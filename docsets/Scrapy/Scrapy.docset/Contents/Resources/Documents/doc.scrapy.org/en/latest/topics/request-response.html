

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->

<!-- Mirrored from doc.scrapy.org/en/latest/topics/request-response.html by HTTrack Website Copier/3.x [XR&CO'2013], Wed, 23 Apr 2014 03:06:41 GMT -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Requests and Responses &mdash; Scrapy 0.22.2 documentation</title>
  

  
  

  
  <link href='../../../../fonts.googleapis.com/css732b.css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  

  
    <link rel="stylesheet" href="../../../../media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  
    <link rel="stylesheet" href="../../../../media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />
  
    <link rel="top" title="Scrapy 0.22.2 documentation" href="../index-2.html"/>
        <link rel="next" title="Settings" href="settings.html"/>
        <link rel="prev" title="Core API" href="api.html"/>
 
<!-- RTD Extra Head -->



  
  <!-- 
  Always link to the latest version, as canonical.
  http://docs.readthedocs.org/en/latest/canonical.html
  -->
  <link rel="canonical" href="request-response.html" />
  

<script type="text/javascript">
  // This is included here because other places don't have access to the pagename variable.
  var READTHEDOCS_DATA = {
    project: "scrapy",
    version: "latest",
    language: "en",
    page: "topics/request-response",
    theme: "sphinx_rtd_theme",
    docroot: "/docs/",
    source_suffix: ".rst",
    api_host: "https://readthedocs.org"
  }
  // Old variables
  var doc_version = "latest";
  var doc_slug = "scrapy";
  var page_name = "topics/request-response";
  var html_theme = "sphinx_rtd_theme";
</script>
<!-- RTD Analytics Code -->
<!-- Included in the header because you don't have a footer block. -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17997319-1']);
  _gaq.push(['_trackPageview']);

  // User Analytics Code
  _gaq.push(['user._setAccount', 'UA-10231918-2']);
  _gaq.push(['user._trackPageview']);
  // End User Analytics Code


  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<!-- end RTD Analytics Code -->
<!-- end RTD <extrahead> -->


  
  <script src="../../../../cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../index-2.html" class="fa fa-home"> Scrapy</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="http://doc.scrapy.org/en/latest/search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy at a glance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#pick-a-website">Pick a website</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#define-the-data-you-want-to-scrape">Define the data you want to scrape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#write-a-spider-to-extract-the-data">Write a Spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#run-the-spider-to-extract-the-data">Run the spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#review-scraped-data">Review scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#what-else">What else?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#what-s-next">What&#8217;s next?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">Installation guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#pre-requisites">Pre-requisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#installing-scrapy">Installing Scrapy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#platform-specific-installation-notes">Platform specific installation notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#creating-a-project">Creating a project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#defining-our-item">Defining our Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#our-first-spider">Our first Spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#storing-the-scraped-data">Storing the scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="commands.html">Command line tool</a><ul>
<li class="toctree-l2"><a class="reference internal" href="commands.html#default-structure-of-scrapy-projects">Default structure of Scrapy projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#using-the-scrapy-tool">Using the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#available-tool-commands">Available tool commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#custom-project-commands">Custom project commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a><ul>
<li class="toctree-l2"><a class="reference internal" href="items.html#declaring-items">Declaring Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#item-fields">Item Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#working-with-items">Working with Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#extending-items">Extending Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#item-objects">Item objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#field-objects">Field objects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="spiders.html#spider-arguments">Spider arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="spiders.html#built-in-spiders-reference">Built-in spiders reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">Selectors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="selectors.html#using-selectors">Using selectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="selectors.html#module-scrapy.selector">Built-in Selectors reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#using-item-loaders-to-populate-items">Using Item Loaders to populate items</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#input-and-output-processors">Input and Output processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#declaring-item-loaders">Declaring Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#declaring-input-and-output-processors">Declaring Input and Output Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#item-loader-context">Item Loader Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#itemloader-objects">ItemLoader objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#reusing-and-extending-item-loaders">Reusing and extending Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#module-scrapy.contrib.loader.processor">Available built-in processors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a><ul>
<li class="toctree-l2"><a class="reference internal" href="shell.html#launch-the-shell">Launch the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#using-the-shell">Using the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#example-of-shell-session">Example of shell session</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#invoking-the-shell-from-spiders-to-inspect-responses">Invoking the shell from spiders to inspect responses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#writing-your-own-item-pipeline">Writing your own item pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#item-pipeline-example">Item pipeline example</a></li>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#activating-an-item-pipeline-component">Activating an Item Pipeline component</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed exports</a><ul>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#serialization-formats">Serialization formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storages">Storages</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storage-uri-parameters">Storage URI parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storage-backends">Storage backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="link-extractors.html#module-scrapy.contrib.linkextractors">Built-in link extractors reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="logging.html#log-levels">Log levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#how-to-set-the-log-level">How to set the log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#how-to-log-messages">How to log messages</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#logging-from-spiders">Logging from Spiders</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#module-scrapy.log">scrapy.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#logging-settings">Logging settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Stats Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="stats.html#common-stats-collector-uses">Common Stats Collector uses</a></li>
<li class="toctree-l2"><a class="reference internal" href="stats.html#available-stats-collectors">Available Stats Collectors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="email.html">Sending e-mail</a><ul>
<li class="toctree-l2"><a class="reference internal" href="email.html#quick-example">Quick example</a></li>
<li class="toctree-l2"><a class="reference internal" href="email.html#mailsender-class-reference">MailSender class reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="email.html#mail-settings">Mail settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet Console</a><ul>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#how-to-access-the-telnet-console">How to access the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#available-variables-in-the-telnet-console">Available variables in the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-console-usage-examples">Telnet console usage examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-console-signals">Telnet Console signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-settings">Telnet settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="webservice.html">Web Service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#web-service-resources">Web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#web-service-settings">Web service settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#writing-a-web-service-resource">Writing a web service resource</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#examples-of-web-service-resources">Examples of web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#example-of-web-service-client">Example of web service client</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-does-scrapy-compare-to-beautifulsoup-or-lxml">How does Scrapy compare to BeautifulSoup or lxml?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-python-versions-does-scrapy-support">What Python versions does Scrapy support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-work-with-python-3">Does Scrapy work with Python 3?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#did-scrapy-steal-x-from-django">Did Scrapy &#8220;steal&#8221; X from Django?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-work-with-http-proxies">Does Scrapy work with HTTP proxies?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-scrape-an-item-with-attributes-in-different-pages">How can I scrape an item with attributes in different pages?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#scrapy-crashes-with-importerror-no-module-named-win32api">Scrapy crashes with: ImportError: No module named win32api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-simulate-a-user-login-in-my-spider">How can I simulate a user login in my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-crawl-in-breadth-first-or-depth-first-order">Does Scrapy crawl in breadth-first or depth-first order?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#my-scrapy-crawler-has-memory-leaks-what-can-i-do">My Scrapy crawler has memory leaks. What can I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-make-scrapy-consume-less-memory">How can I make Scrapy consume less memory?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-basic-http-authentication-in-my-spiders">Can I use Basic HTTP Authentication in my spiders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-does-scrapy-download-pages-in-english-instead-of-my-native-language">Why does Scrapy download pages in English instead of my native language?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-can-i-find-some-example-scrapy-projects">Where can I find some example Scrapy projects?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-run-a-spider-without-creating-a-project">Can I run a spider without creating a project?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-get-filtered-offsite-request-messages-how-can-i-fix-them">I get &#8220;Filtered offsite request&#8221; messages. How can I fix them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">What is the recommended way to deploy a Scrapy crawler in production?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-json-for-large-exports">Can I use JSON for large exports?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-return-twisted-deferreds-from-signal-handlers">Can I return (Twisted) deferreds from signal handlers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-does-the-response-status-code-999-means">What does the response status code 999 means?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">Can I call <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> from my spiders to debug them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">Simplest way to dump all my scraped items into a JSON/CSV/XML file?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">What&#8217;s this huge cryptic <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> parameter used in some forms?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-s-the-best-way-to-parse-big-xml-csv-data-feeds">What&#8217;s the best way to parse big XML/CSV data feeds?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-manage-cookies-automatically">Does Scrapy manage cookies automatically?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">How can I see the cookies being sent and received from Scrapy?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-instruct-a-spider-to-stop-itself">How can I instruct a spider to stop itself?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-prevent-my-scrapy-bot-from-getting-banned">How can I prevent my Scrapy bot from getting banned?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#should-i-use-spider-arguments-or-settings-to-configure-my-spider">Should I use spider arguments or settings to configure my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">I&#8217;m scraping a XML document and my XPath selector doesn&#8217;t return any items</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-m-getting-an-error-cannot-import-name-crawler">I&#8217;m getting an error: &#8220;cannot import name crawler&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="debug.html#parse-command">Parse Command</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#scrapy-shell">Scrapy Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#open-in-browser">Open in browser</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#logging">Logging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contracts.html#custom-contracts">Custom Contracts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="practices.html#run-scrapy-from-a-script">Run Scrapy from a script</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#running-multiple-spiders-in-the-same-process">Running multiple spiders in the same process</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#distributed-crawls">Distributed crawls</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#avoiding-getting-banned">Avoiding getting banned</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#dynamic-creation-of-item-classes">Dynamic Creation of Item Classes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#increase-concurrency">Increase concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#reduce-log-level">Reduce log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-cookies">Disable cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-retries">Disable retries</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#reduce-download-timeout">Reduce download timeout</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-redirects">Disable redirects</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#enable-crawling-of-ajax-crawlable-pages">Enable crawling of &#8220;Ajax Crawlable Pages&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="firefox.html">Using Firefox for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="firefox.html#caveats-with-inspecting-the-live-browser-dom">Caveats with inspecting the live browser DOM</a></li>
<li class="toctree-l2"><a class="reference internal" href="firefox.html#useful-firefox-add-ons-for-scraping">Useful Firefox add-ons for scraping</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="firebug.html">Using Firebug for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#getting-links-to-follow">Getting links to follow</a></li>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#extracting-the-data">Extracting the data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#common-causes-of-memory-leaks">Common causes of memory leaks</a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#debugging-memory-leaks-with-trackref">Debugging memory leaks with <tt class="docutils literal"><span class="pre">trackref</span></tt></a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#debugging-memory-leaks-with-guppy">Debugging memory leaks with Guppy</a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#leaks-without-leaks">Leaks without leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="images.html">Downloading Item Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="images.html#using-the-images-pipeline">Using the Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#usage-example">Usage example</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#enabling-your-images-pipeline">Enabling your Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#images-storage">Images Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#additional-features">Additional features</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#module-scrapy.contrib.pipeline.images">Implementing your custom Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#custom-images-pipeline-example">Custom Images pipeline example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ubuntu.html">Ubuntu packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="scrapyd.html">Scrapyd</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle extension</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#design-goals">Design goals</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#how-it-works">How it works</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#throttling-algorithm">Throttling algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#job-directory">Job directory</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#how-to-use-it">How to use it</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#keeping-persistent-state-between-batches">Keeping persistent state between batches</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#persistence-gotchas">Persistence gotchas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="djangoitem.html">DjangoItem</a><ul>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#using-djangoitem">Using DjangoItem</a></li>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#djangoitem-caveats">DjangoItem caveats</a></li>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#django-settings-set-up">Django settings set up</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#components">Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#data-flow">Data flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#event-driven-networking">Event-driven networking</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="downloader-middleware.html">Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="downloader-middleware.html#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="downloader-middleware.html#writing-your-own-downloader-middleware">Writing your own downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="downloader-middleware.html#built-in-downloader-middleware-reference">Built-in downloader middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#activating-a-spider-middleware">Activating a spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#writing-your-own-spider-middleware">Writing your own spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#built-in-spider-middleware-reference">Built-in spider middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#extension-settings">Extension settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#loading-activating-extensions">Loading &amp; activating extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#available-enabled-and-disabled-extensions">Available, enabled and disabled extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#disabling-an-extension">Disabling an extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#writing-your-own-extension">Writing your own extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#built-in-extensions-reference">Built-in extensions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Core API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api.html#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#module-scrapy.settings">Settings API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#module-scrapy.signalmanager">Signals API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#stats-collector-api">Stats Collector API</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Requests and Responses</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#request-objects">Request objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#request-meta-special-keys">Request.meta special keys</a></li>
<li class="toctree-l2"><a class="reference internal" href="#request-subclasses">Request subclasses</a></li>
<li class="toctree-l2"><a class="reference internal" href="#response-objects">Response objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#response-subclasses">Response subclasses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="settings.html#designating-the-settings">Designating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#populating-the-settings">Populating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#how-to-access-settings">How to access settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#rationale-for-setting-names">Rationale for setting names</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#built-in-settings-reference">Built-in settings reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">Signals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="signals.html#deferred-signal-handlers">Deferred signal handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="signals.html#module-scrapy.signals">Built-in signals reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="exceptions.html#built-in-exceptions-reference">Built-in Exceptions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#using-item-exporters">Using Item Exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#serialization-of-item-fields">Serialization of item fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#built-in-item-exporters-reference">Built-in Item Exporters reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-02-14">0.22.2 (released 2014-02-14)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-02-08">0.22.1 (released 2014-02-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-01-17">0.22.0 (released 2014-01-17)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-12-09">0.20.2 (released 2013-12-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-11-28">0.20.1 (released 2013-11-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-11-08">0.20.0 (released 2013-11-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-10-10">0.18.4 (released 2013-10-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-10-03">0.18.3 (released 2013-10-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-09-03">0.18.2 (released 2013-09-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-08-27">0.18.1 (released 2013-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-08-09">0.18.0 (released 2013-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-05-30">0.16.5 (released 2013-05-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-01-23">0.16.4 (released 2013-01-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-12-07">0.16.3 (released 2012-12-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-11-09">0.16.2 (released 2012-11-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-10-26">0.16.1 (released 2012-10-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-10-18">0.16.0 (released 2012-10-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id2">0.14.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id3">0.14.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id4">0.14.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id5">0.14.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id6">0.14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id7">0.12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id8">0.10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id11">0.9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id14">0.8</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id15">0.7</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#reporting-bugs">Reporting bugs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#writing-patches">Writing patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#submitting-patches">Submitting patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#coding-style">Coding style</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#scrapy-contrib">Scrapy Contrib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#documentation-policies">Documentation policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#tests">Tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API Stability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../versioning.html#id1">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../versioning.html#api-stability">API Stability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../experimental/index.html">Experimental features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../experimental/index.html#add-commands-using-external-libraries">Add commands using external libraries</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index-2.html">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index-2.html">Docs</a> &raquo;</li>
      
    <li>Requests and Responses</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/scrapy/scrapy/blob/0.22/docs/topics/request-response.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h1>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="Permalink to this headline">¶</a></h1>
<p>Scrapy uses <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="#topics-request-response-ref-request-subclasses"><em>Request subclasses</em></a> and
<a class="reference internal" href="#topics-request-response-ref-response-subclasses"><em>Response subclasses</em></a>.</p>
<div class="section" id="request-objects">
<h2>Request objects<a class="headerlink" href="#request-objects" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="scrapy.http.Request">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Request</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>callback</em>, <em>method='GET'</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this request</li>
<li><strong>callback</strong> (<em>callable</em>) &#8211; the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see <a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a> below.
If a Request doesn&#8217;t specify a callback, the spider&#8217;s
<a class="reference internal" href="spiders.html#scrapy.spider.Spider.parse" title="scrapy.spider.Spider.parse"><tt class="xref py py-meth docutils literal"><span class="pre">parse()</span></tt></a> method will be used.
Note that if exceptions are raised during processing, errback is called instead.</li>
<li><strong>method</strong> (<em>string</em>) &#8211; the HTTP method of this request. Defaults to <tt class="docutils literal"><span class="pre">'GET'</span></tt>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</li>
<li><strong>body</strong> (<em>str or unicode</em>) &#8211; the request body. If a <tt class="docutils literal"><span class="pre">unicode</span></tt> is passed, then it&#8217;s encoded to
<tt class="docutils literal"><span class="pre">str</span></tt> using the <cite>encoding</cite> passed (which defaults to <tt class="docutils literal"><span class="pre">utf-8</span></tt>). If
<tt class="docutils literal"><span class="pre">body</span></tt> is not given,, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a <tt class="docutils literal"><span class="pre">str</span></tt> (never
<tt class="docutils literal"><span class="pre">unicode</span></tt> or <tt class="docutils literal"><span class="pre">None</span></tt>).</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<tt class="docutils literal"><span class="pre">None</span></tt> is passed as value, the HTTP header will not be sent at all.</li>
<li><strong>cookies</strong> (<em>dict or list</em>) &#8211; <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic">
<li>Using a dict:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li>Using a list of dicts:<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;currency&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;value&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;domain&#39;</span><span class="p">:</span> <span class="s">&#39;example.com&#39;</span><span class="p">,</span>
                                        <span class="s">&#39;path&#39;</span><span class="p">:</span> <span class="s">&#39;/currency&#39;</span><span class="p">}])</span>
</pre></div>
</div>
</li>
</ol>
<p>The latter form allows for customizing the <tt class="docutils literal"><span class="pre">domain</span></tt> and <tt class="docutils literal"><span class="pre">path</span></tt>
attributes of the cookie. These is only useful if the cookies are saved
for later requests.</p>
<p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That&#8217;s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> key in the
<a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a>.</p>
<p>Example of request without merging cookies:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;currency&#39;</span><span class="p">:</span> <span class="s">&#39;USD&#39;</span><span class="p">,</span> <span class="s">&#39;country&#39;</span><span class="p">:</span> <span class="s">&#39;UY&#39;</span><span class="p">},</span>
                               <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;dont_merge_cookies&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</pre></div>
</div>
<p>For more info see <a class="reference internal" href="downloader-middleware.html#cookies-mw"><em>CookiesMiddleware</em></a>.</p>
</li>
<li><strong>encoding</strong> (<em>string</em>) &#8211; the encoding of this request (defaults to <tt class="docutils literal"><span class="pre">'utf-8'</span></tt>).
This encoding will be used to percent-encode the URL and to convert the
body to <tt class="docutils literal"><span class="pre">str</span></tt> (if given as <tt class="docutils literal"><span class="pre">unicode</span></tt>).</li>
<li><strong>priority</strong> (<em>int</em>) &#8211; the priority of this request (defaults to <tt class="docutils literal"><span class="pre">0</span></tt>).
The priority is used by the scheduler to define the order used to process
requests.</li>
<li><strong>dont_filter</strong> (<em>boolean</em>) &#8211; indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <tt class="docutils literal"><span class="pre">False</span></tt>.</li>
<li><strong>errback</strong> (<em>callable</em>) &#8211; a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a <a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance
as first parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Request.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Request.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.method">
<tt class="descname">method</tt><a class="headerlink" href="#scrapy.http.Request.method" title="Permalink to this definition">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <tt class="docutils literal"><span class="pre">&quot;GET&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;POST&quot;</span></tt>, <tt class="docutils literal"><span class="pre">&quot;PUT&quot;</span></tt>, etc</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Request.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Request.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Request.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="#topics-request-meta"><em>Request.meta special keys</em></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="http://docs.python.org/library/copy.html">shallow copied</a> when the request is cloned using the
<tt class="docutils literal"><span class="pre">copy()</span></tt> or <tt class="docutils literal"><span class="pre">replace()</span></tt> methods, and can also be accessed, in your
spider, from the <tt class="docutils literal"><span class="pre">response.meta</span></tt> attribute.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Request.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>method</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding</em>, <em>dont_filter</em>, <em>callback</em>, <em>errback</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Request.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> is copied by default (unless a new value
is given in the <tt class="docutils literal"><span class="pre">meta</span></tt> argument). See also
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><em>Passing additional data to callback functions</em></a>.</p>
</dd></dl>

</dd></dl>

<div class="section" id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h3>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="Permalink to this headline">¶</a></h3>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                      <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Visited </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute for that.</p>
<p>Here&#8217;s an example of how to pass an item using this mechanism, to populate
different fields from different pages:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;main_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                      <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>
    <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">request</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;item&#39;</span><span class="p">]</span>
    <span class="n">item</span><span class="p">[</span><span class="s">&#39;other_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="request-meta-special-keys">
<span id="topics-request-meta"></span><h2>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><a class="reference internal" href="downloader-middleware.html#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:reqmeta-dont_retry"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_retry</span></tt></a></li>
<li><a class="reference internal" href="spider-middleware.html#std:reqmeta-handle_httpstatus_list"><tt class="xref std std-reqmeta docutils literal"><span class="pre">handle_httpstatus_list</span></tt></a></li>
<li><tt class="docutils literal"><span class="pre">dont_merge_cookies</span></tt> (see <tt class="docutils literal"><span class="pre">cookies</span></tt> parameter of <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> constructor)</li>
<li><a class="reference internal" href="downloader-middleware.html#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a></li>
<li><a class="reference internal" href="downloader-middleware.html#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a></li>
<li><a class="reference internal" href="#std:reqmeta-bindaddress"><tt class="xref std std-reqmeta docutils literal"><span class="pre">bindaddress</span></tt></a></li>
</ul>
<div class="section" id="bindaddress">
<span id="std:reqmeta-bindaddress"></span><h3>bindaddress<a class="headerlink" href="#bindaddress" title="Permalink to this headline">¶</a></h3>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</div>
</div>
<div class="section" id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h2>Request subclasses<a class="headerlink" href="#request-subclasses" title="Permalink to this headline">¶</a></h2>
<p>Here is the list of built-in <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<div class="section" id="formrequest-objects">
<h3>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="Permalink to this headline">¶</a></h3>
<p>The FormRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects.</p>
<dl class="class">
<dt id="scrapy.http.FormRequest">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">FormRequest</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>formdata</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> class and are
not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>formdata</strong> (<em>dict or iterable of tuples</em>) &#8211; is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> objects support the following class method in
addition to the standard <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> methods:</p>
<dl class="classmethod">
<dt id="scrapy.http.FormRequest.from_response">
<em class="property">classmethod </em><tt class="descname">from_response</tt><big>(</big><em>response</em><span class="optional">[</span>, <em>formname=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>dont_click=False</em>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.FormRequest.from_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object with its form field values
pre-populated with those found in the HTML <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element contained
in the given response. For an example see
<a class="reference internal" href="#topics-request-response-ref-request-userlogin"><em>Using FormRequest.from_response() to simulate a user login</em></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <tt class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></tt>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">from_response()</span></tt></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<tt class="docutils literal"><span class="pre">dont_click</span></tt> argument to <tt class="docutils literal"><span class="pre">True</span></tt>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<tt class="docutils literal"><span class="pre">clickdata</span></tt> argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response containing a HTML form which will be used
to pre-populate the form fields</li>
<li><strong>formname</strong> (<em>string</em>) &#8211; if given, the form with name attribute set to this value will be used.</li>
<li><strong>formxpath</strong> (<em>string</em>) &#8211; if given, the first form that matches the xpath will be used.</li>
<li><strong>formnumber</strong> (<em>integer</em>) &#8211; the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <tt class="docutils literal"><span class="pre">0</span></tt>.</li>
<li><strong>formdata</strong> (<em>dict</em>) &#8211; fields to override in the form data. If a field was
already present in the response <tt class="docutils literal"><span class="pre">&lt;form&gt;</span></tt> element, its value is
overridden by the one passed in this parameter.</li>
<li><strong>dont_click</strong> (<em>boolean</em>) &#8211; If True, the form data will be submitted without
clicking in any element.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.3: </span>The <tt class="docutils literal"><span class="pre">formname</span></tt> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>The <tt class="docutils literal"><span class="pre">formxpath</span></tt> parameter.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="request-usage-examples">
<h3>Request usage examples<a class="headerlink" href="#request-usage-examples" title="Permalink to this headline">¶</a></h3>
<div class="section" id="using-formrequest-to-send-data-via-http-post">
<h4>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this headline">¶</a></h4>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><tt class="xref py py-class docutils literal"><span class="pre">FormRequest</span></tt></a> object (from your
spider) like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;John Doe&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;27&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-formrequest-from-response-to-simulate-a-user-login">
<span id="topics-request-response-ref-request-userlogin"></span><h4>Using FormRequest.from_response() to simulate a user login<a class="headerlink" href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this headline">¶</a></h4>
<p>It is usual for web sites to provide pre-populated form fields through <tt class="docutils literal"><span class="pre">&lt;input</span>
<span class="pre">type=&quot;hidden&quot;&gt;</span></tt> elements, such as session related data or authentication
tokens (for login pages). When scraping, you&#8217;ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><tt class="xref py py-meth docutils literal"><span class="pre">FormRequest.from_response()</span></tt></a>
method for this job. Here&#8217;s an example spider which uses it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;example.com&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;http://www.example.com/users/login.php&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;username&#39;</span><span class="p">:</span> <span class="s">&#39;john&#39;</span><span class="p">,</span> <span class="s">&#39;password&#39;</span><span class="p">:</span> <span class="s">&#39;secret&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c"># check login succeed before going on</span>
        <span class="k">if</span> <span class="s">&quot;authentication failed&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s">&quot;Login failed&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="response-objects">
<h2>Response objects<a class="headerlink" href="#response-objects" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="scrapy.http.Response">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">Response</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>status=200</em>, <em>headers</em>, <em>body</em>, <em>flags</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this response</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</li>
<li><strong>status</strong> (<em>integer</em>) &#8211; the HTTP status of the response. Defaults to <tt class="docutils literal"><span class="pre">200</span></tt>.</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the response body. It must be str, not unicode, unless you&#8217;re
using a encoding-aware <a class="reference internal" href="#topics-request-response-ref-response-subclasses"><em>Response subclass</em></a>, such as
<a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> attribute. If
given, the dict will be shallow copied.</li>
<li><strong>flags</strong> (<em>list</em>) &#8211; is a list containing the initial values for the
<a class="reference internal" href="#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><tt class="xref py py-attr docutils literal"><span class="pre">Response.flags</span></tt></a> attribute. If given, the list will be shallow
copied.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Response.url">
<tt class="descname">url</tt><a class="headerlink" href="#scrapy.http.Response.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.status">
<tt class="descname">status</tt><a class="headerlink" href="#scrapy.http.Response.status" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <tt class="docutils literal"><span class="pre">200</span></tt>,
<tt class="docutils literal"><span class="pre">404</span></tt>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.headers">
<tt class="descname">headers</tt><a class="headerlink" href="#scrapy.http.Response.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.body">
<tt class="descname">body</tt><a class="headerlink" href="#scrapy.http.Response.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str containing the body of this Response. Keep in mind that Reponse.body
is always a str. If you want the unicode version use
<a class="reference internal" href="#scrapy.http.TextResponse.body_as_unicode" title="scrapy.http.TextResponse.body_as_unicode"><tt class="xref py py-meth docutils literal"><span class="pre">TextResponse.body_as_unicode()</span></tt></a> (only available in
<a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><tt class="xref py py-meth docutils literal"><span class="pre">replace()</span></tt></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.request">
<tt class="descname">request</tt><a class="headerlink" href="#scrapy.http.Response.request" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware"><em>Downloader Middlewares</em></a>.
In particular, this means that:</p>
<ul class="simple">
<li>HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).</li>
<li>Response.request.url doesn&#8217;t always equal Response.url</li>
<li>This attribute is only available in the spider code, and in the
<a class="reference internal" href="spider-middleware.html#topics-spider-middleware"><em>Spider Middlewares</em></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="reference internal" href="signals.html#std:signal-response_downloaded"><tt class="xref std std-signal docutils literal"><span class="pre">response_downloaded</span></tt></a> signal.</li>
</ul>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.meta">
<tt class="descname">meta</tt><a class="headerlink" href="#scrapy.http.Response.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> object (ie. <tt class="docutils literal"><span class="pre">self.request.meta</span></tt>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><tt class="xref py py-attr docutils literal"><span class="pre">Response.request</span></tt></a> attribute, the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> attribute</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.flags">
<tt class="descname">flags</tt><a class="headerlink" href="#scrapy.http.Response.flags" title="Permalink to this definition">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <cite>&#8216;cached&#8217;</cite>, <cite>&#8216;redirected</cite>&#8216;, etc. And
they&#8217;re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.copy">
<tt class="descname">copy</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.Response.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.replace">
<tt class="descname">replace</tt><big>(</big><span class="optional">[</span><em>url</em>, <em>status</em>, <em>headers</em>, <em>body</em>, <em>request</em>, <em>flags</em>, <em>cls</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.Response.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></tt></a> is copied by default.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h2>Response subclasses<a class="headerlink" href="#response-subclasses" title="Permalink to this headline">¶</a></h2>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<div class="section" id="textresponse-objects">
<h3>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.http.TextResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">TextResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>encoding</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects adds encoding capabilities to the base
<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> class and is not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>encoding</strong> (<em>string</em>) &#8211; is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If <tt class="docutils literal"><span class="pre">encoding</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt> (default value), the
encoding will be looked up in the response headers and body instead.</td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="attribute">
<dt id="scrapy.http.TextResponse.encoding">
<tt class="descname">encoding</tt><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li>the encoding passed in the constructor <cite>encoding</cite> argument</li>
<li>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.</li>
<li>the encoding declared in the response body. The TextResponse class
doesn&#8217;t provide any special functionality for this. However, the
<a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> and <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> classes do.</li>
<li>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</li>
</ol>
</dd></dl>

<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> objects support the following methods in addition to
the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> ones:</p>
<dl class="method">
<dt id="scrapy.http.TextResponse.body_as_unicode">
<tt class="descname">body_as_unicode</tt><big>(</big><big>)</big><a class="headerlink" href="#scrapy.http.TextResponse.body_as_unicode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the body of the response as unicode. This is equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
<p>But <strong>not</strong> equivalent to:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nb">unicode</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<p>Since, in the latter case, you would be using you system default encoding
(typically <cite>ascii</cite>) to convert the body to unicode, instead of the response
encoding.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="htmlresponse-objects">
<h3>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.http.HtmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">HtmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="http://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
<div class="section" id="xmlresponse-objects">
<h3>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.http.XmlResponse">
<em class="property">class </em><tt class="descclassname">scrapy.http.</tt><tt class="descname">XmlResponse</tt><big>(</big><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><tt class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></tt></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><tt class="xref py py-class docutils literal"><span class="pre">TextResponse</span></tt></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><tt class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></tt></a>.</p>
</dd></dl>

</div>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="settings.html" class="btn btn-neutral float-right" title="Settings">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="api.html" class="btn btn-neutral" title="Core API"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2013, Scrapy developers.
      Last updated on Apr 17, 2014.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="http://doc.scrapy.org/en/master/">master</a></dd>
        
          <dd><a href="../index.html">latest</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.20/">0.20</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.18/">0.18</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.16/">0.16</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.14/">0.14</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.12/">0.12</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.10.3/">0.10.3</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.9/">0.9</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.8/">0.8</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.7/">0.7</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="https://media.readthedocs.org/pdf/scrapy/latest/scrapy.pdf">PDF</a></dd>
        
          <dd><a href="https://media.readthedocs.org/htmlzip/scrapy/latest/scrapy.zip">HTML</a></dd>
        
          <dd><a href="https://media.readthedocs.org/epub/scrapy/latest/scrapy.epub">Epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="http://readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="http://readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org/">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.22.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/jquery/jquery-2.0.3.min.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/jquery/jquery-migrate-1.2.1.min.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/underscore.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/doctools.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/readthedocs-doc-embed.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>

<!-- Mirrored from doc.scrapy.org/en/latest/topics/request-response.html by HTTrack Website Copier/3.x [XR&CO'2013], Wed, 23 Apr 2014 03:06:41 GMT -->
</html>