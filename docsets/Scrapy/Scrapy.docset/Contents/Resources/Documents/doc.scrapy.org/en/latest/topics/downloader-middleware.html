

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->

<!-- Mirrored from doc.scrapy.org/en/latest/topics/downloader-middleware.html by HTTrack Website Copier/3.x [XR&CO'2013], Wed, 23 Apr 2014 03:06:41 GMT -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Downloader Middleware &mdash; Scrapy 0.22.2 documentation</title>
  

  
  

  
  <link href='../../../../fonts.googleapis.com/css732b.css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  

  
    <link rel="stylesheet" href="../../../../media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  
    <link rel="stylesheet" href="../../../../media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />
  
    <link rel="top" title="Scrapy 0.22.2 documentation" href="../index-2.html"/>
        <link rel="next" title="Spider Middleware" href="spider-middleware.html"/>
        <link rel="prev" title="Architecture overview" href="architecture.html"/>
 
<!-- RTD Extra Head -->



  
  <!-- 
  Always link to the latest version, as canonical.
  http://docs.readthedocs.org/en/latest/canonical.html
  -->
  <link rel="canonical" href="downloader-middleware.html" />
  

<script type="text/javascript">
  // This is included here because other places don't have access to the pagename variable.
  var READTHEDOCS_DATA = {
    project: "scrapy",
    version: "latest",
    language: "en",
    page: "topics/downloader-middleware",
    theme: "sphinx_rtd_theme",
    docroot: "/docs/",
    source_suffix: ".rst",
    api_host: "https://readthedocs.org"
  }
  // Old variables
  var doc_version = "latest";
  var doc_slug = "scrapy";
  var page_name = "topics/downloader-middleware";
  var html_theme = "sphinx_rtd_theme";
</script>
<!-- RTD Analytics Code -->
<!-- Included in the header because you don't have a footer block. -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-17997319-1']);
  _gaq.push(['_trackPageview']);

  // User Analytics Code
  _gaq.push(['user._setAccount', 'UA-10231918-2']);
  _gaq.push(['user._trackPageview']);
  // End User Analytics Code


  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<!-- end RTD Analytics Code -->
<!-- end RTD <extrahead> -->


  
  <script src="../../../../cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../index-2.html" class="fa fa-home"> Scrapy</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="http://doc.scrapy.org/en/latest/search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy at a glance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#pick-a-website">Pick a website</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#define-the-data-you-want-to-scrape">Define the data you want to scrape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#write-a-spider-to-extract-the-data">Write a Spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#run-the-spider-to-extract-the-data">Run the spider to extract the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#review-scraped-data">Review scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#what-else">What else?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/overview.html#what-s-next">What&#8217;s next?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">Installation guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#pre-requisites">Pre-requisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#installing-scrapy">Installing Scrapy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/install.html#platform-specific-installation-notes">Platform specific installation notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#creating-a-project">Creating a project</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#defining-our-item">Defining our Item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#our-first-spider">Our first Spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#storing-the-scraped-data">Storing the scraped data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/tutorial.html#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="commands.html">Command line tool</a><ul>
<li class="toctree-l2"><a class="reference internal" href="commands.html#default-structure-of-scrapy-projects">Default structure of Scrapy projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#using-the-scrapy-tool">Using the <tt class="docutils literal"><span class="pre">scrapy</span></tt> tool</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#available-tool-commands">Available tool commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="commands.html#custom-project-commands">Custom project commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a><ul>
<li class="toctree-l2"><a class="reference internal" href="items.html#declaring-items">Declaring Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#item-fields">Item Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#working-with-items">Working with Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#extending-items">Extending Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#item-objects">Item objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="items.html#field-objects">Field objects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="spiders.html#spider-arguments">Spider arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="spiders.html#built-in-spiders-reference">Built-in spiders reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">Selectors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="selectors.html#using-selectors">Using selectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="selectors.html#module-scrapy.selector">Built-in Selectors reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#using-item-loaders-to-populate-items">Using Item Loaders to populate items</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#input-and-output-processors">Input and Output processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#declaring-item-loaders">Declaring Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#declaring-input-and-output-processors">Declaring Input and Output Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#item-loader-context">Item Loader Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#itemloader-objects">ItemLoader objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#reusing-and-extending-item-loaders">Reusing and extending Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="loaders.html#module-scrapy.contrib.loader.processor">Available built-in processors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a><ul>
<li class="toctree-l2"><a class="reference internal" href="shell.html#launch-the-shell">Launch the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#using-the-shell">Using the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#example-of-shell-session">Example of shell session</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell.html#invoking-the-shell-from-spiders-to-inspect-responses">Invoking the shell from spiders to inspect responses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#writing-your-own-item-pipeline">Writing your own item pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#item-pipeline-example">Item pipeline example</a></li>
<li class="toctree-l2"><a class="reference internal" href="item-pipeline.html#activating-an-item-pipeline-component">Activating an Item Pipeline component</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed exports</a><ul>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#serialization-formats">Serialization formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storages">Storages</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storage-uri-parameters">Storage URI parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#storage-backends">Storage backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="feed-exports.html#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="link-extractors.html#module-scrapy.contrib.linkextractors">Built-in link extractors reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="logging.html#log-levels">Log levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#how-to-set-the-log-level">How to set the log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#how-to-log-messages">How to log messages</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#logging-from-spiders">Logging from Spiders</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#module-scrapy.log">scrapy.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="logging.html#logging-settings">Logging settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Stats Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="stats.html#common-stats-collector-uses">Common Stats Collector uses</a></li>
<li class="toctree-l2"><a class="reference internal" href="stats.html#available-stats-collectors">Available Stats Collectors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="email.html">Sending e-mail</a><ul>
<li class="toctree-l2"><a class="reference internal" href="email.html#quick-example">Quick example</a></li>
<li class="toctree-l2"><a class="reference internal" href="email.html#mailsender-class-reference">MailSender class reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="email.html#mail-settings">Mail settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet Console</a><ul>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#how-to-access-the-telnet-console">How to access the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#available-variables-in-the-telnet-console">Available variables in the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-console-usage-examples">Telnet console usage examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-console-signals">Telnet Console signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="telnetconsole.html#telnet-settings">Telnet settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="webservice.html">Web Service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#web-service-resources">Web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#web-service-settings">Web service settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#writing-a-web-service-resource">Writing a web service resource</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#examples-of-web-service-resources">Examples of web service resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="webservice.html#example-of-web-service-client">Example of web service client</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-does-scrapy-compare-to-beautifulsoup-or-lxml">How does Scrapy compare to BeautifulSoup or lxml?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-python-versions-does-scrapy-support">What Python versions does Scrapy support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-work-with-python-3">Does Scrapy work with Python 3?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#did-scrapy-steal-x-from-django">Did Scrapy &#8220;steal&#8221; X from Django?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-work-with-http-proxies">Does Scrapy work with HTTP proxies?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-scrape-an-item-with-attributes-in-different-pages">How can I scrape an item with attributes in different pages?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#scrapy-crashes-with-importerror-no-module-named-win32api">Scrapy crashes with: ImportError: No module named win32api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-simulate-a-user-login-in-my-spider">How can I simulate a user login in my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-crawl-in-breadth-first-or-depth-first-order">Does Scrapy crawl in breadth-first or depth-first order?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#my-scrapy-crawler-has-memory-leaks-what-can-i-do">My Scrapy crawler has memory leaks. What can I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-make-scrapy-consume-less-memory">How can I make Scrapy consume less memory?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-basic-http-authentication-in-my-spiders">Can I use Basic HTTP Authentication in my spiders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#why-does-scrapy-download-pages-in-english-instead-of-my-native-language">Why does Scrapy download pages in English instead of my native language?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#where-can-i-find-some-example-scrapy-projects">Where can I find some example Scrapy projects?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-run-a-spider-without-creating-a-project">Can I run a spider without creating a project?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-get-filtered-offsite-request-messages-how-can-i-fix-them">I get &#8220;Filtered offsite request&#8221; messages. How can I fix them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">What is the recommended way to deploy a Scrapy crawler in production?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-use-json-for-large-exports">Can I use JSON for large exports?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-return-twisted-deferreds-from-signal-handlers">Can I return (Twisted) deferreds from signal handlers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-does-the-response-status-code-999-means">What does the response status code 999 means?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">Can I call <tt class="docutils literal"><span class="pre">pdb.set_trace()</span></tt> from my spiders to debug them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">Simplest way to dump all my scraped items into a JSON/CSV/XML file?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">What&#8217;s this huge cryptic <tt class="docutils literal"><span class="pre">__VIEWSTATE</span></tt> parameter used in some forms?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#what-s-the-best-way-to-parse-big-xml-csv-data-feeds">What&#8217;s the best way to parse big XML/CSV data feeds?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#does-scrapy-manage-cookies-automatically">Does Scrapy manage cookies automatically?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">How can I see the cookies being sent and received from Scrapy?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-instruct-a-spider-to-stop-itself">How can I instruct a spider to stop itself?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#how-can-i-prevent-my-scrapy-bot-from-getting-banned">How can I prevent my Scrapy bot from getting banned?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#should-i-use-spider-arguments-or-settings-to-configure-my-spider">Should I use spider arguments or settings to configure my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">I&#8217;m scraping a XML document and my XPath selector doesn&#8217;t return any items</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq.html#i-m-getting-an-error-cannot-import-name-crawler">I&#8217;m getting an error: &#8220;cannot import name crawler&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="debug.html#parse-command">Parse Command</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#scrapy-shell">Scrapy Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#open-in-browser">Open in browser</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#logging">Logging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contracts.html#custom-contracts">Custom Contracts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="practices.html#run-scrapy-from-a-script">Run Scrapy from a script</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#running-multiple-spiders-in-the-same-process">Running multiple spiders in the same process</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#distributed-crawls">Distributed crawls</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#avoiding-getting-banned">Avoiding getting banned</a></li>
<li class="toctree-l2"><a class="reference internal" href="practices.html#dynamic-creation-of-item-classes">Dynamic Creation of Item Classes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#increase-concurrency">Increase concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#reduce-log-level">Reduce log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-cookies">Disable cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-retries">Disable retries</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#reduce-download-timeout">Reduce download timeout</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#disable-redirects">Disable redirects</a></li>
<li class="toctree-l2"><a class="reference internal" href="broad-crawls.html#enable-crawling-of-ajax-crawlable-pages">Enable crawling of &#8220;Ajax Crawlable Pages&#8221;</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="firefox.html">Using Firefox for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="firefox.html#caveats-with-inspecting-the-live-browser-dom">Caveats with inspecting the live browser DOM</a></li>
<li class="toctree-l2"><a class="reference internal" href="firefox.html#useful-firefox-add-ons-for-scraping">Useful Firefox add-ons for scraping</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="firebug.html">Using Firebug for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#getting-links-to-follow">Getting links to follow</a></li>
<li class="toctree-l2"><a class="reference internal" href="firebug.html#extracting-the-data">Extracting the data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#common-causes-of-memory-leaks">Common causes of memory leaks</a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#debugging-memory-leaks-with-trackref">Debugging memory leaks with <tt class="docutils literal"><span class="pre">trackref</span></tt></a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#debugging-memory-leaks-with-guppy">Debugging memory leaks with Guppy</a></li>
<li class="toctree-l2"><a class="reference internal" href="leaks.html#leaks-without-leaks">Leaks without leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="images.html">Downloading Item Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="images.html#using-the-images-pipeline">Using the Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#usage-example">Usage example</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#enabling-your-images-pipeline">Enabling your Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#images-storage">Images Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#additional-features">Additional features</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#module-scrapy.contrib.pipeline.images">Implementing your custom Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="images.html#custom-images-pipeline-example">Custom Images pipeline example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ubuntu.html">Ubuntu packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="scrapyd.html">Scrapyd</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle extension</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#design-goals">Design goals</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#how-it-works">How it works</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#throttling-algorithm">Throttling algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="autothrottle.html#settings">Settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#job-directory">Job directory</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#how-to-use-it">How to use it</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#keeping-persistent-state-between-batches">Keeping persistent state between batches</a></li>
<li class="toctree-l2"><a class="reference internal" href="jobs.html#persistence-gotchas">Persistence gotchas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="djangoitem.html">DjangoItem</a><ul>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#using-djangoitem">Using DjangoItem</a></li>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#djangoitem-caveats">DjangoItem caveats</a></li>
<li class="toctree-l2"><a class="reference internal" href="djangoitem.html#django-settings-set-up">Django settings set up</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#components">Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#data-flow">Data flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#event-driven-networking">Event-driven networking</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#writing-your-own-downloader-middleware">Writing your own downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-downloader-middleware-reference">Built-in downloader middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#activating-a-spider-middleware">Activating a spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#writing-your-own-spider-middleware">Writing your own spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="spider-middleware.html#built-in-spider-middleware-reference">Built-in spider middleware reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#extension-settings">Extension settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#loading-activating-extensions">Loading &amp; activating extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#available-enabled-and-disabled-extensions">Available, enabled and disabled extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#disabling-an-extension">Disabling an extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#writing-your-own-extension">Writing your own extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html#built-in-extensions-reference">Built-in extensions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Core API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api.html#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#module-scrapy.settings">Settings API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#module-scrapy.signalmanager">Signals API</a></li>
<li class="toctree-l2"><a class="reference internal" href="api.html#stats-collector-api">Stats Collector API</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="request-response.html">Requests and Responses</a><ul>
<li class="toctree-l2"><a class="reference internal" href="request-response.html#request-objects">Request objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="request-response.html#request-meta-special-keys">Request.meta special keys</a></li>
<li class="toctree-l2"><a class="reference internal" href="request-response.html#request-subclasses">Request subclasses</a></li>
<li class="toctree-l2"><a class="reference internal" href="request-response.html#response-objects">Response objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="request-response.html#response-subclasses">Response subclasses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="settings.html#designating-the-settings">Designating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#populating-the-settings">Populating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#how-to-access-settings">How to access settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#rationale-for-setting-names">Rationale for setting names</a></li>
<li class="toctree-l2"><a class="reference internal" href="settings.html#built-in-settings-reference">Built-in settings reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">Signals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="signals.html#deferred-signal-handlers">Deferred signal handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="signals.html#module-scrapy.signals">Built-in signals reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="exceptions.html#built-in-exceptions-reference">Built-in Exceptions reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#using-item-exporters">Using Item Exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#serialization-of-item-fields">Serialization of item fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="exporters.html#built-in-item-exporters-reference">Built-in Item Exporters reference</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-02-14">0.22.2 (released 2014-02-14)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-02-08">0.22.1 (released 2014-02-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2014-01-17">0.22.0 (released 2014-01-17)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-12-09">0.20.2 (released 2013-12-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-11-28">0.20.1 (released 2013-11-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-11-08">0.20.0 (released 2013-11-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-10-10">0.18.4 (released 2013-10-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-10-03">0.18.3 (released 2013-10-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-09-03">0.18.2 (released 2013-09-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-08-27">0.18.1 (released 2013-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-08-09">0.18.0 (released 2013-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-05-30">0.16.5 (released 2013-05-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2013-01-23">0.16.4 (released 2013-01-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-12-07">0.16.3 (released 2012-12-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-11-09">0.16.2 (released 2012-11-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-10-26">0.16.1 (released 2012-10-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#released-2012-10-18">0.16.0 (released 2012-10-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id2">0.14.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id3">0.14.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id4">0.14.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id5">0.14.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id6">0.14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id7">0.12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id8">0.10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id11">0.9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id14">0.8</a></li>
<li class="toctree-l2"><a class="reference internal" href="../news.html#id15">0.7</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#reporting-bugs">Reporting bugs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#writing-patches">Writing patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#submitting-patches">Submitting patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#coding-style">Coding style</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#scrapy-contrib">Scrapy Contrib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#documentation-policies">Documentation policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#tests">Tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API Stability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../versioning.html#id1">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../versioning.html#api-stability">API Stability</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../experimental/index.html">Experimental features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../experimental/index.html#add-commands-using-external-libraries">Add commands using external libraries</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index-2.html">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index-2.html">Docs</a> &raquo;</li>
      
    <li>Downloader Middleware</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/scrapy/scrapy/blob/0.22/docs/topics/downloader-middleware.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h1>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this headline">¶</a></h1>
<p>The downloader middleware is a framework of hooks into Scrapy&#8217;s
request/response processing.  It&#8217;s a light, low-level system for globally
altering Scrapy&#8217;s requests and responses.</p>
<div class="section" id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h2>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this headline">¶</a></h2>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here&#8217;s an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;myproject.middlewares.CustomDownloaderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting is merged with the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the downloader.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> and enabled by default) you must define it
in your project&#8217;s <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></tt></a> setting and assign <cite>None</cite>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&#39;myproject.middlewares.CustomDownloaderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s">&#39;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-downloader-middleware">
<h2>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this headline">¶</a></h2>
<p>Writing your own downloader middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:</p>
<span class="target" id="module-scrapy.contrib.downloadermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.</tt><tt class="descname">DownloaderMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request">
<tt class="descname">process_request</tt><big>(</big><em>request</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> should either: return <tt class="docutils literal"><span class="pre">None</span></tt>, return a
<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, return a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a>
object, or raise <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a>.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, Scrapy won&#8217;t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> or <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods,
or the appropriate download function; it&#8217;ll return that response. The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception, the
<a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<tt class="docutils literal"><span class="pre">Request.errback</span></tt>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response">
<tt class="descname">process_response</tt><big>(</big><em>request</em>, <em>response</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> should either: return a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a>
object, return a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object or
raise a <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a> of the next middleware in the chain.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a>.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception, the errback
function of the request (<tt class="docutils literal"><span class="pre">Request.errback</span></tt>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that originated the response</li>
<li><strong>response</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object) &#8211; the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception">
<tt class="descname">process_exception</tt><big>(</big><em>request</em>, <em>exception</em>, <em>spider</em><big>)</big><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> when a download handler
or a <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><tt class="xref py py-meth docutils literal"><span class="pre">process_request()</span></tt></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><tt class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></tt></a> exception)</p>
<p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> should return: either <tt class="docutils literal"><span class="pre">None</span></tt>,
a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, or a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object.</p>
<p>If it returns <tt class="docutils literal"><span class="pre">None</span></tt>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><tt class="xref py py-class docutils literal"><span class="pre">Response</span></tt></a> object, the <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><tt class="xref py py-meth docutils literal"><span class="pre">process_response()</span></tt></a>
method chain of installed middleware is started, and Scrapy won&#8217;t bother calling
any other <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of middleware.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><tt class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></tt></a> methods of the middleware the same as returning a
response would.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> object) &#8211; the request that generated the exception</li>
<li><strong>exception</strong> (an <tt class="docutils literal"><span class="pre">Exception</span></tt> object) &#8211; the raised exception</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.Spider" title="scrapy.spider.Spider"><tt class="xref py py-class docutils literal"><span class="pre">Spider</span></tt></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h2>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this headline">¶</a></h2>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="#topics-downloader-middleware"><em>downloader middleware usage guide</em></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></tt></a> setting.</p>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h3>CookiesMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.cookies" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.cookies.</tt><tt class="descname">CookiesMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-COOKIES_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a></li>
</ul>
<div class="section" id="multiple-cookie-sessions-per-spider">
<span id="std:reqmeta-cookiejar"></span><h4>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this headline">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com&quot;</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;cookiejar&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that the <a class="reference internal" href="#std:reqmeta-cookiejar"><tt class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></tt></a> meta key is not &#8220;sticky&#8221;. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c"># do some processing</span>
    <span class="k">return</span> <span class="n">Request</span><span class="p">(</span><span class="s">&quot;http://www.example.com/otherpage&quot;</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;cookiejar&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">&#39;cookiejar&#39;</span><span class="p">]},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_other_page</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h4>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this headline">¶</a></h4>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h4>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this headline">¶</a></h4>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <tt class="docutils literal"><span class="pre">Cookie</span></tt>
header) and all cookies received in responses (ie. <tt class="docutils literal"><span class="pre">Set-Cookie</span></tt> header).</p>
<p>Here&#8217;s an example of a log with <a class="reference internal" href="#std:setting-COOKIES_DEBUG"><tt class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></tt></a> enabled:</p>
<div class="highlight-python"><div class="highlight"><pre>2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened
2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.defaultheaders">
<span id="defaultheadersmiddleware"></span><h3>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.defaultheaders" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.defaultheaders.</tt><tt class="descname">DefaultHeadersMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="settings.html#std:setting-DEFAULT_REQUEST_HEADERS"><tt class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></tt></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h3>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.downloadtimeout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.downloadtimeout.</tt><tt class="descname">DownloadTimeoutMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="settings.html#std:setting-DOWNLOAD_TIMEOUT"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></tt></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpauth">
<span id="httpauthmiddleware"></span><h3>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpauth" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpauth.</tt><tt class="descname">HttpAuthMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="http://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <tt class="docutils literal"><span class="pre">http_user</span></tt>
and <tt class="docutils literal"><span class="pre">http_pass</span></tt> attributes of those spiders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>

<span class="k">class</span> <span class="nc">SomeIntranetSiteSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">http_user</span> <span class="o">=</span> <span class="s">&#39;someuser&#39;</span>
    <span class="n">http_pass</span> <span class="o">=</span> <span class="s">&#39;somepass&#39;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;intranet.example.com&#39;</span>

    <span class="c"># .. rest of the spider code omitted ...</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcache">
<span id="httpcachemiddleware"></span><h3>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcache" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcache.</tt><tt class="descname">HttpCacheMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with two HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-storage-fs"><em>Filesystem storage backend (default)</em></a></li>
<li><a class="reference internal" href="#httpcache-storage-dbm"><em>DBM storage backend</em></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a>
setting. Or you can also implement your own storage backend.</p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-policy-rfc2616"><em>RFC2616 policy</em></a></li>
<li><a class="reference internal" href="#httpcache-policy-dummy"><em>Dummy policy (default)</em></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a>
setting. Or you can also implement your own policy.</p>
</dd></dl>

<div class="section" id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h4>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this headline">¶</a></h4>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
&#8220;replay&#8221; a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DummyPolicy</span></tt></li>
</ul>
</div>
<div class="section" id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h4>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this headline">¶</a></h4>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<ul class="simple">
<li>Do not attempt to store responses/requests with <cite>no-store</cite> cache-control directive set</li>
<li>Do not serve responses from cache if <cite>no-cache</cite> cache-control directive is set even for fresh responses</li>
<li>Compute freshness lifetime from <cite>max-age</cite> cache-control directive</li>
<li>Compute freshness lifetime from <cite>Expires</cite> response header</li>
<li>Compute freshness lifetime from <cite>Last-Modified</cite> response header (heuristic used by Firefox)</li>
<li>Compute current age from <cite>Age</cite> response header</li>
<li>Compute current age from <cite>Date</cite> header</li>
<li>Revalidate stale responses based on <cite>Last-Modified</cite> response header</li>
<li>Revalidate stale responses based on <cite>ETag</cite> response header</li>
<li>Set <cite>Date</cite> header for any received response missing it</li>
</ul>
<p>what is missing:</p>
<ul class="simple">
<li><cite>Pragma: no-cache</cite> support <a class="reference external" href="http://www.mnot.net/cache_docs/#PRAGMA">http://www.mnot.net/cache_docs/#PRAGMA</a></li>
<li><cite>Vary</cite> header support <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>Invalidation after updates or deletes <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>... probably others ..</li>
</ul>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.RFC2616Policy</span></tt></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h4>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this headline">¶</a></h4>
<p>File system storage backend is available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.FilesystemCacheStorage</span></tt></li>
</ul>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">request_body</span></tt> - the plain request body</li>
<li><tt class="docutils literal"><span class="pre">request_headers</span></tt> - the request headers (in raw HTTP format)</li>
<li><tt class="docutils literal"><span class="pre">response_body</span></tt> - the plain response body</li>
<li><tt class="docutils literal"><span class="pre">response_headers</span></tt> - the request headers (in raw HTTP format)</li>
<li><tt class="docutils literal"><span class="pre">meta</span></tt> - some metadata of this cache resource in Python <tt class="docutils literal"><span class="pre">repr()</span></tt> format
(grep-friendly format)</li>
<li><tt class="docutils literal"><span class="pre">pickled_meta</span></tt> - the same metadata in <tt class="docutils literal"><span class="pre">meta</span></tt> but pickled for more
efficient deserialization</li>
</ul>
</div></blockquote>
<p>The directory name is made from the request fingerprint (see
<tt class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></tt>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-python"><div class="highlight"><pre>/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</pre></div>
</div>
</div>
<div class="section" id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h4>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this headline">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>A <a class="reference external" href="http://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="http://docs.python.org/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="#std:setting-HTTPCACHE_DBM_MODULE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></tt></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></tt></a> to <tt class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></tt></li>
</ul>
</div>
<div class="section" id="httpcache-middleware-settings">
<h4>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></tt></a> can be configured through the following
settings:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h5>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether the HTTP cache will be enabled.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="#std:setting-HTTPCACHE_DIR"><tt class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></tt></a> was used to enable cache.</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h5>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">0</span></tt></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h5>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'httpcache'</span></tt></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="commands.html#topics-project-structure"><em>Default structure of Scrapy projects</em></a>.</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h5>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">[]</span></tt></p>
<p>Don&#8217;t cache response with these HTTP codes.</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h5>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h5>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">['file']</span></tt></p>
<p>Don&#8217;t cache responses with these URI schemes.</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h5>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.FilesystemCacheStorage'</span></tt></p>
<p>The class which implements the cache storage backend.</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h5>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">'anydbm'</span></tt></p>
<p>The database module to use in the <a class="reference internal" href="#httpcache-storage-dbm"><em>DBM storage backend</em></a>. This setting is specific to the DBM backend.</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h5>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.DummyPolicy'</span></tt></p>
<p>The class which implements the cache policy.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcompression">
<span id="httpcompressionmiddleware"></span><h3>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcompression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpcompression.</tt><tt class="descname">HttpCompressionMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h4>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h5>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Compression middleware will be enabled.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.chunked">
<span id="chunkedtransfermiddleware"></span><h3>ChunkedTransferMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.chunked" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.chunked.</tt><tt class="descname">ChunkedTransferMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware adds support for <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpproxy">
<span id="httpproxymiddleware"></span><h3>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpproxy" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.8.</span></p>
</div>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.httpproxy.</tt><tt class="descname">HttpProxyMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<tt class="docutils literal"><span class="pre">proxy</span></tt> meta value to <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><tt class="xref py py-class docutils literal"><span class="pre">Request</span></tt></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="http://docs.python.org/library/urllib.html">urllib</a> and <a class="reference external" href="http://docs.python.org/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">http_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">https_proxy</span></tt></li>
<li><tt class="docutils literal"><span class="pre">no_proxy</span></tt></li>
</ul>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.redirect">
<span id="redirectmiddleware"></span><h3>RedirectMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.redirect" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">RedirectMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <tt class="docutils literal"><span class="pre">redirect_urls</span></tt> <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> key.</p>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-REDIRECT_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="settings.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> contains the
<tt class="docutils literal"><span class="pre">dont_redirect</span></tt> key, the request will be ignored by this middleware.</p>
<div class="section" id="redirectmiddleware-settings">
<h4>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h5>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Redirect middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">20</span></tt></p>
<p>The maximum number of redirections that will be follow for a single request.</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h3>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</tt><tt class="descname">MetaRefreshMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-METAREFRESH_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_ENABLED</span></tt></a></li>
<li><tt class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_MAXDELAY</span></tt></li>
</ul>
<p>This middleware obey <a class="reference internal" href="settings.html#std:setting-REDIRECT_MAX_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></tt></a> setting, <a class="reference internal" href="#std:reqmeta-dont_redirect"><tt class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></tt></a>
and <a class="reference internal" href="#std:reqmeta-redirect_urls"><tt class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></tt></a> request meta keys as described for <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></tt></a></p>
<div class="section" id="metarefreshmiddleware-settings">
<h4>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h5>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h5>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">100</span></tt></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.retry">
<span id="retrymiddleware"></span><h3>RetryMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.retry" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.retry.</tt><tt class="descname">RetryMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middlware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.</p>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware"><tt class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></tt></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-RETRY_ENABLED"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></tt></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_TIMES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></tt></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a></li>
</ul>
<p>About HTTP errors to consider:</p>
<p>You may want to remove 400 from <a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><tt class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></tt></a>, if you stick to the
HTTP protocol. It&#8217;s included by default because it&#8217;s a common code used
to indicate server overload, which would be something we want to retry.</p>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><tt class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></tt></a> contains the <tt class="docutils literal"><span class="pre">dont_retry</span></tt>
key, the request will be ignored by this middleware.</p>
<div class="section" id="retrymiddleware-settings">
<h4>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h5>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">True</span></tt></p>
<p>Whether the Retry middleware will be enabled.</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h5>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">2</span></tt></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h5>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this headline">¶</a></h5>
<p>Default: <tt class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">400,</span> <span class="pre">408]</span></tt></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h3>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.robotstxt" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.robotstxt.</tt><tt class="descname">RobotsTxtMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="settings.html#std:setting-ROBOTSTXT_OBEY"><tt class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></tt></a> setting is enabled.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Keep in mind that, if you crawl using multiple concurrent
requests per domain, Scrapy could still  download some forbidden pages
if they were requested before the robots.txt file was downloaded. This
is a known limitation of the current robots.txt middleware and will
be fixed in the future.</p>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.stats">
<span id="downloaderstats"></span><h3>DownloaderStats<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.stats" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.stats.DownloaderStats">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.stats.</tt><tt class="descname">DownloaderStats</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_STATS"><tt class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></tt></a>
setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.useragent">
<span id="useragentmiddleware"></span><h3>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.useragent" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.useragent.</tt><tt class="descname">UserAgentMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <cite>user_agent</cite>
attribute must be set.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h3>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.ajaxcrawl" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property">class </em><tt class="descclassname">scrapy.contrib.downloadermiddleware.ajaxcrawl.</tt><tt class="descname">AjaxCrawlMiddleware</tt><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds &#8216;AJAX crawlable&#8217; page variants based
on meta-fragment html tag. See
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
for more info.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scrapy finds &#8216;AJAX crawlable&#8217; pages for URLs like
<tt class="docutils literal"><span class="pre">'http://example.com/!#foo=bar'</span></tt> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn&#8217;t contain <tt class="docutils literal"><span class="pre">'!#'</span></tt>.
This is often a case for &#8216;index&#8217; or &#8216;main&#8217; website pages.</p>
</div>
</dd></dl>

<div class="section" id="ajaxcrawlmiddleware-settings">
<h4>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="ajaxcrawl-enabled">
<span id="std:setting-AJAXCRAWL_ENABLED"></span><h5>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.21.</span></p>
</div>
<p>Default: <tt class="docutils literal"><span class="pre">False</span></tt></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="reference internal" href="broad-crawls.html#topics-broad-crawls"><em>broad crawls</em></a>.</p>
</div>
</div>
</div>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="spider-middleware.html" class="btn btn-neutral float-right" title="Spider Middleware">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="architecture.html" class="btn btn-neutral" title="Architecture overview"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2013, Scrapy developers.
      Last updated on Apr 17, 2014.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="http://doc.scrapy.org/en/master/">master</a></dd>
        
          <dd><a href="../index.html">latest</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.20/">0.20</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.18/">0.18</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.16/">0.16</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.14/">0.14</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.12/">0.12</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.10.3/">0.10.3</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.9/">0.9</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.8/">0.8</a></dd>
        
          <dd><a href="http://doc.scrapy.org/en/0.7/">0.7</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="https://media.readthedocs.org/pdf/scrapy/latest/scrapy.pdf">PDF</a></dd>
        
          <dd><a href="https://media.readthedocs.org/htmlzip/scrapy/latest/scrapy.zip">HTML</a></dd>
        
          <dd><a href="https://media.readthedocs.org/epub/scrapy/latest/scrapy.epub">Epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="http://readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="http://readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org/">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.22.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/jquery/jquery-2.0.3.min.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/jquery/jquery-migrate-1.2.1.min.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/underscore.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/doctools.js"></script>
      <script type="text/javascript" src="../../../../media.readthedocs.org/javascript/readthedocs-doc-embed.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>

<!-- Mirrored from doc.scrapy.org/en/latest/topics/downloader-middleware.html by HTTrack Website Copier/3.x [XR&CO'2013], Wed, 23 Apr 2014 03:06:41 GMT -->
</html>