
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
<title>The Autodiff Cookbook — JAX  documentation</title>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" rel="stylesheet" type="text/css">
<link href="../_static/plot_directive.css" rel="stylesheet" type="text/css">
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../_static/style.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../_static/favicon.png" rel="shortcut icon">
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="vmapped_log_probs.html" rel="next" title="Autobatching log-densities example"/>
<link href="../changelog.html" rel="prev" title="Change log"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../_static/jax_logo_250px.png"/>
</a>
</div><form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search the docs ..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../installation.html">
   Installing JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="quickstart.html">
   JAX Quickstart
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="thinking_in_jax.html">
   How to Think in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Common_Gotchas_in_JAX.html">
   🔪 JAX - The Sharp Bits 🔪
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax-101/index.html">
   Tutorial: JAX 101
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/02-jitting.html">
     Just In Time Compilation with JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/05.1-pytrees.html">
     Working with Pytrees
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/07-state.html">
     Stateful Computations in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/08-pjit.html">
     Introduction to pjit
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../debugging/index.html">
   Runtime value debugging in JAX
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/print_breakpoint.html">
<code class="docutils literal notranslate">
<span class="pre">
       jax.debug.print
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jax.debug.breakpoint
      </span>
</code>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
<span class="pre">
       checkify
      </span>
</code>
     transformation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/flags.html">
     JAX debugging flags
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../async_dispatch.html">
   Asynchronous dispatch
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../aot.html">
   Ahead-of-time lowering and compilation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jaxpr.html">
   Understanding Jaxprs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="convolutions.html">
   Convolutions in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../pytrees.html">
   Pytrees
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../type_promotion.html">
   Type promotion semantics
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../errors.html">
   JAX Errors
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../transfer_guard.html">
   Transfer guard
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../glossary.html">
   JAX Glossary of Terms
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../changelog.html">
   Change log
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active">
<a class="current reference internal" href="#">
   The Autodiff Cookbook
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../api_compatibility.html">
   API compatibility
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../deprecation.html">
   Python and NumPy version support policy
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../concurrency.html">
   Concurrency
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../profiling.html">
   Profiling JAX programs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../device_memory_profiling.html">
   Device Memory Profiling
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../rank_promotion_warning.html">
   Rank promotion warning
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../contributing.html">
   Contributing to JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../developer.html">
   Building from source
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jax_internal_api.html">
   Internal APIs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax.html">
   Public API: jax package
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax.numpy.html">
     jax.numpy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.scipy.html">
     jax.scipy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.config.html">
     JAX configuration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.debug.html">
     jax.debug package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.dlpack.html">
     jax.dlpack module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.distributed.html">
     jax.distributed module
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.example_libraries.html">
     jax.example_libraries package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.experimental.html">
     jax.experimental package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.flatten_util.html">
     jax.flatten_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.image.html">
     jax.image package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lax.html">
     jax.lax package
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.nn.html">
     jax.nn package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.ops.html">
     jax.ops package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.profiler.html">
     jax.profiler module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.random.html">
     jax.random package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.stages.html">
     jax.stages package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.tree_util.html">
     jax.tree_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lib.html">
     jax.lib package
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<a class="headerbtn" data-placement="bottom" data-toggle="tooltip" href="https://github.com/google/jax" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
</a>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../_sources/notebooks/autodiff_cookbook.ipynb.txt" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#gradients">
   Gradients
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#starting-with-grad">
     Starting with
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts">
     Differentiating with respect to nested lists, tuples, and dicts
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluate-a-function-and-its-gradient-using-value-and-grad">
     Evaluate a function and its gradient using
     <code class="docutils literal notranslate">
<span class="pre">
       value_and_grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#checking-against-numerical-differences">
     Checking against numerical differences
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#hessian-vector-products-with-grad-of-grad">
     Hessian-vector products with
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
     -of-
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobians-and-hessians-using-jacfwd-and-jacrev">
     Jacobians and Hessians using
     <code class="docutils literal notranslate">
<span class="pre">
       jacfwd
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jacrev
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#how-it-s-made-two-foundational-autodiff-functions">
   How it’s made: two foundational autodiff functions
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff">
     Jacobian-Vector products (JVPs, aka forward-mode autodiff)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#jvps-in-math">
       JVPs in math
      </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#jvps-in-jax-code">
       JVPs in JAX code
      </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff">
     Vector-Jacobian products (VJPs, aka reverse-mode autodiff)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#vjps-in-math">
       VJPs in math
      </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#vjps-in-jax-code">
       VJPs in JAX code
      </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#vector-valued-gradients-with-vjps">
     Vector-valued gradients with VJPs
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#hessian-vector-products-using-both-forward-and-reverse-mode">
     Hessian-vector products using both forward- and reverse-mode
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#composing-vjps-jvps-and-vmap">
   Composing VJPs, JVPs, and
   <code class="docutils literal notranslate">
<span class="pre">
     vmap
    </span>
</code>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobian-matrix-and-matrix-jacobian-products">
     Jacobian-Matrix and Matrix-Jacobian products
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#the-implementation-of-jacfwd-and-jacrev">
     The implementation of
     <code class="docutils literal notranslate">
<span class="pre">
       jacfwd
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jacrev
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#complex-numbers-and-differentiation">
   Complex numbers and differentiation
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#more-advanced-autodiff">
   More advanced autodiff
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>The Autodiff Cookbook</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#gradients">
   Gradients
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#starting-with-grad">
     Starting with
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts">
     Differentiating with respect to nested lists, tuples, and dicts
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluate-a-function-and-its-gradient-using-value-and-grad">
     Evaluate a function and its gradient using
     <code class="docutils literal notranslate">
<span class="pre">
       value_and_grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#checking-against-numerical-differences">
     Checking against numerical differences
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#hessian-vector-products-with-grad-of-grad">
     Hessian-vector products with
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
     -of-
     <code class="docutils literal notranslate">
<span class="pre">
       grad
      </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobians-and-hessians-using-jacfwd-and-jacrev">
     Jacobians and Hessians using
     <code class="docutils literal notranslate">
<span class="pre">
       jacfwd
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jacrev
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#how-it-s-made-two-foundational-autodiff-functions">
   How it’s made: two foundational autodiff functions
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff">
     Jacobian-Vector products (JVPs, aka forward-mode autodiff)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#jvps-in-math">
       JVPs in math
      </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#jvps-in-jax-code">
       JVPs in JAX code
      </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff">
     Vector-Jacobian products (VJPs, aka reverse-mode autodiff)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#vjps-in-math">
       VJPs in math
      </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#vjps-in-jax-code">
       VJPs in JAX code
      </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#vector-valued-gradients-with-vjps">
     Vector-valued gradients with VJPs
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#hessian-vector-products-using-both-forward-and-reverse-mode">
     Hessian-vector products using both forward- and reverse-mode
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#composing-vjps-jvps-and-vmap">
   Composing VJPs, JVPs, and
   <code class="docutils literal notranslate">
<span class="pre">
     vmap
    </span>
</code>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#jacobian-matrix-and-matrix-jacobian-products">
     Jacobian-Matrix and Matrix-Jacobian products
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#the-implementation-of-jacfwd-and-jacrev">
     The implementation of
     <code class="docutils literal notranslate">
<span class="pre">
       jacfwd
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jacrev
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#complex-numbers-and-differentiation">
   Complex numbers and differentiation
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#more-advanced-autodiff">
   More advanced autodiff
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/The Autodiff Cookbook"></a><section class="tex2jax_ignore mathjax_ignore" id="the-autodiff-cookbook">
<h1>The Autodiff Cookbook<a class="headerlink" href="#the-autodiff-cookbook" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p><em>alexbw@, mattjj@</em></p>
<p>JAX has a pretty general automatic differentiation system. In this notebook, we’ll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Gradients"></a><section id="gradients">
<h2>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">#</a></h2>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Starting with grad"></a><section id="starting-with-grad">
<h3>Starting with <code class="docutils literal notranslate"><span class="pre">grad</span></code><a class="headerlink" href="#starting-with-grad" title="Permalink to this headline">#</a></h3>
<p>You can differentiate a function with <code class="docutils literal notranslate"><span class="pre">grad</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grad_tanh</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_tanh</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.070650816
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">grad</span></code> takes a function and returns a function. If you have a Python function <code class="docutils literal notranslate"><span class="pre">f</span></code> that evaluates the mathematical function <span class="math notranslate nohighlight">\(f\)</span>, then <code class="docutils literal notranslate"><span class="pre">grad(f)</span></code> is a Python function that evaluates the mathematical function <span class="math notranslate nohighlight">\(\nabla f\)</span>. That means <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code> represents the value <span class="math notranslate nohighlight">\(\nabla f(x)\)</span>.</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">grad</span></code> operates on functions, you can apply it to its own output to differentiate as many times as you like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">))(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)))(</span><span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.13621868
0.25265405
</pre></div>
</div>
</div>
</div>
<p>Let’s look at computing gradients with <code class="docutils literal notranslate"><span class="pre">grad</span></code> in a linear logistic regression model. First, the setup:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Outputs probability of a label being true.</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Build a toy dataset.</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">1.12</span><span class="p">,</span>  <span class="mf">0.77</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.88</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.30</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.74</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.49</span><span class="p">,</span> <span class="mf">1.39</span><span class="p">]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>

<span class="c1"># Training loss is the negative log-likelihood of the training examples.</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">label_probs</span> <span class="o">=</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">label_probs</span><span class="p">))</span>

<span class="c1"># Initialize random model coefficients</span>
<span class="n">key</span><span class="p">,</span> <span class="n">W_key</span><span class="p">,</span> <span class="n">b_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">W_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">b_key</span><span class="p">,</span> <span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">grad</span></code> function with its <code class="docutils literal notranslate"><span class="pre">argnums</span></code> argument to differentiate a function with respect to positional arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Differentiate `loss` with respect to the first positional argument:</span>
<span class="n">W_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'W_grad'</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>

<span class="c1"># Since argnums=0 is the default, this does the same thing:</span>
<span class="n">W_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'W_grad'</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>

<span class="c1"># But we can choose different values too, and drop the keyword:</span>
<span class="n">b_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b_grad'</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">)</span>

<span class="c1"># Including tuple values</span>
<span class="n">W_grad</span><span class="p">,</span> <span class="n">b_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'W_grad'</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b_grad'</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W_grad [-0.16965583 -0.8774644  -1.4901346 ]
W_grad [-0.16965583 -0.8774644  -1.4901346 ]
b_grad -0.29227245
W_grad [-0.16965583 -0.8774644  -1.4901346 ]
b_grad -0.29227245
</pre></div>
</div>
</div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">grad</span></code> API has a direct correspondence to the excellent notation in Spivak’s classic <em>Calculus on Manifolds</em> (1965), also used in Sussman and Wisdom’s <a class="reference external" href="https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics"><em>Structure and Interpretation of Classical Mechanics</em></a> (2015) and their <a class="reference external" href="https://mitpress.mit.edu/9780262019347/functional-differential-geometry"><em>Functional Differential Geometry</em></a> (2013). Both books are open-access. See in particular the “Prologue” section of <em>Functional Differential Geometry</em> for a defense of this notation.</p>
<p>Essentially, when using the <code class="docutils literal notranslate"><span class="pre">argnums</span></code> argument, if <code class="docutils literal notranslate"><span class="pre">f</span></code> is a Python function for evaluating the mathematical function <span class="math notranslate nohighlight">\(f\)</span>, then the Python expression <code class="docutils literal notranslate"><span class="pre">grad(f,</span> <span class="pre">i)</span></code> evaluates to a Python function for evaluating <span class="math notranslate nohighlight">\(\partial_i f\)</span>.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Differentiating with respect to nested lists, tuples, and dicts"></a><section id="differentiating-with-respect-to-nested-lists-tuples-and-dicts">
<h3>Differentiating with respect to nested lists, tuples, and dicts<a class="headerlink" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts" title="Permalink to this headline">#</a></h3>
<p>Differentiating with respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss2</span><span class="p">(</span><span class="n">params_dict</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">'W'</span><span class="p">],</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">'b'</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">label_probs</span> <span class="o">=</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">label_probs</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">loss2</span><span class="p">)({</span><span class="s1">'W'</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">:</span> <span class="n">b</span><span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{'W': DeviceArray([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': DeviceArray(-0.29227245, dtype=float32)}
</pre></div>
</div>
</div>
</div>
<p>You can <a class="reference external" href="https://github.com/google/jax/issues/446#issuecomment-467105048">register your own container types</a> to work with not just <code class="docutils literal notranslate"><span class="pre">grad</span></code> but all the JAX transformations (<code class="docutils literal notranslate"><span class="pre">jit</span></code>, <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, etc.).</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Evaluate a function and its gradient using value_and_grad"></a><section id="evaluate-a-function-and-its-gradient-using-value-and-grad">
<h3>Evaluate a function and its gradient using <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code><a class="headerlink" href="#evaluate-a-function-and-its-gradient-using-value-and-grad" title="Permalink to this headline">#</a></h3>
<p>Another convenient function is <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code> for efficiently computing both a function’s value as well as its gradient’s value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">value_and_grad</span>
<span class="n">loss_value</span><span class="p">,</span> <span class="n">Wb_grad</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'loss value'</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'loss value'</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss value 3.0519388
loss value 3.0519388
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Checking against numerical differences"></a><section id="checking-against-numerical-differences">
<h3>Checking against numerical differences<a class="headerlink" href="#checking-against-numerical-differences" title="Permalink to this headline">#</a></h3>
<p>A great thing about derivatives is that they’re straightforward to check with finite differences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set a step size for finite differences calculations</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>

<span class="c1"># Check b_grad with scalar finite differences</span>
<span class="n">b_grad_numerical</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span> <span class="o">-</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">))</span> <span class="o">/</span> <span class="n">eps</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b_grad_numerical'</span><span class="p">,</span> <span class="n">b_grad_numerical</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b_grad_autodiff'</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

<span class="c1"># Check W_grad with finite differences in a random direction</span>
<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">unitvec</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">vec</span><span class="p">))</span>
<span class="n">W_grad_numerical</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">unitvec</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">unitvec</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="n">eps</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'W_dirderiv_numerical'</span><span class="p">,</span> <span class="n">W_grad_numerical</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'W_dirderiv_autodiff'</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">unitvec</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b_grad_numerical -0.29325485
b_grad_autodiff -0.29227245
W_dirderiv_numerical -0.2002716
W_dirderiv_autodiff -0.19909117
</pre></div>
</div>
</div>
</div>
<p>JAX provides a simple convenience function that does essentially the same thing, but checks up to any order of differentiation that you like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax.test_util</span> <span class="kn">import</span> <span class="n">check_grads</span>
<span class="n">check_grads</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># check up to 2nd order derivatives</span>
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Hessian-vector products with grad-of-grad"></a><section id="hessian-vector-products-with-grad-of-grad">
<h3>Hessian-vector products with <code class="docutils literal notranslate"><span class="pre">grad</span></code>-of-<code class="docutils literal notranslate"><span class="pre">grad</span></code><a class="headerlink" href="#hessian-vector-products-with-grad-of-grad" title="Permalink to this headline">#</a></h3>
<p>One thing we can do with higher-order <code class="docutils literal notranslate"><span class="pre">grad</span></code> is build a Hessian-vector product function. (Later on we’ll write an even more efficient implementation that mixes both forward- and reverse-mode, but this one will use pure reverse-mode.)</p>
<p>A Hessian-vector product function can be useful in a <a class="reference external" href="https://en.wikipedia.org/wiki/Truncated_Newton_method">truncated Newton Conjugate-Gradient algorithm</a> for minimizing smooth convex functions, or for studying the curvature of neural network training objectives (e.g. <a class="reference external" href="https://arxiv.org/abs/1406.2572">1</a>, <a class="reference external" href="https://arxiv.org/abs/1811.07062">2</a>, <a class="reference external" href="https://arxiv.org/abs/1706.04454">3</a>, <a class="reference external" href="https://arxiv.org/abs/1802.03451">4</a>).</p>
<p>For a scalar-valued function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is written as <span class="math notranslate nohighlight">\(\partial^2 f(x)\)</span>. A Hessian-vector product function is then able to evaluate</p>
<p><span class="math notranslate nohighlight">\(\qquad v \mapsto \partial^2 f(x) \cdot v\)</span></p>
<p>for any <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>.</p>
<p>The trick is not to instantiate the full Hessian matrix: if <span class="math notranslate nohighlight">\(n\)</span> is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store.</p>
<p>Luckily, <code class="docutils literal notranslate"><span class="pre">grad</span></code> already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial^2 f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial g(x)\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(g(x) = \partial f(x) \cdot v\)</span> is a new scalar-valued function that dots the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> with the vector <span class="math notranslate nohighlight">\(v\)</span>. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know <code class="docutils literal notranslate"><span class="pre">grad</span></code> is efficient.</p>
<p>In JAX code, we can just write this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This example shows that you can freely use lexical closure, and JAX will never get perturbed or confused.</p>
<p>We’ll check this implementation a few cells down, once we see how to compute dense Hessian matrices. We’ll also write an even better version that uses both forward-mode and reverse-mode.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Jacobians and Hessians using jacfwd and jacrev"></a><section id="jacobians-and-hessians-using-jacfwd-and-jacrev">
<h3>Jacobians and Hessians using <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code><a class="headerlink" href="#jacobians-and-hessians-using-jacfwd-and-jacrev" title="Permalink to this headline">#</a></h3>
<p>You can compute full Jacobian matrices using the <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacfwd</span><span class="p">,</span> <span class="n">jacrev</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">J</span> <span class="o">=</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"jacfwd result, with shape"</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>

<span class="n">J</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"jacrev result, with shape"</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>jacfwd result, with shape (4, 3)
[[ 0.05981758  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188288  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]]
jacrev result, with shape (4, 3)
[[ 0.05981757  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188289  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]]
</pre></div>
</div>
</div>
</div>
<p>These two functions compute the same values (up to machine numerics), but differ in their implementation: <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> probably has an edge over <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> with container types:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_dict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">predict</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">'W'</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">'b'</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">J_dict</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">predict_dict</span><span class="p">)({</span><span class="s1">'W'</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">:</span> <span class="n">b</span><span class="p">},</span> <span class="n">inputs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">J_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Jacobian from </span><span class="si">{}</span><span class="s2"> to logits is"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jacobian from W to logits is
[[ 0.05981757  0.12883787  0.08857603]
 [ 0.04015916 -0.04928625  0.00684531]
 [ 0.12188289  0.01406341 -0.3047072 ]
 [ 0.00140431 -0.00472531  0.00263782]]
Jacobian from b to logits is
[0.11503381 0.04563541 0.23439017 0.00189771]
</pre></div>
</div>
</div>
</div>
<p>For more details on forward- and reverse-mode, as well as how to implement <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> as efficiently as possible, read on!</p>
<p>Using a composition of two of these functions gives us a way to compute dense Hessian matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>

<span class="n">H</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"hessian, with shape"</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hessian, with shape (4, 3, 3)
[[[ 0.02285465  0.04922541  0.03384247]
  [ 0.04922541  0.10602397  0.07289147]
  [ 0.03384247  0.07289147  0.05011288]]

 [[-0.03195215  0.03921401 -0.00544639]
  [ 0.03921401 -0.04812629  0.00668421]
  [-0.00544639  0.00668421 -0.00092836]]

 [[-0.01583708 -0.00182736  0.03959271]
  [-0.00182736 -0.00021085  0.00456839]
  [ 0.03959271  0.00456839 -0.09898177]]

 [[-0.00103524  0.00348343 -0.00194457]
  [ 0.00348343 -0.01172127  0.0065432 ]
  [-0.00194457  0.0065432  -0.00365263]]]
</pre></div>
</div>
</div>
</div>
<p>This shape makes sense: if we start with a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>, then at a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> we expect to get the shapes</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x) \in \mathbb{R}^m\)</span>, the value of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial f(x) \in \mathbb{R}^{m \times n}\)</span>, the Jacobian matrix at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial^2 f(x) \in \mathbb{R}^{m \times n \times n}\)</span>, the Hessian at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
</ul>
<p>and so on.</p>
<p>To implement <code class="docutils literal notranslate"><span class="pre">hessian</span></code>, we could have used <code class="docutils literal notranslate"><span class="pre">jacfwd(jacrev(f))</span></code> or <code class="docutils literal notranslate"><span class="pre">jacrev(jacfwd(f))</span></code> or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since <span class="math notranslate nohighlight">\(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)</span>), which is where forward-mode wins out.</p>
</section>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/How it's made: two foundational autodiff functions"></a><section id="how-it-s-made-two-foundational-autodiff-functions">
<h2>How it’s made: two foundational autodiff functions<a class="headerlink" href="#how-it-s-made-two-foundational-autodiff-functions" title="Permalink to this headline">#</a></h2>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Jacobian-Vector products (JVPs, aka forward-mode autodiff)"></a><section id="jacobian-vector-products-jvps-aka-forward-mode-autodiff">
<a class="dashAnchor" name="//apple_ref/cpp/Section/Jacobian-Vector products (JVPs, aka forward-mode autodiff)"></a><span id="jacobian-vector-product"></span><h3>Jacobian-Vector products (JVPs, aka forward-mode autodiff)<a class="headerlink" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff" title="Permalink to this headline">#</a></h3>
<p>JAX includes efficient and general implementations of both forward- and reverse-mode automatic differentiation. The familiar <code class="docutils literal notranslate"><span class="pre">grad</span></code> function is built on reverse-mode, but to explain the difference in the two modes, and when each can be useful, we need a bit of math background.</p>
<section id="jvps-in-math">
<h4>JVPs in math<a class="headerlink" href="#jvps-in-math" title="Permalink to this headline">#</a></h4>
<p>Mathematically, given a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian of <span class="math notranslate nohighlight">\(f\)</span> evaluated at an input point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, denoted <span class="math notranslate nohighlight">\(\partial f(x)\)</span>, is often thought of as a matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^m \times \mathbb{R}^n\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\)</span>.</p>
<p>But we can also think of <span class="math notranslate nohighlight">\(\partial f(x)\)</span> as a linear map, which maps the tangent space of the domain of <span class="math notranslate nohighlight">\(f\)</span> at the point <span class="math notranslate nohighlight">\(x\)</span> (which is just another copy of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>) to the tangent space of the codomain of <span class="math notranslate nohighlight">\(f\)</span> at the point <span class="math notranslate nohighlight">\(f(x)\)</span> (a copy of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>):</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)</span>.</p>
<p>This map is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Pushforward_(differential)">pushforward map</a> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. The Jacobian matrix is just the matrix for this linear map in a standard basis.</p>
<p>If we don’t commit to one specific input point <span class="math notranslate nohighlight">\(x\)</span>, then we can think of the function <span class="math notranslate nohighlight">\(\partial f\)</span> as first taking an input point and returning the Jacobian linear map at that input point:</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\)</span>.</p>
<p>In particular, we can uncurry things so that given input point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> and a tangent vector <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>, we get back an output tangent vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>. We call that mapping, from <span class="math notranslate nohighlight">\((x, v)\)</span> pairs to output tangent vectors, the <em>Jacobian-vector product</em>, and write it as</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto \partial f(x) v\)</span></p>
</section>
<section id="jvps-in-jax-code">
<h4>JVPs in JAX code<a class="headerlink" href="#jvps-in-jax-code" title="Permalink to this headline">#</a></h4>
<p>Back in Python code, JAX’s <code class="docutils literal notranslate"><span class="pre">jvp</span></code> function models this transformation. Given a Python function that evaluates <span class="math notranslate nohighlight">\(f\)</span>, JAX’s <code class="docutils literal notranslate"><span class="pre">jvp</span></code> is a way to get a Python function for evaluating <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), \partial f(x) v)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jvp</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Push forward the vector `v` along `f` evaluated at `W`</span>
<span class="n">y</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">v</span><span class="p">,))</span>
</pre></div>
</div>
</div>
</div>
<p>In terms of <a class="reference external" href="https://wiki.haskell.org/Type_signature">Haskell-like type signatures</a>,
we could write</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">jvp</span><span class="w"> </span><span class="ow">::</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"></span>
</pre></div>
</div>
<p>where we use <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code> to denote the type of the tangent space for <code class="docutils literal notranslate"><span class="pre">a</span></code>. In words, <code class="docutils literal notranslate"><span class="pre">jvp</span></code> takes as arguments a function of type <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">-&gt;</span> <span class="pre">b</span></code>, a value of type <code class="docutils literal notranslate"><span class="pre">a</span></code>, and a tangent vector value of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code>. It gives back a pair consisting of a value of type <code class="docutils literal notranslate"><span class="pre">b</span></code> and an output tangent vector of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">b</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function is evaluated much like the original function, but paired up with each primal value of type <code class="docutils literal notranslate"><span class="pre">a</span></code> it pushes along tangent values of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code>. For each primitive numerical operation that the original function would have applied, the <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function executes a “JVP rule” for that primitive that both evaluates the primitive on the primals and applies the primitive’s JVP at those primal values.</p>
<p>That evaluation strategy has some immediate implications about computational complexity: since we evaluate JVPs as we go, we don’t need to store anything for later, and so the memory cost is independent of the depth of the computation. In addition, the FLOP cost of the <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function is about 3x the cost of just evaluating the function (one unit of work for evaluating the original function, for example <code class="docutils literal notranslate"><span class="pre">sin(x)</span></code>; one unit for linearizing, like <code class="docutils literal notranslate"><span class="pre">cos(x)</span></code>; and one unit for applying the linearized function to a vector, like <code class="docutils literal notranslate"><span class="pre">cos_x</span> <span class="pre">*</span> <span class="pre">v</span></code>). Put another way, for a fixed primal point <span class="math notranslate nohighlight">\(x\)</span>, we can evaluate <span class="math notranslate nohighlight">\(v \mapsto \partial f(x) \cdot v\)</span> for about the same marginal cost as evaluating <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>That memory complexity sounds pretty compelling! So why don’t we see forward-mode very often in machine learning?</p>
<p>To answer that, first think about how you could use a JVP to build a full Jacobian matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build a full Jacobian one column at a time, and to get each column costs about the same as one function evaluation. That will be efficient for functions with “tall” Jacobians, but inefficient for “wide” Jacobians.</p>
<p>If you’re doing gradient-based optimization in machine learning, you probably want to minimize a loss function from parameters in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> to a scalar loss value in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. That means the Jacobian of this function is a very wide matrix: <span class="math notranslate nohighlight">\(\partial f(x) \in \mathbb{R}^{1 \times n}\)</span>, which we often identify with the Gradient vector <span class="math notranslate nohighlight">\(\nabla f(x) \in \mathbb{R}^n\)</span>. Building that matrix one column at a time, with each call taking a similar number of FLOPs to evaluate the original function, sure seems inefficient! In particular, for training neural networks, where <span class="math notranslate nohighlight">\(f\)</span> is a training loss function and <span class="math notranslate nohighlight">\(n\)</span> can be in the millions or billions, this approach just won’t scale.</p>
<p>To do better for functions like this, we just need to use reverse-mode.</p>
</section>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Vector-Jacobian products (VJPs, aka reverse-mode autodiff)"></a><section id="vector-jacobian-products-vjps-aka-reverse-mode-autodiff">
<a class="dashAnchor" name="//apple_ref/cpp/Section/Vector-Jacobian products (VJPs, aka reverse-mode autodiff)"></a><span id="vector-jacobian-product"></span><h3>Vector-Jacobian products (VJPs, aka reverse-mode autodiff)<a class="headerlink" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff" title="Permalink to this headline">#</a></h3>
<p>Where forward-mode gives us back a function for evaluating Jacobian-vector products, which we can then use to build Jacobian matrices one column at a time, reverse-mode is a way to get back a function for evaluating vector-Jacobian products (equivalently Jacobian-transpose-vector products), which we can use to build Jacobian matrices one row at a time.</p>
<section id="vjps-in-math">
<h4>VJPs in math<a class="headerlink" href="#vjps-in-math" title="Permalink to this headline">#</a></h4>
<p>Let’s again consider a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>.
Starting from our notation for JVPs, the notation for VJPs is pretty simple:</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto v \partial f(x)\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(v\)</span> is an element of the cotangent space of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> (isomorphic to another copy of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>). When being rigorous, we should think of <span class="math notranslate nohighlight">\(v\)</span> as a linear map <span class="math notranslate nohighlight">\(v : \mathbb{R}^m \to \mathbb{R}\)</span>, and when we write <span class="math notranslate nohighlight">\(v \partial f(x)\)</span> we mean function composition <span class="math notranslate nohighlight">\(v \circ \partial f(x)\)</span>, where the types work out because <span class="math notranslate nohighlight">\(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)</span>. But in the common case we can identify <span class="math notranslate nohighlight">\(v\)</span> with a vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> and use the two almost interchangeably, just like we might sometimes flip between “column vectors” and “row vectors” without much comment.</p>
<p>With that identification, we can alternatively think of the linear part of a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\)</span>.</p>
<p>For a given point <span class="math notranslate nohighlight">\(x\)</span>, we can write the signature as</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\)</span>.</p>
<p>The corresponding map on cotangent spaces is often called the <a class="reference external" href="https://en.wikipedia.org/wiki/Pullback_(differential_geometry)">pullback</a>
of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. The key for our purposes is that it goes from something that looks like the output of <span class="math notranslate nohighlight">\(f\)</span> to something that looks like the input of <span class="math notranslate nohighlight">\(f\)</span>, just like we might expect from a transposed linear function.</p>
</section>
<section id="vjps-in-jax-code">
<h4>VJPs in JAX code<a class="headerlink" href="#vjps-in-jax-code" title="Permalink to this headline">#</a></h4>
<p>Switching from math back to Python, the JAX function <code class="docutils literal notranslate"><span class="pre">vjp</span></code> can take a Python function for evaluating <span class="math notranslate nohighlight">\(f\)</span> and give us back a Python function for evaluating the VJP <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vjp</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Pull back the covector `u` along `f` evaluated at `W`</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">vjp_fun</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In terms of <a class="reference external" href="https://wiki.haskell.org/Type_signature">Haskell-like type signatures</a>,
we could write</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">vjp</span><span class="w"> </span><span class="ow">::</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">CT</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">CT</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"></span>
</pre></div>
</div>
<p>where we use <code class="docutils literal notranslate"><span class="pre">CT</span> <span class="pre">a</span></code> to denote the type for the cotangent space for <code class="docutils literal notranslate"><span class="pre">a</span></code>. In words, <code class="docutils literal notranslate"><span class="pre">vjp</span></code> takes as arguments a function of type <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">-&gt;</span> <span class="pre">b</span></code> and a point of type <code class="docutils literal notranslate"><span class="pre">a</span></code>, and gives back a pair consisting of a value of type <code class="docutils literal notranslate"><span class="pre">b</span></code> and a linear map of type <code class="docutils literal notranslate"><span class="pre">CT</span> <span class="pre">b</span> <span class="pre">-&gt;</span> <span class="pre">CT</span> <span class="pre">a</span></code>.</p>
<p>This is great because it lets us build Jacobian matrices one row at a time, and the FLOP cost for evaluating <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\)</span> is only about three times the cost of evaluating <span class="math notranslate nohighlight">\(f\)</span>. In particular, if we want the gradient of a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, we can do it in just one call. That’s how <code class="docutils literal notranslate"><span class="pre">grad</span></code> is efficient for gradient-based optimization, even for objectives like neural network training loss functions on millions or billions of parameters.</p>
<p>There’s a cost, though: though the FLOPs are friendly, memory scales with the depth of the computation. Also, the implementation is traditionally more complex than that of forward-mode, though JAX has some tricks up its sleeve (that’s a story for a future notebook!).</p>
<p>For more on how reverse-mode works, see <a class="reference external" href="http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/">this tutorial video from the Deep Learning Summer School in 2017</a>.</p>
</section>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Vector-valued gradients with VJPs"></a><section id="vector-valued-gradients-with-vjps">
<h3>Vector-valued gradients with VJPs<a class="headerlink" href="#vector-valued-gradients-with-vjps" title="Permalink to this headline">#</a></h3>
<p>If you’re interested in taking vector-valued gradients (like <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vjp</span>

<span class="k">def</span> <span class="nf">vgrad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">vgrad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[6. 6.]
 [6. 6.]]
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Hessian-vector products using both forward- and reverse-mode"></a><section id="hessian-vector-products-using-both-forward-and-reverse-mode">
<h3>Hessian-vector products using both forward- and reverse-mode<a class="headerlink" href="#hessian-vector-products-using-both-forward-and-reverse-mode" title="Permalink to this headline">#</a></h3>
<p>In a previous section, we implemented a Hessian-vector product function just using reverse-mode (assuming continuous second derivatives):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That’s efficient, but we can do even better and save some memory by using forward-mode together with reverse-mode.</p>
<p>Mathematically, given a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> to differentiate, a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> at which to linearize the function, and a vector <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>, the Hessian-vector product function we want is</p>
<p><span class="math notranslate nohighlight">\((x, v) \mapsto \partial^2 f(x) v\)</span></p>
<p>Consider the helper function <span class="math notranslate nohighlight">\(g : \mathbb{R}^n \to \mathbb{R}^n\)</span> defined to be the derivative (or gradient) of <span class="math notranslate nohighlight">\(f\)</span>, namely <span class="math notranslate nohighlight">\(g(x) = \partial f(x)\)</span>. All we need is its JVP, since that will give us</p>
<p><span class="math notranslate nohighlight">\((x, v) \mapsto \partial g(x) v = \partial^2 f(x) v\)</span>.</p>
<p>We can translate that almost directly into code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jvp</span><span class="p">,</span> <span class="n">grad</span>

<span class="c1"># forward-over-reverse</span>
<span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jvp</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Even better, since we didn’t have to call <code class="docutils literal notranslate"><span class="pre">jnp.dot</span></code> directly, this <code class="docutils literal notranslate"><span class="pre">hvp</span></code> function works with arrays of any shape and with arbitrary container types (like vectors stored as nested lists/dicts/tuples), and doesn’t even have a dependence on <code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code>.</p>
<p>Here’s an example of how to use it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey1</span><span class="p">,</span> <span class="n">subkey2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey1</span><span class="p">,</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey2</span><span class="p">,</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

<span class="n">ans1</span> <span class="o">=</span> <span class="n">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,),</span> <span class="p">(</span><span class="n">V</span><span class="p">,))</span>
<span class="n">ans2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">X</span><span class="p">),</span> <span class="n">V</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans1</span><span class="p">,</span> <span class="n">ans2</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Another way you might consider writing this is using reverse-over-forward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reverse-over-forward</span>
<span class="k">def</span> <span class="nf">hvp_revfwd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">primals</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="n">g</span><span class="p">)(</span><span class="n">primals</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That’s not quite as good, though, because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reverse-over-reverse, only works for single arguments</span>
<span class="k">def</span> <span class="nf">hvp_revrev</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">primals</span>
  <span class="n">v</span><span class="p">,</span> <span class="o">=</span> <span class="n">tangents</span>
  <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">"Forward over reverse"</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp(f, (X,), (V,))
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reverse over forward"</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp_revfwd(f, (X,), (V,))
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reverse over reverse"</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp_revrev(f, (X,), (V,))

<span class="nb">print</span><span class="p">(</span><span class="s2">"Naive full Hessian materialization"</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Forward over reverse
1.69 ms ± 62.7 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over forward
The slowest run took 4.13 times longer than the fastest. This could mean that an intermediate result is being cached.
3.99 ms ± 2.88 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over reverse
The slowest run took 4.27 times longer than the fastest. This could mean that an intermediate result is being cached.
5.59 ms ± 4.12 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Naive full Hessian materialization
16.6 ms ± 141 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Composing VJPs, JVPs, and vmap"></a><section id="composing-vjps-jvps-and-vmap">
<h2>Composing VJPs, JVPs, and <code class="docutils literal notranslate"><span class="pre">vmap</span></code><a class="headerlink" href="#composing-vjps-jvps-and-vmap" title="Permalink to this headline">#</a></h2>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Jacobian-Matrix and Matrix-Jacobian products"></a><section id="jacobian-matrix-and-matrix-jacobian-products">
<h3>Jacobian-Matrix and Matrix-Jacobian products<a class="headerlink" href="#jacobian-matrix-and-matrix-jacobian-products" title="Permalink to this headline">#</a></h3>
<p>Now that we have <code class="docutils literal notranslate"><span class="pre">jvp</span></code> and <code class="docutils literal notranslate"><span class="pre">vjp</span></code> transformations that give us functions to push-forward or pull-back single vectors at a time, we can use JAX’s <code class="docutils literal notranslate"><span class="pre">vmap</span></code> <a class="reference external" href="https://github.com/google/jax#auto-vectorization-with-vmap">transformation</a> to push and pull entire bases at once. In particular, we can use that to write fast matrix-Jacobian and Jacobian-matrix products.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.</span>
<span class="c1"># First, use a list comprehension to loop over rows in the matrix M.</span>
<span class="k">def</span> <span class="nf">loop_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">vjp_fun</span><span class="p">(</span><span class="n">mi</span><span class="p">)</span> <span class="k">for</span> <span class="n">mi</span> <span class="ow">in</span> <span class="n">M</span><span class="p">])</span>

<span class="c1"># Now, use vmap to build a computation that does a single fast matrix-matrix</span>
<span class="c1"># multiply, rather than an outer loop over vector-matrix multiplies.</span>
<span class="k">def</span> <span class="nf">vmap_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">outs</span><span class="p">,</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp_fun</span><span class="p">)(</span><span class="n">M</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outs</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_covecs</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">num_covecs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">loop_vs</span> <span class="o">=</span> <span class="n">loop_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">U</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Non-vmapped Matrix-Jacobian product'</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 loop_mjp(f, W, M=U)

<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Vmapped Matrix-Jacobian product'</span><span class="p">)</span>
<span class="n">vmap_vs</span> <span class="o">=</span> <span class="n">vmap_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">U</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 vmap_mjp(f, W, M=U)

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">loop_vs</span><span class="p">,</span> <span class="n">vmap_vs</span><span class="p">),</span> <span class="s1">'Vmap and non-vmapped Matrix-Jacobian Products should be identical'</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Non-vmapped Matrix-Jacobian product
44.2 ms ± 185 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Matrix-Jacobian product
1.68 ms ± 23.1 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loop_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="c1"># jvp immediately returns the primal and tangent values as a tuple,</span>
    <span class="c1"># so we'll compute and select the tangents in a list comprehension</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">mi</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">mi</span> <span class="ow">in</span> <span class="n">M</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">vmap_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">_jvp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">s</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">_jvp</span><span class="p">)(</span><span class="n">M</span><span class="p">)</span>

<span class="n">num_vecs</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">num_vecs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">loop_vs</span> <span class="o">=</span> <span class="n">loop_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Non-vmapped Jacobian-Matrix product'</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 loop_jmp(f, W, M=S)
<span class="n">vmap_vs</span> <span class="o">=</span> <span class="n">vmap_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Vmapped Jacobian-Matrix product'</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 vmap_jmp(f, W, M=S)

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">loop_vs</span><span class="p">,</span> <span class="n">vmap_vs</span><span class="p">),</span> <span class="s1">'Vmap and non-vmapped Jacobian-Matrix products should be identical'</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Non-vmapped Jacobian-Matrix product
84.6 ms ± 414 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Jacobian-Matrix product
949 µs ± 15.4 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/The implementation of jacfwd and jacrev"></a><section id="the-implementation-of-jacfwd-and-jacrev">
<h3>The implementation of <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code><a class="headerlink" href="#the-implementation-of-jacfwd-and-jacrev" title="Permalink to this headline">#</a></h3>
<p>Now that we’ve seen fast Jacobian-matrix and matrix-Jacobian products, it’s not hard to guess how to write <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>. We just use the same technique to push-forward or pull-back an entire standard basis (isomorphic to an identity matrix) at once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacrev</span> <span class="k">as</span> <span class="n">builtin_jacrev</span>

<span class="k">def</span> <span class="nf">our_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="c1"># Use vmap to do a matrix-Jacobian product.</span>
        <span class="c1"># Here, the matrix is the Euclidean basis, so we get all</span>
        <span class="c1"># entries in the Jacobian at once. </span>
        <span class="n">J</span><span class="p">,</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp_fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">J</span>
    <span class="k">return</span> <span class="n">jacfun</span>

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">builtin_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">),</span> <span class="n">our_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)),</span> <span class="s1">'Incorrect reverse-mode Jacobian results!'</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacfwd</span> <span class="k">as</span> <span class="n">builtin_jacfwd</span>

<span class="k">def</span> <span class="nf">our_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">_jvp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="n">s</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">Jt</span> <span class="o">=</span><span class="n">vmap</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Jt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacfun</span>

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">builtin_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">),</span> <span class="n">our_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)),</span> <span class="s1">'Incorrect forward-mode Jacobian results!'</span>
</pre></div>
</div>
</div>
</div>
<p>Interestingly, <a class="reference external" href="https://github.com/hips/autograd">Autograd</a> couldn’t do this. Our <a class="reference external" href="https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60">implementation</a> of reverse-mode <code class="docutils literal notranslate"><span class="pre">jacobian</span></code> in Autograd had to pull back one vector at a time with an outer-loop <code class="docutils literal notranslate"><span class="pre">map</span></code>. Pushing one vector at a time through the computation is much less efficient than batching it all together with <code class="docutils literal notranslate"><span class="pre">vmap</span></code>.</p>
<p>Another thing that Autograd couldn’t do is <code class="docutils literal notranslate"><span class="pre">jit</span></code>. Interestingly, no matter how much Python dynamism you use in your function to be differentiated, we could always use <code class="docutils literal notranslate"><span class="pre">jit</span></code> on the linear part of the computation. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span>

<span class="n">y</span><span class="p">,</span> <span class="n">f_vjp</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jit</span><span class="p">(</span><span class="n">f_vjp</span><span class="p">)(</span><span class="mf">1.</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(DeviceArray(3.1415927, dtype=float32, weak_type=True),)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Complex numbers and differentiation"></a><section id="complex-numbers-and-differentiation">
<h2>Complex numbers and differentiation<a class="headerlink" href="#complex-numbers-and-differentiation" title="Permalink to this headline">#</a></h2>
<p>JAX is great at complex numbers and differentiation. To support both <a class="reference external" href="https://en.wikipedia.org/wiki/Holomorphic_function">holomorphic and non-holomorphic differentiation</a>, it helps to think in terms of JVPs and VJPs.</p>
<p>Consider a complex-to-complex function <span class="math notranslate nohighlight">\(f: \mathbb{C} \to \mathbb{C}\)</span> and identify it with a corresponding function <span class="math notranslate nohighlight">\(g: \mathbb{R}^2 \to \mathbb{R}^2\)</span>,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>That is, we’ve decomposed <span class="math notranslate nohighlight">\(f(z) = u(x, y) + v(x, y) i\)</span> where <span class="math notranslate nohighlight">\(z = x + y i\)</span>, and identified <span class="math notranslate nohighlight">\(\mathbb{C}\)</span> with <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> to get <span class="math notranslate nohighlight">\(g\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(g\)</span> only involves real inputs and outputs, we already know how to write a Jacobian-vector product for it, say given a tangent vector <span class="math notranslate nohighlight">\((c, d) \in \mathbb{R}^2\)</span>, namely</p>
<p><span class="math notranslate nohighlight">\(\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} c \\ d \end{bmatrix}\)</span>.</p>
<p>To get a JVP for the original function <span class="math notranslate nohighlight">\(f\)</span> applied to a tangent vector <span class="math notranslate nohighlight">\(c + di \in \mathbb{C}\)</span>, we just use the same definition and identify the result as another complex number,</p>
<p><span class="math notranslate nohighlight">\(\partial f(x + y i)(c + d i) =
\begin{matrix} \begin{bmatrix} 1 &amp; i \end{bmatrix} \\ ~ \end{matrix}
\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} c \\ d \end{bmatrix}\)</span>.</p>
<p>That’s our definition of the JVP of a <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function! Notice it doesn’t matter whether or not <span class="math notranslate nohighlight">\(f\)</span> is holomorphic: the JVP is unambiguous.</p>
<p>Here’s a check:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># random coeffs for u and v</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">y</span>

  <span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">y</span>

  <span class="c1"># primal point</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># tangent vector</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z_dot</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># check jvp</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">ans</span> <span class="o">=</span> <span class="n">jvp</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="p">(</span><span class="n">z</span><span class="p">,),</span> <span class="p">(</span><span class="n">z_dot</span><span class="p">,))</span>
  <span class="n">expected</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">check</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
True
True
</pre></div>
</div>
</div>
</div>
<p>What about VJPs? We do something pretty similar: for a cotangent vector <span class="math notranslate nohighlight">\(c + di \in \mathbb{C}\)</span> we define the VJP of <span class="math notranslate nohighlight">\(f\)</span> as</p>
<p><span class="math notranslate nohighlight">\((c + di)^* \; \partial f(x + y i) =
\begin{matrix} \begin{bmatrix} c &amp; -d \end{bmatrix} \\ ~ \end{matrix}
\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} 1 \\ -i \end{bmatrix}\)</span>.</p>
<p>What’s with the negatives? They’re just to take care of complex conjugation, and the fact that we’re working with covectors.</p>
<p>Here’s a check of the VJP rules:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># random coeffs for u and v</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">y</span>

  <span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">y</span>

  <span class="c1"># primal point</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># cotangent vector</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z_bar</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>  <span class="c1"># for dtype control</span>

  <span class="c1"># check vjp</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">fun_vjp</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
  <span class="n">ans</span><span class="p">,</span> <span class="o">=</span> <span class="n">fun_vjp</span><span class="p">(</span><span class="n">z_bar</span><span class="p">)</span>
  <span class="n">expected</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">d</span><span class="p">)</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">))</span>
  <span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">check</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What about convenience wrappers like <code class="docutils literal notranslate"><span class="pre">grad</span></code>, <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code>, and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>?</p>
<p>For <span class="math notranslate nohighlight">\(\mathbb{R} \to \mathbb{R}\)</span> functions, recall we defined <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code> as being <code class="docutils literal notranslate"><span class="pre">vjp(f,</span> <span class="pre">x)[1](1.0)</span></code>, which works because applying a VJP to a <code class="docutils literal notranslate"><span class="pre">1.0</span></code> value reveals the gradient (i.e. Jacobian, or derivative). We can do the same thing for <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{R}\)</span> functions: we can still use <code class="docutils literal notranslate"><span class="pre">1.0</span></code> as the cotangent vector, and we just get out a complex number result summarizing the full Jacobian:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray(6.-8.j, dtype=complex64)
</pre></div>
</div>
</div>
</div>
<p>For general <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> functions, the Jacobian has 4 real-valued degrees of freedom (as in the 2x2 Jacobian matrices above), so we can’t hope to represent all of them within a complex number. But we can for holomorphic functions! A holomorphic function is precisely a <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function with the special property that its derivative can be represented as a single complex number. (The <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a> ensure that the above 2x2 Jacobians have the special form of a scale-and-rotate matrix in the complex plane, i.e. the action of a single complex number under multiplication.) And we can reveal that one complex number using a single call to <code class="docutils literal notranslate"><span class="pre">vjp</span></code> with a covector of <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<p>Because this only works for holomorphic functions, to use this trick we need to promise JAX that our function is holomorphic; otherwise, JAX will raise an error when <code class="docutils literal notranslate"><span class="pre">grad</span></code> is used for a complex-output function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray(-27.034946-3.8511534j, dtype=complex64, weak_type=True)
</pre></div>
</div>
</div>
</div>
<p>All the <code class="docutils literal notranslate"><span class="pre">holomorphic=True</span></code> promise does is disable the error when the output is complex-valued. We can still write <code class="docutils literal notranslate"><span class="pre">holomorphic=True</span></code> when the function isn’t holomorphic, but the answer we get out won’t represent the full Jacobian. Instead, it’ll be the Jacobian of the function where we just discard the imaginary part of the output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">conjugate</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># f is not actually holomorphic!</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray(1.-0.j, dtype=complex64, weak_type=True)
</pre></div>
</div>
</div>
</div>
<p>There are some useful upshots for how <code class="docutils literal notranslate"><span class="pre">grad</span></code> works here:</p>
<ol class="arabic simple">
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">grad</span></code> on holomorphic <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> functions.</p></li>
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">grad</span></code> to optimize <span class="math notranslate nohighlight">\(f : \mathbb{C} \to \mathbb{R}\)</span> functions, like real-valued loss functions of complex parameters <code class="docutils literal notranslate"><span class="pre">x</span></code>, by taking steps in the direction of the conjugate of <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code>.</p></li>
<li><p>If we have an <span class="math notranslate nohighlight">\(\mathbb{R} \to \mathbb{R}\)</span> function that just happens to use some complex-valued operations internally (some of which must be non-holomorphic, e.g. FFTs used in convolutions) then <code class="docutils literal notranslate"><span class="pre">grad</span></code> still works and we get the same result that an implementation using only real values would have given.</p></li>
</ol>
<p>In any case, JVPs and VJPs are always unambiguous. And if we wanted to compute the full Jacobian matrix of a non-holomorphic <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function, we can do it with JVPs or VJPs!</p>
<p>You should expect complex numbers to work everywhere in JAX. Here’s differentiating through a Cholesky decomposition of a complex matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span>    <span class="mf">2.</span><span class="o">+</span><span class="mi">3</span><span class="n">j</span><span class="p">,</span>    <span class="mi">5</span><span class="n">j</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">2.</span><span class="o">-</span><span class="mi">3</span><span class="n">j</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">,</span>  <span class="mf">1.</span><span class="o">+</span><span class="mi">7</span><span class="n">j</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="n">j</span><span class="p">,</span>  <span class="mf">1.</span><span class="o">-</span><span class="mi">7</span><span class="n">j</span><span class="p">,</span>    <span class="mf">12.</span><span class="p">]])</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">L</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">L</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[-0.7534131  +0.j       , -3.0509033 -10.9405365j,
               5.989681   +3.5423033j],
             [-3.0509033 +10.9405365j, -8.904484   +0.j       ,
              -5.135148   -6.5593696j],
             [ 5.989681   -3.5423033j, -5.135148   +6.5593696j,
               0.01320434 +0.j       ]], dtype=complex64)
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/More advanced autodiff"></a><section id="more-advanced-autodiff">
<h2>More advanced autodiff<a class="headerlink" href="#more-advanced-autodiff" title="Permalink to this headline">#</a></h2>
<p>In this notebook, we worked through some easy, and then progressively more complicated, applications of automatic differentiation in JAX. We hope you now feel that taking derivatives in JAX is easy and powerful.</p>
<p>There’s a whole world of other autodiff tricks and functionality out there. Topics we didn’t cover, but hope to in an “Advanced Autodiff Cookbook” include:</p>
<ul class="simple">
<li><p>Gauss-Newton Vector Products, linearizing once</p></li>
<li><p>Custom VJPs and JVPs</p></li>
<li><p>Efficient derivatives at fixed-points</p></li>
<li><p>Estimating the trace of a Hessian using random Hessian-vector products.</p></li>
<li><p>Forward-mode autodiff using only reverse-mode autodiff.</p></li>
<li><p>Taking derivatives with respect to custom data types.</p></li>
<li><p>Checkpointing (binomial checkpointing for efficient reverse-mode, not model snapshotting).</p></li>
<li><p>Optimizing VJPs with Jacobian pre-accumulation.</p></li>
</ul>
</section>
</section>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="../changelog.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Change log</p>
</div>
</a>
<a class="right-next" href="vmapped_log_probs.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Autobatching log-densities example</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      © Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>