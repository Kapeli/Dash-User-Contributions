
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Training a Simple Neural Network, with PyTorch Data Loading — JAX  documentation</title>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" rel="stylesheet" type="text/css">
<link href="../_static/plot_directive.css" rel="stylesheet" type="text/css">
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../_static/style.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<link href="../_static/favicon.png" rel="shortcut icon">
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="xmap_tutorial.html" rel="next" title="Named axes and easy-to-revise parallelism"/>
<link href="Writing_custom_interpreters_in_Jax.html" rel="prev" title="Writing custom Jaxpr interpreters in JAX"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../_static/jax_logo_250px.png"/>
</a>
</div><form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search the docs ..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../installation.html">
   Installing JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="quickstart.html">
   JAX Quickstart
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="thinking_in_jax.html">
   How to Think in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Common_Gotchas_in_JAX.html">
   🔪 JAX - The Sharp Bits 🔪
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax-101/index.html">
   Tutorial: JAX 101
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/02-jitting.html">
     Just In Time Compilation with JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/05.1-pytrees.html">
     Working with Pytrees
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/07-state.html">
     Stateful Computations in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax-101/08-pjit.html">
     Introduction to pjit
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../debugging/index.html">
   Runtime value debugging in JAX
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/print_breakpoint.html">
<code class="docutils literal notranslate">
<span class="pre">
       jax.debug.print
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jax.debug.breakpoint
      </span>
</code>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
<span class="pre">
       checkify
      </span>
</code>
     transformation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/flags.html">
     JAX debugging flags
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../async_dispatch.html">
   Asynchronous dispatch
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../aot.html">
   Ahead-of-time lowering and compilation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jaxpr.html">
   Understanding Jaxprs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="convolutions.html">
   Convolutions in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../pytrees.html">
   Pytrees
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../type_promotion.html">
   Type promotion semantics
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../errors.html">
   JAX Errors
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../transfer_guard.html">
   Transfer guard
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../glossary.html">
   JAX Glossary of Terms
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../changelog.html">
   Change log
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
</li>
<li class="toctree-l1 current active">
<a class="current reference internal" href="#">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../api_compatibility.html">
   API compatibility
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../deprecation.html">
   Python and NumPy version support policy
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../concurrency.html">
   Concurrency
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../profiling.html">
   Profiling JAX programs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../device_memory_profiling.html">
   Device Memory Profiling
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../rank_promotion_warning.html">
   Rank promotion warning
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../contributing.html">
   Contributing to JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../developer.html">
   Building from source
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jax_internal_api.html">
   Internal APIs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax.html">
   Public API: jax package
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax.numpy.html">
     jax.numpy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.scipy.html">
     jax.scipy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.config.html">
     JAX configuration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.debug.html">
     jax.debug package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.dlpack.html">
     jax.dlpack module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.distributed.html">
     jax.distributed module
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.example_libraries.html">
     jax.example_libraries package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.experimental.html">
     jax.experimental package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.flatten_util.html">
     jax.flatten_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.image.html">
     jax.image package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lax.html">
     jax.lax package
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.nn.html">
     jax.nn package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.ops.html">
     jax.ops package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.profiler.html">
     jax.profiler module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.random.html">
     jax.random package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.stages.html">
     jax.stages package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.tree_util.html">
     jax.tree_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lib.html">
     jax.lib package
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<a class="headerbtn" data-placement="bottom" data-toggle="tooltip" href="https://github.com/google/jax" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
</a>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../_sources/notebooks/Neural_Network_and_Data_Loading.ipynb.txt" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#auto-batching-predictions">
   Auto-batching predictions
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#utility-and-loss-functions">
   Utility and loss functions
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-loading-with-pytorch">
   Data Loading with PyTorch
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-loop">
   Training Loop
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Training a Simple Neural Network, with PyTorch Data Loading</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#auto-batching-predictions">
   Auto-batching predictions
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#utility-and-loss-functions">
   Utility and loss functions
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-loading-with-pytorch">
   Data Loading with PyTorch
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-loop">
   Training Loop
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Training a Simple Neural Network, with PyTorch Data Loading"></a><section class="tex2jax_ignore mathjax_ignore" id="training-a-simple-neural-network-with-pytorch-data-loading">
<h1>Training a Simple Neural Network, with PyTorch Data Loading<a class="headerlink" href="#training-a-simple-neural-network-with-pytorch-data-loading" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/Neural_Network_and_Data_Loading.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p><strong>Copyright 2018 The JAX Authors.</strong></p>
<p>Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<p>https://www.apache.org/licenses/LICENSE-2.0</p>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<p><img alt="JAX" src="https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png"/></p>
<p>Let’s combine everything we showed in the <a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/quickstart.ipynb">quickstart notebook</a> to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch’s data loading API to load images and labels (because it’s pretty great, and the world doesn’t need yet another data loading library).</p>
<p>Of course, you can use JAX with any API that is compatible with NumPy to make specifying the model a bit more plug-and-play. Here, just for explanatory purposes, we won’t use any neural network libraries or special APIs for building our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
</pre></div>
</div>
</div>
</div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Hyperparameters"></a><section id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this headline">#</a></h2>
<p>Let’s get a few bookkeeping items out of the way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A helper function to randomly initialize weights and biases</span>
<span class="c1"># for a dense neural network layer</span>
<span class="k">def</span> <span class="nf">random_layer_params</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
  <span class="n">w_key</span><span class="p">,</span> <span class="n">b_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">w_key</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)),</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">b_key</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,))</span>

<span class="c1"># Initialize all layers for a fully-connected neural network with sizes "sizes"</span>
<span class="k">def</span> <span class="nf">init_network_params</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
  <span class="n">keys</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">))</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">random_layer_params</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">keys</span><span class="p">)]</span>

<span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">n_targets</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">init_network_params</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Auto-batching predictions"></a><section id="auto-batching-predictions">
<h2>Auto-batching predictions<a class="headerlink" href="#auto-batching-predictions" title="Permalink to this headline">#</a></h2>
<p>Let us first define our prediction function. Note that we’re defining this for a <em>single</em> image example. We’re going to use JAX’s <code class="docutils literal notranslate"><span class="pre">vmap</span></code> function to automatically handle mini-batches, with no performance penalty.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
  <span class="c1"># per-example predictions</span>
  <span class="n">activations</span> <span class="o">=</span> <span class="n">image</span>
  <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
  
  <span class="n">final_w</span><span class="p">,</span> <span class="n">final_b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">final_w</span><span class="p">,</span> <span class="n">activations</span><span class="p">)</span> <span class="o">+</span> <span class="n">final_b</span>
  <span class="k">return</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check that our prediction function only works on single images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This works on single examples</span>
<span class="n">random_flattened_image</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,))</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">random_flattened_image</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Doesn't work with a batch</span>
<span class="n">random_flattened_images</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>
<span class="k">try</span><span class="p">:</span>
  <span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">random_flattened_images</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">'Invalid shapes!'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Invalid shapes!
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let's upgrade it to handle batches using `vmap`</span>

<span class="c1"># Make a batched version of the `predict` function</span>
<span class="n">batched_predict</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="c1"># `batched_predict` has the same call signature as `predict`</span>
<span class="n">batched_preds</span> <span class="o">=</span> <span class="n">batched_predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">random_flattened_images</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batched_preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10, 10)
</pre></div>
</div>
</div>
</div>
<p>At this point, we have all the ingredients we need to define our neural network and train it. We’ve built an auto-batched version of <code class="docutils literal notranslate"><span class="pre">predict</span></code>, which we should be able to use in a loss function. We should be able to use <code class="docutils literal notranslate"><span class="pre">grad</span></code> to take the derivative of the loss with respect to the neural network parameters. Last, we should be able to use <code class="docutils literal notranslate"><span class="pre">jit</span></code> to speed up everything.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Utility and loss functions"></a><section id="utility-and-loss-functions">
<h2>Utility and loss functions<a class="headerlink" href="#utility-and-loss-functions" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">"""Create a one-hot encoding of x of size k."""</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">dtype</span><span class="p">)</span>
  
<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
  <span class="n">target_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">batched_predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predicted_class</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
  <span class="n">preds</span> <span class="o">=</span> <span class="n">batched_predict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[(</span><span class="n">w</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">dw</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">db</span><span class="p">)</span>
          <span class="k">for</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="p">(</span><span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Data Loading with PyTorch"></a><section id="data-loading-with-pytorch">
<h2>Data Loading with PyTorch<a class="headerlink" href="#data-loading-with-pytorch" title="Permalink to this headline">#</a></h2>
<p>JAX is laser-focused on program transformations and accelerator-backed NumPy, so we don’t include data loading or munging in the JAX library. There are already a lot of great data loaders out there, so let’s just use them instead of reinventing anything. We’ll grab PyTorch’s data loader, and make a tiny shim to make it work with NumPy arrays.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install torch torchvision
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: torch in /opt/anaconda3/lib/python3.7/site-packages (1.4.0)
Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.7/site-packages (0.5.0)
Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.17.2)
Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.12.0)
Requirement already satisfied: pillow&gt;=4.1.1 in /opt/anaconda3/lib/python3.7/site-packages (from torchvision) (6.2.0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>

<span class="k">def</span> <span class="nf">numpy_collate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span><span class="nb">list</span><span class="p">)):</span>
    <span class="n">transposed</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">numpy_collate</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">samples</span> <span class="ow">in</span> <span class="n">transposed</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">NumpyLoader</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">batch_sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
        <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="o">=</span><span class="n">batch_sampler</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">numpy_collate</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
        <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">worker_init_fn</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">FlattenAndCast</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pic</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define our dataset, using torch datasets</span>
<span class="n">mnist_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">'/tmp/mnist/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">FlattenAndCast</span><span class="p">())</span>
<span class="n">training_generator</span> <span class="o">=</span> <span class="n">NumpyLoader</span><span class="p">(</span><span class="n">mnist_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz
Extracting /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw
Processing...
Done!
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "75806ce83ace4f69b81bbc4251c5573f", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "274ed4ab05f34f70b7a5bb6cf427ffd0", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "d38fa4eabf3c4d4494eb59e078ac94e8", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "523ac9565c5f4509a1ee8fdbb1e6d66d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the full train dataset (for checking accuracy while training)</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mnist_dataset</span><span class="o">.</span><span class="n">train_data</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist_dataset</span><span class="o">.</span><span class="n">train_data</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mnist_dataset</span><span class="o">.</span><span class="n">train_labels</span><span class="p">),</span> <span class="n">n_targets</span><span class="p">)</span>

<span class="c1"># Get full test dataset</span>
<span class="n">mnist_dataset_test</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">'/tmp/mnist/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mnist_dataset_test</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist_dataset_test</span><span class="o">.</span><span class="n">test_data</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mnist_dataset_test</span><span class="o">.</span><span class="n">test_labels</span><span class="p">),</span> <span class="n">n_targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data
  warnings.warn("train_data has been renamed data")
/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data
  warnings.warn("test_data has been renamed data")
/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets
  warnings.warn("test_labels has been renamed targets")
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Training Loop"></a><section id="training-loop">
<h2>Training Loop<a class="headerlink" href="#training-loop" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">training_generator</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

  <span class="n">train_acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
  <span class="n">test_acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">{}</span><span class="s2"> in </span><span class="si">{:0.2f}</span><span class="s2"> sec"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">epoch_time</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Training set accuracy </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_acc</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Test set accuracy </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0 in 55.15 sec
Training set accuracy 0.9157500267028809
Test set accuracy 0.9195000529289246
Epoch 1 in 42.26 sec
Training set accuracy 0.9372166991233826
Test set accuracy 0.9384000301361084
Epoch 2 in 44.37 sec
Training set accuracy 0.9491666555404663
Test set accuracy 0.9469000697135925
Epoch 3 in 41.75 sec
Training set accuracy 0.9568166732788086
Test set accuracy 0.9534000158309937
Epoch 4 in 41.16 sec
Training set accuracy 0.9631333351135254
Test set accuracy 0.9577000737190247
Epoch 5 in 38.89 sec
Training set accuracy 0.9675000309944153
Test set accuracy 0.9616000652313232
Epoch 6 in 40.68 sec
Training set accuracy 0.9708333611488342
Test set accuracy 0.9650000333786011
Epoch 7 in 41.50 sec
Training set accuracy 0.973716676235199
Test set accuracy 0.9672000408172607
</pre></div>
</div>
</div>
</div>
<p>We’ve now used the whole of the JAX API: <code class="docutils literal notranslate"><span class="pre">grad</span></code> for derivatives, <code class="docutils literal notranslate"><span class="pre">jit</span></code> for speedups and <code class="docutils literal notranslate"><span class="pre">vmap</span></code> for auto-vectorization.
We used NumPy to specify all of our computation, and borrowed the great data loaders from PyTorch, and ran the whole thing on the GPU.</p>
</section>
</section>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="Writing_custom_interpreters_in_Jax.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Writing custom Jaxpr interpreters in JAX</p>
</div>
</a>
<a class="right-next" href="xmap_tutorial.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Named axes and easy-to-revise parallelism</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      © Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>