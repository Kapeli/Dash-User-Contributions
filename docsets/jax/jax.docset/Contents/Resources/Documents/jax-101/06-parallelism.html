
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Parallel Evaluation in JAX â€” JAX  documentation</title>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" rel="stylesheet" type="text/css">
<link href="../_static/plot_directive.css" rel="stylesheet" type="text/css">
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../_static/style.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<link href="../_static/favicon.png" rel="shortcut icon">
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="07-state.html" rel="next" title="Stateful Computations in JAX"/>
<link href="05.1-pytrees.html" rel="prev" title="Working with Pytrees"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../_static/jax_logo_250px.png"/>
</a>
</div><form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search the docs ..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../installation.html">
   Installing JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/quickstart.html">
   JAX Quickstart
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/thinking_in_jax.html">
   How to Think in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Common_Gotchas_in_JAX.html">
   ðŸ”ª JAX - The Sharp Bits ðŸ”ª
  </a>
</li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="index.html">
   Tutorial: JAX 101
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="02-jitting.html">
     Just In Time Compilation with JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="05.1-pytrees.html">
     Working with Pytrees
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Parallel Evaluation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="07-state.html">
     Stateful Computations in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="08-pjit.html">
     Introduction to pjit
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../debugging/index.html">
   Runtime value debugging in JAX
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/print_breakpoint.html">
<code class="docutils literal notranslate">
<span class="pre">
       jax.debug.print
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jax.debug.breakpoint
      </span>
</code>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
<span class="pre">
       checkify
      </span>
</code>
     transformation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/flags.html">
     JAX debugging flags
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../async_dispatch.html">
   Asynchronous dispatch
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../aot.html">
   Ahead-of-time lowering and compilation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jaxpr.html">
   Understanding Jaxprs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/convolutions.html">
   Convolutions in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../pytrees.html">
   Pytrees
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../type_promotion.html">
   Type promotion semantics
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../errors.html">
   JAX Errors
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../transfer_guard.html">
   Transfer guard
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../glossary.html">
   JAX Glossary of Terms
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../changelog.html">
   Change log
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../api_compatibility.html">
   API compatibility
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../deprecation.html">
   Python and NumPy version support policy
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../concurrency.html">
   Concurrency
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../profiling.html">
   Profiling JAX programs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../device_memory_profiling.html">
   Device Memory Profiling
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../rank_promotion_warning.html">
   Rank promotion warning
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../contributing.html">
   Contributing to JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../developer.html">
   Building from source
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jax_internal_api.html">
   Internal APIs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax.html">
   Public API: jax package
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax.numpy.html">
     jax.numpy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.scipy.html">
     jax.scipy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.config.html">
     JAX configuration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.debug.html">
     jax.debug package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.dlpack.html">
     jax.dlpack module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.distributed.html">
     jax.distributed module
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.example_libraries.html">
     jax.example_libraries package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.experimental.html">
     jax.experimental package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.flatten_util.html">
     jax.flatten_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.image.html">
     jax.image package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lax.html">
     jax.lax package
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.nn.html">
     jax.nn package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.ops.html">
     jax.ops package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.profiler.html">
     jax.profiler module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.random.html">
     jax.random package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.stages.html">
     jax.stages package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.tree_util.html">
     jax.tree_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lib.html">
     jax.lib package
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<a class="headerbtn" data-placement="bottom" data-toggle="tooltip" href="https://github.com/google/jax" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
</a>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../_sources/jax-101/06-parallelism.ipynb.txt" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#colab-tpu-setup">
   Colab TPU Setup
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#the-basics">
   The basics
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#specifying-in-axes">
   Specifying
   <code class="docutils literal notranslate">
<span class="pre">
     in_axes
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#pmap-and-jit">
<code class="docutils literal notranslate">
<span class="pre">
     pmap
    </span>
</code>
   and
   <code class="docutils literal notranslate">
<span class="pre">
     jit
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#communication-between-devices">
   Communication between devices
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#nesting-jax-pmap-and-jax-vmap">
   Nesting
   <code class="docutils literal notranslate">
<span class="pre">
     jax.pmap
    </span>
</code>
   and
   <code class="docutils literal notranslate">
<span class="pre">
     jax.vmap
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#example">
   Example
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#aside-hosts-and-devices-in-jax">
   Aside: hosts and devices in JAX
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Parallel Evaluation in JAX</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#colab-tpu-setup">
   Colab TPU Setup
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#the-basics">
   The basics
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#specifying-in-axes">
   Specifying
   <code class="docutils literal notranslate">
<span class="pre">
     in_axes
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#pmap-and-jit">
<code class="docutils literal notranslate">
<span class="pre">
     pmap
    </span>
</code>
   and
   <code class="docutils literal notranslate">
<span class="pre">
     jit
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#communication-between-devices">
   Communication between devices
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#nesting-jax-pmap-and-jax-vmap">
   Nesting
   <code class="docutils literal notranslate">
<span class="pre">
     jax.pmap
    </span>
</code>
   and
   <code class="docutils literal notranslate">
<span class="pre">
     jax.vmap
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#example">
   Example
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#aside-hosts-and-devices-in-jax">
   Aside: hosts and devices in JAX
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Parallel Evaluation in JAX"></a><section class="tex2jax_ignore mathjax_ignore" id="parallel-evaluation-in-jax">
<h1>Parallel Evaluation in JAX<a class="headerlink" href="#parallel-evaluation-in-jax" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/06-parallelism.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p><em>Authors: Vladimir Mikulik &amp; Roman Ring</em></p>
<p>In this section we will discuss the facilities built into JAX for single-program, multiple-data (SPMD) code.</p>
<p>SPMD refers to a parallelism technique where the same computation (e.g., the forward pass of a neural net) is run on different input data (e.g., different inputs in a batch) in parallel on different devices (e.g., several TPUs).</p>
<p>Conceptually, this is not very different from vectorisation, where the same operations occur in parallel in different parts of memory on the same device. We have already seen that vectorisation is supported in JAX as a program transformation, <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>. JAX supports device parallelism analogously, using <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> to transform a function written for one device into a function that runs in parallel on multiple devices. This colab will teach you all about it.</p>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Colab TPU Setup"></a><section id="colab-tpu-setup">
<h2>Colab TPU Setup<a class="headerlink" href="#colab-tpu-setup" title="Permalink to this headline">#</a></h2>
<p>If youâ€™re running this code in Google Colab, be sure to choose <em>Runtime</em>â†’<em>Change Runtime Type</em> and choose <strong>TPU</strong> from the Hardware Accelerator menu.</p>
<p>Once this is done, you can run the following to set up the Colab TPU for use with JAX:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.tools.colab_tpu</span>
<span class="n">jax</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">colab_tpu</span><span class="o">.</span><span class="n">setup_tpu</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Next run the following to see the TPU devices you have available:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[TpuDevice(id=0, host_id=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, host_id=0, coords=(0,0,0), core_on_chip=1),
 TpuDevice(id=2, host_id=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=3, host_id=0, coords=(1,0,0), core_on_chip=1),
 TpuDevice(id=4, host_id=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=5, host_id=0, coords=(0,1,0), core_on_chip=1),
 TpuDevice(id=6, host_id=0, coords=(1,1,0), core_on_chip=0),
 TpuDevice(id=7, host_id=0, coords=(1,1,0), core_on_chip=1)]
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/The basics"></a><section id="the-basics">
<h2>The basics<a class="headerlink" href="#the-basics" title="Permalink to this headline">#</a></h2>
<p>The most basic use of <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> is completely analogous to <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>, so letâ€™s return to the convolution example from the <a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/03-vectorization.ipynb">Vectorisation notebook</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">convolve</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">w</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="n">convolve</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([11., 20., 29.], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Now, letâ€™s convert our <code class="docutils literal notranslate"><span class="pre">convolve</span></code> function into one that runs on entire batches of data. In anticipation of spreading the batch across several devices, weâ€™ll make the batch size equal to the number of devices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span> 
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">n_devices</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">ws</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">w</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_devices</span><span class="p">)</span>

<span class="n">xs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19],
       [20, 21, 22, 23, 24],
       [25, 26, 27, 28, 29],
       [30, 31, 32, 33, 34],
       [35, 36, 37, 38, 39]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ws</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.],
       [2., 3., 4.]])
</pre></div>
</div>
</div>
</div>
<p>As before, we can vectorise using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ws</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[ 11.,  20.,  29.],
             [ 56.,  65.,  74.],
             [101., 110., 119.],
             [146., 155., 164.],
             [191., 200., 209.],
             [236., 245., 254.],
             [281., 290., 299.],
             [326., 335., 344.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>To spread out the computation across multiple devices, just replace <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> with <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ws</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ShardedDeviceArray([[ 11.,  20.,  29.],
                    [ 56.,  65.,  74.],
                    [101., 110., 119.],
                    [146., 155., 164.],
                    [191., 200., 209.],
                    [236., 245., 254.],
                    [281., 290., 299.],
                    [326., 335., 344.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Note that the parallelized <code class="docutils literal notranslate"><span class="pre">convolve</span></code> returns a <code class="docutils literal notranslate"><span class="pre">ShardedDeviceArray</span></code>. That is because the elements of this array are sharded across all of the devices used in the parallelism. If we were to run another parallel computation, the elements would stay on their respective devices, without incurring cross-device communication costs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ws</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ShardedDeviceArray([[   78.,   138.,   198.],
                    [ 1188.,  1383.,  1578.],
                    [ 3648.,  3978.,  4308.],
                    [ 7458.,  7923.,  8388.],
                    [12618., 13218., 13818.],
                    [19128., 19863., 20598.],
                    [26988., 27858., 28728.],
                    [36198., 37203., 38208.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>The outputs of the inner <code class="docutils literal notranslate"><span class="pre">jax.pmap(convolve)</span></code> never left their devices when being fed into the outer <code class="docutils literal notranslate"><span class="pre">jax.pmap(convolve)</span></code>.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Specifying in_axes"></a><section id="specifying-in-axes">
<h2>Specifying <code class="docutils literal notranslate"><span class="pre">in_axes</span></code><a class="headerlink" href="#specifying-in-axes" title="Permalink to this headline">#</a></h2>
<p>Like with <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, we can use <code class="docutils literal notranslate"><span class="pre">in_axes</span></code> to specify whether an argument to the parallelized function should be broadcast (<code class="docutils literal notranslate"><span class="pre">None</span></code>), or whether it should be split along a given axis. Note, however, that unlike <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, only the leading axis (<code class="docutils literal notranslate"><span class="pre">0</span></code>) is supported by <code class="docutils literal notranslate"><span class="pre">pmap</span></code> at the time of writing this guide.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">convolve</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">xs</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ShardedDeviceArray([[ 11.,  20.,  29.],
                    [ 56.,  65.,  74.],
                    [101., 110., 119.],
                    [146., 155., 164.],
                    [191., 200., 209.],
                    [236., 245., 254.],
                    [281., 290., 299.],
                    [326., 335., 344.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Notice how we get equivalent output to what we observe above with <code class="docutils literal notranslate"><span class="pre">jax.pmap(convolve)(xs,</span> <span class="pre">ws)</span></code>, where we manually replicated <code class="docutils literal notranslate"><span class="pre">w</span></code> when creating <code class="docutils literal notranslate"><span class="pre">ws</span></code>. Here, it is replicated via broadcasting, by specifying it as <code class="docutils literal notranslate"><span class="pre">None</span></code> in <code class="docutils literal notranslate"><span class="pre">in_axes</span></code>.</p>
<p>Keep in mind that when calling the transformed function, the size of the specified axis in arguments must not exceed the number of devices available to the host.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/pmap and jit"></a><section id="pmap-and-jit">
<h2><code class="docutils literal notranslate"><span class="pre">pmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jit</span></code><a class="headerlink" href="#pmap-and-jit" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> JIT-compiles the function given to it as part of its operation, so there is no need to additionally <code class="docutils literal notranslate"><span class="pre">jax.jit</span></code> it.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Communication between devices"></a><section id="communication-between-devices">
<h2>Communication between devices<a class="headerlink" href="#communication-between-devices" title="Permalink to this headline">#</a></h2>
<p>The above is enough to perform simple parallel operations, e.g. batching a simple MLP forward pass across several devices. However, sometimes we need to pass information between the devices. For example, perhaps we are interested in normalizing the output of each device so they sum to 1.
For that, we can use special <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators">collective ops</a> (such as the <code class="docutils literal notranslate"><span class="pre">jax.lax.p*</span></code> ops <code class="docutils literal notranslate"><span class="pre">psum</span></code>, <code class="docutils literal notranslate"><span class="pre">pmean</span></code>, <code class="docutils literal notranslate"><span class="pre">pmax</span></code>, â€¦). In order to use the collective ops we must specify the name of the <code class="docutils literal notranslate"><span class="pre">pmap</span></code>-ed axis through <code class="docutils literal notranslate"><span class="pre">axis_name</span></code> argument, and then refer to it when calling the op. Hereâ€™s how to do that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">normalized_convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">w</span><span class="p">))</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">output</span> <span class="o">/</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">psum</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'p'</span><span class="p">)</span>

<span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">normalized_convolution</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'p'</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ws</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ShardedDeviceArray([[0.00816024, 0.01408451, 0.019437  ],
                    [0.04154303, 0.04577465, 0.04959785],
                    [0.07492582, 0.07746479, 0.07975871],
                    [0.10830861, 0.10915492, 0.10991956],
                    [0.14169139, 0.14084506, 0.14008042],
                    [0.17507419, 0.17253521, 0.17024128],
                    [0.20845698, 0.20422535, 0.20040214],
                    [0.24183977, 0.23591548, 0.23056298]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">axis_name</span></code> is just a string label that allows collective operations like <code class="docutils literal notranslate"><span class="pre">jax.lax.psum</span></code> to refer to the axis bound by <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>. It can be named anything you want â€“ in this case, <code class="docutils literal notranslate"><span class="pre">p</span></code>. This name is essentially invisible to anything but those functions, and those functions use it to know which axis to communicate across.</p>
<p><code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> also supports <code class="docutils literal notranslate"><span class="pre">axis_name</span></code>, which allows <code class="docutils literal notranslate"><span class="pre">jax.lax.p*</span></code> operations to be used in the vectorisation context in the same way they would be used in a <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">normalized_convolution</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'p'</span><span class="p">)(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ws</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[0.00816024, 0.01408451, 0.019437  ],
             [0.04154303, 0.04577465, 0.04959785],
             [0.07492582, 0.07746479, 0.07975871],
             [0.10830861, 0.10915492, 0.10991956],
             [0.14169139, 0.14084506, 0.14008042],
             [0.17507419, 0.17253521, 0.17024128],
             [0.20845698, 0.20422535, 0.20040214],
             [0.24183977, 0.23591548, 0.23056298]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">normalized_convolution</span></code> will no longer work without being transformed by <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> or <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>, because <code class="docutils literal notranslate"><span class="pre">jax.lax.psum</span></code> expects there to be a named axis (<code class="docutils literal notranslate"><span class="pre">'p'</span></code>, in this case), and those two transformations are the only way to bind one.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Nesting jax.pmap and jax.vmap"></a><section id="nesting-jax-pmap-and-jax-vmap">
<h2>Nesting <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code><a class="headerlink" href="#nesting-jax-pmap-and-jax-vmap" title="Permalink to this headline">#</a></h2>
<p>The reason we specify <code class="docutils literal notranslate"><span class="pre">axis_name</span></code> as a string is so we can use collective operations when nesting <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'i'</span><span class="p">),</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'j'</span><span class="p">)</span>
</pre></div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">jax.lax.psum(...,</span> <span class="pre">axis_name='i')</span></code> in <code class="docutils literal notranslate"><span class="pre">f</span></code> would refer only to the pmapped axis, since they share the <code class="docutils literal notranslate"><span class="pre">axis_name</span></code>.</p>
<p>In general, <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code> can be nested in any order, and with themselves (so you can have a <code class="docutils literal notranslate"><span class="pre">pmap</span></code> within another <code class="docutils literal notranslate"><span class="pre">pmap</span></code>, for instance).</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Example"></a><section id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">#</a></h2>
<p>Hereâ€™s an example of a regression training loop with data parallelism, where each batch is split into sub-batches which are evaluated on separate devices.</p>
<p>There are two places to pay attention to:</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">update()</span></code> function</p></li>
<li><p>the replication of parameters and splitting of data across devices.</p></li>
</ul>
<p>If this example is too confusing, you can find the same example, but without parallelism, in the next notebook, <a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/07-state.ipynb">State in JAX</a>. Once that example makes sense, you can compare the differences to understand how parallelism changes the picture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="k">class</span> <span class="nc">Params</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">weight</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
  <span class="n">bias</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>


<span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Params</span><span class="p">:</span>
  <span class="sd">"""Returns the initial model params."""</span>
  <span class="n">weights_key</span><span class="p">,</span> <span class="n">bias_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
  <span class="n">weight</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">weights_key</span><span class="p">,</span> <span class="p">())</span>
  <span class="n">bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">bias_key</span><span class="p">,</span> <span class="p">())</span>
  <span class="k">return</span> <span class="n">Params</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
  <span class="sd">"""Computes the least squares error of the model's predictions on x against y."""</span>
  <span class="n">pred</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">xs</span> <span class="o">+</span> <span class="n">params</span><span class="o">.</span><span class="n">bias</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">ys</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.005</span>

<span class="c1"># So far, the code is identical to the single-device case. Here's what's new:</span>


<span class="c1"># Remember that the `axis_name` is just an arbitrary string label used</span>
<span class="c1"># to later tell `jax.lax.pmean` which axis to reduce over. Here, we call it</span>
<span class="c1"># 'num_devices', but could have used anything, so long as `pmean` used the same.</span>
<span class="nd">@functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'num_devices'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span> <span class="n">xs</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Params</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
  <span class="sd">"""Performs one SGD update step on params using the given data."""</span>

  <span class="c1"># Compute the gradients on the given minibatch (individually on each device).</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>

  <span class="c1"># Combine the gradient across all devices (by taking their mean).</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">pmean</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'num_devices'</span><span class="p">)</span>

  <span class="c1"># Also combine the loss. Unnecessary for the update, but useful for logging.</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">pmean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="s1">'num_devices'</span><span class="p">)</span>

  <span class="c1"># Each device performs its own update, but since we start with the same params</span>
  <span class="c1"># and synchronise gradients, the params stay in sync.</span>
  <span class="n">new_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">param</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">param</span> <span class="o">-</span> <span class="n">g</span> <span class="o">*</span> <span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Hereâ€™s how <code class="docutils literal notranslate"><span class="pre">update()</span></code> works:</p>
<p>Undecorated and without the <code class="docutils literal notranslate"><span class="pre">pmean</span></code>s, <code class="docutils literal notranslate"><span class="pre">update()</span></code> takes data tensors of shape <code class="docutils literal notranslate"><span class="pre">[batch,</span> <span class="pre">...]</span></code>, computes the loss function on that batch and evaluates its gradients.</p>
<p>We want to spread the <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension across all available devices. To do that, we add a new axis using <code class="docutils literal notranslate"><span class="pre">pmap</span></code>. The arguments to the decorated <code class="docutils literal notranslate"><span class="pre">update()</span></code> thus need to have shape <code class="docutils literal notranslate"><span class="pre">[num_devices,</span> <span class="pre">batch_per_device,</span> <span class="pre">...]</span></code>. So, to call the new <code class="docutils literal notranslate"><span class="pre">update()</span></code>, weâ€™ll need to reshape data batches so that what used to be <code class="docutils literal notranslate"><span class="pre">batch</span></code> is reshaped to <code class="docutils literal notranslate"><span class="pre">[num_devices,</span> <span class="pre">batch_per_device]</span></code>. Thatâ€™s what <code class="docutils literal notranslate"><span class="pre">split()</span></code> does below. Additionally, weâ€™ll need to replicate our model parameters, adding the <code class="docutils literal notranslate"><span class="pre">num_devices</span></code> axis. This reshaping is how a pmapped function knows which devices to send which data.</p>
<p>At some point during the update step, we need to combine the gradients computed by each device â€“ otherwise, the updates performed by each device would be different. Thatâ€™s why we use <code class="docutils literal notranslate"><span class="pre">jax.lax.pmean</span></code> to compute the mean across the <code class="docutils literal notranslate"><span class="pre">num_devices</span></code> axis, giving us the average gradient of the batch. That average gradient is what we use to compute the update.</p>
<p>Aside on naming: here, we use <code class="docutils literal notranslate"><span class="pre">num_devices</span></code> for the <code class="docutils literal notranslate"><span class="pre">axis_name</span></code> for didactic clarity while introducing <code class="docutils literal notranslate"><span class="pre">jax.pmap</span></code>. However, in some sense that is tautologous: any axis introduced by a pmap will represent a number of devices. Therefore, itâ€™s common to see the axis be named something semantically meaningful, like <code class="docutils literal notranslate"><span class="pre">batch</span></code>, <code class="docutils literal notranslate"><span class="pre">data</span></code> (signifying data parallelism) or <code class="docutils literal notranslate"><span class="pre">model</span></code> (signifying model parallelism).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate true data from y = w*x + b + noise</span>
<span class="n">true_w</span><span class="p">,</span> <span class="n">true_b</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">xs</span> <span class="o">*</span> <span class="n">true_w</span> <span class="o">+</span> <span class="n">true_b</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1"># Initialise parameters and replicate across devices.</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">init</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">))</span>
<span class="n">n_devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>
<span class="n">replicated_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_devices</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So far, weâ€™ve just constructed arrays with an additional leading dimension. The params are all still all on the host (CPU). <code class="docutils literal notranslate"><span class="pre">pmap</span></code> will communicate them to the devices when <code class="docutils literal notranslate"><span class="pre">update()</span></code> is first called, and each copy will stay on its own device subsequently. You can tell because they are a DeviceArray, not a ShardedDeviceArray:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">replicated_params</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>jax.interpreters.xla._DeviceArray
</pre></div>
</div>
</div>
</div>
<p>The params will become a ShardedDeviceArray when they are returned by our pmapped <code class="docutils literal notranslate"><span class="pre">update()</span></code> (see further down).</p>
<p>We do the same to the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
  <span class="sd">"""Splits the first axis of `arr` evenly across the number of devices."""</span>
  <span class="k">return</span> <span class="n">arr</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_devices</span><span class="p">,</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">n_devices</span><span class="p">,</span> <span class="o">*</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="c1"># Reshape xs and ys for the pmapped `update()`.</span>
<span class="n">x_split</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">y_split</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="nb">type</span><span class="p">(</span><span class="n">x_split</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>numpy.ndarray
</pre></div>
</div>
</div>
</div>
<p>The data is just a reshaped vanilla NumPy array. Hence, it cannot be anywhere but on the host, as NumPy runs on CPU only. Since we never modify it, it will get sent to the device at each <code class="docutils literal notranslate"><span class="pre">update</span></code> call, like in a real pipeline where data is typically streamed from CPU to the device at each step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">type_after_update</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"after first `update()`, `</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">` is a"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>

<span class="c1"># Actual training loop.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>

  <span class="c1"># This is where the params and data gets communicated to devices:</span>
  <span class="n">replicated_params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">replicated_params</span><span class="p">,</span> <span class="n">x_split</span><span class="p">,</span> <span class="n">y_split</span><span class="p">)</span>

  <span class="c1"># The returned `replicated_params` and `loss` are now both ShardedDeviceArrays,</span>
  <span class="c1"># indicating that they're on the devices.</span>
  <span class="c1"># `x_split`, of course, remains a NumPy array on the host.</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">type_after_update</span><span class="p">(</span><span class="s1">'replicated_params.weight'</span><span class="p">,</span> <span class="n">replicated_params</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">type_after_update</span><span class="p">(</span><span class="s1">'loss'</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">type_after_update</span><span class="p">(</span><span class="s1">'x_split'</span><span class="p">,</span> <span class="n">x_split</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Note that loss is actually an array of shape [num_devices], with identical</span>
    <span class="c1"># entries, because each device returns its copy of the loss.</span>
    <span class="c1"># So, we take the first element to print it.</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">3d</span><span class="si">}</span><span class="s2">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="c1"># Plot results.</span>

<span class="c1"># Like the loss, the leaves of params have an extra leading dimension,</span>
<span class="c1"># so we take the params from the first device.</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_get</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replicated_params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>after first `update()`, `replicated_params.weight` is a &lt;class 'jax.interpreters.pxla.ShardedDeviceArray'&gt;
after first `update()`, `loss` is a &lt;class 'jax.interpreters.pxla.ShardedDeviceArray'&gt;
after first `update()`, `x_split` is a &lt;class 'numpy.ndarray'&gt;
Step   0, loss: 0.228
Step 100, loss: 0.228
Step 200, loss: 0.228
Step 300, loss: 0.228
Step 400, loss: 0.228
Step 500, loss: 0.228
Step 600, loss: 0.228
Step 700, loss: 0.228
Step 800, loss: 0.228
Step 900, loss: 0.228
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">xs</span> <span class="o">+</span> <span class="n">params</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Model Prediction'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b996a59f03b6f21077a804669190e4d15550c44e4362fc476d23305d2bdd6512.png" src="../_images/b996a59f03b6f21077a804669190e4d15550c44e4362fc476d23305d2bdd6512.png"/>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Aside: hosts and devices in JAX"></a><section id="aside-hosts-and-devices-in-jax">
<h2>Aside: hosts and devices in JAX<a class="headerlink" href="#aside-hosts-and-devices-in-jax" title="Permalink to this headline">#</a></h2>
<p>When running on TPU, the idea of a â€˜hostâ€™ becomes important. A host is the CPU that manages several devices. A single host can only manage so many devices (usually 8), so when running very large parallel programs, multiple hosts are needed, and some finesse is required to manage them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[TpuDevice(id=0, host_id=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, host_id=0, coords=(0,0,0), core_on_chip=1),
 TpuDevice(id=2, host_id=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=3, host_id=0, coords=(1,0,0), core_on_chip=1),
 TpuDevice(id=4, host_id=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=5, host_id=0, coords=(0,1,0), core_on_chip=1),
 TpuDevice(id=6, host_id=0, coords=(1,1,0), core_on_chip=0),
 TpuDevice(id=7, host_id=0, coords=(1,1,0), core_on_chip=1)]
</pre></div>
</div>
</div>
</div>
<p>When running on CPU you can always emulate an arbitrary number of devices with a nifty <code class="docutils literal notranslate"><span class="pre">--xla_force_host_platform_device_count</span></code> XLA flag, e.g. by executing the following before importing JAX:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'XLA_FLAGS'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'--xla_force_host_platform_device_count=8'</span>
<span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">6</span><span class="p">),</span>
 <span class="n">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">7</span><span class="p">)]</span>
</pre></div>
</div>
<p>This is especially useful for debugging and testing locally or even for prototyping in Colab since a CPU runtime is faster to (re-)start.</p>
</section>
</section>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="05.1-pytrees.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Working with Pytrees</p>
</div>
</a>
<a class="right-next" href="07-state.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Stateful Computations in JAX</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      Â© Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>