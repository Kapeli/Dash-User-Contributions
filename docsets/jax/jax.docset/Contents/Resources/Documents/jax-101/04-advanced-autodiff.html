
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Advanced Automatic Differentiation in JAX â€” JAX  documentation</title>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" rel="stylesheet" type="text/css">
<link href="../_static/plot_directive.css" rel="stylesheet" type="text/css">
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css">
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../_static/style.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="../_static/favicon.png" rel="shortcut icon">
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="05-random-numbers.html" rel="next" title="Pseudo Random Numbers in JAX"/>
<link href="03-vectorization.html" rel="prev" title="Automatic Vectorization in JAX"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../_static/jax_logo_250px.png"/>
</a>
</div><form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search the docs ..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../installation.html">
   Installing JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/quickstart.html">
   JAX Quickstart
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/thinking_in_jax.html">
   How to Think in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Common_Gotchas_in_JAX.html">
   ðŸ”ª JAX - The Sharp Bits ðŸ”ª
  </a>
</li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="index.html">
   Tutorial: JAX 101
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="02-jitting.html">
     Just In Time Compilation with JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Advanced Automatic Differentiation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="05.1-pytrees.html">
     Working with Pytrees
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="07-state.html">
     Stateful Computations in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="08-pjit.html">
     Introduction to pjit
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../debugging/index.html">
   Runtime value debugging in JAX
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/print_breakpoint.html">
<code class="docutils literal notranslate">
<span class="pre">
       jax.debug.print
      </span>
</code>
     and
     <code class="docutils literal notranslate">
<span class="pre">
       jax.debug.breakpoint
      </span>
</code>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
<span class="pre">
       checkify
      </span>
</code>
     transformation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../debugging/flags.html">
     JAX debugging flags
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../async_dispatch.html">
   Asynchronous dispatch
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../aot.html">
   Ahead-of-time lowering and compilation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jaxpr.html">
   Understanding Jaxprs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/convolutions.html">
   Convolutions in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../pytrees.html">
   Pytrees
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../type_promotion.html">
   Type promotion semantics
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../errors.html">
   JAX Errors
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../transfer_guard.html">
   Transfer guard
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../glossary.html">
   JAX Glossary of Terms
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../changelog.html">
   Change log
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../notebooks/xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../api_compatibility.html">
   API compatibility
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../deprecation.html">
   Python and NumPy version support policy
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../concurrency.html">
   Concurrency
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../profiling.html">
   Profiling JAX programs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../device_memory_profiling.html">
   Device Memory Profiling
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../rank_promotion_warning.html">
   Rank promotion warning
  </a>
</li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../contributing.html">
   Contributing to JAX
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../developer.html">
   Building from source
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../jax_internal_api.html">
   Internal APIs
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
</li>
</ul>
</input></li>
</ul>
<p aria-level="2" class="caption" role="heading">
<span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../jax.html">
   Public API: jax package
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../jax.numpy.html">
     jax.numpy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.scipy.html">
     jax.scipy package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.config.html">
     JAX configuration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.debug.html">
     jax.debug package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.dlpack.html">
     jax.dlpack module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.distributed.html">
     jax.distributed module
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.example_libraries.html">
     jax.example_libraries package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.experimental.html">
     jax.experimental package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.flatten_util.html">
     jax.flatten_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.image.html">
     jax.image package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lax.html">
     jax.lax package
    </a>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../jax.nn.html">
     jax.nn package
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.ops.html">
     jax.ops package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.profiler.html">
     jax.profiler module
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.random.html">
     jax.random package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.stages.html">
     jax.stages package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.tree_util.html">
     jax.tree_util package
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../jax.lib.html">
     jax.lib package
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<a class="headerbtn" data-placement="bottom" data-toggle="tooltip" href="https://github.com/google/jax" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
</a>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../_sources/jax-101/04-advanced-autodiff.ipynb.txt" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#higher-order-derivatives">
   Higher-order derivatives
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#higher-order-optimization">
   Higher order optimization
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#stopping-gradients">
   Stopping gradients
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#straight-through-estimator-using-stop-gradient">
   Straight-through estimator using
   <code class="docutils literal notranslate">
<span class="pre">
     stop_gradient
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#per-example-gradients">
   Per-example gradients
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Advanced Automatic Differentiation in JAX</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#higher-order-derivatives">
   Higher-order derivatives
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#higher-order-optimization">
   Higher order optimization
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#stopping-gradients">
   Stopping gradients
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#straight-through-estimator-using-stop-gradient">
   Straight-through estimator using
   <code class="docutils literal notranslate">
<span class="pre">
     stop_gradient
    </span>
</code>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#per-example-gradients">
   Per-example gradients
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Advanced Automatic Differentiation in JAX"></a><section class="tex2jax_ignore mathjax_ignore" id="advanced-automatic-differentiation-in-jax">
<h1>Advanced Automatic Differentiation in JAX<a class="headerlink" href="#advanced-automatic-differentiation-in-jax" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/04-advanced-autodiff.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p><em>Authors: Vlatimir Mikulik &amp; Matteo Hessel</em></p>
<p>Computing gradients is a critical part of modern machine learning methods. This section considers a few advanced topics in the areas of automatic differentiation as it relates to modern machine learning.</p>
<p>While understanding how automatic differentiation works under the hood isnâ€™t crucial for using JAX in most contexts, we encourage the reader to check out this quite accessible <a class="reference external" href="https://www.youtube.com/watch?v=wG_nF1awSSY">video</a>  to get a deeper sense of whatâ€™s going on.</p>
<p><a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a> is a more advanced and more detailed explanation of how these ideas are implemented in the JAX backend. Itâ€™s not necessary to understand this to do most things in JAX. However, some features (like defining <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html">custom derivatives</a>) depend on understanding this, so itâ€™s worth knowing this explanation exists if you ever need to use them.</p>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Higher-order derivatives"></a><section id="higher-order-derivatives">
<h2>Higher-order derivatives<a class="headerlink" href="#higher-order-derivatives" title="Permalink to this headline">#</a></h2>
<p>JAXâ€™s autodiff makes it easy to compute higher-order derivatives, because the functions that compute derivatives are themselves differentiable. Thus, higher-order derivatives are as easy as stacking transformations.</p>
<p>We illustrate this in the single-variable case:</p>
<p>The derivative of <span class="math notranslate nohighlight">\(f(x) = x^3 + 2x^2 - 3x + 1\)</span> can be computed as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>

<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">dfdx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The higher-order derivatives of <span class="math notranslate nohighlight">\(f\)</span> are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l}
f'(x) = 3x^2 + 4x -3\\
f''(x) = 6x + 4\\
f'''(x) = 6\\
f^{iv}(x) = 0
\end{array}
\end{split}\]</div>
<p>Computing any of these in JAX is as easy as chaining the <code class="docutils literal notranslate"><span class="pre">grad</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2fdx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">dfdx</span><span class="p">)</span>
<span class="n">d3fdx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">d2fdx</span><span class="p">)</span>
<span class="n">d4fdx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">d3fdx</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Evaluating the above in <span class="math notranslate nohighlight">\(x=1\)</span> would give us:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l}
f'(1) = 4\\
f''(1) = 10\\
f'''(1) = 6\\
f^{iv}(1) = 0
\end{array}
\end{split}\]</div>
<p>Using JAX:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dfdx</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d2fdx</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d3fdx</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d4fdx</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4.0
10.0
6.0
0.0
</pre></div>
</div>
</div>
</div>
<p>In the multivariable case, higher-order derivatives are more complicated. The second-order derivative of a function is represented by its <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>, defined according to</p>
<div class="math notranslate nohighlight">
\[(\mathbf{H}f)_{i,j} = \frac{\partial^2 f}{\partial_i\partial_j}.\]</div>
<p>The Hessian of a real-valued function of several variables, <span class="math notranslate nohighlight">\(f: \mathbb R^n\to\mathbb R\)</span>, can be identified with the Jacobian of its gradient. JAX provides two transformations for computing the Jacobian of a function, <code class="docutils literal notranslate"><span class="pre">jax.jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.jacrev</span></code>, corresponding to forward- and reverse-mode autodiff. They give the same answer, but one can be more efficient than the other in different circumstances â€“ see the <a class="reference external" href="https://www.youtube.com/watch?v=wG_nF1awSSY">video about autodiff</a> linked above for an explanation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Letâ€™s double check this is correct on the dot-product <span class="math notranslate nohighlight">\(f: \mathbf{x} \mapsto \mathbf{x} ^\top \mathbf{x}\)</span>.</p>
<p>if <span class="math notranslate nohighlight">\(i=j\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial_i\partial_j}(\mathbf{x}) = 2\)</span>. Otherwise, <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial_i\partial_j}(\mathbf{x}) = 0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[2., 0., 0.],
             [0., 2., 0.],
             [0., 0., 2.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Often, however, we arenâ€™t interested in computing the full Hessian itself, and doing so can be very inefficient. <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a> explains some tricks, like the Hessian-vector product, that allow to use it without materialising the whole matrix.</p>
<p>If you plan to work with higher-order derivatives in JAX, we strongly recommend reading the Autodiff Cookbook.</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Higher order optimization"></a><section id="higher-order-optimization">
<h2>Higher order optimization<a class="headerlink" href="#higher-order-optimization" title="Permalink to this headline">#</a></h2>
<p>Some meta-learning techniques, such as Model-Agnostic Meta-Learning (<a class="reference external" href="https://arxiv.org/abs/1703.03400">MAML</a>), require differentiating through gradient updates. In other frameworks this can be quite cumbersome, but in JAX itâ€™s much easier:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">meta_loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
  <span class="sd">"""Computes the loss after one step of SGD."""</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">params</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="n">meta_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">meta_loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Stopping gradients"></a><section id="stopping-gradients">
<h2>Stopping gradients<a class="headerlink" href="#stopping-gradients" title="Permalink to this headline">#</a></h2>
<p>Auto-diff enables automatic computation of the gradient of a function with respect to its inputs. Sometimes, however, we might want some additional control: for instance, we might want to avoid back-propagating gradients through some subset of the computational graph.</p>
<p>Consider for instance the TD(0) (<a class="reference external" href="https://en.wikipedia.org/wiki/Temporal_difference_learning">temporal difference</a>) reinforcement learning update. This is used to learn to estimate the <em>value</em> of a state in an environment from experience of interacting with the environment. Letâ€™s assume the value estimate <span class="math notranslate nohighlight">\(v_{\theta}(s_{t-1}\)</span>) in a state <span class="math notranslate nohighlight">\(s_{t-1}\)</span> is parameterised by a linear function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Value function and initial parameters</span>
<span class="n">value_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Consider a transition from a state <span class="math notranslate nohighlight">\(s_{t-1}\)</span> to a state <span class="math notranslate nohighlight">\(s_t\)</span> during which we observed the reward <span class="math notranslate nohighlight">\(r_t\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># An example transition.</span>
<span class="n">s_tm1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">r_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">s_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The TD(0) update to the network parameters is:</p>
<div class="math notranslate nohighlight">
\[
\Delta \theta = (r_t + v_{\theta}(s_t) - v_{\theta}(s_{t-1})) \nabla v_{\theta}(s_{t-1})
\]</div>
<p>This update is not the gradient of any loss function.</p>
<p>However, it can be <strong>written</strong> as the gradient of the pseudo loss function</p>
<div class="math notranslate nohighlight">
\[
L(\theta) = [r_t + v_{\theta}(s_t) - v_{\theta}(s_{t-1})]^2
\]</div>
<p>if the dependency of the target <span class="math notranslate nohighlight">\(r_t + v_{\theta}(s_t)\)</span> on the parameter <span class="math notranslate nohighlight">\(\theta\)</span> is ignored.</p>
<p>How can we implement this in JAX? If we write the pseudo loss naively we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">td_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
  <span class="n">v_tm1</span> <span class="o">=</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">r_t</span> <span class="o">+</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v_tm1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">td_update</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">td_loss</span><span class="p">)</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">td_update</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>

<span class="n">delta_theta</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([ 2.4, -2.4,  2.4], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>But <code class="docutils literal notranslate"><span class="pre">td_update</span></code> will <strong>not</strong> compute a TD(0) update, because the gradient computation will include the dependency of <code class="docutils literal notranslate"><span class="pre">target</span></code> on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>We can use <code class="docutils literal notranslate"><span class="pre">jax.lax.stop_gradient</span></code> to force JAX to ignore the dependency of the target on <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">td_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">):</span>
  <span class="n">v_tm1</span> <span class="o">=</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">r_t</span> <span class="o">+</span> <span class="n">value_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">v_tm1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">td_update</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">td_loss</span><span class="p">)</span>
<span class="n">delta_theta</span> <span class="o">=</span> <span class="n">td_update</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>

<span class="n">delta_theta</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([-2.4, -4.8,  2.4], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>This will treat <code class="docutils literal notranslate"><span class="pre">target</span></code> as if it did <strong>not</strong> depend on the parameters <span class="math notranslate nohighlight">\(\theta\)</span> and compute the correct update to the parameters.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">jax.lax.stop_gradient</span></code> may also be useful in other settings, for instance if you want the gradient from some loss to only affect a subset of the parameters of the neural network (because, for instance, the other parameters are trained using a different loss).</p>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Straight-through estimator using stop_gradient"></a><section id="straight-through-estimator-using-stop-gradient">
<h2>Straight-through estimator using <code class="docutils literal notranslate"><span class="pre">stop_gradient</span></code><a class="headerlink" href="#straight-through-estimator-using-stop-gradient" title="Permalink to this headline">#</a></h2>
<p>The straight-through estimator is a trick for defining a â€˜gradientâ€™ of a function that is otherwise non-differentiable. Given a non-differentiable function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^n\)</span> that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that <span class="math notranslate nohighlight">\(f\)</span> is the identity function. This can be implemented neatly using <code class="docutils literal notranslate"><span class="pre">jax.lax.stop_gradient</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># non-differentiable</span>

<span class="k">def</span> <span class="nf">straight_through_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="c1"># Create an exactly-zero expression with Sterbenz lemma that has</span>
  <span class="c1"># an exactly-one gradient.</span>
  <span class="n">zero</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">zero</span> <span class="o">+</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"f(x): "</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="mf">3.2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"straight_through_f(x):"</span><span class="p">,</span> <span class="n">straight_through_f</span><span class="p">(</span><span class="mf">3.2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"grad(f)(x):"</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="mf">3.2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"grad(straight_through_f)(x):"</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">straight_through_f</span><span class="p">)(</span><span class="mf">3.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>f(x):  3.0
straight_through_f(x): 3.0
grad(f)(x): 0.0
grad(straight_through_f)(x): 1.0
</pre></div>
</div>
</div>
</div>
</section>
<a class="dashAnchor" name="//apple_ref/cpp/Section/Per-example gradients"></a><section id="per-example-gradients">
<h2>Per-example gradients<a class="headerlink" href="#per-example-gradients" title="Permalink to this headline">#</a></h2>
<p>While most ML systems compute gradients and updates from batches of data, for reasons of computational efficiency and/or variance reduction, it is sometimes necessary to have access to the gradient/update associated with each specific sample in the batch.</p>
<p>For instance, this is needed to prioritise data based on gradient magnitude, or to apply clipping / normalisations on a sample by sample basis.</p>
<p>In many frameworks (PyTorch, TF, Theano) it is often not trivial to compute per-example gradients, because the library directly accumulates the gradient over the batch. Naive workarounds, such as computing a separate loss per example and then aggregating the resulting gradients are typically very inefficient.</p>
<p>In JAX we can define the code to compute the gradient per-sample in an easy but efficient way.</p>
<p>Just combine the <code class="docutils literal notranslate"><span class="pre">jit</span></code>, <code class="docutils literal notranslate"><span class="pre">vmap</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code> transformations together:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perex_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">td_loss</span><span class="p">),</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

<span class="c1"># Test it:</span>
<span class="n">batched_s_tm1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s_tm1</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">])</span>
<span class="n">batched_r_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">r_t</span><span class="p">,</span> <span class="n">r_t</span><span class="p">])</span>
<span class="n">batched_s_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">s_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">])</span>

<span class="n">perex_grads</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">batched_s_tm1</span><span class="p">,</span> <span class="n">batched_r_t</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[-2.4, -4.8,  2.4],
             [-2.4, -4.8,  2.4]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Letâ€™s walk through this one transformation at a time.</p>
<p>First, we apply <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> to <code class="docutils literal notranslate"><span class="pre">td_loss</span></code> to obtain a function that computes the gradient of the loss w.r.t. the parameters on single (unbatched) inputs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dtdloss_dtheta</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">td_loss</span><span class="p">)</span>

<span class="n">dtdloss_dtheta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">s_tm1</span><span class="p">,</span> <span class="n">r_t</span><span class="p">,</span> <span class="n">s_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([-2.4, -4.8,  2.4], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>This function computes one row of the array above.</p>
<p>Then, we vectorise this function using <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>. This adds a batch dimension to all inputs and outputs. Now, given a batch of inputs, we produce a batch of outputs â€“ each output in the batch corresponds to the gradient for the corresponding member of the input batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">almost_perex_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">dtdloss_dtheta</span><span class="p">)</span>

<span class="n">batched_theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">theta</span><span class="p">,</span> <span class="n">theta</span><span class="p">])</span>
<span class="n">almost_perex_grads</span><span class="p">(</span><span class="n">batched_theta</span><span class="p">,</span> <span class="n">batched_s_tm1</span><span class="p">,</span> <span class="n">batched_r_t</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[-2.4, -4.8,  2.4],
             [-2.4, -4.8,  2.4]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>This isnâ€™t quite what we want, because we have to manually feed this function a batch of <code class="docutils literal notranslate"><span class="pre">theta</span></code>s, whereas we actually want to use a single <code class="docutils literal notranslate"><span class="pre">theta</span></code>. We fix this by adding <code class="docutils literal notranslate"><span class="pre">in_axes</span></code> to the <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>, specifying theta as <code class="docutils literal notranslate"><span class="pre">None</span></code>, and the other args as <code class="docutils literal notranslate"><span class="pre">0</span></code>. This makes the resulting function add an extra axis only to the other arguments, leaving <code class="docutils literal notranslate"><span class="pre">theta</span></code> unbatched, as we want:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inefficient_perex_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">dtdloss_dtheta</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="n">inefficient_perex_grads</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">batched_s_tm1</span><span class="p">,</span> <span class="n">batched_r_t</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[-2.4, -4.8,  2.4],
             [-2.4, -4.8,  2.4]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Almost there! This does what we want, but is slower than it has to be. Now, we wrap the whole thing in a <code class="docutils literal notranslate"><span class="pre">jax.jit</span></code> to get the compiled, efficient version of the same function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perex_grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">inefficient_perex_grads</span><span class="p">)</span>

<span class="n">perex_grads</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">batched_s_tm1</span><span class="p">,</span> <span class="n">batched_r_t</span><span class="p">,</span> <span class="n">batched_s_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DeviceArray([[-2.4, -4.8,  2.4],
             [-2.4, -4.8,  2.4]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()
<span class="o">%</span><span class="k">timeit</span> perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100 loops, best of 5: 7.74 ms per loop
10000 loops, best of 5: 86.2 Âµs per loop
</pre></div>
</div>
</div>
</div>
</section>
</section>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="03-vectorization.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Automatic Vectorization in JAX</p>
</div>
</a>
<a class="right-next" href="05-random-numbers.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Pseudo Random Numbers in JAX</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      Â© Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>