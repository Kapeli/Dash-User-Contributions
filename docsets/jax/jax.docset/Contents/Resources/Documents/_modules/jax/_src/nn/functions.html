
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>jax._src.nn.functions &#8212; JAX  documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" href="../../../../_static/style.css" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="shortcut icon" href="../../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/jax_logo_250px.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../installation.html">
   Installing JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/quickstart.html">
   JAX Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/thinking_in_jax.html">
   How to Think in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/Common_Gotchas_in_JAX.html">
   ðŸ”ª JAX - The Sharp Bits ðŸ”ª
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../jax-101/index.html">
   Tutorial: JAX 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/02-jitting.html">
     Just In Time Compilation with JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/05.1-pytrees.html">
     Working with Pytrees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/07-state.html">
     Stateful Computations in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax-101/08-pjit.html">
     Introduction to pjit
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../debugging/index.html">
   Runtime value debugging in JAX
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../debugging/print_breakpoint.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.print
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.breakpoint
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
      <span class="pre">
       checkify
      </span>
     </code>
     transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../debugging/flags.html">
     JAX debugging flags
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../async_dispatch.html">
   Asynchronous dispatch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../aot.html">
   Ahead-of-time lowering and compilation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../jaxpr.html">
   Understanding Jaxprs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/convolutions.html">
   Convolutions in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../pytrees.html">
   Pytrees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../type_promotion.html">
   Type promotion semantics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../errors.html">
   JAX Errors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../transfer_guard.html">
   Transfer guard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../glossary.html">
   JAX Glossary of Terms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../changelog.html">
   Change log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../notebooks/xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../api_compatibility.html">
   API compatibility
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../deprecation.html">
   Python and NumPy version support policy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../concurrency.html">
   Concurrency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../profiling.html">
   Profiling JAX programs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../device_memory_profiling.html">
   Device Memory Profiling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../rank_promotion_warning.html">
   Rank promotion warning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../contributing.html">
   Contributing to JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../developer.html">
   Building from source
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../jax_internal_api.html">
   Internal APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../jax.html">
   Public API: jax package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.numpy.html">
     jax.numpy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.scipy.html">
     jax.scipy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.config.html">
     JAX configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.debug.html">
     jax.debug package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.dlpack.html">
     jax.dlpack module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.distributed.html">
     jax.distributed module
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../jax.example_libraries.html">
     jax.example_libraries package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../jax.experimental.html">
     jax.experimental package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.flatten_util.html">
     jax.flatten_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.image.html">
     jax.image package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.lax.html">
     jax.lax package
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../jax.nn.html">
     jax.nn package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.ops.html">
     jax.ops package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.profiler.html">
     jax.profiler module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.random.html">
     jax.random package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.stages.html">
     jax.stages package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.tree_util.html">
     jax.tree_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../jax.lib.html">
     jax.lib package
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/google/jax"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for jax._src.nn.functions</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2019 The JAX Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Shared neural network activations and other functions.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">custom_jvp</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">AxisName</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">jax.scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">from</span> <span class="nn">jax.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span> <span class="k">as</span> <span class="n">_logsumexp</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">Array</span> <span class="o">=</span> <span class="n">Any</span>

<span class="c1"># activations</span>

<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.relu.html#jax.nn.relu">[docs]</a><span class="nd">@custom_jvp</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rectified linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{relu}(x) = \max(x, 0)</span>

<span class="sd">  except under differentiation, we take:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \nabla \mathrm{relu}(0) = 0</span>

<span class="sd">  For more information see</span>
<span class="sd">  `Numerical influence of ReLUâ€™(0) on backpropagation</span>
<span class="sd">  &lt;https://openreview.net/forum?id=urrcVI-_jRm&gt;`_.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>
<span class="c1"># For behavior at 0, see https://openreview.net/forum?id=urrcVI-_jRm</span>
<span class="n">relu</span><span class="o">.</span><span class="n">defjvps</span><span class="p">(</span><span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">ans</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">lax</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

<div class="viewcode-block" id="softplus"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.softplus.html#jax.nn.softplus">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Softplus activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{softplus}(x) = \log(1 + e^x)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="soft_sign"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.soft_sign.html#jax.nn.soft_sign">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">soft_sign</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Soft-sign activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{soft\_sign}(x) = \frac{x}{|x| + 1}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.sigmoid.html#jax.nn.sigmoid">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">lax</span><span class="o">.</span><span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="silu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.silu.html#jax.nn.silu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SiLU activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{silu}(x) = x \cdot \mathrm{sigmoid}(x) = \frac{x}{1 + e^{-x}}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<span class="n">swish</span> <span class="o">=</span> <span class="n">silu</span>

<div class="viewcode-block" id="log_sigmoid"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.log_sigmoid.html#jax.nn.log_sigmoid">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Log-sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{log\_sigmoid}(x) = \log(\mathrm{sigmoid}(x)) = -\log(1 + e^{-x})</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.elu.html#jax.nn.elu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Exponential linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{elu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha \left(\exp(x) - 1\right), &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    alpha : scalar or array of alpha values (default: 1.0)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">safe_x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">safe_x</span><span class="p">))</span></div>

<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.leaky_relu.html#jax.nn.leaky_relu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Leaky rectified linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{leaky\_relu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x \ge 0\\</span>
<span class="sd">      \alpha x, &amp; x &lt; 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  where :math:`\alpha` = :code:`negative_slope`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    negative_slope : array or scalar specifying the negative slope (default: 0.01)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">negative_slope</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="hard_tanh"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.hard_tanh.html#jax.nn.hard_tanh">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">hard_tanh</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard :math:`\mathrm{tanh}` activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_tanh}(x) = \begin{cases}</span>
<span class="sd">      -1, &amp; x &lt; -1\\</span>
<span class="sd">      x, &amp; -1 \le x \le 1\\</span>
<span class="sd">      1, &amp; 1 &lt; x</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span></div>

<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.celu.html#jax.nn.celu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Continuously-differentiable exponential linear unit activation.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{celu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha \left(\exp(\frac{x}{\alpha}) - 1\right), &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  For more information, see</span>
<span class="sd">  `Continuously Differentiable Exponential Linear Units</span>
<span class="sd">  &lt;https://arxiv.org/pdf/1704.07483.pdf&gt;`_.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    alpha : array or scalar (default: 1.0)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span></div>

<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.selu.html#jax.nn.selu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Scaled exponential linear unit activation.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{selu}(x) = \lambda \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha e^x - \alpha, &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  where :math:`\lambda = 1.0507009873554804934193349852946` and</span>
<span class="sd">  :math:`\alpha = 1.6732632423543772848170429916717`.</span>

<span class="sd">  For more information, see</span>
<span class="sd">  `Self-Normalizing Neural Networks</span>
<span class="sd">  &lt;https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf&gt;`_.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.6732632423543772848170429916717</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0507009873554804934193349852946</span>
  <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span></div>

<span class="c1"># TODO(phawkins): this jit was found to change numerics in a test. Debug this.</span>
<span class="c1"># @partial(jax.jit, static_argnames=(&quot;approximate&quot;,))</span>
<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.gelu.html#jax.nn.gelu">[docs]</a><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">approximate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gaussian error linear unit activation function.</span>

<span class="sd">  If ``approximate=False``, computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{gelu}(x) = \frac{x}{2} \left(1 + \mathrm{erf} \left(</span>
<span class="sd">      \frac{x}{\sqrt{2}} \right) \right)</span>

<span class="sd">  If ``approximate=True``, uses the approximate formulation of GELU:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{gelu}(x) = \frac{x}{2} \left(1 + \mathrm{tanh} \left(</span>
<span class="sd">      \sqrt{\frac{2}{\pi}} \left(x + 0.044715 x^3 \right) \right) \right)</span>

<span class="sd">  For more information, see `Gaussian Error Linear Units (GELUs)</span>
<span class="sd">  &lt;https://arxiv.org/abs/1606.08415&gt;`_, section 2.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    approximate: whether to use the approximate or exact formulation.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Promote to nearest float-like dtype.</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">to_inexact_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
    <span class="n">sqrt_2_over_pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">sqrt_2_over_pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">cdf</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">sqrt_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">sqrt_2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.glu.html#jax.nn.glu">[docs]</a><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Gated linear unit activation function.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis along which the split should be computed (default: -1)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
  <span class="k">assert</span> <span class="n">size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;axis size must be divisible by 2&quot;</span>
  <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span></div>

<span class="c1"># other functions</span>

<span class="n">logsumexp</span> <span class="o">=</span> <span class="n">_logsumexp</span>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.log_softmax.html#jax.nn.log_softmax">[docs]</a><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
                <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">where</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">initial</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Log-Softmax function.</span>

<span class="sd">  Computes the logarithm of the :code:`softmax` function, which rescales</span>
<span class="sd">  elements to the range :math:`[-\infty, 0)`.</span>

<span class="sd">  .. math ::</span>
<span class="sd">    \mathrm{log\_softmax}(x) = \log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>
<span class="sd">    \right)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis or axes along which the :code:`log_softmax` should be</span>
<span class="sd">      computed. Either an integer or a tuple of integers.</span>
<span class="sd">    where: Elements to include in the :code:`log_softmax`.</span>
<span class="sd">    initial: The minimum value used to shift the input array. Must be present</span>
<span class="sd">      when :code:`where` is not None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">shifted</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x_max</span><span class="p">)</span>
  <span class="n">shifted_logsumexp</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
      <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shifted</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">shifted</span> <span class="o">-</span> <span class="n">shifted_logsumexp</span></div>


<span class="c1"># TODO(phawkins): this jit was found to change numerics in a test. Debug this.</span>
<span class="c1">#@partial(jax.jit, static_argnames=(&quot;axis&quot;,))</span>
<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.softmax.html#jax.nn.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
            <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">where</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">initial</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Softmax function.</span>

<span class="sd">  Computes the function which rescales elements to the range :math:`[0, 1]`</span>
<span class="sd">  such that the elements along :code:`axis` sum to :math:`1`.</span>

<span class="sd">  .. math ::</span>
<span class="sd">    \mathrm{softmax}(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis or axes along which the softmax should be computed. The</span>
<span class="sd">      softmax output summed across these dimensions should sum to :math:`1`.</span>
<span class="sd">      Either an integer or a tuple of integers.</span>
<span class="sd">    where: Elements to include in the :code:`softmax`.</span>
<span class="sd">    initial: The minimum value used to shift the input array. Must be present</span>
<span class="sd">      when :code:`where` is not None.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">x_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="n">initial</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">unnormalized</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x_max</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">unnormalized</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unnormalized</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
              <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">mean</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">variance</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">epsilon</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
              <span class="n">where</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Normalizes an array by subtracting ``mean`` and dividing by :math:`\sqrt{\mathrm{variance}}`.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># this definition is traditionally seen as less accurate than jnp.var&#39;s</span>
    <span class="c1"># mean((x - mean(x))**2) but may be faster and even, given typical</span>
    <span class="c1"># activation distributions and low-precision arithmetic, more accurate</span>
    <span class="c1"># when used in neural network normalization layers</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="n">where</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.normalize.html#jax.nn.normalize">[docs]</a><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
              <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">mean</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">variance</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">epsilon</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
              <span class="n">where</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Normalizes an array by subtracting ``mean`` and dividing by :math:`\sqrt{\mathrm{variance}}`.&quot;&quot;&quot;</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;jax.nn.normalize will be deprecated. Use jax.nn.standardize instead.&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">standardize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">where</span><span class="p">)</span></div>

<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;num_classes&quot;</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">_one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
             <span class="n">dtype</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">AxisName</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="n">num_classes</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">concrete_or_error</span><span class="p">(</span>
      <span class="nb">int</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
      <span class="s2">&quot;The error arose in jax.nn.one_hot argument `num_classes`.&quot;</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">output_pos_axis</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="n">axis_size</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">psum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_classes</span> <span class="o">!=</span> <span class="n">axis_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected num_classes to match the size of axis </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">, &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;but </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
    <span class="n">axis_idx</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">axis_index</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">axis_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">operator</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
  <span class="n">lhs</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
  <span class="n">rhs_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">rhs_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">output_pos_axis</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
  <span class="n">rhs</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">broadcasted_iota</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">rhs_shape</span><span class="p">,</span> <span class="n">output_pos_axis</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">lhs</span> <span class="o">==</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.one_hot.html#jax.nn.one_hot">[docs]</a><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float_</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;One-hot encodes the given indicies.</span>

<span class="sd">  Each index in the input ``x`` is encoded as a vector of zeros of length</span>
<span class="sd">  ``num_classes`` with the element at ``index`` set to one::</span>

<span class="sd">    &gt;&gt;&gt; jax.nn.one_hot(jnp.array([0, 1, 2]), 3)</span>
<span class="sd">    DeviceArray([[1., 0., 0.],</span>
<span class="sd">                  [0., 1., 0.],</span>
<span class="sd">                  [0., 0., 1.]], dtype=float32)</span>

<span class="sd">  Indicies outside the range [0, num_classes) will be encoded as zeros::</span>

<span class="sd">    &gt;&gt;&gt; jax.nn.one_hot(jnp.array([-1, 3]), 3)</span>
<span class="sd">    DeviceArray([[0., 0., 0.],</span>
<span class="sd">                 [0., 0., 0.]], dtype=float32)</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A tensor of indices.</span>
<span class="sd">    num_classes: Number of classes in the one-hot dimension.</span>
<span class="sd">    dtype: optional, a float dtype for the returned values (default :obj:`jnp.float_`).</span>
<span class="sd">    axis: the axis or axes along which the function should be</span>
<span class="sd">      computed.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_classes</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">concrete_or_error</span><span class="p">(</span>
      <span class="nb">int</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
      <span class="s2">&quot;The error arose in jax.nn.one_hot argument `num_classes`.&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.relu6.html#jax.nn.relu6">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rectified Linear Unit 6 activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{relu6}(x) = \min(\max(x, 0), 6)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mf">6.</span><span class="p">)</span></div>

<div class="viewcode-block" id="hard_sigmoid"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.hard_sigmoid.html#jax.nn.hard_sigmoid">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">hard_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard Sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_sigmoid}(x) = \frac{\mathrm{relu6}(x + 3)}{6}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">relu6</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">3.</span><span class="p">)</span> <span class="o">/</span> <span class="mf">6.</span></div>

<div class="viewcode-block" id="hard_silu"><a class="viewcode-back" href="../../../../_autosummary/jax.nn.hard_silu.html#jax.nn.hard_silu">[docs]</a><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">hard_silu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard SiLU activation function</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_silu}(x) = x \cdot \mathrm{hard\_sigmoid}(x)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">hard_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<span class="n">hard_swish</span> <span class="o">=</span> <span class="n">hard_silu</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      &copy; Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>