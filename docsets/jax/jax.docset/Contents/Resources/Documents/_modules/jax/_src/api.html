
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>jax._src.api &#8212; JAX  documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" href="../../../_static/style.css" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/jax_logo_250px.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../installation.html">
   Installing JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/quickstart.html">
   JAX Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/thinking_in_jax.html">
   How to Think in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Common_Gotchas_in_JAX.html">
   ðŸ”ª JAX - The Sharp Bits ðŸ”ª
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jax-101/index.html">
   Tutorial: JAX 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/02-jitting.html">
     Just In Time Compilation with JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/05.1-pytrees.html">
     Working with Pytrees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/07-state.html">
     Stateful Computations in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/08-pjit.html">
     Introduction to pjit
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../debugging/index.html">
   Runtime value debugging in JAX
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/print_breakpoint.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.print
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.breakpoint
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
      <span class="pre">
       checkify
      </span>
     </code>
     transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/flags.html">
     JAX debugging flags
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../async_dispatch.html">
   Asynchronous dispatch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../aot.html">
   Ahead-of-time lowering and compilation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../jaxpr.html">
   Understanding Jaxprs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/convolutions.html">
   Convolutions in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../pytrees.html">
   Pytrees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../type_promotion.html">
   Type promotion semantics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../errors.html">
   JAX Errors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../transfer_guard.html">
   Transfer guard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../glossary.html">
   JAX Glossary of Terms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../changelog.html">
   Change log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../api_compatibility.html">
   API compatibility
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deprecation.html">
   Python and NumPy version support policy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../concurrency.html">
   Concurrency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../profiling.html">
   Profiling JAX programs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../device_memory_profiling.html">
   Device Memory Profiling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rank_promotion_warning.html">
   Rank promotion warning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../contributing.html">
   Contributing to JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../developer.html">
   Building from source
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../jax_internal_api.html">
   Internal APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jax.html">
   Public API: jax package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.numpy.html">
     jax.numpy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.scipy.html">
     jax.scipy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.config.html">
     JAX configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.debug.html">
     jax.debug package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.dlpack.html">
     jax.dlpack module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.distributed.html">
     jax.distributed module
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.example_libraries.html">
     jax.example_libraries package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.experimental.html">
     jax.experimental package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.flatten_util.html">
     jax.flatten_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.image.html">
     jax.image package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.lax.html">
     jax.lax package
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.nn.html">
     jax.nn package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.ops.html">
     jax.ops package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.profiler.html">
     jax.profiler module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.random.html">
     jax.random package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.stages.html">
     jax.stages package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.tree_util.html">
     jax.tree_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.lib.html">
     jax.lib package
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/google/jax"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for jax._src.api</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The JAX Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;JAX user-facing transformations and utilities.</span>

<span class="sd">The transformations here mostly wrap internal transformations, providing</span>
<span class="sd">convenience flags to control behavior and handling Python containers of</span>
<span class="sd">arguments and outputs. The Python containers handled are pytrees (see</span>
<span class="sd">tree_util.py), which include nested tuples/lists/dicts, where the leaves are</span>
<span class="sd">arrays.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">itertools</span> <span class="k">as</span> <span class="nn">it</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span>
                    <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span>
                    <span class="n">Hashable</span><span class="p">,</span> <span class="n">List</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">ExitStack</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">linear_util</span> <span class="k">as</span> <span class="n">lu</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">stages</span>
<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">eval_jaxpr</span>
<span class="kn">from</span> <span class="nn">jax.tree_util</span> <span class="kn">import</span> <span class="p">(</span><span class="n">tree_map</span><span class="p">,</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">,</span>
                           <span class="n">tree_structure</span><span class="p">,</span> <span class="n">tree_transpose</span><span class="p">,</span> <span class="n">tree_leaves</span><span class="p">,</span>
                           <span class="n">treedef_is_leaf</span><span class="p">,</span> <span class="n">treedef_children</span><span class="p">,</span>
                           <span class="n">Partial</span><span class="p">,</span> <span class="n">PyTreeDef</span><span class="p">,</span> <span class="n">all_leaves</span><span class="p">,</span> <span class="n">treedef_tuple</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">callback</span> <span class="k">as</span> <span class="n">jcb</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">device_array</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">source_info_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">traceback_util</span>
<span class="kn">from</span> <span class="nn">jax._src.api_util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">flatten_fun</span><span class="p">,</span> <span class="n">apply_flat_fun</span><span class="p">,</span> <span class="n">flatten_fun_nokwargs</span><span class="p">,</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">,</span>
    <span class="n">argnums_partial</span><span class="p">,</span> <span class="n">argnums_partial_except</span><span class="p">,</span> <span class="n">flatten_axes</span><span class="p">,</span> <span class="n">donation_vector</span><span class="p">,</span>
    <span class="n">rebase_donate_argnums</span><span class="p">,</span> <span class="n">_ensure_index</span><span class="p">,</span> <span class="n">_ensure_index_tuple</span><span class="p">,</span>
    <span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">_ensure_str_tuple</span><span class="p">,</span> <span class="n">argnames_partial_except</span><span class="p">,</span>
    <span class="n">validate_argnames</span><span class="p">,</span> <span class="n">validate_argnums</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax._src.lax</span> <span class="kn">import</span> <span class="n">lax</span> <span class="k">as</span> <span class="n">lax_internal</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">jax_jit</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_bridge</span> <span class="k">as</span> <span class="n">xb</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_client</span> <span class="k">as</span> <span class="n">xc</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">pmap_lib</span>
<span class="kn">from</span> <span class="nn">jax._src.traceback_util</span> <span class="kn">import</span> <span class="n">api_boundary</span>
<span class="kn">from</span> <span class="nn">jax._src.tree_util</span> <span class="kn">import</span> <span class="n">broadcast_prefix</span>
<span class="kn">from</span> <span class="nn">jax._src.util</span> <span class="kn">import</span> <span class="p">(</span><span class="n">unzip2</span><span class="p">,</span> <span class="n">curry</span><span class="p">,</span> <span class="n">safe_map</span><span class="p">,</span> <span class="n">safe_zip</span><span class="p">,</span> <span class="n">prod</span><span class="p">,</span> <span class="n">split_list</span><span class="p">,</span>
                           <span class="n">extend_name_stack</span><span class="p">,</span> <span class="n">new_name_stack</span><span class="p">,</span> <span class="n">wrap_name</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span>
                           <span class="n">wraps</span><span class="p">,</span> <span class="n">HashableFunction</span><span class="p">,</span> <span class="n">weakref_lru_cache</span><span class="p">)</span>

<span class="c1"># Unused imports to be exported</span>
<span class="kn">from</span> <span class="nn">jax._src.lib.xla_bridge</span> <span class="kn">import</span> <span class="p">(</span><span class="n">device_count</span><span class="p">,</span> <span class="n">local_device_count</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span>
                                     <span class="n">local_devices</span><span class="p">,</span> <span class="n">process_index</span><span class="p">,</span>
                                     <span class="n">process_count</span><span class="p">,</span> <span class="n">host_id</span><span class="p">,</span> <span class="n">host_ids</span><span class="p">,</span>
                                     <span class="n">host_count</span><span class="p">,</span> <span class="n">default_backend</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax.ad_checkpoint</span> <span class="kn">import</span> <span class="n">checkpoint_policies</span><span class="p">,</span> <span class="n">checkpoint</span> <span class="k">as</span> <span class="n">new_checkpoint</span>
<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">ShapedArray</span><span class="p">,</span> <span class="n">raise_to_shaped</span>
<span class="kn">from</span> <span class="nn">jax.custom_batching</span> <span class="kn">import</span> <span class="n">custom_vmap</span>
<span class="kn">from</span> <span class="nn">jax.custom_derivatives</span> <span class="kn">import</span> <span class="p">(</span><span class="n">closure_convert</span><span class="p">,</span> <span class="n">custom_gradient</span><span class="p">,</span> <span class="n">custom_jvp</span><span class="p">,</span>
                                    <span class="n">custom_vjp</span><span class="p">,</span> <span class="n">linear_call</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">jax.custom_transpose</span> <span class="kn">import</span> <span class="n">custom_transpose</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">partial_eval</span> <span class="k">as</span> <span class="n">pe</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">mlir</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">xla</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">pxla</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">ad</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">batching</span>

<span class="kn">from</span> <span class="nn">jax._src.config</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">flags</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">bool_env</span><span class="p">,</span>
    <span class="n">disable_jit</span> <span class="k">as</span> <span class="n">_disable_jit</span><span class="p">,</span>
    <span class="n">debug_nans</span> <span class="k">as</span> <span class="n">config_debug_nans</span><span class="p">,</span>
    <span class="n">debug_infs</span> <span class="k">as</span> <span class="n">config_debug_infs</span><span class="p">,</span>
    <span class="n">_thread_local_state</span> <span class="k">as</span> <span class="n">config_thread_local_state</span><span class="p">,</span>
    <span class="n">explicit_device_put_scope</span> <span class="k">as</span> <span class="n">config_explicit_device_put_scope</span><span class="p">,</span>
    <span class="n">explicit_device_get_scope</span> <span class="k">as</span> <span class="n">config_explicit_device_get_scope</span><span class="p">)</span>


<span class="n">traceback_util</span><span class="o">.</span><span class="n">register_exclusion</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>

<span class="n">_dtype</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">canonicalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">AxisName</span> <span class="o">=</span> <span class="n">Any</span>

<span class="c1"># These TypeVars are used below to express the fact that function types</span>
<span class="c1"># (i.e. call signatures) are invariant under the vmap transformation.</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;F&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">Callable</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">)</span>

<span class="nb">map</span><span class="p">,</span> <span class="n">unsafe_map</span> <span class="o">=</span> <span class="n">safe_map</span><span class="p">,</span> <span class="nb">map</span>
<span class="nb">zip</span><span class="p">,</span> <span class="n">unsafe_zip</span> <span class="o">=</span> <span class="n">safe_zip</span><span class="p">,</span> <span class="nb">zip</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>

<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s2">&quot;experimental_cpp_jit&quot;</span><span class="p">,</span> <span class="n">bool_env</span><span class="p">(</span><span class="s2">&quot;JAX_CPP_JIT&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="s2">&quot;A flag enabling the C++ jax.jit fast path.&quot;</span>
    <span class="s2">&quot;Set this to `False` only if it crashes otherwise and report &quot;</span>
    <span class="s2">&quot;the error to the jax-team.&quot;</span><span class="p">)</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s2">&quot;experimental_cpp_pmap&quot;</span><span class="p">,</span> <span class="n">bool_env</span><span class="p">(</span><span class="s2">&quot;JAX_CPP_PMAP&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="s2">&quot;A flag enabling the C++ jax.pmap fast path. Until the default &quot;</span>
    <span class="s2">&quot;is switched to True, the feature is not supported and possibly broken &quot;</span>
    <span class="s2">&quot;(e.g. it may use unreleased code from jaxlib.&quot;</span><span class="p">)</span>
<span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_bool</span><span class="p">(</span>
    <span class="s2">&quot;experimental_cpp_pjit&quot;</span><span class="p">,</span> <span class="n">bool_env</span><span class="p">(</span><span class="s2">&quot;JAX_CPP_PJIT&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="s2">&quot;A flag enabling the C++ pjit fast path. Until the default &quot;</span>
    <span class="s2">&quot;is switched to True, the feature is not supported and possibly broken &quot;</span>
    <span class="s2">&quot;(e.g. it may use unreleased code from jaxlib.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_nan_check_posthook</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Hook function called by the C++ jit/pmap to perform NaN checking.&quot;&quot;&quot;</span>
  <span class="n">leaves</span> <span class="o">=</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

  <span class="n">buffers</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">da_or_sda</span> <span class="ow">in</span> <span class="n">leaves</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">da_or_sda</span><span class="p">,</span> <span class="s2">&quot;device_buffer&quot;</span><span class="p">):</span>
      <span class="n">buffers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">da_or_sda</span><span class="o">.</span><span class="n">device_buffer</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">da_or_sda</span><span class="p">,</span> <span class="s2">&quot;device_buffers&quot;</span><span class="p">):</span>
      <span class="n">buffers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">da_or_sda</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">)</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">check_special</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">xla_call_p</span><span class="p">,</span> <span class="n">buffers</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">FloatingPointError</span><span class="p">:</span>
    <span class="c1"># compiled_fun can only raise in this case</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_debug_nans</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_debug_infs</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Invalid nan value encountered in the output of a C++-jit/pmap &quot;</span>
          <span class="s2">&quot;function. Calling the de-optimized version.&quot;</span><span class="p">)</span>
    <span class="n">fun</span><span class="o">.</span><span class="n">_cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># probably won&#39;t return</span>

<span class="k">def</span> <span class="nf">_update_debug_special_global</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">_read</span><span class="p">(</span><span class="s2">&quot;jax_debug_nans&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">_read</span><span class="p">(</span><span class="s2">&quot;jax_debug_infs&quot;</span><span class="p">):</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">global_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="n">_nan_check_posthook</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">global_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_update_debug_special_thread_local</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">config_thread_local_state</span><span class="p">,</span> <span class="s2">&quot;jax_debug_nans&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span>
      <span class="nb">getattr</span><span class="p">(</span><span class="n">config_thread_local_state</span><span class="p">,</span> <span class="s2">&quot;jax_debug_infs&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)):</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">thread_local_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="n">_nan_check_posthook</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jax_jit</span><span class="o">.</span><span class="n">thread_local_state</span><span class="p">()</span><span class="o">.</span><span class="n">post_hook</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">config_debug_nans</span><span class="o">.</span><span class="n">_add_hooks</span><span class="p">(</span><span class="n">_update_debug_special_global</span><span class="p">,</span>
                             <span class="n">_update_debug_special_thread_local</span><span class="p">)</span>
<span class="n">config_debug_infs</span><span class="o">.</span><span class="n">_add_hooks</span><span class="p">(</span><span class="n">_update_debug_special_global</span><span class="p">,</span>
                             <span class="n">_update_debug_special_thread_local</span><span class="p">)</span>


<span class="n">float0</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float0</span>

<span class="k">def</span> <span class="nf">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
  <span class="c1"># In Python 3.10+, the only thing stopping us from supporting staticmethods</span>
  <span class="c1"># is that we can&#39;t take weak references to them, which the C++ JIT requires.</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="nb">staticmethod</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;staticmethod arguments are not supported, got </span><span class="si">{</span><span class="n">fun</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected a callable value, got </span><span class="si">{</span><span class="n">fun</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">_isgeneratorfunction</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected a function, got a generator function: </span><span class="si">{</span><span class="n">fun</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_isgeneratorfunction</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
  <span class="c1"># TODO 3.9+: remove</span>
  <span class="c1"># re-implemented here because of https://bugs.python.org/issue33261</span>
  <span class="k">while</span> <span class="n">inspect</span><span class="o">.</span><span class="n">ismethod</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
    <span class="n">fun</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__func__</span>
  <span class="k">while</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">):</span>
    <span class="n">fun</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="n">func</span>
  <span class="k">return</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isfunction</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">bool</span><span class="p">(</span><span class="n">fun</span><span class="o">.</span><span class="vm">__code__</span><span class="o">.</span><span class="n">co_flags</span> <span class="o">&amp;</span> <span class="n">inspect</span><span class="o">.</span><span class="n">CO_GENERATOR</span><span class="p">)</span>

<span class="n">_POSITIONAL_OR_KEYWORD</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span>

<span class="k">def</span> <span class="nf">_infer_argnums_and_argnames</span><span class="p">(</span>
    <span class="n">sig</span><span class="p">:</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Signature</span><span class="p">,</span>
    <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">argnames</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
  <span class="sd">&quot;&quot;&quot;Infer missing argnums and argnames for a function with inspect.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">argnums</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">argnames</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(),</span> <span class="p">()</span>

  <span class="k">if</span> <span class="n">argnums</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">argnames</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>
    <span class="n">argnames</span> <span class="o">=</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">argnames</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">argnames</span>

  <span class="n">parameters</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span>
  <span class="k">if</span> <span class="n">argnums</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">argnames</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">argnames</span> <span class="o">=</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">argnames</span><span class="p">)</span>
    <span class="n">argnums</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
        <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">_POSITIONAL_OR_KEYWORD</span> <span class="ow">and</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">argnames</span>
    <span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>
    <span class="n">argnames</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
        <span class="n">k</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">_POSITIONAL_OR_KEYWORD</span> <span class="ow">and</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">argnums</span>
    <span class="p">)</span>

  <span class="k">return</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">argnames</span>

<div class="viewcode-block" id="jit"><a class="viewcode-back" href="../../../_autosummary/jax.jit.html#jax.jit">[docs]</a><span class="k">def</span> <span class="nf">jit</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">static_argnames</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">inline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Wrapped</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Sets up ``fun`` for just-in-time compilation with XLA.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be jitted. ``fun`` should be a pure function, as</span>
<span class="sd">      side-effects may only be executed once.</span>

<span class="sd">      The arguments and return value of ``fun`` should be arrays,</span>
<span class="sd">      scalars, or (nested) standard Python containers (tuple/list/dict) thereof.</span>
<span class="sd">      Positional arguments indicated by ``static_argnums`` can be anything at</span>
<span class="sd">      all, provided they are hashable and have an equality operation defined.</span>
<span class="sd">      Static arguments are included as part of a compilation cache key, which is</span>
<span class="sd">      why hash and equality operators must be defined.</span>

<span class="sd">      JAX keeps a weak reference to ``fun`` for use as a compilation cache key,</span>
<span class="sd">      so the object ``fun`` must be weakly-referenceable. Most :class:`Callable`</span>
<span class="sd">      objects will already satisfy this requirement.</span>
<span class="sd">    static_argnums: An optional int or collection of ints that specify which</span>
<span class="sd">      positional arguments to treat as static (compile-time constant).</span>
<span class="sd">      Operations that only depend on static arguments will be constant-folded in</span>
<span class="sd">      Python (during tracing), and so the corresponding argument values can be</span>
<span class="sd">      any Python object.</span>

<span class="sd">      Static arguments should be hashable, meaning both ``__hash__`` and</span>
<span class="sd">      ``__eq__`` are implemented, and immutable. Calling the jitted function</span>
<span class="sd">      with different values for these constants will trigger recompilation.</span>
<span class="sd">      Arguments that are not arrays or containers thereof must be marked as</span>
<span class="sd">      static.</span>

<span class="sd">      If neither ``static_argnums`` nor ``static_argnames`` is provided, no</span>
<span class="sd">      arguments are treated as static. If ``static_argnums`` is not provided but</span>
<span class="sd">      ``static_argnames`` is, or vice versa, JAX uses</span>
<span class="sd">      :code:`inspect.signature(fun)` to find any positional arguments that</span>
<span class="sd">      correspond to ``static_argnames``</span>
<span class="sd">      (or vice versa). If both ``static_argnums`` and ``static_argnames`` are</span>
<span class="sd">      provided, ``inspect.signature`` is not used, and only actual</span>
<span class="sd">      parameters listed in either ``static_argnums`` or ``static_argnames`` will</span>
<span class="sd">      be treated as static.</span>
<span class="sd">    static_argnames: An optional string or collection of strings specifying</span>
<span class="sd">      which named arguments to treat as static (compile-time constant). See the</span>
<span class="sd">      comment on ``static_argnums`` for details. If not</span>
<span class="sd">      provided but ``static_argnums`` is set, the default is based on calling</span>
<span class="sd">      ``inspect.signature(fun)`` to find corresponding named arguments.</span>
<span class="sd">    device: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, the Device the jitted function will run on. (Available devices</span>
<span class="sd">      can be retrieved via :py:func:`jax.devices`.) The default is inherited</span>
<span class="sd">      from XLA&#39;s DeviceAssignment logic and is usually to use</span>
<span class="sd">      ``jax.devices()[0]``.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend: ``&#39;cpu&#39;``, ``&#39;gpu&#39;``, or</span>
<span class="sd">      ``&#39;tpu&#39;``.</span>
<span class="sd">    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to</span>
<span class="sd">      the computation. It is safe to donate argument buffers if you no longer</span>
<span class="sd">      need them once the computation has finished. In some cases XLA can make</span>
<span class="sd">      use of donated buffers to reduce the amount of memory needed to perform a</span>
<span class="sd">      computation, for example recycling one of your input buffers to store a</span>
<span class="sd">      result. You should not reuse buffers that you donate to a computation, JAX</span>
<span class="sd">      will raise an error if you try to. By default, no argument buffers are</span>
<span class="sd">      donated.</span>
<span class="sd">      Note that donate_argnums only work for positional arguments, and keyword</span>
<span class="sd">      arguments will not be donated.</span>

<span class="sd">      For more details on buffer donation see the</span>
<span class="sd">      [FAQ](https://jax.readthedocs.io/en/latest/faq.html#buffer-donation).</span>

<span class="sd">    inline: Specify whether this function should be inlined into enclosing</span>
<span class="sd">      jaxprs (rather than being represented as an application of the xla_call</span>
<span class="sd">      primitive with its own subjaxpr). Default False.</span>
<span class="sd">    keep_unused: If `False` (the default), arguments that JAX determines to be</span>
<span class="sd">      unused by `fun` *may* be dropped from resulting compiled XLA executables.</span>
<span class="sd">      Such arguments will not be transferred to the device nor provided to the</span>
<span class="sd">      underlying executable. If `True`, unused arguments will not be pruned.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun``, set up for just-in-time compilation.</span>

<span class="sd">  Examples:</span>
<span class="sd">    In the following example, ``selu`` can be compiled into a single fused kernel</span>
<span class="sd">    by XLA:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... def selu(x, alpha=1.67, lmbda=1.05):</span>
<span class="sd">    ...   return lmbda * jax.numpy.where(x &gt; 0, x, alpha * jax.numpy.exp(x) - alpha)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; key = jax.random.PRNGKey(0)</span>
<span class="sd">    &gt;&gt;&gt; x = jax.random.normal(key, (10,))</span>
<span class="sd">    &gt;&gt;&gt; print(selu(x))  # doctest: +SKIP</span>
<span class="sd">    [-0.54485  0.27744 -0.29255 -0.91421 -0.62452 -0.24748</span>
<span class="sd">    -0.85743 -0.78232  0.76827  0.59566 ]</span>

<span class="sd">    To pass arguments such as ``static_argnames`` when decorating a function, a common</span>
<span class="sd">    pattern is to use :func:`functools.partial`:</span>

<span class="sd">    &gt;&gt;&gt; from functools import partial</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @partial(jax.jit, static_argnames=[&#39;n&#39;])</span>
<span class="sd">    ... def g(x, n):</span>
<span class="sd">    ...   for i in range(n):</span>
<span class="sd">    ...     x = x ** 2</span>
<span class="sd">    ...   return x</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g(jnp.arange(4), 3)</span>
<span class="sd">    DeviceArray([   0,    1,  256, 6561], dtype=int32)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">abstracted_axes</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;abstracted_axes must be used with --jax_dynamic_shapes&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">experimental_cpp_jit</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_jit</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span>
                    <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_jit</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span>
                      <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="p">,</span> <span class="n">abstracted_axes</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_jit</span><span class="p">(</span>
    <span class="n">use_cpp_jit</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">static_argnames</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">inline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Wrapped</span><span class="p">:</span>
  <span class="c1"># Implemements common logic between CPP and Python backends</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>

  <span class="c1"># Coerce input</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">)</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="c1"># Some built-in functions don&#39;t support signature.</span>
    <span class="c1"># See: https://github.com/python/cpython/issues/73485</span>
    <span class="c1"># In this case no validation is done</span>
    <span class="n">static_argnums</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">static_argnums</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_argnums</span><span class="p">)</span>
    <span class="n">static_argnames</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">static_argnames</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">static_argnames</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Infer argnums and argnames according to docstring</span>
    <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span> <span class="o">=</span> <span class="n">_infer_argnums_and_argnames</span><span class="p">(</span>
        <span class="n">sig</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">)</span>

    <span class="c1"># Validation</span>
    <span class="n">validate_argnums</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="s2">&quot;static_argnums&quot;</span><span class="p">)</span>
    <span class="n">validate_argnums</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="s2">&quot;donate_argnums&quot;</span><span class="p">)</span>

    <span class="n">validate_argnames</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="s2">&quot;static_argnames&quot;</span><span class="p">)</span>

  <span class="c1"># Compensate for static argnums absorbing args</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">rebase_donate_argnums</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">use_cpp_jit</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_cpp_jit</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="n">static_argnames</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
        <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_python_jit</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="n">static_argnames</span><span class="p">,</span>
      <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
      <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">,</span> <span class="n">abstracted_axes</span><span class="o">=</span><span class="n">abstracted_axes</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_prepare_jit</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span>
                 <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
  <span class="c1"># Validate donate_argnums</span>
  <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;jitted function has donate_argnums=</span><span class="si">{</span><span class="n">donate_argnums</span><span class="si">}</span><span class="s2"> but &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;was called with only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional arguments.&quot;</span><span class="p">)</span>

  <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">f</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">argnums_partial_except</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">allow_invalid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">f</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">argnames_partial_except</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
  <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">donate_argnums</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">donation_vector</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">args_flat</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">donated_invars</span>


<span class="n">PytreeOfAbstractedAxesSpec</span> <span class="o">=</span> <span class="n">Any</span>

<span class="k">def</span> <span class="nf">_python_jit</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">static_argnames</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">inline</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PytreeOfAbstractedAxesSpec</span><span class="p">],</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Wrapped</span><span class="p">:</span>
  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">f_jitted</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_disable_jit</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">_prepare_jit</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args_flat</span><span class="p">:</span>
      <span class="n">_check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span><span class="p">:</span>
      <span class="n">axes_specs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">abstracted_axes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                    <span class="n">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
      <span class="n">in_type</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">infer_lambda_input_type</span><span class="p">(</span><span class="n">axes_specs</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
      <span class="n">flat_fun</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">in_type</span><span class="p">)</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">xla_call</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="o">*</span><span class="n">args_flat</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">,</span>
        <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_flat</span><span class="p">)</span>

  <span class="n">f_jitted</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">_jit_lower</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                              <span class="n">backend</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="p">,</span>
                              <span class="n">abstracted_axes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">clear_cache</span><span class="p">():</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_callable</span><span class="o">.</span><span class="n">evict_function</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">f_jitted</span><span class="o">.</span><span class="n">clear_cache</span> <span class="o">=</span> <span class="n">clear_cache</span>

  <span class="k">return</span> <span class="n">f_jitted</span>

<span class="k">def</span> <span class="nf">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">pe</span><span class="o">.</span><span class="n">AbstractedAxesSpec</span><span class="p">]:</span>
  <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span>
  <span class="k">def</span> <span class="nf">ax_leaf</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">all_leaves</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="ow">or</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">all_leaves</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">broadcast_prefix</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">ax_leaf</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_BackendAndDeviceInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">default_device</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">Device</span>
  <span class="n">committed_to_device</span><span class="p">:</span> <span class="nb">bool</span>

<span class="k">class</span> <span class="nc">_FastpathData</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">xla_executable</span><span class="p">:</span> <span class="n">xla</span><span class="o">.</span><span class="n">XlaLoadedExecutable</span>
  <span class="n">out_pytree_def</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">sticky_device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span>
  <span class="n">avals</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">lazy_exprs</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">kept_var_bitvec</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
  <span class="n">shardings</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">committed</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

<span class="n">_cpp_jit_cache</span> <span class="o">=</span> <span class="n">jax_jit</span><span class="o">.</span><span class="n">CompiledFunctionCache</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_cpp_jit_clear_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_clear_cache</span><span class="p">()</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_callable</span><span class="o">.</span><span class="n">evict_function</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fun</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_jax_array_use_fast_path</span><span class="p">(</span><span class="n">execute</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">):</span>
  <span class="n">use_fastpath</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">xc</span><span class="o">.</span><span class="n">_version</span> <span class="o">&gt;=</span> <span class="mi">96</span> <span class="ow">and</span>
      <span class="c1"># This is if we have already executed this code-path (most-recent entry</span>
      <span class="c1"># has been reset to None). Thus, we do not support the fast-path.</span>
      <span class="n">execute</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="nb">type</span><span class="p">(</span><span class="n">execute</span><span class="p">)</span> <span class="ow">is</span> <span class="n">pxla</span><span class="o">.</span><span class="n">ExecuteReplicated</span> <span class="ow">and</span>
      <span class="c1"># No effects in computation</span>
      <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">has_unordered_effects</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">has_host_callbacks</span> <span class="ow">and</span>
      <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">)</span> <span class="ow">and</span>
      <span class="c1"># Not supported: dynamic shapes</span>
      <span class="ow">not</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span>
      <span class="c1"># TODO(chky): Check sharding is SingleDeviceSharding</span>
  <span class="p">)</span>

  <span class="k">if</span> <span class="n">use_fastpath</span><span class="p">:</span>
    <span class="n">sticky_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">lazy_exprs</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_flat</span><span class="p">)</span>
    <span class="n">kept_var_bitvec</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="ow">in</span> <span class="n">execute</span><span class="o">.</span><span class="n">kept_var_idx</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args_flat</span><span class="p">))]</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">aval</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
    <span class="n">shardings</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">sharding</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
    <span class="n">committed</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">_committed</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">_FastpathData</span><span class="p">(</span><span class="n">execute</span><span class="o">.</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">sticky_device</span><span class="p">,</span>
                         <span class="n">avals</span><span class="p">,</span> <span class="n">lazy_exprs</span><span class="p">,</span> <span class="n">kept_var_bitvec</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span>
                         <span class="n">committed</span><span class="p">)</span>

  <span class="k">return</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_device_array_use_fast_path</span><span class="p">(</span><span class="n">execute</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">):</span>
  <span class="c1"># TODO(sharadmv): Clean up usage of `execute.args`</span>
  <span class="n">use_fastpath</span> <span class="o">=</span> <span class="p">(</span>
      <span class="c1"># This is if we have already executed this code-path (most-recent entry</span>
      <span class="c1"># has been reset to None). Thus, we do not support the fast-path.</span>
      <span class="n">execute</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="n">execute</span><span class="o">.</span><span class="n">func</span> <span class="ow">is</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">_execute_compiled</span> <span class="ow">and</span>  <span class="c1"># not trivial, not pmap</span>
      <span class="c1"># No effects in computation</span>
      <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="ow">and</span>
      <span class="c1"># Has no host callbacks</span>
      <span class="ow">not</span> <span class="n">execute</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="ow">and</span>
      <span class="c1"># impl rule must have been called, i.e. top trace is an EvalTrace</span>
      <span class="nb">isinstance</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">find_top_trace</span><span class="p">(</span><span class="n">args_flat</span><span class="p">),</span> <span class="n">core</span><span class="o">.</span><span class="n">EvalTrace</span><span class="p">)</span> <span class="ow">and</span>
      <span class="c1"># Not supported: ShardedDeviceArray</span>
      <span class="nb">all</span><span class="p">(</span><span class="n">device_array</span><span class="o">.</span><span class="n">type_is_device_array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">)</span> <span class="ow">and</span>
      <span class="c1"># Not supported: dynamic shapes</span>
      <span class="ow">not</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span>
      <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">execute</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span> <span class="ow">is</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">SimpleResultHandler</span><span class="p">)</span>

  <span class="c1">### If we can use the fastpath, we return required info to the caller.</span>
  <span class="k">if</span> <span class="n">use_fastpath</span><span class="p">:</span>
    <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">xla_executable</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">result_handlers</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">,</span>
     <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">execute</span><span class="o">.</span><span class="n">args</span>  <span class="c1"># pytype: disable=attribute-error</span>
    <span class="n">sticky_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lazy_exprs</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">result_handlers</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">result_handler</span> <span class="ow">in</span> <span class="n">result_handlers</span><span class="p">:</span>
      <span class="n">aval</span><span class="p">,</span> <span class="n">sticky_device</span> <span class="o">=</span> <span class="n">result_handler</span><span class="o">.</span><span class="n">args</span>
      <span class="n">avals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">avals</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_flat</span><span class="p">)</span>
    <span class="n">kept_var_bitvec</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args_flat</span><span class="p">))]</span>
    <span class="n">shardings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">committed</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">return</span> <span class="n">_FastpathData</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">sticky_device</span><span class="p">,</span> <span class="n">avals</span><span class="p">,</span>
                         <span class="n">lazy_exprs</span><span class="p">,</span> <span class="n">kept_var_bitvec</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">committed</span><span class="p">)</span>

  <span class="k">return</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_cpp_jit</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">static_argnames</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">inline</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Wrapped</span><span class="p">:</span>
  <span class="c1"># An implementation of `jit` that tries to do as much as possible in C++.</span>
  <span class="c1"># The goal of this function is to speed up the time it takes to process the</span>
  <span class="c1"># arguments, find the correct C++ executable, start the transfer of arguments</span>
  <span class="c1"># and schedule the computation.</span>
  <span class="c1"># As long as it does not support all features of the Python implementation</span>
  <span class="c1"># the C++ code will fallback to `_python_jit` when it faces some unsupported</span>
  <span class="c1"># feature.</span>
  <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;can&#39;t specify both a device and a backend for jit, &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;got device=</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2"> and backend=</span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1">### This first part is basically the same code as in _python_jit.</span>
    <span class="c1"># An alternative would be for cache_miss to accept from C++ the arguments</span>
    <span class="c1"># (dyn_args, donated_invars, args_flat, in_tree), since otherwise we have</span>
    <span class="c1"># work/code that is redundant between C++ and Python. We can try that later.</span>
    <span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">_prepare_jit</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args_flat</span><span class="p">:</span>
      <span class="n">_check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span><span class="p">:</span>
      <span class="n">in_type</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">infer_lambda_input_type</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
      <span class="n">flat_fun</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">in_type</span><span class="p">)</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">xla_call</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="o">*</span><span class="n">args_flat</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="o">=</span><span class="n">keep_unused</span><span class="p">)</span>
    <span class="n">out_pytree_def</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span>

    <span class="c1">### Decide whether we can support the C++ fast path</span>
    <span class="c1"># High level note: The Python tracing mechanism is complex; in particular</span>
    <span class="c1"># to know whether `jax.jit(f)(x)` will execute or trace, it&#39;s not enough to</span>
    <span class="c1"># inspect the argument x, we actually do need to execute it and look at the</span>
    <span class="c1"># outputs that could be tracers (if f is capturing `Tracer` by closure).</span>
    <span class="n">execute</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_callable</span><span class="o">.</span><span class="n">most_recent_entry</span><span class="p">()</span>

    <span class="n">fastpath_data</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># TODO(sharadmv): Enable fast path for effectful jaxprs</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="n">_jax_array_use_fast_path</span><span class="p">(</span><span class="n">execute</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="n">_device_array_use_fast_path</span><span class="p">(</span><span class="n">execute</span><span class="p">,</span> <span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">fastpath_data</span>

  <span class="k">def</span> <span class="nf">get_device_info</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Backends do not exist before __main__ is being executed.&quot;&quot;&quot;</span>
    <span class="n">committed_to_device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">default_device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">backend_</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
      <span class="n">default_device</span> <span class="o">=</span> <span class="n">backend_</span><span class="o">.</span><span class="n">get_default_device_assignment</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">_BackendAndDeviceInfo</span><span class="p">(</span><span class="n">default_device</span><span class="p">,</span> <span class="n">committed_to_device</span><span class="p">)</span>

  <span class="n">jitted_f_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">jitted_f_kwargs</span><span class="p">[</span><span class="s2">&quot;has_explicit_device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">cpp_jitted_f</span> <span class="o">=</span> <span class="n">jax_jit</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span>
      <span class="n">cache_miss</span><span class="p">,</span>
      <span class="n">get_device_info</span><span class="p">,</span>
      <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span>
      <span class="n">static_argnames</span><span class="o">=</span><span class="n">static_argnames</span><span class="p">,</span>
      <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
      <span class="n">cache</span><span class="o">=</span><span class="n">_cpp_jit_cache</span><span class="p">,</span>
      <span class="o">**</span><span class="n">jitted_f_kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
  <span class="n">f_jitted</span> <span class="o">=</span> <span class="n">wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)(</span><span class="n">cpp_jitted_f</span><span class="p">)</span>

  <span class="n">f_jitted</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">_jit_lower</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                              <span class="n">backend</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span> <span class="n">keep_unused</span><span class="p">,</span>
                              <span class="kc">None</span><span class="p">)</span>
  <span class="n">f_jitted</span><span class="o">.</span><span class="n">_fun</span> <span class="o">=</span> <span class="n">fun</span>
  <span class="nb">type</span><span class="p">(</span><span class="n">f_jitted</span><span class="p">)</span><span class="o">.</span><span class="n">clear_cache</span> <span class="o">=</span> <span class="n">_cpp_jit_clear_cache</span>

  <span class="k">return</span> <span class="n">f_jitted</span>


<span class="k">def</span> <span class="nf">_jit_lower</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span>
               <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">inline</span><span class="p">,</span>  <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PytreeOfAbstractedAxesSpec</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Make a ``lower`` method for jitted functions.&quot;&quot;&quot;</span>
  <span class="c1"># If the function we returned from ``jit`` were a class instance,</span>
  <span class="c1"># this might naturally be a method, with ``fun`` as a ``self`` and</span>
  <span class="c1"># all the other arguments stored as attributes.</span>

  <span class="k">def</span> <span class="nf">arg_spec</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">PmapSharding</span>
    <span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">pjit</span>
    <span class="c1"># like xla.arg_spec but duck-types on x.shape and x.dtype</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span> <span class="k">else</span> <span class="n">shaped_abstractify</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;sharding&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="n">PmapSharding</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">aval</span><span class="p">,</span> <span class="kc">None</span>
        <span class="c1"># If `x` has a sharding attribute but not `_committed` attribute,</span>
        <span class="c1"># assume that `x` is committed. This might happen when the input is</span>
        <span class="c1"># a `ShapedDtypeStruct` or `types.SimpleNamespace`, etc that might</span>
        <span class="c1"># only have a `sharding` attribute on them.</span>
        <span class="k">return</span> <span class="n">aval</span><span class="p">,</span> <span class="p">(</span><span class="n">pjit</span><span class="o">.</span><span class="n">to_op_sharding_sharding</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
                      <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_committed&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">aval</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">device</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_device&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">aval</span><span class="p">,</span> <span class="n">device</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">lower</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Lower this function for the given arguments.</span>

<span class="sd">    A lowered function is staged out of Python and translated to a</span>
<span class="sd">    compiler&#39;s input language, possibly in a backend-dependent</span>
<span class="sd">    manner. It is ready for compilation but not yet compiled.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A ``Lowered`` instance representing the lowering.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">_prepare_jit</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">static_argnames</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">closed_fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">arg_specs_and_devices</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">arg_spec</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_dynamic_shapes</span><span class="p">:</span>
      <span class="n">axes_specs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">abstracted_axes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
                    <span class="n">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
      <span class="n">in_type</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">infer_lambda_input_type</span><span class="p">(</span><span class="n">axes_specs</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
      <span class="n">flat_fun</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">in_type</span><span class="p">)</span>
      <span class="n">in_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">aval</span> <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">explicit</span> <span class="ow">in</span> <span class="n">in_type</span> <span class="k">if</span> <span class="n">explicit</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">abstracted_axes</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;abstracted_axes must be used with --jax_dynamic_shapes&quot;</span><span class="p">)</span>
      <span class="n">in_avals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">arg_specs_and_devices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
      <span class="n">computation</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">sharded_lowering</span><span class="p">(</span>
          <span class="n">flat_fun</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
          <span class="n">keep_unused</span><span class="p">,</span> <span class="o">*</span><span class="n">arg_specs_and_devices</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="o">.</span><span class="n">from_flat_info</span><span class="p">(</span>
          <span class="n">computation</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">computation</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">lower_xla_callable</span><span class="p">(</span>
          <span class="n">flat_fun</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
          <span class="n">keep_unused</span><span class="p">,</span> <span class="o">*</span><span class="n">arg_specs_and_devices</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="o">.</span><span class="n">from_flat_info</span><span class="p">(</span>
          <span class="n">computation</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">lower</span>


<div class="viewcode-block" id="disable_jit"><a class="viewcode-back" href="../../../_autosummary/jax.disable_jit.html#jax.disable_jit">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">disable_jit</span><span class="p">(</span><span class="n">disable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Context manager that disables :py:func:`jit` behavior under its dynamic context.</span>

<span class="sd">  For debugging it is useful to have a mechanism that disables :py:func:`jit`</span>
<span class="sd">  everywhere in a dynamic context. Note that this not only disables explicit</span>
<span class="sd">  uses of `jit` by the user, but will also remove any implicit JIT compilation</span>
<span class="sd">  used by the JAX library: this includes implicit JIT computation of `body` and</span>
<span class="sd">  `cond` functions passed to higher-level primitives like :func:`scan` and</span>
<span class="sd">  :func:`while_loop`, JIT used in implementations of :mod:`jax.numpy` functions,</span>
<span class="sd">  and any other case where `jit` is used within an API&#39;s implementation.</span>

<span class="sd">  Values that have a data dependence on the arguments to a jitted function are</span>
<span class="sd">  traced and abstracted. For example, an abstract value may be a</span>
<span class="sd">  :py:class:`ShapedArray` instance, representing the set of all possible arrays</span>
<span class="sd">  with a given shape and dtype, but not representing one concrete array with</span>
<span class="sd">  specific values. You might notice those if you use a benign side-effecting</span>
<span class="sd">  operation in a jitted function, like a print:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @jax.jit</span>
<span class="sd">  ... def f(x):</span>
<span class="sd">  ...   y = x * 2</span>
<span class="sd">  ...   print(&quot;Value of y is&quot;, y)</span>
<span class="sd">  ...   return y + 3</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(f(jax.numpy.array([1, 2, 3])))</span>
<span class="sd">  Value of y is Traced&lt;ShapedArray(int32[3])&gt;with&lt;DynamicJaxprTrace(level=0/1)&gt;</span>
<span class="sd">  [5 7 9]</span>

<span class="sd">  Here ``y`` has been abstracted by :py:func:`jit` to a :py:class:`ShapedArray`,</span>
<span class="sd">  which represents an array with a fixed shape and type but an arbitrary value.</span>
<span class="sd">  The value of ``y`` is also traced. If we want to see a concrete value while</span>
<span class="sd">  debugging, and avoid the tracer too, we can use the :py:func:`disable_jit`</span>
<span class="sd">  context manager:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; with jax.disable_jit():</span>
<span class="sd">  ...   print(f(jax.numpy.array([1, 2, 3])))</span>
<span class="sd">  ...</span>
<span class="sd">  Value of y is [2 4 6]</span>
<span class="sd">  [5 7 9]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">_disable_jit</span><span class="p">(</span><span class="n">disable</span><span class="p">):</span>
    <span class="k">yield</span></div>


<div class="viewcode-block" id="xla_computation"><a class="viewcode-back" href="../../../_autosummary/jax.xla_computation.html#jax.xla_computation">[docs]</a><span class="k">def</span> <span class="nf">xla_computation</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                    <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
                    <span class="n">axis_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">in_parts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_parts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">tuple_args</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">instantiate_const_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">return_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Creates a function that produces its XLA computation given example args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function from which to form XLA computations.</span>
<span class="sd">    static_argnums: See the :py:func:`jax.jit` docstring.</span>
<span class="sd">    axis_env: Optional, a sequence of pairs where the first element is an axis</span>
<span class="sd">      name and the second element is a positive integer representing the size of</span>
<span class="sd">      the mapped axis with that name. This parameter is useful when lowering</span>
<span class="sd">      functions that involve parallel communication collectives, and it</span>
<span class="sd">      specifies the axis name/size environment that would be set up by</span>
<span class="sd">      applications of :py:func:`jax.pmap`. See the examples below.</span>
<span class="sd">    in_parts: Optional, how each argument to ``fun`` should be partitioned or</span>
<span class="sd">      replicated. This is used to specify partitioned XLA computations, see</span>
<span class="sd">      ``sharded_jit`` for more info.</span>
<span class="sd">    out_parts: Optional, how each output of ``fun`` should be partitioned or</span>
<span class="sd">      replicated. This is used to specify partitioned XLA computations, see</span>
<span class="sd">      ``sharded_jit`` for more info.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend: ``&#39;cpu&#39;``, ``&#39;gpu&#39;``, or</span>
<span class="sd">      ``&#39;tpu&#39;``.</span>
<span class="sd">    tuple_args: Optional bool, defaults to ``False``. If ``True``, the resulting</span>
<span class="sd">      XLA computation will have a single tuple argument that is unpacked into</span>
<span class="sd">      the specified function arguments. If `None`, tupling will be enabled when</span>
<span class="sd">      there are more than 100 arguments, since some platforms have limits on</span>
<span class="sd">      argument arity.</span>
<span class="sd">    instantiate_const_outputs: Deprecated argument, does nothing.</span>
<span class="sd">    return_shape: Optional boolean, defaults to ``False``. If ``True``, the</span>
<span class="sd">      wrapped function returns a pair where the first element is the XLA</span>
<span class="sd">      computation and the second element is a pytree with the same structure as</span>
<span class="sd">      the output of ``fun`` and where the leaves are objects with ``shape``,</span>
<span class="sd">      ``dtype``, and ``named_shape`` attributes representing the corresponding</span>
<span class="sd">      types of the output leaves.</span>
<span class="sd">    donate_argnums: Specify which arguments are &quot;donated&quot; to the computation.</span>
<span class="sd">      It is safe to donate arguments if you no longer need them once the</span>
<span class="sd">      computation has finished. In some cases XLA can make use of donated</span>
<span class="sd">      buffers to reduce the amount of memory needed to perform a computation,</span>
<span class="sd">      for example recycling one of your input buffers to store a result. You</span>
<span class="sd">      should not reuse buffers that you donate to a computation, JAX will raise</span>
<span class="sd">      an error if you try to.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun`` that when applied to example arguments returns</span>
<span class="sd">    a built XLA Computation (see xla_client.py), from which representations of</span>
<span class="sd">    the unoptimized XLA HLO computation can be extracted using methods like</span>
<span class="sd">    ``as_hlo_text``, ``as_serialized_hlo_module_proto``, and</span>
<span class="sd">    ``as_hlo_dot_graph``. If the argument ``return_shape`` is ``True``, then the</span>
<span class="sd">    wrapped function returns a pair where the first element is the XLA</span>
<span class="sd">    Computation and the second element is a pytree representing the structure,</span>
<span class="sd">    shapes, dtypes, and named shapes of the output of ``fun``.</span>

<span class="sd">    Concrete example arguments are not always necessary. For those arguments not</span>
<span class="sd">    indicated by ``static_argnums``, any object with ``shape`` and ``dtype``</span>
<span class="sd">    attributes is acceptable (excepting namedtuples, which are treated as Python</span>
<span class="sd">    containers).</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x))</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f)(3.)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule xla_computation_f.6</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  ENTRY xla_computation_f.6 {</span>
<span class="sd">    constant.2 = pred[] constant(false)</span>
<span class="sd">    parameter.1 = f32[] parameter(0)</span>
<span class="sd">    cosine.3 = f32[] cosine(parameter.1)</span>
<span class="sd">    sine.4 = f32[] sine(cosine.3)</span>
<span class="sd">    ROOT tuple.5 = (f32[]) tuple(sine.4)</span>
<span class="sd">  }</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>


<span class="sd">  Alternatively, the assignment to ``c`` above could be written:</span>

<span class="sd">  &gt;&gt;&gt; import types</span>
<span class="sd">  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32))</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f)(scalar)</span>


<span class="sd">  Here&#39;s an example that involves a parallel collective and axis name:</span>

<span class="sd">  &gt;&gt;&gt; def f(x): return x - jax.lax.psum(x, &#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; c = jax.xla_computation(f, axis_env=[(&#39;i&#39;, 4)])(2)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule jaxpr_computation.9</span>
<span class="sd">  primitive_computation.3 {</span>
<span class="sd">    parameter.4 = s32[] parameter(0)</span>
<span class="sd">    parameter.5 = s32[] parameter(1)</span>
<span class="sd">    ROOT add.6 = s32[] add(parameter.4, parameter.5)</span>
<span class="sd">  }</span>
<span class="sd">  ENTRY jaxpr_computation.9 {</span>
<span class="sd">    tuple.1 = () tuple()</span>
<span class="sd">    parameter.2 = s32[] parameter(0)</span>
<span class="sd">    all-reduce.7 = s32[] all-reduce(parameter.2), replica_groups={{0,1,2,3}}, to_apply=primitive_computation.3</span>
<span class="sd">    ROOT subtract.8 = s32[] subtract(parameter.2, all-reduce.7)</span>
<span class="sd">  }</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>
<span class="sd">  &lt;BLANKLINE&gt;</span>

<span class="sd">  Notice the ``replica_groups`` that were generated. Here&#39;s an example that</span>
<span class="sd">  generates more interesting ``replica_groups``:</span>

<span class="sd">  &gt;&gt;&gt; from jax import lax</span>
<span class="sd">  &gt;&gt;&gt; def g(x):</span>
<span class="sd">  ...   rowsum = lax.psum(x, &#39;i&#39;)</span>
<span class="sd">  ...   colsum = lax.psum(x, &#39;j&#39;)</span>
<span class="sd">  ...   allsum = lax.psum(x, (&#39;i&#39;, &#39;j&#39;))</span>
<span class="sd">  ...   return rowsum, colsum, allsum</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; axis_env = [(&#39;i&#39;, 4), (&#39;j&#39;, 2)]</span>
<span class="sd">  &gt;&gt;&gt; c = xla_computation(g, axis_env=axis_env)(5.)</span>
<span class="sd">  &gt;&gt;&gt; print(c.as_hlo_text())  # doctest: +SKIP</span>
<span class="sd">  HloModule jaxpr_computation__1.19</span>
<span class="sd">  [removed uninteresting text here]</span>
<span class="sd">  ENTRY jaxpr_computation__1.19 {</span>
<span class="sd">    tuple.1 = () tuple()</span>
<span class="sd">    parameter.2 = f32[] parameter(0)</span>
<span class="sd">    all-reduce.7 = f32[] all-reduce(parameter.2), replica_groups={{0,2,4,6},{1,3,5,7}}, to_apply=primitive_computation__1.3</span>
<span class="sd">    all-reduce.12 = f32[] all-reduce(parameter.2), replica_groups={{0,1},{2,3},{4,5},{6,7}}, to_apply=primitive_computation__1.8</span>
<span class="sd">    all-reduce.17 = f32[] all-reduce(parameter.2), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=primitive_computation__1.13</span>
<span class="sd">    ROOT tuple.18 = (f32[], f32[], f32[]) tuple(all-reduce.7, all-reduce.12, all-reduce.17)</span>
<span class="sd">  }</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">del</span> <span class="n">instantiate_const_outputs</span>  <span class="c1"># Unused</span>

  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">static_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_argnums</span><span class="p">)</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">)</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="n">rebase_donate_argnums</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">)</span>

  <span class="n">fun_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">,</span> <span class="s2">&quot;unknown&quot;</span><span class="p">)</span>

  <span class="n">platform</span> <span class="o">=</span> <span class="n">backend</span> <span class="k">if</span> <span class="n">backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span><span class="o">.</span><span class="n">platform</span>

  <span class="k">def</span> <span class="nf">make_axis_env</span><span class="p">(</span><span class="n">nreps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis_env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">xla</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="p">(),</span> <span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">nreps</span> <span class="o">=</span> <span class="n">nreps</span> <span class="o">*</span> <span class="n">prod</span><span class="p">(</span><span class="n">size</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span><span class="p">)</span>
      <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">axis_env</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">xla</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">computation_maker</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">static_argnums</span> <span class="o">+</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;jitted function has static_argnums=</span><span class="si">{</span><span class="n">static_argnums</span><span class="si">}</span><span class="s2">,&quot;</span>
                       <span class="sa">f</span><span class="s2">&quot; donate_argnums=</span><span class="si">{</span><span class="n">donate_argnums</span><span class="si">}</span><span class="s2"> but &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;was called with only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional arguments.&quot;</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial_except</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">static_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">allow_invalid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">donate_argnums</span><span class="p">:</span>
      <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">donation_vector</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">donated_invars</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">args_flat</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">in_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">in_parts_flat</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">in_parts_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_axes</span><span class="p">(</span>
          <span class="s2">&quot;xla_computation in_parts&quot;</span><span class="p">,</span> <span class="n">in_tree</span><span class="o">.</span><span class="n">children</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">in_parts</span><span class="p">))</span>
    <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span> <span class="ow">or</span> <span class="p">[]:</span>
        <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_dynamic</span><span class="p">(</span><span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">avals</span><span class="p">)</span>
      <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">apply_outfeed_rewriter</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
      <span class="n">axis_env_</span> <span class="o">=</span> <span class="n">make_axis_env</span><span class="p">(</span><span class="n">dispatch</span><span class="o">.</span><span class="n">jaxpr_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">out_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_parts_flat</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out_parts_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_axes</span><span class="p">(</span>
            <span class="s2">&quot;xla_computation out_parts&quot;</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_parts</span><span class="p">))</span>
      <span class="n">unordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">effects</span>
                           <span class="k">if</span> <span class="n">eff</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
      <span class="n">ordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">effects</span>
                         <span class="k">if</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
      <span class="n">lowering_result</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">lower_jaxpr_to_module</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;xla_computation_</span><span class="si">{</span><span class="n">fun_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
          <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">),</span>
          <span class="n">unordered_effects</span><span class="o">=</span><span class="n">unordered_effects</span><span class="p">,</span>
          <span class="n">ordered_effects</span><span class="o">=</span><span class="n">ordered_effects</span><span class="p">,</span>
          <span class="n">backend_or_name</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
          <span class="n">platform</span><span class="o">=</span><span class="n">platform</span><span class="p">,</span>
          <span class="n">axis_context</span><span class="o">=</span><span class="n">mlir</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">axis_env_</span><span class="p">),</span>
          <span class="n">name_stack</span><span class="o">=</span><span class="n">new_name_stack</span><span class="p">(</span><span class="n">wrap_name</span><span class="p">(</span><span class="n">fun_name</span><span class="p">,</span> <span class="s2">&quot;xla_computation&quot;</span><span class="p">)),</span>
          <span class="n">donated_args</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span>
          <span class="n">arg_shardings</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">in_parts_flat</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">map</span><span class="p">(</span>
              <span class="n">xla</span><span class="o">.</span><span class="n">sharding_to_proto</span><span class="p">,</span> <span class="n">in_parts_flat</span><span class="p">)),</span>
          <span class="n">result_shardings</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">out_parts_flat</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">map</span><span class="p">(</span>
              <span class="n">xla</span><span class="o">.</span><span class="n">sharding_to_proto</span><span class="p">,</span> <span class="n">out_parts_flat</span><span class="p">)))</span>
      <span class="k">if</span> <span class="n">tuple_args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">should_tuple</span> <span class="o">=</span> <span class="n">tuple_args</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">should_tuple_args</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">avals</span><span class="p">),</span> <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>
      <span class="n">built</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">mlir_module_to_xla_computation</span><span class="p">(</span>
          <span class="n">mlir</span><span class="o">.</span><span class="n">module_to_string</span><span class="p">(</span><span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">),</span>
          <span class="n">use_tuple_args</span><span class="o">=</span><span class="n">should_tuple</span><span class="p">,</span>
          <span class="n">return_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out_shapes_flat</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">]</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_shapes_flat</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">out_aval</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_aval</span><span class="p">,</span> <span class="n">xla</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;As we want to propagate the weak_type, we need &quot;</span>
                           <span class="s2">&quot;to get a ShapedArray, otherwise this &quot;</span>
                           <span class="s2">&quot;information is lost&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">return_shape</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">built</span><span class="p">,</span> <span class="n">out_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">built</span>

  <span class="k">return</span> <span class="n">computation_maker</span></div>

<div class="viewcode-block" id="grad"><a class="viewcode-back" href="../../../_autosummary/jax.grad.html#jax.grad">[docs]</a><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
         <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
         <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Creates a function that evaluates the gradient of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments at positions specified by</span>
<span class="sd">      ``argnums`` should be arrays, scalars, or standard Python containers.</span>
<span class="sd">      Argument arrays in the positions specified by ``argnums`` must be of</span>
<span class="sd">      inexact (i.e., floating-point or complex) type. It</span>
<span class="sd">      should return a scalar (which includes arrays with shape ``()`` but not</span>
<span class="sd">      arrays with shape ``(1,)`` etc.)</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default 0).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. If True, inputs and outputs must be complex. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>
<span class="sd">    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and</span>
<span class="sd">      ``fun`` implicitly broadcasts a value over that axis, the backward pass</span>
<span class="sd">      will perform a ``psum`` of the corresponding gradient. Otherwise, the</span>
<span class="sd">      gradient will be per-example over named axes. For example, if ``&#39;batch&#39;``</span>
<span class="sd">      is a named batch axis, ``grad(f, reduce_axes=(&#39;batch&#39;,))`` will create a</span>
<span class="sd">      function that computes the total gradient while ``grad(f)`` will create</span>
<span class="sd">      one that computes the per-example gradient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the gradient</span>
<span class="sd">    of ``fun``. If ``argnums`` is an integer then the gradient has the same</span>
<span class="sd">    shape and type as the positional argument indicated by that integer. If</span>
<span class="sd">    argnums is a tuple of integers, the gradient is a tuple of values with the</span>
<span class="sd">    same shapes and types as the corresponding arguments. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (gradient, auxiliary_data) is returned.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; grad_tanh = jax.grad(jax.numpy.tanh)</span>
<span class="sd">  &gt;&gt;&gt; print(grad_tanh(0.2))</span>
<span class="sd">  0.961043</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">value_and_grad_f</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span>
                                    <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">,</span>
                                    <span class="n">allow_int</span><span class="o">=</span><span class="n">allow_int</span><span class="p">,</span>
                                    <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Gradient of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;gradient, which has the same shape as the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">grad_f_aux</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">aux</span><span class="p">),</span> <span class="n">g</span> <span class="o">=</span> <span class="n">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">grad_f_aux</span> <span class="k">if</span> <span class="n">has_aux</span> <span class="k">else</span> <span class="n">grad_f</span></div>

<div class="viewcode-block" id="value_and_grad"><a class="viewcode-back" href="../../../_autosummary/jax.value_and_grad.html#jax.value_and_grad">[docs]</a><span class="k">def</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                   <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                   <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">()</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="sd">&quot;&quot;&quot;Create a function that evaluates both ``fun`` and the gradient of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments at positions specified by</span>
<span class="sd">      ``argnums`` should be arrays, scalars, or standard Python containers. It</span>
<span class="sd">      should return a scalar (which includes arrays with shape ``()`` but not</span>
<span class="sd">      arrays with shape ``(1,)`` etc.)</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default 0).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. If True, inputs and outputs must be complex. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>
<span class="sd">    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and</span>
<span class="sd">      ``fun`` implicitly broadcasts a value over that axis, the backward pass</span>
<span class="sd">      will perform a ``psum`` of the corresponding gradient. Otherwise, the</span>
<span class="sd">      gradient will be per-example over named axes. For example, if ``&#39;batch&#39;``</span>
<span class="sd">      is a named batch axis, ``value_and_grad(f, reduce_axes=(&#39;batch&#39;,))`` will</span>
<span class="sd">      create a function that computes the total gradient while</span>
<span class="sd">      ``value_and_grad(f)`` will create one that computes the per-example</span>
<span class="sd">      gradient.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun`` that evaluates both ``fun``</span>
<span class="sd">    and the gradient of ``fun`` and returns them as a pair (a two-element</span>
<span class="sd">    tuple). If ``argnums`` is an integer then the gradient has the same shape</span>
<span class="sd">    and type as the positional argument indicated by that integer. If argnums is</span>
<span class="sd">    a sequence of integers, the gradient is a tuple of values with the same</span>
<span class="sd">    shapes and types as the corresponding arguments. If ``has_aux`` is True</span>
<span class="sd">    then a tuple of ((value, auxiliary_data), gradient) is returned.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Value and gradient of </span><span class="si">{fun}</span><span class="s2"> with respect to positional &quot;</span>
            <span class="s2">&quot;argument(s) </span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but &quot;</span>
            <span class="s2">&quot;returns a two-element tuple where the first element is the value &quot;</span>
            <span class="s2">&quot;of </span><span class="si">{fun}</span><span class="s2"> and the second element is the gradient, which has the &quot;</span>
            <span class="s2">&quot;same shape as the arguments at positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">argnums</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">concrete_or_error</span><span class="p">(</span><span class="n">_ensure_index</span><span class="p">,</span> <span class="n">argnums</span><span class="p">)</span>
  <span class="n">reduce_axes</span> <span class="o">=</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">reduce_axes</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">value_and_grad_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">max_argnum</span> <span class="o">=</span> <span class="n">argnums</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_argnum</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;differentiating with respect to argnums=</span><span class="si">{</span><span class="n">argnums</span><span class="si">}</span><span class="s2"> requires at least &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">max_argnum</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> positional arguments to be passed by the caller, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional arguments.&quot;</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">):</span>
      <span class="n">_check_input_dtype_grad</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">,</span> <span class="n">leaf</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">ans</span><span class="p">,</span> <span class="n">vjp_py</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ans</span><span class="p">,</span> <span class="n">vjp_py</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span>
          <span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
    <span class="n">_check_scalar</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_grad</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">ans</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">vjp_py</span><span class="p">(</span><span class="n">lax_internal</span><span class="o">.</span><span class="n">_one</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">g</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ans</span><span class="p">,</span> <span class="n">g</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">aux</span><span class="p">),</span> <span class="n">g</span>

  <span class="k">return</span> <span class="n">value_and_grad_f</span></div>

<span class="k">def</span> <span class="nf">_check_scalar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Gradient only defined for scalar-output functions. Output </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;was </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span> <span class="kn">from</span> <span class="nn">e</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">aval</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">():</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;had shape: </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;had abstract value </span><span class="si">{</span><span class="n">aval</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_check_input_dtype_revderiv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">_check_arg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with input element type </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with holomorphic=True requires inputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span> <span class="ow">or</span>
      <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_int</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real- or complex-valued inputs (input dtype &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;that is a sub-dtype of np.inexact), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                      <span class="s2">&quot;If you want to use Boolean- or integer-valued inputs, use vjp &quot;</span>
                      <span class="s2">&quot;or set allow_int to True.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inexact</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires numerical-valued inputs (input dtype that is a &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;sub-dtype of np.bool_ or np.number), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="n">_check_input_dtype_grad</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_output_dtype_revderiv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with output element type </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s2">&quot;For differentiation of non-holomorphic functions involving complex &quot;</span>
                    <span class="s2">&quot;outputs, use jax.vjp directly.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> requires real-valued outputs (output dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For differentiation of functions with integer outputs, use &quot;</span>
                    <span class="s2">&quot;jax.vjp directly.&quot;</span><span class="p">)</span>
<span class="n">_check_output_dtype_grad</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="jacfwd"><a class="viewcode-back" href="../../../_autosummary/jax.jacfwd.html#jax.jacfwd">[docs]</a><span class="k">def</span> <span class="nf">jacfwd</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
           <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Jacobian of ``fun`` evaluated column-by-column using forward-mode AD.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Jacobian is to be computed.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Jacobian of</span>
<span class="sd">    ``fun`` using forward-mode automatic differentiation. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (jacobian, auxiliary_data) is returned.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x):</span>
<span class="sd">  ...   return jnp.asarray(</span>
<span class="sd">  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(jax.jacfwd(f)(jnp.array([1., 2., 3.])))</span>
<span class="sd">  [[ 1.       0.       0.     ]</span>
<span class="sd">   [ 0.       0.       5.     ]</span>
<span class="sd">   [ 0.      16.      -2.     ]</span>
<span class="sd">   [ 1.6209   0.       0.84147]]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">argnums</span> <span class="o">=</span> <span class="n">_ensure_index</span><span class="p">(</span><span class="n">argnums</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Jacobian of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_jacfwd</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">pushfwd</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">)</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pushfwd</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">pushfwd</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">jac</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pushfwd</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">dyn_args</span><span class="p">))</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_jacfwd</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">example_args</span> <span class="o">=</span> <span class="n">dyn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dyn_args</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_jacfwd_unravel</span><span class="p">,</span> <span class="n">example_args</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">jac</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">jacfun</span></div>

<span class="k">def</span> <span class="nf">_check_input_dtype_jacfwd</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">_check_arg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;jacfwd with input element type </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd with holomorphic=True requires inputs with complex &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;dtype, but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd requires real-valued inputs (input dtype that is &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;a sub-dtype of np.floating), but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;For holomorphic differentiation, pass holomorphic=True. &quot;</span>
                    <span class="s2">&quot;For differentiation of non-holomorphic functions involving &quot;</span>
                    <span class="s2">&quot;complex inputs or integer inputs, use jax.jvp directly.&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_output_dtype_jacfwd</span><span class="p">(</span><span class="n">holomorphic</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">holomorphic</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;jacfwd with holomorphic=True requires outputs with complex dtype, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="jacrev"><a class="viewcode-back" href="../../../_autosummary/jax.jacrev.html#jax.jacrev">[docs]</a><span class="k">def</span> <span class="nf">jacrev</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
           <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Jacobian of ``fun`` evaluated row-by-row using reverse-mode AD.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Jacobian is to be computed.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>
<span class="sd">    allow_int: Optional, bool. Whether to allow differentiating with</span>
<span class="sd">      respect to integer valued inputs. The gradient of an integer input will</span>
<span class="sd">      have a trivial vector-space dtype (float0). Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Jacobian of</span>
<span class="sd">    ``fun`` using reverse-mode automatic differentiation. If ``has_aux`` is True</span>
<span class="sd">    then a pair of (jacobian, auxiliary_data) is returned.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x):</span>
<span class="sd">  ...   return jnp.asarray(</span>
<span class="sd">  ...     [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; print(jax.jacrev(f)(jnp.array([1., 2., 3.])))</span>
<span class="sd">  [[ 1.       0.       0.     ]</span>
<span class="sd">   [ 0.       0.       5.     ]</span>
<span class="sd">   [ 0.      16.      -2.     ]</span>
<span class="sd">   [ 1.6209   0.       0.84147]]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>

  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Jacobian of </span><span class="si">{fun}</span><span class="s2"> with respect to positional argument(s) &quot;</span>
            <span class="s2">&quot;</span><span class="si">{argnums}</span><span class="s2">. Takes the same arguments as </span><span class="si">{fun}</span><span class="s2"> but returns the &quot;</span>
            <span class="s2">&quot;jacobian of the output with respect to the arguments at &quot;</span>
            <span class="s2">&quot;positions </span><span class="si">{argnums}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="n">argnums</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">f_partial</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                                          <span class="n">require_static_args_hashable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_jacrev</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">,</span> <span class="n">allow_int</span><span class="p">),</span> <span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">pullback</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">pullback</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">_vjp</span><span class="p">(</span><span class="n">f_partial</span><span class="p">,</span> <span class="o">*</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_jacrev</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">pullback</span><span class="p">)(</span><span class="n">_std_basis</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">jac</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">jac</span>
    <span class="n">example_args</span> <span class="o">=</span> <span class="n">dyn_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">argnums</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dyn_args</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_jacrev_unravel</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">example_args</span><span class="p">,</span> <span class="n">jac</span><span class="p">)</span>
    <span class="n">jac_tree</span> <span class="o">=</span> <span class="n">tree_transpose</span><span class="p">(</span><span class="n">tree_structure</span><span class="p">(</span><span class="n">example_args</span><span class="p">),</span> <span class="n">tree_structure</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">jac_tree</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">jac_tree</span><span class="p">,</span> <span class="n">aux</span>

  <span class="k">return</span> <span class="n">jacfun</span></div>
<span class="n">jacobian</span> <span class="o">=</span> <span class="n">jacrev</span>

<span class="n">_check_input_dtype_jacrev</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_input_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;jacrev&quot;</span><span class="p">)</span>
<span class="n">_check_output_dtype_jacrev</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_check_output_dtype_revderiv</span><span class="p">,</span> <span class="s2">&quot;jacrev&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hessian"><a class="viewcode-back" href="../../../_autosummary/jax.hessian.html#jax.hessian">[docs]</a><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">holomorphic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Hessian of ``fun`` as a dense array.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function whose Hessian is to be computed.  Its arguments at positions</span>
<span class="sd">      specified by ``argnums`` should be arrays, scalars, or standard Python</span>
<span class="sd">      containers thereof. It should return arrays, scalars, or standard Python</span>
<span class="sd">      containers thereof.</span>
<span class="sd">    argnums: Optional, integer or sequence of integers. Specifies which</span>
<span class="sd">      positional argument(s) to differentiate with respect to (default ``0``).</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">      first element is considered the output of the mathematical function to be</span>
<span class="sd">      differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be</span>
<span class="sd">      holomorphic. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function with the same arguments as ``fun``, that evaluates the Hessian of</span>
<span class="sd">    ``fun``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; g = lambda x: x[0]**3 - 2*x[0]*x[1] - x[1]**6</span>
<span class="sd">  &gt;&gt;&gt; print(jax.hessian(g)(jax.numpy.array([1., 2.])))</span>
<span class="sd">  [[   6.   -2.]</span>
<span class="sd">   [  -2. -480.]]</span>

<span class="sd">  :py:func:`hessian` is a generalization of the usual definition of the Hessian</span>
<span class="sd">  that supports nested Python containers (i.e. pytrees) as inputs and outputs.</span>
<span class="sd">  The tree structure of ``jax.hessian(fun)(x)`` is given by forming a tree</span>
<span class="sd">  product of the structure of ``fun(x)`` with a tree product of two copies of</span>
<span class="sd">  the structure of ``x``. A tree product of two tree structures is formed by</span>
<span class="sd">  replacing each leaf of the first tree with a copy of the second. For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt; f = lambda dct: {&quot;c&quot;: jnp.power(dct[&quot;a&quot;], dct[&quot;b&quot;])}</span>
<span class="sd">  &gt;&gt;&gt; print(jax.hessian(f)({&quot;a&quot;: jnp.arange(2.) + 1., &quot;b&quot;: jnp.arange(2.) + 2.}))</span>
<span class="sd">  {&#39;c&#39;: {&#39;a&#39;: {&#39;a&#39;: DeviceArray([[[ 2.,  0.], [ 0.,  0.]],</span>
<span class="sd">                                 [[ 0.,  0.], [ 0., 12.]]], dtype=float32),</span>
<span class="sd">               &#39;b&#39;: DeviceArray([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],</span>
<span class="sd">                                 [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32)},</span>
<span class="sd">         &#39;b&#39;: {&#39;a&#39;: DeviceArray([[[ 1.      ,  0.      ], [ 0.      ,  0.      ]],</span>
<span class="sd">                                 [[ 0.      ,  0.      ], [ 0.      , 12.317766]]], dtype=float32),</span>
<span class="sd">               &#39;b&#39;: DeviceArray([[[0.      , 0.      ], [0.      , 0.      ]],</span>
<span class="sd">                                [[0.      , 0.      ], [0.      , 3.843624]]], dtype=float32)}}}</span>

<span class="sd">  Thus each leaf in the tree structure of ``jax.hessian(fun)(x)`` corresponds to</span>
<span class="sd">  a leaf of ``fun(x)`` and a pair of leaves of ``x``. For each leaf in</span>
<span class="sd">  ``jax.hessian(fun)(x)``, if the corresponding array leaf of ``fun(x)`` has</span>
<span class="sd">  shape ``(out_1, out_2, ...)`` and the corresponding array leaves of ``x`` have</span>
<span class="sd">  shape ``(in_1_1, in_1_2, ...)`` and ``(in_2_1, in_2_2, ...)`` respectively,</span>
<span class="sd">  then the Hessian leaf has shape ``(out_1, out_2, ..., in_1_1, in_1_2, ...,</span>
<span class="sd">  in_2_1, in_2_2, ...)``. In other words, the Python tree structure represents</span>
<span class="sd">  the block structure of the Hessian, with blocks determined by the input and</span>
<span class="sd">  output pytrees.</span>

<span class="sd">  In particular, an array is produced (with no pytrees involved) when the</span>
<span class="sd">  function input ``x`` and output ``fun(x)`` are each a single array, as in the</span>
<span class="sd">  ``g`` example above. If ``fun(x)`` has shape ``(out1, out2, ...)`` and ``x``</span>
<span class="sd">  has shape ``(in1, in2, ...)`` then ``jax.hessian(fun)(x)`` has shape</span>
<span class="sd">  ``(out1, out2, ..., in1, in2, ..., in1, in2, ...)``. To flatten pytrees into</span>
<span class="sd">  1D vectors, consider using :py:func:`jax.flatten_util.flatten_pytree`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">),</span>
                <span class="n">argnums</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="n">holomorphic</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_std_basis</span><span class="p">(</span><span class="n">pytree</span><span class="p">):</span>
  <span class="n">leaves</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">pytree</span><span class="p">)</span>
  <span class="n">ndim</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">leaves</span><span class="p">))</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">result_type</span><span class="p">(</span><span class="o">*</span><span class="n">leaves</span><span class="p">)</span>
  <span class="n">flat_basis</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span><span class="n">pytree</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">flat_basis</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_jacfwd_unravel</span><span class="p">(</span><span class="n">input_pytree</span><span class="p">,</span> <span class="n">output_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span>
    <span class="n">input_pytree</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_jacrev_unravel</span><span class="p">(</span><span class="n">output_pytree</span><span class="p">,</span> <span class="n">input_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_unravel_array_into_pytree</span><span class="p">(</span>
    <span class="n">output_pytree</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">input_pytree_leaf</span><span class="p">,</span> <span class="n">arr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_possible_downcast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">example</span><span class="p">):</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">)</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">example</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">complexfloating</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">real</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_dtype</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
  <span class="n">weak_type</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">is_weakly_typed</span><span class="p">(</span><span class="n">example</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">lax_internal</span><span class="o">.</span><span class="n">_convert_element_type</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">weak_type</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_unravel_array_into_pytree</span><span class="p">(</span><span class="n">pytree</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">example</span><span class="p">,</span> <span class="n">arr</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Unravel an array into a PyTree with a given structure.</span>
<span class="sd">  Args:</span>
<span class="sd">      pytree: The pytree that provides the structure.</span>
<span class="sd">      axis: The parameter axis is either -1, 0, or 1.  It controls the</span>
<span class="sd">        resulting shapes.</span>
<span class="sd">      example: If specified, cast the components to the matching dtype/weak_type,</span>
<span class="sd">        or else use the pytree leaf type if example is None.</span>
<span class="sd">      arr: The array to be unraveled.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">leaves</span><span class="p">,</span> <span class="n">treedef</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">pytree</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">leaves</span><span class="p">]</span>
  <span class="n">parts</span> <span class="o">=</span> <span class="n">_split</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">leaves</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="n">axis</span><span class="p">)</span>
  <span class="n">reshaped_parts</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">_possible_downcast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">leaf</span> <span class="k">if</span> <span class="n">example</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">example</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parts</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">leaves</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">treedef</span><span class="p">,</span> <span class="n">reshaped_parts</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="vmap"><a class="viewcode-back" href="../../../_autosummary/jax.vmap.html#jax.vmap">[docs]</a><span class="k">def</span> <span class="nf">vmap</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">F</span><span class="p">,</span>
         <span class="n">in_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">out_axes</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
         <span class="n">axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Hashable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">spmd_axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Hashable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">F</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Vectorizing map. Creates a function which maps ``fun`` over argument axes.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be mapped over additional axes.</span>
<span class="sd">    in_axes: An integer, None, or (nested) standard Python container</span>
<span class="sd">      (tuple/list/dict) thereof specifying which input array axes to map over.</span>

<span class="sd">      If each positional argument to ``fun`` is an array, then ``in_axes`` can</span>
<span class="sd">      be an integer, a None, or a tuple of integers and Nones with length equal</span>
<span class="sd">      to the number of positional arguments to ``fun``. An integer or ``None``</span>
<span class="sd">      indicates which array axis to map over for all arguments (with ``None``</span>
<span class="sd">      indicating not to map any axis), and a tuple indicates which axis to map</span>
<span class="sd">      for each corresponding positional argument. Axis integers must be in the</span>
<span class="sd">      range ``[-ndim, ndim)`` for each array, where ``ndim`` is the number of</span>
<span class="sd">      dimensions (axes) of the corresponding input array.</span>

<span class="sd">      If the positional arguments to ``fun`` are container (pytree) types, the</span>
<span class="sd">      corresponding element of ``in_axes`` can itself be a matching container,</span>
<span class="sd">      so that distinct array axes can be mapped for different container</span>
<span class="sd">      elements. ``in_axes`` must be a container tree prefix of the positional</span>
<span class="sd">      argument tuple passed to ``fun``. See this link for more detail:</span>
<span class="sd">      https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees</span>

<span class="sd">      Either ``axis_size`` must be provided explicitly, or at least one</span>
<span class="sd">      positional argument must have ``in_axes`` not None. The sizes of the</span>
<span class="sd">      mapped input axes for all mapped positional arguments must all be equal.</span>

<span class="sd">      Arguments passed as keywords are always mapped over their leading axis</span>
<span class="sd">      (i.e. axis index 0).</span>

<span class="sd">      See below for examples.</span>

<span class="sd">    out_axes: An integer, None, or (nested) standard Python container</span>
<span class="sd">      (tuple/list/dict) thereof indicating where the mapped axis should appear</span>
<span class="sd">      in the output. All outputs with a mapped axis must have a non-None</span>
<span class="sd">      ``out_axes`` specification. Axis integers must be in the range ``[-ndim,</span>
<span class="sd">      ndim)`` for each output array, where ``ndim`` is the number of dimensions</span>
<span class="sd">      (axes) of the array returned by the :func:`vmap`-ed function, which is one</span>
<span class="sd">      more than the number of dimensions (axes) of the corresponding array</span>
<span class="sd">      returned by ``fun``.</span>
<span class="sd">    axis_name: Optional, a hashable Python object used to identify the mapped</span>
<span class="sd">      axis so that parallel collectives can be applied.</span>
<span class="sd">    axis_size: Optional, an integer indicating the size of the axis to be</span>
<span class="sd">      mapped. If not provided, the mapped axis size is inferred from arguments.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Batched/vectorized version of ``fun`` with arguments that correspond to</span>
<span class="sd">    those of ``fun``, but with extra array axes at positions indicated by</span>
<span class="sd">    ``in_axes``, and a return value that corresponds to that of ``fun``, but</span>
<span class="sd">    with extra array axes at positions indicated by ``out_axes``.</span>

<span class="sd">  For example, we can implement a matrix-matrix product using a vector dot</span>
<span class="sd">  product:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -&gt; []</span>
<span class="sd">  &gt;&gt;&gt; mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -&gt; [b]      (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mm = vmap(mv, (None, 1), 1)      #  ([b,a], [a,c]) -&gt; [b,c]  (c is the mapped axis)</span>

<span class="sd">  Here we use ``[a,b]`` to indicate an array with shape (a,b). Here are some</span>
<span class="sd">  variants:</span>

<span class="sd">  &gt;&gt;&gt; mv1 = vmap(vv, (0, 0), 0)   #  ([b,a], [b,a]) -&gt; [b]        (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mv2 = vmap(vv, (0, 1), 0)   #  ([b,a], [a,b]) -&gt; [b]        (b is the mapped axis)</span>
<span class="sd">  &gt;&gt;&gt; mm2 = vmap(mv2, (1, 1), 0)  #  ([b,c,a], [a,c,b]) -&gt; [c,b]  (c is the mapped axis)</span>

<span class="sd">  Here&#39;s an example of using container types in ``in_axes`` to specify which</span>
<span class="sd">  axes of the container elements to map over:</span>

<span class="sd">  &gt;&gt;&gt; A, B, C, D = 2, 3, 4, 5</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.ones((A, B))</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.ones((B, C))</span>
<span class="sd">  &gt;&gt;&gt; z = jnp.ones((C, D))</span>
<span class="sd">  &gt;&gt;&gt; def foo(tree_arg):</span>
<span class="sd">  ...   x, (y, z) = tree_arg</span>
<span class="sd">  ...   return jnp.dot(x, jnp.dot(y, z))</span>
<span class="sd">  &gt;&gt;&gt; tree = (x, (y, z))</span>
<span class="sd">  &gt;&gt;&gt; print(foo(tree))</span>
<span class="sd">  [[12. 12. 12. 12. 12.]</span>
<span class="sd">   [12. 12. 12. 12. 12.]]</span>
<span class="sd">  &gt;&gt;&gt; from jax import vmap</span>
<span class="sd">  &gt;&gt;&gt; K = 6  # batch size</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.ones((K, A, B))  # batch axis in different locations</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.ones((B, K, C))</span>
<span class="sd">  &gt;&gt;&gt; z = jnp.ones((C, D, K))</span>
<span class="sd">  &gt;&gt;&gt; tree = (x, (y, z))</span>
<span class="sd">  &gt;&gt;&gt; vfoo = vmap(foo, in_axes=((0, (1, 2)),))</span>
<span class="sd">  &gt;&gt;&gt; print(vfoo(tree).shape)</span>
<span class="sd">  (6, 2, 5)</span>

<span class="sd">  Here&#39;s another example using container types in ``in_axes``, this time a</span>
<span class="sd">  dictionary, to specify the elements of the container to map over:</span>

<span class="sd">  &gt;&gt;&gt; dct = {&#39;a&#39;: 0., &#39;b&#39;: jnp.arange(5.)}</span>
<span class="sd">  &gt;&gt;&gt; x = 1.</span>
<span class="sd">  &gt;&gt;&gt; def foo(dct, x):</span>
<span class="sd">  ...  return dct[&#39;a&#39;] + dct[&#39;b&#39;] + x</span>
<span class="sd">  &gt;&gt;&gt; out = vmap(foo, in_axes=({&#39;a&#39;: None, &#39;b&#39;: 0}, None))(dct, x)</span>
<span class="sd">  &gt;&gt;&gt; print(out)</span>
<span class="sd">  [1. 2. 3. 4. 5.]</span>

<span class="sd">  The results of a vectorized function can be mapped or unmapped. For example,</span>
<span class="sd">  the function below returns a pair with the first element mapped and the second</span>
<span class="sd">  unmapped. Only for unmapped results we can specify ``out_axes`` to be ``None``</span>
<span class="sd">  (to keep it unmapped).</span>

<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=(0, None))(jnp.arange(2.), 4.))</span>
<span class="sd">  (DeviceArray([4., 5.], dtype=float32), 8.0)</span>

<span class="sd">  If the ``out_axes`` is specified for an unmapped result, the result is</span>
<span class="sd">  broadcast across the mapped axis:</span>

<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=0)(jnp.arange(2.), 4.))</span>
<span class="sd">  (DeviceArray([4., 5.], dtype=float32), DeviceArray([8., 8.], dtype=float32, weak_type=True))</span>

<span class="sd">  If the ``out_axes`` is specified for a mapped result, the result is transposed</span>
<span class="sd">  accordingly.</span>

<span class="sd">  Finally, here&#39;s an example using ``axis_name`` together with collectives:</span>

<span class="sd">  &gt;&gt;&gt; xs = jnp.arange(3. * 4.).reshape(3, 4)</span>
<span class="sd">  &gt;&gt;&gt; print(vmap(lambda x: lax.psum(x, &#39;i&#39;), axis_name=&#39;i&#39;)(xs))</span>
<span class="sd">  [[12. 15. 18. 21.]</span>
<span class="sd">   [12. 15. 18. 21.]</span>
<span class="sd">   [12. 15. 18. 21.]]</span>

<span class="sd">  See the :py:func:`jax.pmap` docstring for more examples involving collectives.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">docstr</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Vectorized version of </span><span class="si">{fun}</span><span class="s2">. Takes similar arguments as </span><span class="si">{fun}</span><span class="s2"> &quot;</span>
            <span class="s2">&quot;but with additional array axes over which </span><span class="si">{fun}</span><span class="s2"> is mapped.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">:</span>
    <span class="n">docstr</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Original documentation:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
    <span class="n">docstr</span> <span class="o">+=</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__doc__</span>

  <span class="n">axis_name</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">no_axis_name</span> <span class="k">if</span> <span class="n">axis_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis_name</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="c1"># To be a tree prefix of the positional args tuple, in_axes can never be a</span>
    <span class="c1"># list: if in_axes is not a leaf, it must be a tuple of trees. However,</span>
    <span class="c1"># in cases like these users expect tuples and lists to be treated</span>
    <span class="c1"># essentially interchangeably, so we canonicalize lists to tuples here</span>
    <span class="c1"># rather than raising an error. https://github.com/google/jax/issues/2367</span>
    <span class="n">in_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batching</span><span class="o">.</span><span class="n">spec_types</span>
             <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;vmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">in_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batching</span><span class="o">.</span><span class="n">spec_types</span>
               <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;vmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">out_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">docstr</span><span class="o">=</span><span class="n">docstr</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">vmap_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span>  <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">batching</span><span class="o">.</span><span class="n">is_vmappable</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">flatten_fun_for_vmap</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">in_axes_flat</span> <span class="o">=</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;vmap in_axes&quot;</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">kws</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">axis_size_</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis_size</span> <span class="k">if</span> <span class="n">axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                  <span class="n">_mapped_axis_size</span><span class="p">(</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span> <span class="s2">&quot;vmap&quot;</span><span class="p">,</span>
                                    <span class="n">kws</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size_</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;vmap out_axes&quot;</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_axes</span><span class="p">),</span>
        <span class="n">spmd_axis_name</span><span class="o">=</span><span class="n">spmd_axis_name</span>
    <span class="p">)</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_flat</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vmap_f</span></div>

<span class="k">def</span> <span class="nf">_mapped_axis_size</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">kws</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">vals</span><span class="p">:</span>
    <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">vals</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> wrapped function must be passed at least one argument &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;containing an array, got empty *args=</span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2"> and **kwargs=</span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

  <span class="k">def</span> <span class="nf">_get_axis_size</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisSize</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisSize</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">IndexError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="n">min_rank</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="n">axis</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> was requested to map its argument along axis </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;which implies that its rank should be at least </span><span class="si">{</span><span class="n">min_rank</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;but is only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> (its shape is </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

  <span class="n">axis_sizes</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">dedup_referents</span><span class="p">(</span>
      <span class="n">_get_axis_size</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">axis_sizes</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> must have at least one non-None value in in_axes&quot;</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> got inconsistent sizes for array axes to be mapped:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span>
  <span class="c1"># we switch the error message based on whether args is a tuple of arrays,</span>
  <span class="c1"># in which case we can produce an error message based on argument indices,</span>
  <span class="c1"># or if it has nested containers.</span>
  <span class="k">if</span> <span class="n">kws</span><span class="p">:</span>
    <span class="n">position_only_tree</span><span class="p">,</span> <span class="n">leaf</span> <span class="o">=</span> <span class="n">treedef_children</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">treedef_is_leaf</span><span class="p">(</span><span class="n">leaf</span><span class="p">):</span>
      <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)]</span>
      <span class="n">sizes</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;the tree of axis sizes is:</span><span class="se">\n</span><span class="si">{</span><span class="n">sizes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span> <span class="kn">from</span> <span class="bp">None</span>
    <span class="c1"># if keyword arguments are included in the tree, we adapt the error</span>
    <span class="c1"># message only to be about the positional arguments</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="n">position_only_tree</span>

  <span class="c1"># TODO(mattjj,phawkins): add a way to inspect pytree kind more directly</span>
  <span class="k">if</span> <span class="n">tree</span> <span class="o">==</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="n">tree</span><span class="o">.</span><span class="n">num_leaves</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">lines1</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;arg </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has shape </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2"> and axis </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2"> is to be mapped&quot;</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">))]</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)):</span>
      <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sizes</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">lines2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> to be mapped of size </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="s2">&quot;args&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;arg&quot;</span><span class="p">,</span>
                <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">idxs</span><span class="p">)),</span>
                <span class="s2">&quot;have&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;has&quot;</span><span class="p">,</span>
                <span class="s2">&quot;axes&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;an axis&quot;</span><span class="p">,</span>
                <span class="n">size</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">size</span><span class="p">,</span> <span class="n">idxs</span> <span class="ow">in</span> <span class="n">sizes</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines1</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;so&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">lines2</span><span class="p">)))</span> <span class="kn">from</span> <span class="bp">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">dims</span><span class="p">)]</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;the tree of axis sizes is:</span><span class="se">\n</span><span class="si">{</span><span class="n">sizes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span> <span class="kn">from</span> <span class="bp">None</span>


<div class="viewcode-block" id="pmap"><a class="viewcode-back" href="../../../_autosummary/jax.pmap.html#jax.pmap">[docs]</a><span class="k">def</span> <span class="nf">pmap</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Parallel map with support for collective operations.</span>

<span class="sd">  The purpose of :py:func:`pmap` is to express single-program multiple-data</span>
<span class="sd">  (SPMD) programs. Applying :py:func:`pmap` to a function will compile the</span>
<span class="sd">  function with XLA (similarly to :py:func:`jit`), then execute it in parallel</span>
<span class="sd">  on XLA devices, such as multiple GPUs or multiple TPU cores. Semantically it</span>
<span class="sd">  is comparable to :py:func:`vmap` because both transformations map a function</span>
<span class="sd">  over array axes, but where :py:func:`vmap` vectorizes functions by pushing the</span>
<span class="sd">  mapped axis down into primitive operations, :py:func:`pmap` instead replicates</span>
<span class="sd">  the function and executes each replica on its own XLA device in parallel.</span>

<span class="sd">  The mapped axis size must be less than or equal to the number of local XLA</span>
<span class="sd">  devices available, as returned by :py:func:`jax.local_device_count()` (unless</span>
<span class="sd">  ``devices`` is specified, see below). For nested :py:func:`pmap` calls, the</span>
<span class="sd">  product of the mapped axis sizes must be less than or equal to the number of</span>
<span class="sd">  XLA devices.</span>

<span class="sd">  .. note::</span>
<span class="sd">    :py:func:`pmap` compiles ``fun``, so while it can be combined with</span>
<span class="sd">    :py:func:`jit`, it&#39;s usually unnecessary.</span>

<span class="sd">  **Multi-process platforms:** On multi-process platforms such as TPU pods,</span>
<span class="sd">  :py:func:`pmap` is designed to be used in SPMD Python programs, where every</span>
<span class="sd">  process is running the same Python code such that all processes run the same</span>
<span class="sd">  pmapped function in the same order. Each process should still call the pmapped</span>
<span class="sd">  function with mapped axis size equal to the number of *local* devices (unless</span>
<span class="sd">  ``devices`` is specified, see below), and an array of the same leading axis</span>
<span class="sd">  size will be returned as usual. However, any collective operations in ``fun``</span>
<span class="sd">  will be computed over *all* participating devices, including those on other</span>
<span class="sd">  processes, via device-to-device communication.  Conceptually, this can be</span>
<span class="sd">  thought of as running a pmap over a single array sharded across processes,</span>
<span class="sd">  where each process &quot;sees&quot; only its local shard of the input and output. The</span>
<span class="sd">  SPMD model requires that the same multi-process pmaps must be run in the same</span>
<span class="sd">  order on all devices, but they can be interspersed with arbitrary operations</span>
<span class="sd">  running in a single process.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be mapped over argument axes. Its arguments and return</span>
<span class="sd">      value should be arrays, scalars, or (nested) standard Python containers</span>
<span class="sd">      (tuple/list/dict) thereof. Positional arguments indicated by</span>
<span class="sd">      ``static_broadcasted_argnums`` can be anything at all, provided they are</span>
<span class="sd">      hashable and have an equality operation defined.</span>
<span class="sd">    axis_name: Optional, a hashable Python object used to identify the mapped</span>
<span class="sd">      axis so that parallel collectives can be applied.</span>
<span class="sd">    in_axes: A non-negative integer, None, or nested Python container thereof</span>
<span class="sd">      that specifies which axes of positional arguments to map over. Arguments</span>
<span class="sd">      passed as keywords are always mapped over their leading axis (i.e. axis</span>
<span class="sd">      index 0). See :py:func:`vmap` for details.</span>
<span class="sd">    out_axes: A non-negative integer, None, or nested Python container thereof</span>
<span class="sd">      indicating where the mapped axis should appear in the output. All outputs</span>
<span class="sd">      with a mapped axis must have a non-None ``out_axes`` specification</span>
<span class="sd">      (see :py:func:`vmap`).</span>
<span class="sd">    static_broadcasted_argnums: An int or collection of ints specifying which</span>
<span class="sd">      positional arguments to treat as static (compile-time constant).</span>
<span class="sd">      Operations that only depend on static arguments will be constant-folded.</span>
<span class="sd">      Calling the pmapped function with different values for these constants</span>
<span class="sd">      will trigger recompilation. If the pmapped function is called with fewer</span>
<span class="sd">      positional arguments than indicated by ``static_argnums`` then an error is</span>
<span class="sd">      raised. Each of the static arguments will be broadcasted to all devices.</span>
<span class="sd">      Arguments that are not arrays or containers thereof must be marked as</span>
<span class="sd">      static. Defaults to ().</span>

<span class="sd">      Static arguments must be hashable, meaning both ``__hash__`` and</span>
<span class="sd">      ``__eq__`` are implemented, and should be immutable.</span>

<span class="sd">    devices: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a sequence of Devices to map over. (Available devices can be</span>
<span class="sd">      retrieved via jax.devices()). Must be given identically for each process</span>
<span class="sd">      in multi-process settings (and will therefore include devices across</span>
<span class="sd">      processes). If specified, the size of the mapped axis must be equal to</span>
<span class="sd">      the number of devices in the sequence local to the given process. Nested</span>
<span class="sd">      :py:func:`pmap` s with ``devices`` specified in either the inner or outer</span>
<span class="sd">      :py:func:`pmap` are not yet supported.</span>
<span class="sd">    backend: This is an experimental feature and the API is likely to change.</span>
<span class="sd">      Optional, a string representing the XLA backend. &#39;cpu&#39;, &#39;gpu&#39;, or &#39;tpu&#39;.</span>
<span class="sd">    axis_size: Optional; the size of the mapped axis.</span>
<span class="sd">    donate_argnums: Specify which positional argument buffers are &quot;donated&quot; to</span>
<span class="sd">      the computation. It is safe to donate argument buffers if you no longer need</span>
<span class="sd">      them once the computation has finished. In some cases XLA can make use of</span>
<span class="sd">      donated buffers to reduce the amount of memory needed to perform a</span>
<span class="sd">      computation, for example recycling one of your input buffers to store a</span>
<span class="sd">      result. You should not reuse buffers that you donate to a computation, JAX</span>
<span class="sd">      will raise an error if you try to.</span>
<span class="sd">      Note that donate_argnums only work for positional arguments, and keyword</span>
<span class="sd">      arguments will not be donated.</span>

<span class="sd">      For more details on buffer donation see the</span>
<span class="sd">      [FAQ](https://jax.readthedocs.io/en/latest/faq.html#buffer-donation).</span>

<span class="sd">    global_arg_shapes: Optional, must be set when using pmap(sharded_jit) and</span>
<span class="sd">      the partitioned values span multiple processes. The global cross-process</span>
<span class="sd">      per-replica shape of each argument, i.e. does not include the leading</span>
<span class="sd">      pmapped dimension. Can be None for replicated arguments. This API is</span>
<span class="sd">      likely to change in the future.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A parallelized version of ``fun`` with arguments that correspond to those of</span>
<span class="sd">    ``fun`` but with extra array axes at positions indicated by ``in_axes`` and</span>
<span class="sd">    with output that has an additional leading array axis (with the same size).</span>

<span class="sd">  For example, assuming 8 XLA devices are available, :py:func:`pmap` can be used</span>
<span class="sd">  as a map along a leading array axis:</span>

<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(lambda x: x ** 2)(jnp.arange(8))  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [0, 1, 4, 9, 16, 25, 36, 49]</span>

<span class="sd">  When the leading dimension is smaller than the number of available devices JAX</span>
<span class="sd">  will simply run on a subset of devices:</span>

<span class="sd">  &gt;&gt;&gt; x = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2))</span>
<span class="sd">  &gt;&gt;&gt; y = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) ** 2</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(jnp.dot)(x, y)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [[[    4.     9.]</span>
<span class="sd">    [   12.    29.]]</span>
<span class="sd">   [[  244.   345.]</span>
<span class="sd">    [  348.   493.]]</span>
<span class="sd">   [[ 1412.  1737.]</span>
<span class="sd">    [ 1740.  2141.]]]</span>

<span class="sd">  If your leading dimension is larger than the number of available devices you</span>
<span class="sd">  will get an error:</span>

<span class="sd">  &gt;&gt;&gt; pmap(lambda x: x ** 2)(jnp.arange(9))  # doctest: +SKIP</span>
<span class="sd">  ValueError: ... requires 9 replicas, but only 8 XLA devices are available</span>

<span class="sd">  As with :py:func:`vmap`, using ``None`` in ``in_axes`` indicates that an</span>
<span class="sd">  argument doesn&#39;t have an extra axis and should be broadcasted, rather than</span>
<span class="sd">  mapped, across the replicas:</span>

<span class="sd">  &gt;&gt;&gt; x, y = jnp.arange(2.), 4.</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None))(x, y)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  ([4., 5.], [8., 8.])</span>

<span class="sd">  Note that :py:func:`pmap` always returns values mapped over their leading axis,</span>
<span class="sd">  equivalent to using ``out_axes=0`` in :py:func:`vmap`.</span>

<span class="sd">  In addition to expressing pure maps, :py:func:`pmap` can also be used to express</span>
<span class="sd">  parallel single-program multiple-data (SPMD) programs that communicate via</span>
<span class="sd">  collective operations. For example:</span>

<span class="sd">  &gt;&gt;&gt; f = lambda x: x / jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(f, axis_name=&#39;i&#39;)(jnp.arange(4.))  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [ 0.          0.16666667  0.33333334  0.5       ]</span>
<span class="sd">  &gt;&gt;&gt; print(out.sum())  # doctest: +SKIP</span>
<span class="sd">  1.0</span>

<span class="sd">  In this example, ``axis_name`` is a string, but it can be any Python object</span>
<span class="sd">  with ``__hash__`` and ``__eq__`` defined.</span>

<span class="sd">  The argument ``axis_name`` to :py:func:`pmap` names the mapped axis so that</span>
<span class="sd">  collective operations, like :func:`jax.lax.psum`, can refer to it. Axis names</span>
<span class="sd">  are important particularly in the case of nested :py:func:`pmap` functions,</span>
<span class="sd">  where collective operations can operate over distinct axes:</span>

<span class="sd">  &gt;&gt;&gt; from functools import partial</span>
<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;rows&#39;)</span>
<span class="sd">  ... @partial(pmap, axis_name=&#39;cols&#39;)</span>
<span class="sd">  ... def normalize(x):</span>
<span class="sd">  ...   row_normed = x / jax.lax.psum(x, &#39;rows&#39;)</span>
<span class="sd">  ...   col_normed = x / jax.lax.psum(x, &#39;cols&#39;)</span>
<span class="sd">  ...   doubly_normed = x / jax.lax.psum(x, (&#39;rows&#39;, &#39;cols&#39;))</span>
<span class="sd">  ...   return row_normed, col_normed, doubly_normed</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; x = jnp.arange(8.).reshape((4, 2))</span>
<span class="sd">  &gt;&gt;&gt; row_normed, col_normed, doubly_normed = normalize(x)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(row_normed.sum(0))  # doctest: +SKIP</span>
<span class="sd">  [ 1.  1.]</span>
<span class="sd">  &gt;&gt;&gt; print(col_normed.sum(1))  # doctest: +SKIP</span>
<span class="sd">  [ 1.  1.  1.  1.]</span>
<span class="sd">  &gt;&gt;&gt; print(doubly_normed.sum((0, 1)))  # doctest: +SKIP</span>
<span class="sd">  1.0</span>

<span class="sd">  On multi-process platforms, collective operations operate over all devices,</span>
<span class="sd">  including those on other processes. For example, assuming the following code</span>
<span class="sd">  runs on two processes with 4 XLA devices each:</span>

<span class="sd">  &gt;&gt;&gt; f = lambda x: x + jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt; data = jnp.arange(4) if jax.process_index() == 0 else jnp.arange(4, 8)</span>
<span class="sd">  &gt;&gt;&gt; out = pmap(f, axis_name=&#39;i&#39;)(data)  # doctest: +SKIP</span>
<span class="sd">  &gt;&gt;&gt; print(out)  # doctest: +SKIP</span>
<span class="sd">  [28 29 30 31] # on process 0</span>
<span class="sd">  [32 33 34 35] # on process 1</span>

<span class="sd">  Each process passes in a different length-4 array, corresponding to its 4</span>
<span class="sd">  local devices, and the psum operates over all 8 values. Conceptually, the two</span>
<span class="sd">  length-4 arrays can be thought of as a sharded length-8 array (in this example</span>
<span class="sd">  equivalent to jnp.arange(8)) that is mapped over, with the length-8 mapped</span>
<span class="sd">  axis given name &#39;i&#39;. The pmap call on each process then returns the</span>
<span class="sd">  corresponding length-4 output shard.</span>

<span class="sd">  The ``devices`` argument can be used to specify exactly which devices are used</span>
<span class="sd">  to run the parallel computation. For example, again assuming a single process</span>
<span class="sd">  with 8 devices, the following code defines two parallel computations, one</span>
<span class="sd">  which runs on the first six devices and one on the remaining two:</span>

<span class="sd">  &gt;&gt;&gt; from functools import partial</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;i&#39;, devices=jax.devices()[:6])</span>
<span class="sd">  ... def f1(x):</span>
<span class="sd">  ...   return x / jax.lax.psum(x, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; @partial(pmap, axis_name=&#39;i&#39;, devices=jax.devices()[-2:])</span>
<span class="sd">  ... def f2(x):</span>
<span class="sd">  ...   return jax.lax.psum(x ** 2, axis_name=&#39;i&#39;)</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; print(f1(jnp.arange(6.)))  # doctest: +SKIP</span>
<span class="sd">  [0.         0.06666667 0.13333333 0.2        0.26666667 0.33333333]</span>
<span class="sd">  &gt;&gt;&gt; print(f2(jnp.array([2., 3.])))  # doctest: +SKIP</span>
<span class="sd">  [ 13.  13.]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">experimental_cpp_pmap</span><span class="p">:</span>
    <span class="n">func</span> <span class="o">=</span> <span class="n">_cpp_pmap</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">func</span> <span class="o">=</span> <span class="n">_python_pmap</span>

  <span class="k">return</span> <span class="n">func</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span>
      <span class="n">axis_name</span><span class="p">,</span>
      <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
      <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
      <span class="n">static_broadcasted_argnums</span><span class="o">=</span><span class="n">static_broadcasted_argnums</span><span class="p">,</span>
      <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
      <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
      <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
      <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
      <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">global_arg_shapes</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">PmapCallInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">flat_fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span>
  <span class="n">in_tree</span><span class="p">:</span> <span class="n">PyTreeDef</span>
  <span class="n">out_tree</span><span class="p">:</span> <span class="n">PyTreeDef</span>
  <span class="n">flat_args</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
  <span class="n">in_axes_flat</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
  <span class="n">local_axis_size</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">global_arg_shapes_flat</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span>
  <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">HashableFunction</span>
  <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">_prepare_pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
                  <span class="n">donate_tuple</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">in_devices</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">static_broadcasted_tuple</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">static_broadcasted_tuple</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;pmapped function has static_broadcasted_argnums=</span><span class="si">{</span><span class="n">static_broadcasted_tuple</span><span class="si">}</span><span class="s2">&quot;</span>
          <span class="sa">f</span><span class="s2">&quot; but was called with only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span><span class="si">}</span><span class="s2"> positional &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;argument</span><span class="si">{</span><span class="s1">&#39;s&#39;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">. &quot;</span>
          <span class="s2">&quot;All static broadcasted arguments must be passed positionally.&quot;</span><span class="p">)</span>
    <span class="n">dyn_argnums</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
                   <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">static_broadcasted_tuple</span><span class="p">]</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">dyn_args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dyn_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">in_axes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dyn_argnums</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="n">in_axes</span>
      <span class="n">dyn_global_arg_shapes</span> <span class="o">=</span> <span class="n">global_arg_shapes</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">global_arg_shapes</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
      <span class="n">dyn_global_arg_shapes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">global_arg_shapes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dyn_argnums</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dyn_global_arg_shapes</span> <span class="o">=</span> <span class="n">global_arg_shapes</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">dyn_args</span><span class="p">,</span> <span class="n">dyn_in_axes</span> <span class="o">=</span> <span class="n">args</span><span class="p">,</span> <span class="n">in_axes</span>
    <span class="n">dyn_global_arg_shapes</span> <span class="o">=</span> <span class="n">global_arg_shapes</span>
  <span class="n">args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">donate_tuple</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">donation_vector</span><span class="p">(</span><span class="n">donate_tuple</span><span class="p">,</span> <span class="n">dyn_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="p">(</span><span class="kc">False</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
  <span class="n">in_axes_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;pmap in_axes&quot;</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">(</span><span class="n">dyn_in_axes</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
  <span class="n">global_arg_shapes_flat</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_axes</span><span class="p">(</span>
      <span class="s2">&quot;pmap global_arg_shapes&quot;</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">(</span><span class="n">dyn_global_arg_shapes</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
      <span class="n">kws</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
  <span class="n">local_axis_size</span> <span class="o">=</span> <span class="n">_mapped_axis_size</span><span class="p">(</span>
      <span class="n">in_tree</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">in_axes_flat</span><span class="p">,</span> <span class="s2">&quot;pmap&quot;</span><span class="p">,</span> <span class="n">kws</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">out_axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">out_axis</span> <span class="ow">in</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;None out_axes in pmap are not supported yet&quot;</span><span class="p">)</span>
  <span class="c1"># NOTE: We don&#39;t put out_tree() in the closure, because it&#39;s (1) non-hashable,</span>
  <span class="c1">#       (2) depends deterministically on flat_fun (at least that&#39;s the assumption</span>
  <span class="c1">#       that we make).</span>
  <span class="k">if</span> <span class="n">out_axes</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># TODO(apaszke,mattjj): flatten_axes assumes that the output pytree is</span>
    <span class="c1">#   functorial (i.e. it can hold leaves of any type), but some user code</span>
    <span class="c1">#   breaks this assumption. This is a stop-gap solution to keep the old</span>
    <span class="c1">#   out_axes == 0 path working as we look for a better solution.</span>
    <span class="n">out_axes_thunk</span> <span class="o">=</span> <span class="n">HashableFunction</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="n">out_tree</span><span class="p">()</span><span class="o">.</span><span class="n">num_leaves</span><span class="p">,</span>
        <span class="n">closure</span><span class="o">=</span><span class="n">out_axes</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># out_axes_thunk closes over the out_axes, they are flattened here to make</span>
    <span class="c1"># them hashable.</span>
    <span class="n">out_axes_leaves</span><span class="p">,</span> <span class="n">out_axes_treedef</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)</span>
    <span class="n">out_axes_thunk</span> <span class="o">=</span> <span class="n">HashableFunction</span><span class="p">(</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_axes</span><span class="p">(</span><span class="s2">&quot;pmap out_axes&quot;</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">(),</span>
                                    <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_axes_treedef</span><span class="p">,</span>
                                                  <span class="nb">list</span><span class="p">(</span><span class="n">out_axes_leaves</span><span class="p">)))),</span>
        <span class="n">closure</span><span class="o">=</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">out_axes_leaves</span><span class="p">),</span> <span class="n">out_axes_treedef</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">PmapCallInfo</span><span class="p">(</span><span class="n">flat_fun</span><span class="o">=</span><span class="n">flat_fun</span><span class="p">,</span>
                      <span class="n">in_tree</span><span class="o">=</span><span class="n">in_tree</span><span class="p">,</span>
                      <span class="n">out_tree</span><span class="o">=</span><span class="n">out_tree</span><span class="p">,</span>
                      <span class="n">flat_args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
                      <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span>
                      <span class="n">in_axes_flat</span><span class="o">=</span><span class="n">in_axes_flat</span><span class="p">,</span>
                      <span class="n">local_axis_size</span><span class="o">=</span><span class="n">local_axis_size</span><span class="p">,</span>
                      <span class="n">global_arg_shapes_flat</span><span class="o">=</span><span class="n">global_arg_shapes_flat</span><span class="p">,</span>
                      <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">out_axes_thunk</span><span class="p">,</span>
                      <span class="n">devices</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">in_devices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">in_devices</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_get_f_mapped</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AxisName</span><span class="p">],</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_tuple</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]],</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">donate_tuple</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]],</span>
  <span class="p">):</span>
  <span class="k">def</span> <span class="nf">pmap_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_prepare_pmap</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">:</span>
      <span class="n">_check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">xla_pmap</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="n">axis_name</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">local_axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">in_axes_flat</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">out_axes_thunk</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">donated_invars</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">global_arg_shapes_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out</span>

  <span class="k">return</span> <span class="n">pmap_f</span>


<span class="k">def</span> <span class="nf">_shared_code_pmap</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span>
                      <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">):</span>
  <span class="c1"># axis_size is an optional integer representing the global axis size.  The</span>
  <span class="c1"># aggregate size (across all processes) size of the mapped axis must match the</span>
  <span class="c1"># given value.</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">axis_name</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">_TempAxisName</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span> <span class="k">if</span> <span class="n">axis_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">axis_name</span>
  <span class="n">static_broadcasted_tuple</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_broadcasted_argnums</span><span class="p">)</span>
  <span class="n">donate_tuple</span> <span class="o">=</span> <span class="n">rebase_donate_argnums</span><span class="p">(</span>
      <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">donate_argnums</span><span class="p">),</span> <span class="n">static_broadcasted_tuple</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">in_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pmap in_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">in_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">out_axes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pmap out_axes must be an int, None, or (nested) container &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;with those types as leaves, but got </span><span class="si">{</span><span class="n">out_axes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span>


<span class="k">def</span> <span class="nf">_python_pmap</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Wrapped</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;The Python only implementation.&quot;&quot;&quot;</span>
  <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span> <span class="o">=</span> <span class="n">_shared_code_pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span>
      <span class="n">out_axes</span><span class="p">)</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">pmap_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f_pmapped_</span> <span class="o">=</span> <span class="n">_get_f_mapped</span><span class="p">(</span>
        <span class="n">fun</span><span class="o">=</span><span class="n">fun</span><span class="p">,</span>
        <span class="n">axis_name</span><span class="o">=</span><span class="n">axis_name</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
        <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
        <span class="n">static_broadcasted_tuple</span><span class="o">=</span><span class="n">static_broadcasted_tuple</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">global_arg_shapes</span><span class="p">,</span>
        <span class="n">donate_tuple</span><span class="o">=</span><span class="n">donate_tuple</span><span class="p">)</span>

    <span class="n">out_tree</span><span class="p">,</span> <span class="n">out_flat</span> <span class="o">=</span> <span class="n">f_pmapped_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_flat</span><span class="p">)</span>

  <span class="n">pmap_f</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">_pmap_lower</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span>
      <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">pmap_f</span>


<span class="k">class</span> <span class="nc">_PmapFastpathData</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">version</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># For forward and backward compatibility</span>
  <span class="n">xla_executable</span><span class="p">:</span> <span class="n">xla</span><span class="o">.</span><span class="n">XlaLoadedExecutable</span>
  <span class="n">in_handler</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">out_handler</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">out_pytree_def</span><span class="p">:</span> <span class="n">Any</span>
  <span class="c1"># Data needed to handle the inputs.</span>
  <span class="n">input_sharding_specs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pxla</span><span class="o">.</span><span class="n">ShardingSpec</span><span class="p">]</span>
  <span class="n">input_devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span>
  <span class="n">input_indices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pxla</span><span class="o">.</span><span class="n">Index</span><span class="p">]</span>
  <span class="n">input_array_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="c1"># Data needed to build the ShardedDeviceArray from C++.</span>
  <span class="n">out_sharding_specs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pxla</span><span class="o">.</span><span class="n">ShardingSpec</span><span class="p">]</span>
  <span class="n">out_indices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pxla</span><span class="o">.</span><span class="n">Index</span><span class="p">]</span>
  <span class="n">out_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">out_array_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>
  <span class="n">out_committed</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_cpp_pmap</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">static_broadcasted_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F811</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
  <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span> <span class="o">=</span> <span class="n">_shared_code_pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span>
      <span class="n">out_axes</span><span class="p">)</span>
  <span class="k">del</span> <span class="n">static_broadcasted_argnums</span><span class="p">,</span> <span class="n">donate_argnums</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">cache_miss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f_pmapped_</span> <span class="o">=</span> <span class="n">_get_f_mapped</span><span class="p">(</span>
        <span class="n">fun</span><span class="o">=</span><span class="n">fun</span><span class="p">,</span>
        <span class="n">axis_name</span><span class="o">=</span><span class="n">axis_name</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
        <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axes</span><span class="p">,</span>
        <span class="n">static_broadcasted_tuple</span><span class="o">=</span><span class="n">static_broadcasted_tuple</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">global_arg_shapes</span><span class="p">,</span>
        <span class="n">donate_tuple</span><span class="o">=</span><span class="n">donate_tuple</span><span class="p">)</span>

    <span class="n">out_tree</span><span class="p">,</span> <span class="n">out_flat</span> <span class="o">=</span> <span class="n">f_pmapped_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">out_pytree_def</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_pytree_def</span><span class="p">,</span> <span class="n">out_flat</span><span class="p">)</span>

    <span class="c1">### Decide whether we can support the C++ fast path</span>
    <span class="n">execute</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">execute</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">parallel_callable</span><span class="o">.</span><span class="n">most_recent_entry</span><span class="p">()</span>
    <span class="n">use_fastpath</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">execute</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="c1"># We don&#39;t support JAX extension backends.</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">execute</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pxla</span><span class="o">.</span><span class="n">ExecuteReplicated</span><span class="p">)</span> <span class="ow">and</span>
        <span class="c1"># TODO(sharadmv): Enable effects in replicated computation</span>
        <span class="ow">not</span> <span class="n">execute</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">has_unordered_effects</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">execute</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">has_host_callbacks</span> <span class="ow">and</span>
        <span class="c1"># No tracers in the outputs. Checking for ShardedDeviceArray should be</span>
        <span class="c1"># sufficient, but we use the more general `DeviceArray`.</span>
        <span class="nb">all</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device_array</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">)</span> <span class="ow">or</span>
            <span class="n">xc</span><span class="o">.</span><span class="n">_version</span> <span class="o">&gt;=</span> <span class="mi">96</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">))</span>

    <span class="c1">### If we can use the fastpath, we return required info to the caller.</span>
    <span class="k">if</span> <span class="n">use_fastpath</span><span class="p">:</span>
      <span class="n">execute_replicated</span> <span class="o">=</span> <span class="n">execute</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">out_handler</span> <span class="o">=</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">out_handler</span>
      <span class="n">in_handler</span> <span class="o">=</span> <span class="n">execute_replicated</span><span class="o">.</span><span class="n">in_handler</span>
      <span class="n">out_indices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">devices_indices_map</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
                     <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">out_handler</span><span class="o">.</span><span class="n">out_shardings</span><span class="p">,</span> <span class="n">out_handler</span><span class="o">.</span><span class="n">out_avals</span><span class="p">)]</span>

      <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
        <span class="n">out_array_shardings</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">sharding</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
        <span class="n">out_committed</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">_committed</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">out_flat</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out_array_shardings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">out_committed</span> <span class="o">=</span> <span class="p">[]</span>

      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="n">_PmapFastpathData</span><span class="p">(</span>
          <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">xla_executable</span><span class="o">=</span><span class="n">execute_replicated</span><span class="o">.</span><span class="n">xla_executable</span><span class="p">,</span>
          <span class="n">in_handler</span><span class="o">=</span><span class="n">in_handler</span><span class="p">,</span>
          <span class="n">out_handler</span><span class="o">=</span><span class="n">out_handler</span><span class="p">,</span>
          <span class="n">out_pytree_def</span><span class="o">=</span><span class="n">out_pytree_def</span><span class="p">,</span>
          <span class="n">input_sharding_specs</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">sharding_spec</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">in_handler</span><span class="o">.</span><span class="n">in_shardings</span><span class="p">],</span>
          <span class="n">input_devices</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">local_devices</span><span class="p">,</span>
          <span class="n">input_indices</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">input_indices</span><span class="p">,</span>
          <span class="n">input_array_shardings</span><span class="o">=</span><span class="n">in_handler</span><span class="o">.</span><span class="n">in_shardings</span><span class="p">,</span>
          <span class="n">out_sharding_specs</span><span class="o">=</span><span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">sharding_spec</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">out_handler</span><span class="o">.</span><span class="n">out_shardings</span><span class="p">],</span>
          <span class="n">out_indices</span><span class="o">=</span><span class="n">out_indices</span><span class="p">,</span>
          <span class="n">out_avals</span><span class="o">=</span><span class="n">out_handler</span><span class="o">.</span><span class="n">out_avals</span><span class="p">,</span>
          <span class="n">out_array_shardings</span><span class="o">=</span><span class="n">out_array_shardings</span><span class="p">,</span>
          <span class="n">out_committed</span><span class="o">=</span><span class="n">out_committed</span><span class="p">,</span>
      <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
      <span class="n">fastpath_data</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">fastpath_data</span>

  <span class="n">cpp_mapped_f</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">cache_miss</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
      <span class="n">partial</span><span class="p">(</span><span class="n">pxla</span><span class="o">.</span><span class="n">_shard_arg</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">pxla</span><span class="o">.</span><span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pmap</span><span class="p">))</span>

  <span class="n">pmap_f</span> <span class="o">=</span> <span class="n">wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)(</span><span class="n">cpp_mapped_f</span><span class="p">)</span>

  <span class="n">pmap_f</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">_pmap_lower</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span>
      <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">pmap_f</span>


<span class="k">def</span> <span class="nf">_pmap_lower</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span>
                <span class="n">devices</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
  <span class="sd">&quot;&quot;&quot;Make a ``lower`` method for pmapped functions.&quot;&quot;&quot;</span>
  <span class="c1"># If the function we returned from ``pmap`` were a class instance,</span>
  <span class="c1"># this might naturally be a method, with ``fun`` as a ``self`` and</span>
  <span class="c1"># all the other arguments stored as attributes.</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">lower</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Lower a parallel-mapped form of this function for the given arguments.</span>

<span class="sd">    A parallel-mapped and lowered function is staged out of Python and</span>
<span class="sd">    translated to a compiler&#39;s input language, possibly in a</span>
<span class="sd">    backend-dependent manner. It is ready for compilation but is not yet</span>
<span class="sd">    compiled. It represents a function intended for SPMD execution on</span>
<span class="sd">    multiple devices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A ``Lowered`` instance representing the post-map lowering.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">_prepare_pmap</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">static_broadcasted_tuple</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">abstract_args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">flat_args</span><span class="p">))</span>
    <span class="n">computation</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">lower_parallel_callable</span><span class="p">(</span>
        <span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span>
        <span class="n">axis_size</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">local_axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">flat_fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="n">in_axes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">in_axes_flat</span><span class="p">,</span>
        <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">out_axes_thunk</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">global_arg_shapes_flat</span><span class="p">,</span>
        <span class="n">avals</span><span class="o">=</span><span class="n">abstract_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stages</span><span class="o">.</span><span class="n">Lowered</span><span class="o">.</span><span class="n">from_flat_info</span><span class="p">(</span>
        <span class="n">computation</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">abstract_args</span><span class="p">,</span> <span class="n">donate_tuple</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">out_tree</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">lower</span>

<div class="viewcode-block" id="jvp"><a class="viewcode-back" href="../../../_autosummary/jax.jvp.html#jax.jvp">[docs]</a><span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Computes a (forward-mode) Jacobian-vector product of ``fun``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard Python container of arrays or scalars.</span>
<span class="sd">    primals: The primal values at which the Jacobian of ``fun`` should be</span>
<span class="sd">      evaluated. Should be either a tuple or a list of arguments,</span>
<span class="sd">      and its length should be equal to the number of positional parameters of</span>
<span class="sd">      ``fun``.</span>
<span class="sd">    tangents: The tangent vector for which the Jacobian-vector product should be</span>
<span class="sd">      evaluated. Should be either a tuple or a list of tangents, with the same</span>
<span class="sd">      tree structure and array shapes as ``primals``.</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">     first element is considered the output of the mathematical function to be</span>
<span class="sd">     differentiated and the second element is auxiliary data. Default False.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If ``has_aux`` is ``False``, returns a ``(primals_out, tangents_out)`` pair,</span>
<span class="sd">    where ``primals_out`` is ``fun(*primals)``,</span>
<span class="sd">    and ``tangents_out`` is the Jacobian-vector product of</span>
<span class="sd">    ``function`` evaluated at ``primals`` with ``tangents``. The</span>
<span class="sd">    ``tangents_out`` value has the same Python tree structure and shapes as</span>
<span class="sd">    ``primals_out``. If ``has_aux`` is ``True``, returns a</span>
<span class="sd">    ``(primals_out, tangents_out, aux)`` tuple where ``aux``</span>
<span class="sd">    is the auxiliary data returned by ``fun``.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; primals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,))</span>
<span class="sd">  &gt;&gt;&gt; print(primals)</span>
<span class="sd">  0.09983342</span>
<span class="sd">  &gt;&gt;&gt; print(tangents)</span>
<span class="sd">  0.19900084</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_jvp</span><span class="p">(</span><span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_jvp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variant of jvp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">or</span>
      <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tangents</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp must be tuples or lists; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;found </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="n">ps_flat</span><span class="p">,</span> <span class="n">tree_def</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="n">ts_flat</span><span class="p">,</span> <span class="n">tree_def_2</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">tangents</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">tree_def</span> <span class="o">!=</span> <span class="n">tree_def_2</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp must have the same tree &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;structure; primals have tree structure </span><span class="si">{</span><span class="n">tree_def</span><span class="si">}</span><span class="s2"> whereas tangents have &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;tree structure </span><span class="si">{</span><span class="n">tree_def_2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="o">!=</span> <span class="n">_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;primal and tangent arguments to jax.jvp do not match; &quot;</span>
                      <span class="s2">&quot;dtypes must be equal, or in case of int/bool primal dtype &quot;</span>
                      <span class="s2">&quot;the tangent dtype must be float0.&quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Got primal dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2"> and so expected tangent dtype &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="si">}</span><span class="s2">, but got &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;tangent dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;jvp called with different primal and tangent shapes;&quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;Got primal shape </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2"> and tangent shape as </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">tree_def</span><span class="p">)</span>
    <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_tangents</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">)</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">)</span>
    <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_aux_trees</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">tree_def</span><span class="p">)</span>
    <span class="n">jvp_fun</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_tangents</span> <span class="o">=</span> <span class="n">jvp_fun</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="n">ps_flat</span><span class="p">,</span> <span class="n">ts_flat</span><span class="p">)</span>
    <span class="n">out_tree</span><span class="p">,</span> <span class="n">aux_tree</span> <span class="o">=</span> <span class="n">out_aux_trees</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_tangents</span><span class="p">),</span>
            <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">aux_tree</span><span class="p">,</span> <span class="n">aux</span><span class="p">()))</span>

<div class="viewcode-block" id="linearize"><a class="viewcode-back" href="../../../_autosummary/jax.linearize.html#jax.linearize">[docs]</a><span class="k">def</span> <span class="nf">linearize</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Produces a linear approximation to ``fun`` using :py:func:`jvp` and partial eval.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard python container of arrays or scalars.</span>
<span class="sd">    primals: The primal values at which the Jacobian of ``fun`` should be</span>
<span class="sd">      evaluated. Should be a tuple of arrays, scalar, or standard Python</span>
<span class="sd">      container thereof. The length of the tuple is equal to the number of</span>
<span class="sd">      positional parameters of ``fun``.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pair where the first element is the value of ``f(*primals)`` and the</span>
<span class="sd">    second element is a function that evaluates the (forward-mode)</span>
<span class="sd">    Jacobian-vector product of ``fun`` evaluated at ``primals`` without re-doing</span>
<span class="sd">    the linearization work.</span>

<span class="sd">  In terms of values computed, :py:func:`linearize` behaves much like a curried</span>
<span class="sd">  :py:func:`jvp`, where these two code blocks compute the same values::</span>

<span class="sd">    y, out_tangent = jax.jvp(f, (x,), (in_tangent,))</span>

<span class="sd">    y, f_jvp = jax.linearize(f, x)</span>
<span class="sd">    out_tangent = f_jvp(in_tangent)</span>

<span class="sd">  However, the difference is that :py:func:`linearize` uses partial evaluation</span>
<span class="sd">  so that the function ``f`` is not re-linearized on calls to ``f_jvp``. In</span>
<span class="sd">  general that means the memory usage scales with the size of the computation,</span>
<span class="sd">  much like in reverse-mode. (Indeed, :py:func:`linearize` has a similar</span>
<span class="sd">  signature to :py:func:`vjp`!)</span>

<span class="sd">  This function is mainly useful if you want to apply ``f_jvp`` multiple times,</span>
<span class="sd">  i.e. to evaluate a pushforward for many different input tangent vectors at the</span>
<span class="sd">  same linearization point. Moreover if all the input tangent vectors are known</span>
<span class="sd">  at once, it can be more efficient to vectorize using :py:func:`vmap`, as in::</span>

<span class="sd">    pushfwd = partial(jvp, f, (x,))</span>
<span class="sd">    y, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,))</span>

<span class="sd">  By using :py:func:`vmap` and :py:func:`jvp` together like this we avoid the stored-linearization</span>
<span class="sd">  memory cost that scales with the depth of the computation, which is incurred</span>
<span class="sd">  by both :py:func:`linearize` and :py:func:`vjp`.</span>

<span class="sd">  Here&#39;s a more complete example of using :py:func:`linearize`:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return 3. * jnp.sin(x) + jnp.cos(x / 2.)</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; jax.jvp(f, (2.,), (3.,))</span>
<span class="sd">  (DeviceArray(3.26819, dtype=float32, weak_type=True), DeviceArray(-5.00753, dtype=float32, weak_type=True))</span>
<span class="sd">  &gt;&gt;&gt; y, f_jvp = jax.linearize(f, 2.)</span>
<span class="sd">  &gt;&gt;&gt; print(y)</span>
<span class="sd">  3.2681944</span>
<span class="sd">  &gt;&gt;&gt; print(f_jvp(3.))</span>
<span class="sd">  -5.007528</span>
<span class="sd">  &gt;&gt;&gt; print(f_jvp(4.))</span>
<span class="sd">  -6.676704</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">primals</span><span class="p">,</span> <span class="p">{}))</span>
  <span class="n">jaxtree_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">out_primals</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">linearize</span><span class="p">(</span><span class="n">jaxtree_fun</span><span class="p">,</span> <span class="o">*</span><span class="n">primals_flat</span><span class="p">)</span>
  <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
  <span class="n">out_primal_py</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primals</span><span class="p">)</span>
  <span class="n">primal_avals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">))</span>
  <span class="c1"># Ensure that lifted_jvp is a PyTree</span>
  <span class="n">lifted_jvp</span> <span class="o">=</span> <span class="n">Partial</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_lift_linearized</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">primal_avals</span><span class="p">,</span>
                               <span class="p">(</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">out_tree</span><span class="p">),</span> <span class="n">out_pvals</span><span class="p">),</span> <span class="n">consts</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">lifted_jvp</span></div>

<span class="k">def</span> <span class="nf">_lift_linearized</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">primal_avals</span><span class="p">,</span> <span class="n">io_tree</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="o">*</span><span class="n">py_args</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="o">*</span><span class="n">tangents</span><span class="p">):</span>
    <span class="n">tangent_avals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">primal_aval</span><span class="p">,</span> <span class="n">tangent_aval</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">primal_avals</span><span class="p">,</span> <span class="n">tangent_avals</span><span class="p">):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">core</span><span class="o">.</span><span class="n">typecompat</span><span class="p">(</span><span class="n">primal_aval</span><span class="o">.</span><span class="n">at_least_vspace</span><span class="p">(),</span> <span class="n">tangent_aval</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linearized function called on tangent values inconsistent with &quot;</span>
                         <span class="s2">&quot;the original primal values: &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">tangent_aval</span><span class="si">}</span><span class="s2"> for primal aval </span><span class="si">{</span><span class="n">primal_aval</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">tangents_out</span> <span class="o">=</span> <span class="n">eval_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="o">*</span><span class="n">tangents</span><span class="p">)</span>
    <span class="n">tangents_out_</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">tangents_out</span><span class="p">)</span>
    <span class="n">full_out</span> <span class="o">=</span> <span class="p">[</span><span class="n">pval</span><span class="o">.</span><span class="n">get_known</span><span class="p">()</span> <span class="k">if</span> <span class="n">pval</span><span class="o">.</span><span class="n">is_known</span><span class="p">()</span> <span class="k">else</span> <span class="nb">next</span><span class="p">(</span><span class="n">tangents_out_</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pval</span> <span class="ow">in</span> <span class="n">out_pvals</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">next</span><span class="p">(</span><span class="n">tangents_out_</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">full_out</span>

  <span class="k">return</span> <span class="n">apply_flat_fun</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">io_tree</span><span class="p">,</span> <span class="o">*</span><span class="n">py_args</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_vjp_pullback_wrapper</span><span class="p">(</span><span class="n">cotangent_dtypes</span><span class="p">,</span> <span class="n">cotangent_shapes</span><span class="p">,</span>
                          <span class="n">io_tree</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">py_args</span><span class="p">):</span>
  <span class="n">in_tree_expected</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">io_tree</span>
  <span class="n">args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">py_args</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">in_tree</span> <span class="o">!=</span> <span class="n">in_tree_expected</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tree structure of cotangent input </span><span class="si">{</span><span class="n">in_tree</span><span class="si">}</span><span class="s2">, does not match structure of &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;primal output </span><span class="si">{</span><span class="n">in_tree_expected</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">ct_dtype</span><span class="p">,</span> <span class="n">ct_shape</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">cotangent_dtypes</span><span class="p">,</span> <span class="n">cotangent_shapes</span><span class="p">):</span>
    <span class="n">expected_tangent_dtype</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">arg</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">expected_tangent_dtype</span> <span class="o">!=</span> <span class="n">ct_dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Type of cotangent input to vjp pullback function (</span><span class="si">{</span><span class="n">ct_dtype</span><span class="si">}</span><span class="s2">) is not &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;the expected tangent type (</span><span class="si">{</span><span class="n">expected_tangent_dtype</span><span class="si">}</span><span class="s2">) of corresponding primal output &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;with dtype </span><span class="si">{</span><span class="n">_dtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ct_shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Shape of cotangent input to vjp pullback function </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
          <span class="s2">&quot;must be the same as the shape of corresponding primal input &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ct_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
  <span class="n">ans</span> <span class="o">=</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">ans</span><span class="p">)</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">T</span><span class="p">],</span>
        <span class="o">*</span><span class="n">primals</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
  <span class="o">...</span>

<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">]],</span> <span class="o">*</span><span class="n">primals</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">],</span>
        <span class="n">reduce_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">T</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">U</span><span class="p">]:</span>
  <span class="o">...</span>
<div class="viewcode-block" id="vjp"><a class="viewcode-back" href="../../../_autosummary/jax.vjp.html#jax.vjp">[docs]</a><span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="p">()</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
  <span class="sd">&quot;&quot;&quot;Compute a (reverse-mode) vector-Jacobian product of ``fun``.</span>

<span class="sd">  :py:func:`grad` is implemented as a special case of :py:func:`vjp`.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be differentiated. Its arguments should be arrays, scalars,</span>
<span class="sd">      or standard Python containers of arrays or scalars. It should return an</span>
<span class="sd">      array, scalar, or standard Python container of arrays or scalars.</span>
<span class="sd">    primals: A sequence of primal values at which the Jacobian of ``fun``</span>
<span class="sd">      should be evaluated. The length of ``primals`` should be equal to the</span>
<span class="sd">      number of positional parameters to ``fun``. Each primal value should be a</span>
<span class="sd">      tuple of arrays, scalar, or standard Python containers thereof.</span>
<span class="sd">    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the</span>
<span class="sd">     first element is considered the output of the mathematical function to be</span>
<span class="sd">     differentiated and the second element is auxiliary data. Default False.</span>
<span class="sd">    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and</span>
<span class="sd">      ``fun`` implicitly broadcasts a value over that axis, the backward pass</span>
<span class="sd">      will perform a ``psum`` of the corresponding gradient. Otherwise, the</span>
<span class="sd">      VJP will be per-example over named axes. For example, if ``&#39;batch&#39;``</span>
<span class="sd">      is a named batch axis, ``vjp(f, *args, reduce_axes=(&#39;batch&#39;,))`` will</span>
<span class="sd">      create a VJP function that sums over the batch while ``vjp(f, *args)``</span>
<span class="sd">      will create a per-example VJP.</span>

<span class="sd">  Returns:</span>
<span class="sd">    If ``has_aux`` is ``False``, returns a ``(primals_out, vjpfun)`` pair, where</span>
<span class="sd">    ``primals_out`` is ``fun(*primals)``.</span>
<span class="sd">    ``vjpfun`` is a function from a cotangent vector with the same shape as</span>
<span class="sd">    ``primals_out`` to a tuple of cotangent vectors with the same shape as</span>
<span class="sd">    ``primals``, representing the vector-Jacobian product of ``fun`` evaluated at</span>
<span class="sd">    ``primals``. If ``has_aux`` is ``True``, returns a</span>
<span class="sd">    ``(primals_out, vjpfun, aux)`` tuple where ``aux`` is the auxiliary data</span>
<span class="sd">    returned by ``fun``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x, y):</span>
<span class="sd">  ...   return jax.numpy.sin(x), jax.numpy.cos(y)</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; primals, f_vjp = jax.vjp(f, 0.5, 1.0)</span>
<span class="sd">  &gt;&gt;&gt; xbar, ybar = f_vjp((-0.7, 0.3))</span>
<span class="sd">  &gt;&gt;&gt; print(xbar)</span>
<span class="sd">  -0.61430776</span>
<span class="sd">  &gt;&gt;&gt; print(ybar)</span>
<span class="sd">  -0.2524413</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">reduce_axes</span> <span class="o">=</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">reduce_axes</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">_vjp</span><span class="p">(</span>
      <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="n">has_aux</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="p">()):</span>
  <span class="sd">&quot;&quot;&quot;Variant of vjp() that takes an lu.WrappedFun.&quot;&quot;&quot;</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">primals_flat</span><span class="p">:</span> <span class="n">_check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">out_primal</span><span class="p">,</span> <span class="n">out_vjp</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
    <span class="n">out_tree</span> <span class="o">=</span> <span class="n">out_tree</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_aux_trees</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs2</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">out_primal</span><span class="p">,</span> <span class="n">out_vjp</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span>
        <span class="n">flat_fun</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
    <span class="n">out_tree</span><span class="p">,</span> <span class="n">aux_tree</span> <span class="o">=</span> <span class="n">out_aux_trees</span><span class="p">()</span>
  <span class="n">out_primal_py</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">out_primal</span><span class="p">)</span>
  <span class="n">ct_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">primal_dtype_to_tangent_dtype</span><span class="p">(</span><span class="n">_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_primal</span><span class="p">]</span>
  <span class="n">ct_shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out_primal</span><span class="p">]</span>
  <span class="c1"># Ensure that vjp_py is a PyTree so that we can pass it from the forward to the</span>
  <span class="c1"># backward pass in a custom VJP.</span>
  <span class="n">vjp_py</span> <span class="o">=</span> <span class="n">Partial</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">_vjp_pullback_wrapper</span><span class="p">,</span>
                           <span class="n">ct_dtypes</span><span class="p">,</span> <span class="n">ct_shapes</span><span class="p">,</span>
                           <span class="p">(</span><span class="n">out_tree</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)),</span>
                   <span class="n">out_vjp</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_aux</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">vjp_py</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">out_primal_py</span><span class="p">,</span> <span class="n">vjp_py</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">aux_tree</span><span class="p">,</span> <span class="n">aux</span><span class="p">)</span>


<div class="viewcode-block" id="linear_transpose"><a class="viewcode-back" href="../../../_autosummary/jax.linear_transpose.html#jax.linear_transpose">[docs]</a><span class="k">def</span> <span class="nf">linear_transpose</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">primals</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="o">=</span><span class="p">())</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Transpose a function that is promised to be linear.</span>

<span class="sd">  For linear functions, this transformation is equivalent to ``vjp``, but</span>
<span class="sd">  avoids the overhead of computing the forward pass.</span>

<span class="sd">  The outputs of the transposed function will always have the exact same dtypes</span>
<span class="sd">  as ``primals``, even if some values are truncated (e.g., from complex to</span>
<span class="sd">  float, or from float64 to float32). To avoid truncation, use dtypes in</span>
<span class="sd">  ``primals`` that match the full range of desired outputs from the transposed</span>
<span class="sd">  function. Integer dtypes are not supported.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: the linear function to be transposed.</span>
<span class="sd">    *primals: a positional argument tuple of arrays, scalars, or (nested)</span>
<span class="sd">      standard Python containers (tuples, lists, dicts, namedtuples, i.e.,</span>
<span class="sd">      pytrees) of those types used for evaluating the shape/dtype of</span>
<span class="sd">      ``fun(*primals)``. These arguments may be real scalars/ndarrays, but that</span>
<span class="sd">      is not required: only the ``shape`` and ``dtype`` attributes are accessed.</span>
<span class="sd">      See below for an example. (Note that the duck-typed objects cannot be</span>
<span class="sd">      namedtuples because those are treated as standard Python containers.)</span>
<span class="sd">    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and</span>
<span class="sd">      ``fun`` implicitly broadcasts a value over that axis, the backward pass</span>
<span class="sd">      will perform a ``psum`` of the corresponding cotangent. Otherwise, the</span>
<span class="sd">      transposed function will be per-example over named axes. For example, if</span>
<span class="sd">      ``&#39;batch&#39;`` is a named batch axis, ``linear_transpose(f, *args,</span>
<span class="sd">      reduce_axes=(&#39;batch&#39;,))`` will create a transpose function that sums over</span>
<span class="sd">      the batch while ``linear_transpose(f, args)`` will create a per-example</span>
<span class="sd">      transpose.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A callable that calculates the transpose of ``fun``. Valid input into this</span>
<span class="sd">    function must have the same shape/dtypes/structure as the result of</span>
<span class="sd">    ``fun(*primals)``. Output will be a tuple, with the same</span>
<span class="sd">    shape/dtypes/structure as ``primals``.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import types</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; f = lambda x, y: 0.5 * x - 0.5 * y</span>
<span class="sd">  &gt;&gt;&gt; scalar = types.SimpleNamespace(shape=(), dtype=np.dtype(np.float32))</span>
<span class="sd">  &gt;&gt;&gt; f_transpose = jax.linear_transpose(f, scalar, scalar)</span>
<span class="sd">  &gt;&gt;&gt; f_transpose(1.0)</span>
<span class="sd">  (DeviceArray(0.5, dtype=float32), DeviceArray(-0.5, dtype=float32))</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">reduce_axes</span> <span class="o">=</span> <span class="n">_ensure_str_tuple</span><span class="p">(</span><span class="n">reduce_axes</span><span class="p">)</span>
  <span class="n">primals_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
  <span class="n">flat_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">in_avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">primals_flat</span><span class="p">)</span>
  <span class="n">in_dtypes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">)</span>

  <span class="n">in_pvals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">pe</span><span class="o">.</span><span class="n">PartialVal</span><span class="o">.</span><span class="n">unknown</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">)</span>
  <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_pvals</span><span class="p">,</span> <span class="n">const</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_nounits</span><span class="p">(</span><span class="n">flat_fun</span><span class="p">,</span> <span class="n">in_pvals</span><span class="p">,</span>
                                                      <span class="n">instantiate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">out_avals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_pvals</span><span class="p">)</span>
  <span class="n">out_dtypes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">all</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inexact</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">in_dtypes</span> <span class="o">+</span> <span class="n">out_dtypes</span><span class="p">)</span>
          <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span><span class="n">dtypes</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">in_dtypes</span> <span class="o">+</span> <span class="n">out_dtypes</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;linear_transpose only supports [float or complex] -&gt; &quot;</span>
                    <span class="s2">&quot;[float or complex], and integer -&gt; integer functions, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">in_dtypes</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">out_dtypes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">transposed_fun</span><span class="p">(</span><span class="n">const</span><span class="p">,</span> <span class="n">out_cotangent</span><span class="p">):</span>
    <span class="n">out_cts</span><span class="p">,</span> <span class="n">out_tree2</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">out_cotangent</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out_tree</span><span class="p">()</span> <span class="o">!=</span> <span class="n">out_tree2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cotangent tree does not match function output, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="n">out_tree</span><span class="p">()</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">out_tree2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">typecheck</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">,</span> <span class="n">out_cts</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;cotangent type does not match function output, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;expected </span><span class="si">{</span><span class="n">out_avals</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">out_cts</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">dummies</span> <span class="o">=</span> <span class="p">[</span><span class="n">ad</span><span class="o">.</span><span class="n">UndefinedPrimal</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">in_avals</span><span class="p">]</span>
    <span class="n">in_cts</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">backward_pass</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">reduce_axes</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">const</span><span class="p">,</span> <span class="n">dummies</span><span class="p">,</span> <span class="n">out_cts</span><span class="p">)</span>
    <span class="n">in_cts</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">ad</span><span class="o">.</span><span class="n">instantiate_zeros</span><span class="p">,</span> <span class="n">in_cts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">in_tree</span><span class="p">,</span> <span class="n">in_cts</span><span class="p">)</span>

  <span class="c1"># Ensure that transposed_fun is a PyTree</span>
  <span class="k">return</span> <span class="n">Partial</span><span class="p">(</span><span class="n">transposed_fun</span><span class="p">,</span> <span class="n">const</span><span class="p">)</span></div>


<div class="viewcode-block" id="make_jaxpr"><a class="viewcode-back" href="../../../_autosummary/jax.make_jaxpr.html#jax.make_jaxpr">[docs]</a><span class="k">def</span> <span class="nf">make_jaxpr</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
               <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
               <span class="n">axis_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="n">return_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">abstracted_axes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Creates a function that produces its jaxpr given example args.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: The function whose ``jaxpr`` is to be computed. Its positional</span>
<span class="sd">      arguments and return value should be arrays, scalars, or standard Python</span>
<span class="sd">      containers (tuple/list/dict) thereof.</span>
<span class="sd">    static_argnums: See the :py:func:`jax.jit` docstring.</span>
<span class="sd">    axis_env: Optional, a sequence of pairs where the first element is an axis</span>
<span class="sd">      name and the second element is a positive integer representing the size of</span>
<span class="sd">      the mapped axis with that name. This parameter is useful when lowering</span>
<span class="sd">      functions that involve parallel communication collectives, and it</span>
<span class="sd">      specifies the axis name/size environment that would be set up by</span>
<span class="sd">      applications of :py:func:`jax.pmap`.</span>
<span class="sd">    return_shape: Optional boolean, defaults to ``False``. If ``True``, the</span>
<span class="sd">      wrapped function returns a pair where the first element is the XLA</span>
<span class="sd">      computation and the second element is a pytree with the same structure as</span>
<span class="sd">      the output of ``fun`` and where the leaves are objects with ``shape``,</span>
<span class="sd">      ``dtype``, and ``named_shape`` attributes representing the corresponding</span>
<span class="sd">      types of the output leaves.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A wrapped version of ``fun`` that when applied to example arguments returns</span>
<span class="sd">    a ``ClosedJaxpr`` representation of ``fun`` on those arguments. If the</span>
<span class="sd">    argument ``return_shape`` is ``True``, then the returned function instead</span>
<span class="sd">    returns a pair where the first element is the ``ClosedJaxpr``</span>
<span class="sd">    representation of ``fun`` and the second element is a pytree representing</span>
<span class="sd">    the structure, shape, dtypes, and named shapes of the output of ``fun``.</span>

<span class="sd">  A ``jaxpr`` is JAX&#39;s intermediate representation for program traces. The</span>
<span class="sd">  ``jaxpr`` language is based on the simply-typed first-order lambda calculus</span>
<span class="sd">  with let-bindings. :py:func:`make_jaxpr` adapts a function to return its</span>
<span class="sd">  ``jaxpr``, which we can inspect to understand what JAX is doing internally.</span>
<span class="sd">  The ``jaxpr`` returned is a trace of ``fun`` abstracted to</span>
<span class="sd">  :py:class:`ShapedArray` level. Other levels of abstraction exist internally.</span>

<span class="sd">  We do not describe the semantics of the ``jaxpr`` language in detail here, but</span>
<span class="sd">  instead give a few examples.</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; def f(x): return jax.numpy.sin(jax.numpy.cos(x))</span>
<span class="sd">  &gt;&gt;&gt; print(f(3.0))</span>
<span class="sd">  -0.83602</span>
<span class="sd">  &gt;&gt;&gt; jax.make_jaxpr(f)(3.0)</span>
<span class="sd">  { lambda ; a:f32[]. let b:f32[] = cos a; c:f32[] = sin b in (c,) }</span>
<span class="sd">  &gt;&gt;&gt; jax.make_jaxpr(jax.grad(f))(3.0)</span>
<span class="sd">  { lambda ; a:f32[]. let</span>
<span class="sd">      b:f32[] = cos a</span>
<span class="sd">      c:f32[] = sin a</span>
<span class="sd">      _:f32[] = sin b</span>
<span class="sd">      d:f32[] = cos b</span>
<span class="sd">      e:f32[] = mul 1.0 d</span>
<span class="sd">      f:f32[] = neg e</span>
<span class="sd">      g:f32[] = mul f c</span>
<span class="sd">    in (g,) }</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">_check_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="n">static_argnums</span> <span class="o">=</span> <span class="n">_ensure_index_tuple</span><span class="p">(</span><span class="n">static_argnums</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">abstractify</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">flat_args</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">abstracted_axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">),</span> <span class="n">in_tree</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">axes_specs</span> <span class="o">=</span> <span class="n">_flat_axes_specs</span><span class="p">(</span><span class="n">abstracted_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="n">in_type</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">infer_lambda_input_type</span><span class="p">(</span><span class="n">axes_specs</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">)</span>
      <span class="n">in_avals</span><span class="p">,</span> <span class="n">keep_inputs</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">in_type</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">in_avals</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">keep_inputs</span>

  <span class="nd">@wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="nd">@api_boundary</span>
  <span class="k">def</span> <span class="nf">make_jaxpr_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">static_argnums</span><span class="p">:</span>
      <span class="n">dyn_argnums</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">static_argnums</span><span class="p">]</span>
      <span class="n">f</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">argnums_partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dyn_argnums</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">in_avals</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="n">keep_inputs</span> <span class="o">=</span> <span class="n">abstractify</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="n">in_type</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">in_avals</span><span class="p">,</span> <span class="n">keep_inputs</span><span class="p">))</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">in_type</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ExitStack</span><span class="p">()</span> <span class="k">as</span> <span class="n">stack</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">axis_env</span> <span class="ow">or</span> <span class="p">[]:</span>
        <span class="n">stack</span><span class="o">.</span><span class="n">enter_context</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_dynamic2</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">closed_jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_shape</span><span class="p">:</span>
      <span class="n">out_avals</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_type</span><span class="p">)</span>
      <span class="n">out_shapes_flat</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">out_avals</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">closed_jaxpr</span><span class="p">,</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_shapes_flat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">closed_jaxpr</span>

  <span class="n">make_jaxpr_f</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;make_jaxpr(</span><span class="si">{</span><span class="n">make_jaxpr</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">)&quot;</span>
  <span class="k">return</span> <span class="n">make_jaxpr_f</span></div>


<div class="viewcode-block" id="device_put"><a class="viewcode-back" href="../../../_autosummary/jax.device_put.html#jax.device_put">[docs]</a><span class="k">def</span> <span class="nf">device_put</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Sharding</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transfers ``x`` to ``device``.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An array, scalar, or (nested) standard Python container thereof.</span>
<span class="sd">    device: The (optional) :py:class:`Device` or `Sharding` representing the</span>
<span class="sd">      device(s) to which ``x`` should be transferred. If given, then the result</span>
<span class="sd">      is committed to the device(s).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A copy of ``x`` that resides on ``device``.</span>

<span class="sd">  If the ``device`` parameter is ``None``, then this operation behaves like the</span>
<span class="sd">  identity function if the operand is on any device already, otherwise it</span>
<span class="sd">  transfers the data to the default device, uncommitted.</span>

<span class="sd">  For more details on data placement see the</span>
<span class="sd">  :ref:`FAQ on data placement &lt;faq-data-placement&gt;`.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately without</span>
<span class="sd">  blocking the calling Python thread until any transfers are completed.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">config_explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="device_put_sharded"><a class="viewcode-back" href="../../../_autosummary/jax.device_put_sharded.html#jax.device_put_sharded">[docs]</a><span class="k">def</span> <span class="nf">device_put_sharded</span><span class="p">(</span><span class="n">shards</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]):</span>  <span class="c1"># noqa: F811</span>
  <span class="sd">&quot;&quot;&quot;Transfer array shards to specified devices and form ShardedDeviceArray(s).</span>

<span class="sd">  Args:</span>
<span class="sd">    shards: A sequence of arrays, scalars, or (nested) standard Python</span>
<span class="sd">      containers thereof representing the shards to be stacked together to form</span>
<span class="sd">      the output. The length of ``shards`` must equal the length of ``devices``.</span>
<span class="sd">    devices: A sequence of :py:class:`Device` instances representing the devices</span>
<span class="sd">      to which corresponding shards in ``shards`` will be transferred.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A ShardedDeviceArray or (nested) Python container thereof representing the</span>
<span class="sd">    elements of ``shards`` stacked together, with each shard backed by physical</span>
<span class="sd">    device memory specified by the corresponding entry in ``devices``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing a list of arrays for ``shards`` results in a sharded array</span>
<span class="sd">    containing a stacked version of the inputs:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; devices = jax.local_devices()</span>
<span class="sd">    &gt;&gt;&gt; x = [jax.numpy.ones(5) for device in devices]</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack(x))</span>
<span class="sd">    True</span>

<span class="sd">    Passing a list of nested container objects with arrays at the leaves for</span>
<span class="sd">    ``shards`` corresponds to stacking the shards at each leaf. This requires</span>
<span class="sd">    all entries in the list to have the same tree structure:</span>

<span class="sd">    &gt;&gt;&gt; x = [(i, jax.numpy.arange(i, i + 4)) for i in range(len(devices))]</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_sharded(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; type(y)</span>
<span class="sd">    &lt;class &#39;tuple&#39;&gt;</span>
<span class="sd">    &gt;&gt;&gt; y0 = jax.device_put_sharded([a for a, b in x], devices)</span>
<span class="sd">    &gt;&gt;&gt; y1 = jax.device_put_sharded([b for a, b in x], devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y[0], y0)</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y[1], y1)</span>
<span class="sd">    True</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_replicated</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(jakevdp): provide a default for devices that considers both local</span>
  <span class="c1"># devices and pods</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shards</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;device_put_sharded `shards` input must be a sequence; &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;len(shards) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span><span class="si">}</span><span class="s2"> must equal &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;len(devices) = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_device_put_sharded</span><span class="p">(</span><span class="o">*</span><span class="n">xs</span><span class="p">):</span>
    <span class="n">avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">raise_to_shaped</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">a1</span> <span class="o">==</span> <span class="n">a2</span> <span class="k">for</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">avals</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">avals</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
      <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span> <span class="k">for</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">avals</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">avals</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">if</span> <span class="n">a1</span> <span class="o">!=</span> <span class="n">a2</span><span class="p">)</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the shards passed to device_put_sharded must have &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;consistent shape and dtype, but got </span><span class="si">{</span><span class="n">a1</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">a2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">stacked_aval</span> <span class="o">=</span> <span class="n">avals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),)</span> <span class="o">+</span> <span class="n">avals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">buffers</span> <span class="o">=</span> <span class="p">[</span><span class="n">buf</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
      <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span><span class="p">,</span> <span class="n">sharding</span>
      <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">_create_pmap_sharding_spec</span><span class="p">(</span><span class="n">stacked_aval</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">(</span>
          <span class="n">stacked_aval</span><span class="p">,</span>
          <span class="n">sharding</span><span class="o">.</span><span class="n">PmapSharding</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">sharding_spec</span><span class="p">),</span>
          <span class="n">buffers</span><span class="p">,</span> <span class="n">committed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">_skip_checks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pxla</span><span class="o">.</span><span class="n">make_sharded_device_array</span><span class="p">(</span><span class="n">stacked_aval</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">buffers</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">config_explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_put_sharded</span><span class="p">,</span> <span class="o">*</span><span class="n">shards</span><span class="p">)</span></div>


<div class="viewcode-block" id="device_put_replicated"><a class="viewcode-back" href="../../../_autosummary/jax.device_put_replicated.html#jax.device_put_replicated">[docs]</a><span class="k">def</span> <span class="nf">device_put_replicated</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]):</span>  <span class="c1"># noqa: F811</span>
  <span class="sd">&quot;&quot;&quot;Transfer array(s) to each specified device and form ShardedDeviceArray(s).</span>

<span class="sd">  Args:</span>
<span class="sd">    x: an array, scalar, or (nested) standard Python container thereof</span>
<span class="sd">      representing the array to be replicated to form the output.</span>
<span class="sd">    devices: A sequence of :py:class:`Device` instances representing the devices</span>
<span class="sd">      to which ``x`` will be transferred.</span>

<span class="sd">  This function is always asynchronous, i.e. returns immediately.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A ShardedDeviceArray or (nested) Python container thereof representing the</span>
<span class="sd">    value of ``x`` broadcasted along a new leading axis of size</span>
<span class="sd">    ``len(devices)``, with each slice along that new leading axis backed by</span>
<span class="sd">    memory on the device specified by the corresponding entry in ``devices``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing an array:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; devices = jax.local_devices()</span>
<span class="sd">    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.])</span>
<span class="sd">    &gt;&gt;&gt; y = jax.device_put_replicated(x, devices)</span>
<span class="sd">    &gt;&gt;&gt; np.allclose(y, jax.numpy.stack([x for _ in devices]))</span>
<span class="sd">    True</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_sharded</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">devices</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`devices` argument to `device_put_replicated must be &quot;</span>
                     <span class="s2">&quot;a non-empty sequence.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_device_put_replicated</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">unmapped_aval</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">core</span><span class="o">.</span><span class="n">no_axis_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                              <span class="n">core</span><span class="o">.</span><span class="n">raise_to_shaped</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">get_aval</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="k">assert</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">)</span> <span class="ow">and</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">aval_to_xla_shapes</span><span class="p">(</span><span class="n">aval</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">buf</span><span class="p">,</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">rest_bufs</span> <span class="o">=</span> <span class="p">[</span><span class="n">buf</span><span class="o">.</span><span class="n">copy_to_device</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
      <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span><span class="p">,</span> <span class="n">sharding</span>
      <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">pxla</span><span class="o">.</span><span class="n">_create_pmap_sharding_spec</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">(</span>
          <span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">PmapSharding</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="n">sharding_spec</span><span class="p">),</span>
          <span class="p">[</span><span class="n">buf</span><span class="p">,</span> <span class="o">*</span><span class="n">rest_bufs</span><span class="p">],</span> <span class="n">committed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">_skip_checks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">pxla</span><span class="o">.</span><span class="n">make_sharded_device_array</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="n">buf</span><span class="p">,</span> <span class="o">*</span><span class="n">rest_bufs</span><span class="p">])</span>

  <span class="k">with</span> <span class="n">config_explicit_device_put_scope</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_put_replicated</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>


<span class="c1"># TODO(mattjj): consider revising</span>
<span class="k">def</span> <span class="nf">_device_get</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Tracer</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">toarray</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">__array__</span>
  <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">toarray</span><span class="p">()</span>

<div class="viewcode-block" id="device_get"><a class="viewcode-back" href="../../../_autosummary/jax.device_get.html#jax.device_get">[docs]</a><span class="k">def</span> <span class="nf">device_get</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Transfer ``x`` to host.</span>

<span class="sd">  If ``x`` is a pytree, then the individual buffers are copied in parallel.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An array, scalar, DeviceArray or (nested) standard Python container thereof</span>
<span class="sd">      representing the array to be transferred to host.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An array or (nested) Python container thereof representing the</span>
<span class="sd">    value of ``x``.</span>

<span class="sd">  Examples:</span>
<span class="sd">    Passing a DeviceArray:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.])</span>
<span class="sd">    &gt;&gt;&gt; jax.device_get(x)</span>
<span class="sd">    array([1., 2., 3.], dtype=float32)</span>

<span class="sd">    Passing a scalar (has no effect):</span>

<span class="sd">    &gt;&gt;&gt; jax.device_get(1)</span>
<span class="sd">    1</span>

<span class="sd">  See Also:</span>
<span class="sd">    - device_put</span>
<span class="sd">    - device_put_sharded</span>
<span class="sd">    - device_put_replicated</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">config_explicit_device_get_scope</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tree_leaves</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">y</span><span class="o">.</span><span class="n">copy_to_host_async</span><span class="p">()</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">_device_get</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_check_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Tracer</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_valid_jaxtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)):</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument &#39;</span><span class="si">{</span><span class="n">arg</span><span class="si">}</span><span class="s2">&#39; of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not a valid JAX type.&quot;</span><span class="p">)</span>

<span class="c1"># TODO(mattjj,necula): this duplicates code in core.valid_jaxtype, but one</span>
<span class="c1"># internal user relies on it for duck-typing. must fix downstream user!</span>
<span class="k">def</span> <span class="nf">_valid_jaxtype</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>  <span class="c1"># faster than core.get_aval</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">core</span><span class="o">.</span><span class="n">valid_jaxtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">ShapeDtypeStruct</span><span class="p">:</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;named_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;sharding&quot;</span><span class="p">]</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sharding</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sharding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span> <span class="o">=</span> <span class="n">sharding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">named_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">(</span><span class="n">named_shape</span><span class="p">)</span>

  <span class="n">size</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
  <span class="n">ndim</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">IndexError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of unsized object&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span> <span class="c1"># same as numpy error</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">ns</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;, named_shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="si">}{</span><span class="n">ns</span><span class="si">}</span><span class="s2">)&quot;</span>

  <span class="fm">__str__</span> <span class="o">=</span> <span class="fm">__repr__</span>

  <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ShapeDtypeStruct</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># TODO(frostig): avoid the conversion from dict by addressing</span>
    <span class="c1"># https://github.com/google/jax/issues/8182</span>
    <span class="n">named</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_shape</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">named</span><span class="p">))</span>

<span class="n">core</span><span class="o">.</span><span class="n">pytype_aval_mappings</span><span class="p">[</span><span class="n">ShapeDtypeStruct</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">ShapedArray</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                          <span class="n">weak_type</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">named_shape</span><span class="p">))</span>

<div class="viewcode-block" id="eval_shape"><a class="viewcode-back" href="../../../_autosummary/jax.eval_shape.html#jax.eval_shape">[docs]</a><span class="k">def</span> <span class="nf">eval_shape</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute the shape/dtype of ``fun`` without any FLOPs.</span>

<span class="sd">  This utility function is useful for performing shape inference. Its</span>
<span class="sd">  input/output behavior is defined by::</span>

<span class="sd">    def eval_shape(fun, *args, **kwargs):</span>
<span class="sd">      out = fun(*args, **kwargs)</span>
<span class="sd">      return jax.tree_util.tree_map(shape_dtype_struct, out)</span>

<span class="sd">    def shape_dtype_struct(x):</span>
<span class="sd">      return ShapeDtypeStruct(x.shape, x.dtype)</span>

<span class="sd">    class ShapeDtypeStruct:</span>
<span class="sd">      __slots__ = [&quot;shape&quot;, &quot;dtype&quot;]</span>
<span class="sd">      def __init__(self, shape, dtype):</span>
<span class="sd">        self.shape = shape</span>
<span class="sd">        self.dtype = dtype</span>

<span class="sd">  In particular, the output is a pytree of objects that have ``shape`` and</span>
<span class="sd">  ``dtype`` attributes, but nothing else about them is guaranteed by the API.</span>

<span class="sd">  But instead of applying ``fun`` directly, which might be expensive, it uses</span>
<span class="sd">  JAX&#39;s abstract interpretation machinery to evaluate the shapes without doing</span>
<span class="sd">  any FLOPs.</span>

<span class="sd">  Using :py:func:`eval_shape` can also catch shape errors, and will raise same</span>
<span class="sd">  shape errors as evaluating ``fun(*args, **kwargs)``.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: The function whose output shape should be evaluated.</span>
<span class="sd">    *args: a positional argument tuple of arrays, scalars, or (nested) standard</span>
<span class="sd">      Python containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of</span>
<span class="sd">      those types. Since only the ``shape`` and ``dtype`` attributes are</span>
<span class="sd">      accessed, only values that duck-type arrays are required, rather than real</span>
<span class="sd">      ndarrays. The duck-typed objects cannot be namedtuples because those are</span>
<span class="sd">      treated as standard Python containers. See the example below.</span>
<span class="sd">    **kwargs: a keyword argument dict of arrays, scalars, or (nested) standard</span>
<span class="sd">      Python containers (pytrees) of those types. As in ``args``, array values</span>
<span class="sd">      need only be duck-typed to have ``shape`` and ``dtype`` attributes.</span>

<span class="sd">  For example:</span>

<span class="sd">  &gt;&gt;&gt; import jax</span>
<span class="sd">  &gt;&gt;&gt; import jax.numpy as jnp</span>
<span class="sd">  &gt;&gt;&gt;</span>
<span class="sd">  &gt;&gt;&gt; f = lambda A, x: jnp.tanh(jnp.dot(A, x))</span>
<span class="sd">  &gt;&gt;&gt; class MyArgArray(object):</span>
<span class="sd">  ...   def __init__(self, shape, dtype):</span>
<span class="sd">  ...     self.shape = shape</span>
<span class="sd">  ...     self.dtype = jnp.dtype(dtype)</span>
<span class="sd">  ...</span>
<span class="sd">  &gt;&gt;&gt; A = MyArgArray((2000, 3000), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt; x = MyArgArray((3000, 1000), jnp.float32)</span>
<span class="sd">  &gt;&gt;&gt; out = jax.eval_shape(f, A, x)  # no FLOPs performed</span>
<span class="sd">  &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">  (2000, 1000)</span>
<span class="sd">  &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">  float32</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">args_flat</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
  <span class="n">wrapped_fun</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun</span><span class="p">(</span><span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span> <span class="n">in_tree</span><span class="p">)</span>
  <span class="n">debug_info</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">debug_info</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;eval_shape&quot;</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">abstract_eval_fun</span><span class="p">(</span><span class="n">wrapped_fun</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">,</span>
                             <span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">shaped_abstractify</span><span class="p">,</span> <span class="n">args_flat</span><span class="p">),</span>
                             <span class="n">debug_info</span><span class="o">=</span><span class="n">debug_info</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">out</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out</span><span class="p">)</span></div>


<span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">new_checkpoint</span><span class="p">)</span>  <span class="c1"># config.jax_new_checkpoint is True by default</span>
<span class="k">def</span> <span class="nf">checkpoint</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
               <span class="n">concrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">prevent_cse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
               <span class="n">policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">concrete</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The &#39;concrete&#39; option to jax.checkpoint / jax.remat is deprecated; &quot;</span>
           <span class="s2">&quot;in its place, you can use its `static_argnums` option, and if &quot;</span>
           <span class="s2">&quot;necessary the `jax.ensure_compile_time_eval()` context manager.</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;For example, if using `concrete=True` for an `is_training` flag:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  from functools import partial</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  @partial(jax.checkpoint, concrete=True)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  def foo(x, is_training):</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    if is_training:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return f(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    else:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return g(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;replace it with a use of `static_argnums`:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  @partial(jax.checkpoint, static_argnums=(1,))</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  def foo(x, is_training):</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    ...</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;If jax.numpy operations need to be performed on static arguments, &quot;</span>
           <span class="s2">&quot;we can use the `jax.ensure_compile_time_eval()` context manager. &quot;</span>
           <span class="s2">&quot;For example, we can replace this use of `concrete=True`</span><span class="se">\n</span><span class="s2">:&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  @partial(jax.checkpoint, concrete=True)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  def foo(x, y):</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    if y &gt; 0:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return f(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    else:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return g(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;with this combination of `static_argnums` and &quot;</span>
           <span class="s2">&quot;`jax.ensure_compile_time_eval()`:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  @partial(jax.checkpoint, static_argnums=(1,))</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;  def foo(x, y):</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    with jax.ensure_compile_time_eval():</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      y_pos = y &gt; 0</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    if y_pos:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return f(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;    else:</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;      return g(x)</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
           <span class="s2">&quot;See https://jax.readthedocs.io/en/latest/jep/11830-new-remat-checkpoint.html</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_checkpoint</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">prevent_cse</span><span class="o">=</span><span class="n">prevent_cse</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                        <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">)</span>
<span class="n">remat</span> <span class="o">=</span> <span class="n">checkpoint</span>  <span class="c1"># type: ignore</span>


<div class="viewcode-block" id="named_call"><a class="viewcode-back" href="../../../_autosummary/jax.named_call.html#jax.named_call">[docs]</a><span class="k">def</span> <span class="nf">named_call</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Adds a user specified name to a function when staging out JAX computations.</span>

<span class="sd">  When staging out computations for just-in-time compilation to XLA (or other</span>
<span class="sd">  backends such as TensorFlow) JAX runs your Python program but by default does</span>
<span class="sd">  not preserve any of the function names or other metadata associated with it.</span>
<span class="sd">  This can make debugging the staged out (and/or compiled) representation of</span>
<span class="sd">  your program complicated because there is limited context information for each</span>
<span class="sd">  operation being executed.</span>

<span class="sd">  `named_call` tells JAX to stage the given function out as a subcomputation</span>
<span class="sd">  with a specific name. When the staged out program is compiled with XLA these</span>
<span class="sd">  named subcomputations are preserved and show up in debugging utilities like</span>
<span class="sd">  the TensorFlow Profiler in TensorBoard. Names are also preserved when staging</span>
<span class="sd">  out JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.</span>

<span class="sd">  Args:</span>
<span class="sd">    fun: Function to be wrapped. This can be any Callable.</span>
<span class="sd">    name: Optional. The prefix to use to name all sub computations created</span>
<span class="sd">      within the name scope. Use the fun.__name__ if not specified.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A version of `fun` that is wrapped in a name_scope.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span>

  <span class="n">_</span><span class="p">,</span> <span class="n">in_tree</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(())</span>

  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_experimental_name_stack</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">source_info_util</span><span class="o">.</span><span class="n">extend_name_stack</span><span class="p">(</span><span class="n">name</span><span class="p">)(</span><span class="n">fun</span><span class="p">)</span>

  <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">named_call_f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">lu_f</span> <span class="o">=</span> <span class="n">lu</span><span class="o">.</span><span class="n">wrap_init</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="n">flat_f</span><span class="p">,</span> <span class="n">out_tree</span> <span class="o">=</span> <span class="n">flatten_fun_nokwargs</span><span class="p">(</span><span class="n">lu_f</span><span class="p">,</span> <span class="n">in_tree</span><span class="p">)</span>
    <span class="n">out_flat</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">named_call_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">flat_f</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">out_tree</span><span class="p">(),</span> <span class="n">out_flat</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">named_call_f</span></div>

<div class="viewcode-block" id="named_scope"><a class="viewcode-back" href="../../../_autosummary/jax.named_scope.html#jax.named_scope">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">named_scope</span><span class="p">(</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;A context manager that adds a user specified name to the JAX name stack.</span>

<span class="sd">  When staging out computations for just-in-time compilation to XLA (or other</span>
<span class="sd">  backends such as TensorFlow) JAX does not, by default, preserve the names</span>
<span class="sd">  (or other source metadata) of Python functions it encounters.</span>
<span class="sd">  This can make debugging the staged out (and/or compiled) representation of</span>
<span class="sd">  your program complicated because there is limited context information for each</span>
<span class="sd">  operation being executed.</span>

<span class="sd">  ``named_scope`` tells JAX to stage the given function with additional</span>
<span class="sd">  annotations on the underlying operations. JAX internally keeps track of these</span>
<span class="sd">  annotations in a name stack. When the staged out program is compiled with XLA</span>
<span class="sd">  these annotations are preserved and show up in debugging utilities like the</span>
<span class="sd">  TensorFlow Profiler in TensorBoard. Names are also preserved when staging out</span>
<span class="sd">  JAX programs to TensorFlow using :func:`experimental.jax2tf.convert`.</span>


<span class="sd">  Args:</span>
<span class="sd">    name: The prefix to use to name all operations created within the name</span>
<span class="sd">      scope.</span>
<span class="sd">  Yields:</span>
<span class="sd">    Yields ``None``, but enters a context in which `name` will be appended to</span>
<span class="sd">    the active name stack.</span>

<span class="sd">  Examples:</span>
<span class="sd">    ``named_scope`` can be used as a context manager inside compiled functions:</span>

<span class="sd">    &gt;&gt;&gt; import jax</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... def layer(w, x):</span>
<span class="sd">    ...   with jax.named_scope(&quot;dot_product&quot;):</span>
<span class="sd">    ...     logits = w.dot(x)</span>
<span class="sd">    ...   with jax.named_scope(&quot;activation&quot;):</span>
<span class="sd">    ...     return jax.nn.relu(logits)</span>

<span class="sd">    It can also be used as a decorator:</span>

<span class="sd">    &gt;&gt;&gt; @jax.jit</span>
<span class="sd">    ... @jax.named_scope(&quot;layer&quot;)</span>
<span class="sd">    ... def layer(w, x):</span>
<span class="sd">    ...   logits = w.dot(x)</span>
<span class="sd">    ...   return jax.nn.relu(logits)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;named_scope name argument must be a string.&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">source_info_util</span><span class="o">.</span><span class="n">extend_name_stack</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">yield</span></div>

<span class="k">def</span> <span class="nf">effects_barrier</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;Waits until existing functions have completed any side-effects.&quot;&quot;&quot;</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>

<div class="viewcode-block" id="block_until_ready"><a class="viewcode-back" href="../../../_autosummary/jax.block_until_ready.html#jax.block_until_ready">[docs]</a><span class="k">def</span> <span class="nf">block_until_ready</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Tries to call a ``block_until_ready`` method on pytree leaves.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: a pytree, usually with at least some JAX array instances at its leaves.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A pytree with the same structure and values of the input, where the values</span>
<span class="sd">    of all JAX array leaves are ready.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">try_to_block</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">try_to_block</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="pure_callback"><a class="viewcode-back" href="../../../_autosummary/jax.pure_callback.html#jax.pure_callback">[docs]</a><span class="k">def</span> <span class="nf">pure_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">result_shape_dtypes</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
                  <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies a functionally pure Python callable. Works under `jit`/`pmap`/etc.</span>

<span class="sd">  ``pure_callback`` enables calling a Python function in JIT-ed JAX functions.</span>
<span class="sd">  The input ``callback`` will be passed NumPy arrays in place of JAX arrays and</span>
<span class="sd">  should also return NumPy arrays. Execution takes place on CPU, like any</span>
<span class="sd">  Python+NumPy function.</span>

<span class="sd">  The callback is treated as functionally pure, meaning it has no side-effects</span>
<span class="sd">  and its output value depends only on its argument values. As a consequence, it</span>
<span class="sd">  is safe to be called multiple times (e.g. when transformed by ``vmap`` or</span>
<span class="sd">  ``pmap``), or not to be called at all when e.g. the output of a</span>
<span class="sd">  `jit`-decorated function has no data dependence on its value. Pure callbacks</span>
<span class="sd">  may also be reordered if data-dependence allows.</span>

<span class="sd">  When ``pmap``-ed, the pure callback will be called several times (one on each</span>
<span class="sd">  axis of the map). When `vmap`-ed the behavior will depend on the value of the</span>
<span class="sd">  ``vectorized`` keyword argument. When ``vectorized`` is ``True``, the callback</span>
<span class="sd">  is assumed to obey</span>
<span class="sd">  ``jax.vmap(callback)(xs) == callback(xs) == jnp.stack([callback(x) for x in xs])``.</span>
<span class="sd">  Therefore, the callback will be called directly on batched inputs (where the</span>
<span class="sd">  batch axes are the leading dimensions). Additionally, the callbacks should</span>
<span class="sd">  return outputs that have corresponding leading batch axes. If not vectorized</span>
<span class="sd">  ``callback`` will be mapped sequentially across the batched axis.</span>
<span class="sd">  For example, if ``callback = lambda x, y: np.matmul(x, y)``, then we are free</span>
<span class="sd">  to set ``vectorized=True`` because the ``np.matmul`` function handles</span>
<span class="sd">  arbitrary leading batch dimensions.</span>

<span class="sd">  Args:</span>
<span class="sd">    callback: A Python callable. The callable will be passed PyTrees of NumPy</span>
<span class="sd">      arrays as arguments, and should return a PyTree of NumPy arrays that</span>
<span class="sd">      matches ``result_shape_dtypes``.</span>
<span class="sd">    result_shape_dtypes: A PyTree with leaves that are objects with ``shape``</span>
<span class="sd">      and ``dtype`` attributes which represent to the shapes and dtypes of the</span>
<span class="sd">      value of ``callback`` applied to ``args`` and ``kwargs``.</span>
<span class="sd">    *args: The positional arguments to the callback. Must be PyTrees of JAX</span>
<span class="sd">      types.</span>
<span class="sd">    vectorized: A boolean that indicates whether or not ``callback`` is</span>
<span class="sd">      vectorized, meaning it can handle arrays with additional leading</span>
<span class="sd">      dimensions. If ``vectorized`` is `True`, when the callback is mapped</span>
<span class="sd">      via `jax.vmap`, it will be called directly on inputs with leading batch</span>
<span class="sd">      dimensions instead of executing ``callback`` on each mapped input</span>
<span class="sd">      individually. The callback should also return outputs batched across the</span>
<span class="sd">      leading axis.</span>
<span class="sd">    **kwargs: The keyword arguments to the callback. Must be PyTrees of JAX</span>
<span class="sd">      types.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The value of ``callback(*args, **kwargs)``.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jcb</span><span class="o">.</span><span class="n">pure_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="n">result_shape_dtypes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">clear_backends</span><span class="p">():</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Clear all backend clients so that new backend clients can be created later.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xb</span><span class="o">.</span><span class="n">_clear_backends</span><span class="p">()</span>
  <span class="n">jax</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">xla_bridge</span><span class="o">.</span><span class="n">_backends</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_callable</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">xla_primitive_callable</span><span class="o">.</span><span class="n">cache_clear</span><span class="p">()</span>
  <span class="n">_cpp_jit_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
  <span class="n">jax_jit</span><span class="o">.</span><span class="n">CompiledFunctionCache</span><span class="o">.</span><span class="n">clear_all</span><span class="p">()</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      &copy; Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>