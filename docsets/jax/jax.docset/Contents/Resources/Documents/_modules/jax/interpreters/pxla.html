
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>jax.interpreters.pxla &#8212; JAX  documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" href="../../../_static/style.css" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/jax_logo_250px.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../installation.html">
   Installing JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/quickstart.html">
   JAX Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/thinking_in_jax.html">
   How to Think in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Common_Gotchas_in_JAX.html">
   ðŸ”ª JAX - The Sharp Bits ðŸ”ª
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jax-101/index.html">
   Tutorial: JAX 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/01-jax-basics.html">
     JAX As Accelerated NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/02-jitting.html">
     Just In Time Compilation with JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/03-vectorization.html">
     Automatic Vectorization in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/04-advanced-autodiff.html">
     Advanced Automatic Differentiation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/05-random-numbers.html">
     Pseudo Random Numbers in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/05.1-pytrees.html">
     Working with Pytrees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/06-parallelism.html">
     Parallel Evaluation in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/07-state.html">
     Stateful Computations in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax-101/08-pjit.html">
     Introduction to pjit
    </a>
   </li>
  </ul>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../debugging/index.html">
   Runtime value debugging in JAX
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/print_breakpoint.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.print
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       jax.debug.breakpoint
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/checkify_guide.html">
     The
     <code class="docutils literal notranslate">
      <span class="pre">
       checkify
      </span>
     </code>
     transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../debugging/flags.html">
     JAX debugging flags
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../faq.html">
   JAX Frequently Asked Questions (FAQ)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../async_dispatch.html">
   Asynchronous dispatch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../aot.html">
   Ahead-of-time lowering and compilation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../jaxpr.html">
   Understanding Jaxprs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/convolutions.html">
   Convolutions in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../pytrees.html">
   Pytrees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../type_promotion.html">
   Type promotion semantics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../errors.html">
   JAX Errors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../transfer_guard.html">
   Transfer guard
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../glossary.html">
   JAX Glossary of Terms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../changelog.html">
   Change log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Advanced JAX Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/autodiff_cookbook.html">
   The Autodiff Cookbook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/vmapped_log_probs.html">
   Autobatching log-densities example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/neural_network_with_tfds_data.html">
   Training a Simple Neural Network, with tensorflow/datasets Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Custom_derivative_rules_for_Python_code.html">
   Custom derivative rules for JAX-transformable Python functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/How_JAX_primitives_work.html">
   How JAX primitives work
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Writing_custom_interpreters_in_Jax.html">
   Writing custom Jaxpr interpreters in JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/Neural_Network_and_Data_Loading.html">
   Training a Simple Neural Network, with PyTorch Data Loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../notebooks/xmap_tutorial.html">
   Named axes and easy-to-revise parallelism
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../multi_process.html">
   Using JAX in multi-host and multi-process environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../api_compatibility.html">
   API compatibility
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../deprecation.html">
   Python and NumPy version support policy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../concurrency.html">
   Concurrency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../gpu_memory_allocation.html">
   GPU memory allocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../profiling.html">
   Profiling JAX programs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../device_memory_profiling.html">
   Device Memory Profiling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../rank_promotion_warning.html">
   Rank promotion warning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Developer documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../contributing.html">
   Contributing to JAX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../developer.html">
   Building from source
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../jax_internal_api.html">
   Internal APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../autodidax.html">
   Autodidax: JAX core from scratch
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jep/index.html">
   JAX Enhancement Proposals (JEPs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/263-prng.html">
     263: JAX PRNG Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/2026-custom-derivatives.html">
     2026: Custom JVP/VJP rules for JAX-transformable functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/4008-custom-vjp-update.html">
     4008: Custom VJP and `nondiff_argnums` update
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/4410-omnistaging.html">
     4410: Omnistaging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/9407-type-promotion.html">
     9407: Design of Type Promotion Semantics for JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/9419-jax-versioning.html">
     9419: Jax and Jaxlib versioning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/10657-sequencing-effects.html">
     10657: Sequencing side-effects in JAX
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/11830-new-remat-checkpoint.html">
     11830: `jax.remat` / `jax.checkpoint` new implementation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jep/12049-type-annotations.html">
     12049: Type Annotation Roadmap for JAX
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../jax.html">
   Public API: jax package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.numpy.html">
     jax.numpy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.scipy.html">
     jax.scipy package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.config.html">
     JAX configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.debug.html">
     jax.debug package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.dlpack.html">
     jax.dlpack module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.distributed.html">
     jax.distributed module
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.example_libraries.html">
     jax.example_libraries package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.example_libraries.optimizers.html">
       jax.example_libraries.optimizers module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.example_libraries.stax.html">
       jax.example_libraries.stax module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.experimental.html">
     jax.experimental package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.checkify.html">
       jax.experimental.checkify module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.global_device_array.html">
       jax.experimental.global_device_array module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.host_callback.html">
       jax.experimental.host_callback module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.maps.html">
       jax.experimental.maps module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.pjit.html">
       jax.experimental.pjit module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.sparse.html">
       jax.experimental.sparse module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.experimental.jet.html">
       jax.experimental.jet module
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.flatten_util.html">
     jax.flatten_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.image.html">
     jax.image package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.lax.html">
     jax.lax package
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../jax.nn.html">
     jax.nn package
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../jax.nn.initializers.html">
       jax.nn.initializers package
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.ops.html">
     jax.ops package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.profiler.html">
     jax.profiler module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.random.html">
     jax.random package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.stages.html">
     jax.stages package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.tree_util.html">
     jax.tree_util package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../jax.lib.html">
     jax.lib package
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/google/jax"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for jax.interpreters.pxla</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 The JAX Authors.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;Implementation of pmap and related functionality.&quot;&quot;&quot;</span>

<span class="c1"># A ShardingSpec describes at a high level how a logical array is sharded across</span>
<span class="c1"># devices (each ShardedDeviceArray has a ShardingSpec, and ShardingSpecs also</span>
<span class="c1"># describe how to shard inputs to a parallel computation). spec_to_indices()</span>
<span class="c1"># encodes exactly how a given ShardingSpec is translated to device buffers, i.e.</span>
<span class="c1"># how the sharded array is &quot;laid out&quot; across devices. Given a sequence of</span>
<span class="c1"># devices, we shard the data across the devices in row-major order, with</span>
<span class="c1"># replication treated as an extra inner dimension.</span>
<span class="c1">#</span>
<span class="c1"># For example, given the logical data array [1, 2, 3, 4], if we were to</span>
<span class="c1"># partition this array 4 ways with a replication factor of 2, for a total of 8</span>
<span class="c1"># devices, the data on each device would be: [1, 1], [2, 2], [3, 3], [4, 4].</span>
<span class="c1">#</span>
<span class="c1"># This encoding is assumed by various parts of the system, e.g. generating</span>
<span class="c1"># replica groups for collective operations.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span><span class="p">,</span> <span class="n">ContextDecorator</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">OrderedDict</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">lru_cache</span>
<span class="kn">import</span> <span class="nn">itertools</span> <span class="k">as</span> <span class="nn">it</span>
<span class="kn">import</span> <span class="nn">operator</span> <span class="k">as</span> <span class="nn">op</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">types</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">FrozenSet</span><span class="p">,</span>
                    <span class="n">Sequence</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span>
                    <span class="n">TYPE_CHECKING</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">absl</span> <span class="kn">import</span> <span class="n">logging</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">linear_util</span> <span class="k">as</span> <span class="n">lu</span>
<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">ConcreteArray</span><span class="p">,</span> <span class="n">ShapedArray</span>
<span class="kn">from</span> <span class="nn">jax.errors</span> <span class="kn">import</span> <span class="n">JAXTypeError</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">ad</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">batching</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">mlir</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">partial_eval</span> <span class="k">as</span> <span class="n">pe</span>
<span class="kn">from</span> <span class="nn">jax.interpreters</span> <span class="kn">import</span> <span class="n">xla</span>
<span class="kn">from</span> <span class="nn">jax.tree_util</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_map</span>

<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">abstract_arrays</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">api_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">basearray</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">device_array</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">source_info_util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">dispatch</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">profiler</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">stages</span>
<span class="kn">from</span> <span class="nn">jax._src.abstract_arrays</span> <span class="kn">import</span> <span class="n">array_types</span>
<span class="kn">from</span> <span class="nn">jax._src.config</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">can_execute_with_token</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_bridge</span> <span class="k">as</span> <span class="n">xb</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_client</span> <span class="k">as</span> <span class="n">xc</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">xla_extension_version</span>
<span class="kn">from</span> <span class="nn">jax._src.lib</span> <span class="kn">import</span> <span class="n">pmap_lib</span>
<span class="kn">from</span> <span class="nn">jax._src.lib.mlir</span> <span class="kn">import</span> <span class="n">ir</span>
<span class="kn">from</span> <span class="nn">jax._src.lib.mlir.dialects</span> <span class="kn">import</span> <span class="n">mhlo</span>
<span class="kn">from</span> <span class="nn">jax._src.util</span> <span class="kn">import</span> <span class="p">(</span><span class="n">unzip3</span><span class="p">,</span> <span class="n">prod</span><span class="p">,</span> <span class="n">safe_map</span><span class="p">,</span> <span class="n">safe_zip</span><span class="p">,</span> <span class="n">partition_list</span><span class="p">,</span>
                           <span class="n">new_name_stack</span><span class="p">,</span> <span class="n">wrap_name</span><span class="p">,</span> <span class="n">assert_unreachable</span><span class="p">,</span>
                           <span class="n">tuple_insert</span><span class="p">,</span> <span class="n">tuple_delete</span><span class="p">,</span> <span class="n">distributed_debug_log</span><span class="p">,</span>
                           <span class="n">split_dict</span><span class="p">,</span> <span class="n">unzip2</span><span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">MeshPspecSharding</span><span class="p">,</span> <span class="n">XLACompatibleSharding</span>

<span class="c1"># Built in Python lists don&#39;t support weak refs but subclasses of lists do.</span>
<span class="k">class</span> <span class="nc">WeakRefList</span><span class="p">(</span><span class="nb">list</span><span class="p">):</span>
  <span class="k">pass</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">cached_property</span> <span class="k">as</span> <span class="n">maybe_cached_property</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">maybe_cached_property</span> <span class="o">=</span> <span class="nb">property</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">):</span>
  <span class="n">OrderedDictType</span> <span class="o">=</span> <span class="n">OrderedDict</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">OrderedDictType</span> <span class="o">=</span> <span class="n">Dict</span>

<span class="n">xe</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">_xla</span>

<span class="n">unsafe_map</span><span class="p">,</span> <span class="nb">map</span> <span class="o">=</span> <span class="nb">map</span><span class="p">,</span> <span class="n">safe_map</span>  <span class="c1"># type: ignore</span>

<span class="n">Index</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span>

<span class="n">NoSharding</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">NoSharding</span>
<span class="n">Chunked</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">Chunked</span>
<span class="n">Unstacked</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">Unstacked</span>

<span class="n">ShardedAxis</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedAxis</span>
<span class="n">Replicated</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">Replicated</span>

<span class="n">_UNSHARDED_INSTANCE</span> <span class="o">=</span> <span class="n">NoSharding</span><span class="p">()</span>
<span class="n">AvalDimSharding</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Unstacked</span><span class="p">,</span> <span class="n">Chunked</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">]</span>
<span class="n">MeshDimAssignment</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">ShardedAxis</span><span class="p">,</span> <span class="n">Replicated</span><span class="p">]</span>
<span class="n">ShardingSpec</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardingSpec</span>

<span class="n">MeshAxisName</span> <span class="o">=</span> <span class="n">Any</span>
<span class="n">OpShardingType</span> <span class="o">=</span> <span class="n">Any</span>


<span class="k">def</span> <span class="nf">sharding_spec_mesh_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="n">sharded_axis_sizes</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">sharding</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">):</span>
      <span class="k">continue</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">):</span>
      <span class="n">sharded_axis_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Chunked</span><span class="p">):</span>
      <span class="n">sharded_axis_sizes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">chunks</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">assert_unreachable</span><span class="p">(</span><span class="n">sharding</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sharded_axis_sizes</span><span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ShardedAxis</span><span class="p">)</span> <span class="k">else</span> <span class="n">a</span><span class="o">.</span><span class="n">replicas</span>
               <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_logical_mesh_ids</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sharding_spec_sharding_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">special_axes</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">OpShardingType</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}):</span>
  <span class="sd">&quot;&quot;&quot;Converts a ShardingSpec to an OpSharding proto.</span>

<span class="sd">  See</span>
<span class="sd">  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/xla_data.proto#L601</span>
<span class="sd">  for details on the OpSharding proto.</span>
<span class="sd">  Unfortunately the semantics are not very well described in the proto spec, but the code here might help:</span>
<span class="sd">  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/xla/experimental/xla_sharding/xla_sharding.py</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">mesh_shape</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">)</span>
  <span class="n">mesh</span> <span class="o">=</span> <span class="n">_get_logical_mesh_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">)</span>

  <span class="n">sharded_axes</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># maps sharded axis identifiers to mesh axis indices to which they&#39;re mapped</span>
  <span class="n">replicated_maxes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># lists mesh axis identifiers to replicate over</span>
  <span class="k">for</span> <span class="n">maxis</span><span class="p">,</span> <span class="n">assignment</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">assignment</span><span class="p">,</span> <span class="n">Replicated</span><span class="p">):</span>
      <span class="n">replicated_maxes</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">maxis</span><span class="p">,</span> <span class="n">assignment</span><span class="o">.</span><span class="n">replicas</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">assignment</span><span class="p">,</span> <span class="n">ShardedAxis</span><span class="p">):</span>
      <span class="n">sharded_axes</span><span class="p">[</span><span class="n">assignment</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">maxis</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">assert_unreachable</span><span class="p">(</span><span class="n">assignment</span><span class="p">)</span>

  <span class="n">proto</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">replicated_maxes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">special_axes</span><span class="p">:</span>
    <span class="n">proto</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">REPLICATED</span>
    <span class="k">return</span> <span class="n">proto</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">proto</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">OTHER</span>

  <span class="n">mesh_permutation</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">new_mesh_shape</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">next_sharded_axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">sharding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">):</span>
      <span class="n">new_mesh_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Add a dummy mesh axis we won&#39;t be sharding over</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Chunked</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">nchunks</span> <span class="ow">in</span> <span class="n">sharding</span><span class="o">.</span><span class="n">chunks</span><span class="p">:</span>
        <span class="n">maxis</span> <span class="o">=</span> <span class="n">sharded_axes</span><span class="p">[</span><span class="n">next_sharded_axis</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">mesh_shape</span><span class="p">[</span><span class="n">maxis</span><span class="p">]</span> <span class="o">==</span> <span class="n">nchunks</span>
        <span class="n">mesh_permutation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">maxis</span><span class="p">)</span>
        <span class="n">next_sharded_axis</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">new_mesh_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">chunks</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot convert unstacked sharding specs to XLA OpSharding&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">assert_unreachable</span><span class="p">(</span><span class="n">sharding</span><span class="p">)</span>

  <span class="c1"># Create a partial sharding proto if tensor is replicated or partitioned</span>
  <span class="c1"># specially over some mesh axes.</span>
  <span class="k">if</span> <span class="n">replicated_maxes</span><span class="p">:</span>
    <span class="n">last_tile_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">axes_by_type</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">OpShardingType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">size_by_type</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">OpShardingType</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">replicated_maxes</span><span class="p">}</span><span class="o">.</span><span class="n">issuperset</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">special_axes</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">replicated_maxes</span><span class="p">:</span>
      <span class="n">ty</span> <span class="o">=</span> <span class="n">special_axes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">REPLICATED</span><span class="p">)</span>
      <span class="n">axes_by_type</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">ty</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
      <span class="n">size_by_type</span><span class="p">[</span><span class="n">ty</span><span class="p">]</span> <span class="o">*=</span> <span class="n">size</span>
    <span class="k">for</span> <span class="n">ty</span><span class="p">,</span> <span class="n">axes</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes_by_type</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">):</span>
      <span class="n">last_tile_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ty</span><span class="p">)</span>
      <span class="n">new_mesh_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">size_by_type</span><span class="p">[</span><span class="n">ty</span><span class="p">])</span>
      <span class="n">mesh_permutation</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="n">proto</span><span class="o">.</span><span class="n">last_tile_dims</span> <span class="o">=</span> <span class="n">last_tile_dims</span>

  <span class="n">proto_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">mesh_permutation</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_mesh_shape</span><span class="p">)</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">tile_assignment_dimensions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">proto_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">tile_assignment_devices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">proto_mesh</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">proto</span>


<span class="k">def</span> <span class="nf">_get_num_ways_dim_sharded</span><span class="p">(</span><span class="n">op_sharding</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]:</span>
  <span class="n">partitions</span> <span class="o">=</span> <span class="n">op_sharding</span><span class="o">.</span><span class="n">tile_assignment_dimensions</span>
  <span class="k">if</span> <span class="n">op_sharding</span><span class="o">.</span><span class="n">last_tile_dims</span> <span class="o">==</span> <span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">REPLICATED</span><span class="p">]:</span>
    <span class="n">replicate_on_last_tile_dim</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">replicate_on_last_tile_dim</span> <span class="o">=</span> <span class="n">op_sharding</span><span class="o">.</span><span class="n">replicate_on_last_tile_dim</span>
    <span class="k">if</span> <span class="n">op_sharding</span><span class="o">.</span><span class="n">last_tile_dims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Unhandled OpSharding type. Please open a bug report!&quot;</span><span class="p">)</span>
  <span class="n">num_replicas</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">replicate_on_last_tile_dim</span><span class="p">:</span>
    <span class="n">num_replicas</span> <span class="o">=</span> <span class="n">partitions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">partitions</span> <span class="o">=</span> <span class="n">partitions</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">partitions</span><span class="p">,</span> <span class="n">num_replicas</span>


<span class="k">def</span> <span class="nf">_op_sharding_to_numpy_indices</span><span class="p">(</span>
    <span class="n">op_sharding</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object_</span><span class="p">)</span>

  <span class="c1"># num_devices is required as an argument when op_sharding is</span>
  <span class="c1"># REPLICATED. `jax.device_count()` cannot be used because you can create</span>
  <span class="c1"># an opsharding with less number of devices than `jax.device_count()`.</span>
  <span class="k">if</span> <span class="n">is_op_sharding_replicated</span><span class="p">(</span><span class="n">op_sharding</span><span class="p">):</span>
    <span class="n">indices</span><span class="o">.</span><span class="n">fill</span><span class="p">((</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">indices</span>

  <span class="k">assert</span> <span class="n">num_devices</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">op_sharding</span><span class="o">.</span><span class="n">tile_assignment_devices</span><span class="p">)</span>

  <span class="n">partitions</span><span class="p">,</span> <span class="n">num_replicas</span> <span class="o">=</span> <span class="n">_get_num_ways_dim_sharded</span><span class="p">(</span><span class="n">op_sharding</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">partitions</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">partitions</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>

  <span class="n">axis_indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Index</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">n_shards</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">partitions</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n_shards</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">axis_indices</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)])</span>
    <span class="k">elif</span> <span class="n">n_shards</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">shard_size</span><span class="p">,</span> <span class="n">ragged</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_shards</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">ragged</span><span class="p">,</span> <span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_shards</span><span class="p">)</span>
      <span class="n">axis_indices</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">shard_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">shard_size</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_shards</span><span class="p">)])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s1">&#39;Unrecognized number of shards. Please file a bug!&#39;</span><span class="p">)</span>

  <span class="n">device_it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">op_sharding</span><span class="o">.</span><span class="n">tile_assignment_devices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idxs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">axis_indices</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_replicas</span><span class="p">):</span>
      <span class="n">indices</span><span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="n">device_it</span><span class="p">)]</span> <span class="o">=</span> <span class="n">idxs</span>
  <span class="k">return</span> <span class="n">indices</span>


<span class="k">def</span> <span class="nf">op_sharding_to_indices</span><span class="p">(</span><span class="n">op_sharding</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                           <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">slice</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]:</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">_op_sharding_to_numpy_indices</span><span class="p">(</span><span class="n">op_sharding</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sharding_spec_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Returns NumPy-style indices corresponding to a sharding spec.</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: The shape of the logical array being sharded.</span>

<span class="sd">  Returns:</span>
<span class="sd">    An ndarray with the same shape as the logical mesh (as derived form</span>
<span class="sd">    `mesh_mapping`). Each entry is a NumPy-style index selecting the subset of</span>
<span class="sd">    the data array to be placed on a corresponding device. The indices can be</span>
<span class="sd">    ints, slice objects with step=1, or tuples of those.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">),</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">)</span>

  <span class="n">has_unstacked</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">)</span>
  <span class="c1"># Take the op sharding indices generation route for pjit/xmap cases.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">has_unstacked</span><span class="p">:</span>
    <span class="n">op_sharding_proto</span> <span class="o">=</span> <span class="n">sharding_spec_sharding_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_op_sharding_to_numpy_indices</span><span class="p">(</span>
        <span class="n">op_sharding_proto</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">)</span>

  <span class="n">axis_indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Index</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">shard_indices_shape</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sharding</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">):</span>
    <span class="n">axis_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">):</span>
      <span class="n">axis_indices</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)])</span>
      <span class="c1"># NOTE: We don&#39;t append unsharded dimensions to shard_indices_shape here,</span>
      <span class="c1">#       because they do not appear in the mesh mapping.</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">):</span>
      <span class="k">assert</span> <span class="n">axis_size</span> <span class="o">==</span> <span class="n">sharding</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">axis_size</span><span class="si">}</span><span class="s1"> != </span><span class="si">{</span><span class="n">sharding</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s1">&#39;</span>
      <span class="n">axis_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">axis_size</span><span class="p">))</span>
      <span class="n">shard_indices_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis_size</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">Chunked</span><span class="p">):</span>
      <span class="n">total_chunks</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">chunks</span><span class="p">))</span>
      <span class="n">shard_size</span><span class="p">,</span> <span class="n">ragged</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">total_chunks</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">ragged</span><span class="p">,</span> <span class="p">(</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">total_chunks</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
      <span class="n">axis_indices</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">shard_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">shard_size</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_chunks</span><span class="p">)])</span>
      <span class="n">shard_indices_shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">chunks</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">assert_unreachable</span><span class="p">(</span><span class="n">sharding</span><span class="p">)</span>

  <span class="c1"># shard_indices is an ndarray representing the sharded axes of the logical array,</span>
  <span class="c1"># with each dimension having size equal to the number of shards across the corresponding</span>
  <span class="c1"># logical array dimension, and each element containing the multi-dimensional index that</span>
  <span class="c1"># is used to extract the corresponding shard of the logical array.</span>
  <span class="n">shard_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">prod</span><span class="p">(</span><span class="n">shard_indices_shape</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object_</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idxs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">axis_indices</span><span class="p">)):</span>
    <span class="n">shard_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">idxs</span>
  <span class="n">shard_indices</span> <span class="o">=</span> <span class="n">shard_indices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shard_indices_shape</span><span class="p">)</span>

  <span class="c1"># Ensure that each sharded axis is used exactly once in the mesh mapping</span>
  <span class="n">num_sharded_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shard_indices_shape</span><span class="p">)</span>
  <span class="n">sharded_dim_perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">axis</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ShardedAxis</span><span class="p">)]</span>
  <span class="k">assert</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sharded_dim_perm</span><span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_sharded_dim</span><span class="p">))</span> <span class="ow">and</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">sharded_dim_perm</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_sharded_dim</span><span class="p">)</span>
  <span class="c1"># Replicate/reorder the indices according to the mesh mapping</span>
  <span class="n">replica_sizes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">replicas</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Replicated</span><span class="p">))</span>
  <span class="n">replica_dim</span><span class="p">,</span> <span class="n">sharded_dim</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">iter</span><span class="p">(</span><span class="n">sharded_dim_perm</span><span class="p">)</span>
  <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span><span class="nb">next</span><span class="p">(</span><span class="n">replica_dim</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Replicated</span><span class="p">)</span> <span class="k">else</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">replica_sizes</span><span class="p">)</span> <span class="o">+</span> <span class="nb">next</span><span class="p">(</span><span class="n">sharded_dim</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">]</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">shard_indices</span><span class="p">,</span> <span class="n">replica_sizes</span> <span class="o">+</span> <span class="n">shard_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">perm</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sharding_spec_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;ShardingSpec(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="si">}</span><span class="s1">)&#39;</span>


<span class="n">ShardingSpec</span><span class="o">.</span><span class="n">mesh_shape</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="n">sharding_spec_mesh_shape</span><span class="p">)</span>
<span class="n">ShardingSpec</span><span class="o">.</span><span class="n">sharding_proto</span> <span class="o">=</span> <span class="n">sharding_spec_sharding_proto</span>
<span class="n">ShardingSpec</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">sharding_spec_indices</span>
<span class="c1"># mypy raises: error: Cannot assign to a method  [assignment]</span>
<span class="n">ShardingSpec</span><span class="o">.</span><span class="fm">__repr__</span> <span class="o">=</span> <span class="n">sharding_spec_repr</span>  <span class="c1"># type: ignore</span>
<span class="c1"># Do not pollute the namespace</span>
<span class="k">del</span> <span class="n">sharding_spec_mesh_shape</span><span class="p">,</span> <span class="n">sharding_spec_indices</span><span class="p">,</span> <span class="n">sharding_spec_repr</span>

<span class="k">def</span> <span class="nf">spec_to_indices</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                    <span class="n">spec</span><span class="p">:</span> <span class="n">ShardingSpec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Index</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Returns numpy-style indices corresponding to a sharding spec.</span>

<span class="sd">  Each index describes a shard of the array. The order of the indices is the</span>
<span class="sd">  same as the device_buffers of a ShardedDeviceArray (i.e. the data is laid out</span>
<span class="sd">  row-major).</span>

<span class="sd">  Args:</span>
<span class="sd">    shape: The shape of the logical array being sharded.</span>
<span class="sd">    spec: Describes how the array is sharded and how the shards are assigned to</span>
<span class="sd">      the logical mesh.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple of length equal to the size of the mesh (inferred as the product of</span>
<span class="sd">    sharded dimension sizes and all replication factors).  Each element is an</span>
<span class="sd">    int, a slice object with step=1, or a tuple thereof, to be treated as an</span>
<span class="sd">    index into the full logical array.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">indices</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>  <span class="c1"># type: ignore</span>


<span class="c1">### util</span>

<span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">_shard_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">arg_indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a list of size len(devices) containing per-device buffers.</span>

<span class="sd">  For the C++ pmap path, we fallback to Python (this function) to shard</span>
<span class="sd">  arguments that are not supported by the C++ `ShardArg`.</span>

<span class="sd">  Arrgs:</span>
<span class="sd">    arg: The Python argument.</span>
<span class="sd">    devices: The list of devices to shard over.</span>
<span class="sd">    arg_indices: A list of `len(devices)` indices to use to shard the argument.</span>
<span class="sd">    mode: An enum telling whether shard_arg is executed via pmap or pjit/xmap.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">ShardedDeviceArray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">arg_indices</span> <span class="o">==</span> <span class="n">arg</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
    <span class="c1"># The shard_arg_handlers allow an extensible set of types to be sharded, but</span>
    <span class="c1"># inline handling for ShardedDeviceArray as a special case for performance</span>
    <span class="c1"># NOTE: we compare indices instead of sharding_spec because</span>
    <span class="c1"># pmap_benchmark.pmap_shard_args_benchmark indicates this is faster.</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">buf</span> <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">device</span><span class="p">()</span> <span class="o">==</span> <span class="n">d</span> <span class="k">else</span> <span class="n">buf</span><span class="o">.</span><span class="n">copy_to_device</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">)</span>
    <span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">arg</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shard_arg_handlers</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)](</span><span class="n">arg</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">arg_indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>


<span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
<span class="k">def</span> <span class="nf">shard_args</span><span class="p">(</span><span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span>
               <span class="n">indices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Index</span><span class="p">]],</span>
               <span class="n">mode</span><span class="p">:</span> <span class="n">InputsHandlerMode</span><span class="p">,</span>
               <span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">]]]:</span>
  <span class="sd">&quot;&quot;&quot;Shard each argument data array along its leading axis.</span>

<span class="sd">  Args:</span>
<span class="sd">    devices: sequence of Devices mapping replica index to a physical device.</span>
<span class="sd">    indices: sequence of the same length as `args` describing how each arg</span>
<span class="sd">      should be sharded/replicated across `devices`. Each element in `indices`</span>
<span class="sd">      is the same length as `devices`.</span>
<span class="sd">    args: a sequence of JaxTypes representing arguments to be sharded according</span>
<span class="sd">      to `indices` and placed on `devices`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A list of length matching args, containing lists of per-device buffers</span>
<span class="sd">    for each argument.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">_shard_arg</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mode</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)]</span>


<span class="n">shard_arg_handlers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">InputsHandlerMode</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">_shard_token</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">device_put</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)),</span> <span class="n">devices</span><span class="p">,</span> <span class="n">replicate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">shard_arg_handlers</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Token</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_token</span>

<span class="k">def</span> <span class="nf">_shard_array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float0</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="nb">bool</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">device_put</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">],</span> <span class="n">devices</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_t</span> <span class="ow">in</span> <span class="n">array_types</span><span class="p">:</span>
  <span class="n">shard_arg_handlers</span><span class="p">[</span><span class="n">_t</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_array</span>

<span class="k">def</span> <span class="nf">_shard_device_array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="n">start_indices</span><span class="p">,</span> <span class="n">limit_indices</span><span class="p">,</span> <span class="n">removed_dims</span> <span class="o">=</span> <span class="n">unzip3</span><span class="p">(</span>
      <span class="n">_as_slice_indices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">)</span>
  <span class="n">shards</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_multi_slice</span><span class="p">(</span><span class="n">start_indices</span><span class="p">,</span> <span class="n">limit_indices</span><span class="p">,</span> <span class="n">removed_dims</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">device_put</span><span class="p">(</span><span class="n">shards</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">device_array</span><span class="o">.</span><span class="n">device_array_types</span><span class="p">:</span>
  <span class="n">shard_arg_handlers</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_device_array</span>


<span class="c1"># NOTE(skye): we could refactor to generate _multi_slice parameters directly</span>
<span class="c1"># from the input ShardingSpec, rather than the indices. However, this would</span>
<span class="c1"># require duplicating the ordering logic of spec_to_indices, which is more</span>
<span class="c1"># subtle and more likely to change than the index logic we have to support here.</span>
<span class="k">def</span> <span class="nf">_as_slice_indices</span><span class="p">(</span><span class="n">arr</span><span class="p">:</span> <span class="n">device_array</span><span class="o">.</span><span class="n">DeviceArrayProtocol</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="n">Index</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
  <span class="sd">&quot;&quot;&quot;Returns start_indices, limit_indices, removed_dims&quot;&quot;&quot;</span>
  <span class="n">start_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">limit_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">removed_dims</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">tuple_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">idx</span><span class="p">,)</span>
  <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sub_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tuple_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="n">start_indices</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_idx</span>
      <span class="n">limit_indices</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_idx</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="n">removed_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">sub_idx</span> <span class="o">==</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
      <span class="k">continue</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">),</span> <span class="n">sub_idx</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_idx</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="n">sub_idx</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_idx</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="n">sub_idx</span>
      <span class="n">start_indices</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_idx</span><span class="o">.</span><span class="n">start</span>
      <span class="n">limit_indices</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_idx</span><span class="o">.</span><span class="n">stop</span>

  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">start_indices</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">limit_indices</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">removed_dims</span><span class="p">)</span> <span class="c1"># type: ignore</span>


<span class="k">def</span> <span class="nf">shard_aval</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">shard_aval_handlers</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">)](</span><span class="n">size</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">aval</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No shard_aval handler for type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>
<span class="n">shard_aval_handlers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">],</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">def</span> <span class="nf">_shard_abstract_array</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">!=</span> <span class="n">size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Axis size </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2"> does not match dimension </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2"> of &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;shape </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot split a </span><span class="si">{x.dim}</span><span class="s2">D value along axis </span><span class="si">{axis}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">tuple_delete</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
<span class="n">shard_aval_handlers</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_abstract_array</span>


<span class="k">class</span> <span class="nc">_AUTOAxisResource</span><span class="p">:</span>
  <span class="k">pass</span>
<span class="n">AUTO</span> <span class="o">=</span> <span class="n">_AUTOAxisResource</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_is_auto</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_UnspecifiedValue</span><span class="p">:</span>
  <span class="k">pass</span>
<span class="n">_UNSPECIFIED</span> <span class="o">=</span> <span class="n">_UnspecifiedValue</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_is_unspecified</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_UnspecifiedValue</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">ArrayMapping specifies how an ndarray should map to mesh axes.</span>

<span class="sd">Note that the ordering is crucial for the cases when this mapping is non-injective</span>
<span class="sd">(i.e. when multiple mesh axes map to the same positional axis). Then, the</span>
<span class="sd">order of entries of the mapping determines a major-to-minor order on mesh axes,</span>
<span class="sd">according to which chunks of the value along the repeated dimension will be assigned.</span>

<span class="sd">For example, consider a mapping {&#39;x&#39;: 1, &#39;y&#39;: 1} and a mesh with shape {&#39;x&#39;: 2, &#39;y&#39;: 3}.</span>
<span class="sd">The second dimension of the value would get chunked into 6 pieces, and assigned to the</span>
<span class="sd">mesh in a way that treats &#39;y&#39; as the fastest changing (minor) dimension. In this case,</span>
<span class="sd">that would mean that a flat list of chunks would get assigned to a flattened list of</span>
<span class="sd">mesh devices without any modifications. If the mapping was {&#39;y&#39;: 1, &#39;x&#39;: 1}, then the</span>
<span class="sd">mesh devices ndarray would have to be transposed before flattening and assignment.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">ArrayMapping</span> <span class="o">=</span> <span class="n">OrderedDictType</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
<span class="n">ArrayMappingOrAutoOrUnspecified</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">,</span>
                                        <span class="n">_UnspecifiedValue</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">array_mapping_to_axis_resources</span><span class="p">(</span><span class="n">array_mapping</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">):</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">array_mapping</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">PartitionSpec</span><span class="p">()</span>
  <span class="n">max_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
  <span class="n">reverse_map</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">array_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">reverse_map</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;</span> <span class="n">max_index</span><span class="p">:</span>
      <span class="n">max_index</span> <span class="o">=</span> <span class="n">index</span>
  <span class="n">partitions</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">reverse_map</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">if</span> <span class="n">reverse_map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
                     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="o">*</span><span class="n">partitions</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OutputType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="n">Array</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">GlobalDeviceArray</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">ShardedDeviceArray</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">local_aval_to_result_handler</span><span class="p">(</span>
    <span class="n">aval</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">,</span>
    <span class="n">sharding</span><span class="p">:</span> <span class="n">XLACompatibleSharding</span><span class="p">,</span>
    <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Index</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Returns a function for handling the raw buffers of a single output aval.</span>

<span class="sd">  Args:</span>
<span class="sd">    aval: The local output AbstractValue.</span>
<span class="sd">    sharding_spec: Indicates how the output is sharded across devices, or None</span>
<span class="sd">      for non-array avals.</span>
<span class="sd">    indices: The pre-computed result of spec_to_indices, or None for non-array</span>
<span class="sd">      avals.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function for handling the Buffers that will eventually be produced</span>
<span class="sd">    for this output. The function will return an object suitable for returning</span>
<span class="sd">    to the user, e.g. a ShardedDeviceArray.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">Array</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">ShardedDeviceArray</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">local_result_handlers</span><span class="p">[(</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">),</span> <span class="n">output_type</span><span class="p">)](</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;No pxla_result_handler for type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>

<span class="n">PxlaResultHandler</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[</span>
    <span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">],</span> <span class="n">xb</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]]</span>
<span class="n">local_result_handlers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">],</span> <span class="n">OutputType</span><span class="p">],</span> <span class="n">PxlaResultHandler</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">sda_array_result_handler</span><span class="p">(</span><span class="n">aval</span><span class="p">:</span> <span class="n">ShapedArray</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
  <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">_get_sharding_specs</span><span class="p">([</span><span class="n">sharding</span><span class="p">],</span> <span class="p">[</span><span class="n">aval</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">local_sharded_result_handler</span><span class="p">(</span>
        <span class="n">aval</span><span class="p">,</span> <span class="n">sharding</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">bufs</span><span class="p">:</span> <span class="n">make_sharded_device_array</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">bufs</span><span class="p">,</span>
                                                  <span class="n">indices</span><span class="p">)</span>
<span class="n">local_result_handlers</span><span class="p">[(</span><span class="n">ShapedArray</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">ShardedDeviceArray</span><span class="p">)]</span> <span class="o">=</span> <span class="n">sda_array_result_handler</span>
<span class="n">local_result_handlers</span><span class="p">[(</span><span class="n">ConcreteArray</span><span class="p">,</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">ShardedDeviceArray</span><span class="p">)]</span> <span class="o">=</span> <span class="n">sda_array_result_handler</span>


<span class="k">def</span> <span class="nf">global_aval_to_result_handler</span><span class="p">(</span>
    <span class="n">aval</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">,</span> <span class="n">out_sharding</span><span class="p">,</span> <span class="n">committed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">is_out_sharding_from_xla</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Returns a function for handling the raw buffers of a single output aval.</span>

<span class="sd">  Args:</span>
<span class="sd">    aval: The global output AbstractValue.</span>
<span class="sd">    out_axis_resources: A PartitionSpec specifying the sharding of outputs.</span>
<span class="sd">      Used for creating GSDAs.</span>
<span class="sd">    global_mesh: The global device mesh that generated this output. Used</span>
<span class="sd">      for creating GSDAs.</span>
<span class="sd">    is_out_sharding_from_xla: True, if the out_sharding comes from XLA i.e.</span>
<span class="sd">      the sharding is extracted from the HLO.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A function for handling the Buffers that will eventually be produced</span>
<span class="sd">    for this output. The function will return an object suitable for returning</span>
<span class="sd">    to the user, e.g. a ShardedDeviceArray.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">Array</span>
  <span class="k">elif</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_parallel_functions_output_gda</span><span class="p">:</span>
    <span class="n">output_type</span> <span class="o">=</span> <span class="n">OutputType</span><span class="o">.</span><span class="n">GlobalDeviceArray</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">global_result_handlers</span><span class="p">[(</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">),</span> <span class="n">output_type</span><span class="p">)](</span>
        <span class="n">aval</span><span class="p">,</span> <span class="n">out_sharding</span><span class="p">,</span> <span class="n">committed</span><span class="p">,</span> <span class="n">is_out_sharding_from_xla</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;No pxla_result_handler for type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>

<span class="n">global_result_handlers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">],</span> <span class="n">OutputType</span><span class="p">],</span> <span class="n">PxlaResultHandler</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1">### lazy device-memory persistence and result handling</span>

<span class="c1"># TODO(jblespiau): Consider removing this option.</span>
<span class="n">_USE_CPP_SDA</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_create_pmap_sharding_spec</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharded_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sharded_dim_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">sharded_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">sharded_aval</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">sharded_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">sharded_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">if</span> <span class="n">sharded_dim_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">sharded_dim_size</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">sharded_dim</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">sharded_dim_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">sharded_aval</span> <span class="o">=</span> <span class="n">aval</span>

  <span class="k">return</span> <span class="n">_pmap_sharding_spec</span><span class="p">(</span><span class="n">sharded_dim_size</span><span class="p">,</span> <span class="n">sharded_dim_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                             <span class="n">sharded_aval</span><span class="p">,</span> <span class="n">sharded_dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make_sharded_device_array</span><span class="p">(</span>
    <span class="n">aval</span><span class="p">:</span> <span class="n">ShapedArray</span><span class="p">,</span>
    <span class="n">sharding_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShardingSpec</span><span class="p">],</span>
    <span class="c1"># Any is for JAX extensions implementing their own buffer.</span>
    <span class="n">device_buffers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">]],</span>
    <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Index</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a ShardedDeviceArray implementation based on arguments.</span>

<span class="sd">  Returns either a C++ SDA or a Python DeviceArray when the buffers are not</span>
<span class="sd">  JAX buffers.</span>

<span class="sd">  Args:</span>
<span class="sd">    aval: The `ShapedArray` for this array.</span>
<span class="sd">    sharding_spec: If `None`, assumes a pmap-style ShardedDeviceArrays over the</span>
<span class="sd">      first dimension.</span>
<span class="sd">    device_buffers: If a list of Jax `Buffer` objects, a C++ SDA will be</span>
<span class="sd">      returned (if the version is high enough). Otherwise, a Python object will</span>
<span class="sd">      be returned, for JAX extensions not implementing the C++ API.</span>
<span class="sd">    indices: For caching purposes, will be computed if `None`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">sharding_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">_create_pmap_sharding_spec</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">spec_to_indices</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">_USE_CPP_SDA</span> <span class="ow">and</span>
      <span class="p">(</span><span class="ow">not</span> <span class="n">device_buffers</span> <span class="ow">or</span>
       <span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">use_sharded_buffer</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_buffers</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">))</span> <span class="ow">or</span>
       <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_buffers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">))):</span>
    <span class="k">return</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArray</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
        <span class="n">aval</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">device_buffers</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">,</span> <span class="n">aval</span><span class="o">.</span><span class="n">weak_type</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">_ShardedDeviceArray</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">device_buffers</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>


<span class="k">if</span> <span class="n">_USE_CPP_SDA</span><span class="p">:</span>
  <span class="n">ShardedDeviceArrayBase</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArrayBase</span>  <span class="c1"># type: ignore</span>
  <span class="c1"># We want the C++ SDA to extend the DeviceArrayBase. We want this both to</span>
  <span class="c1"># benefit from its methods, and to have isinstance(x, DeviceArray) return true</span>
  <span class="n">ShardedDeviceArrayBase</span><span class="o">.</span><span class="vm">__bases__</span> <span class="o">=</span> <span class="p">((</span><span class="n">device_array</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">,)</span> <span class="o">+</span>  <span class="c1"># type: ignore</span>
                                      <span class="n">ShardedDeviceArrayBase</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">)</span>
  <span class="n">_SDA_BASE_CLASS</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArrayBase</span>  <span class="c1"># type: ignore</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">_SDA_BASE_CLASS</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">device_array</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_array</span><span class="o">.</span><span class="n">DeviceArray</span>  <span class="c1"># type: ignore</span>
<span class="n">basearray</span><span class="o">.</span><span class="n">Array</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">_SDA_BASE_CLASS</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ShardedDeviceArray</span><span class="p">(</span><span class="n">_SDA_BASE_CLASS</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
  <span class="sd">&quot;&quot;&quot;A ShardedDeviceArray is an ndarray sharded across devices.</span>

<span class="sd">  The purpose of a ShardedDeviceArray is to reduce the number of transfers when</span>
<span class="sd">  executing replicated computations, by allowing results to persist on the</span>
<span class="sd">  devices that produced them. That way dispatching a similarly replicated</span>
<span class="sd">  computation that consumes the same sharded memory layout does not incur any</span>
<span class="sd">  transfers.</span>

<span class="sd">  A ShardedDeviceArray represents one logical ndarray value, and simulates the</span>
<span class="sd">  behavior of an ndarray so that it can be treated by user code as an ndarray;</span>
<span class="sd">  that is, it is only an optimization to reduce transfers.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    aval: A ShapedArray indicating the shape and dtype of this array.</span>
<span class="sd">    sharding_spec: describes how this array is sharded across `device_buffers`.</span>
<span class="sd">    device_buffers: the buffers containing the data for this array. Each buffer</span>
<span class="sd">      is the same shape and on a different device. Buffers are in row-major</span>
<span class="sd">      order, with replication treated as an extra innermost dimension.</span>
<span class="sd">    indices: the result of spec_to_indices(sharding_spec). Can optionally be</span>
<span class="sd">      precomputed for efficiency. A list the same length as</span>
<span class="sd">      `device_buffers`. Each index indicates what portion of the full array is</span>
<span class="sd">      stored in the corresponding device buffer, i.e. `array[indices[i]] ==</span>
<span class="sd">      np.asarray(device_buffers[i])`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span>
      <span class="s2">&quot;aval&quot;</span><span class="p">,</span> <span class="s2">&quot;device_buffers&quot;</span><span class="p">,</span> <span class="s2">&quot;sharding_spec&quot;</span><span class="p">,</span> <span class="s2">&quot;indices&quot;</span><span class="p">,</span>
      <span class="s2">&quot;_one_replica_buffer_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;_npy_value&quot;</span>
  <span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">aval</span><span class="p">:</span> <span class="n">ShapedArray</span><span class="p">,</span>
               <span class="n">sharding_spec</span><span class="p">:</span> <span class="n">ShardingSpec</span><span class="p">,</span>
               <span class="n">device_buffers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">],</span>
               <span class="n">indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Index</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># TODO(skye): assert invariants. Keep performance in mind though.</span>
    <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">spec_to_indices</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">aval</span> <span class="o">=</span> <span class="n">aval</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span> <span class="o">=</span> <span class="n">device_buffers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">sharding_spec</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_one_replica_buffer_indices</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_enable_checks</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span> <span class="ow">is</span> <span class="n">ShapedArray</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">:</span>
      <span class="n">buf</span><span class="o">.</span><span class="n">delete</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_one_replica_buffer_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Index</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Returns a set of buffer-indices containing one complete copy of the array.&quot;&quot;&quot;</span>
  <span class="n">one_replica_indices</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">seen_index_hashes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
    <span class="n">hashed_index</span> <span class="o">=</span> <span class="n">_hashable_index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hashed_index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_index_hashes</span><span class="p">:</span>
      <span class="n">one_replica_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="n">seen_index_hashes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hashed_index</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">one_replica_indices</span>


<span class="k">def</span> <span class="nf">_sda_one_replica_buffer_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Indices of buffers containing one complete copy of the array data.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_one_replica_buffer_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_one_replica_buffer_indices</span> <span class="o">=</span> <span class="n">_one_replica_buffer_indices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_one_replica_buffer_indices</span>


<span class="k">def</span> <span class="nf">_sda_copy_to_host_async</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">buffer_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_replica_buffer_indices</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">[</span><span class="n">buffer_index</span><span class="p">]</span><span class="o">.</span><span class="n">copy_to_host_async</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_sda_check_if_deleted</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ShardedDeviceArray has been deleted.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_sda_block_until_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_deleted</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">:</span>
    <span class="n">buf</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">()</span>
  <span class="k">return</span> <span class="bp">self</span>


<span class="k">def</span> <span class="nf">_sda_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">copy_to_host_async</span><span class="p">()</span>
    <span class="n">npy_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_replica_buffer_indices</span><span class="p">:</span>
      <span class="n">npy_value</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span> <span class="o">=</span> <span class="n">npy_value</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span>


<span class="k">def</span> <span class="nf">_sda__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_check_if_deleted</span><span class="p">()</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="n">cidx</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span><span class="p">,)</span> <span class="o">+</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">cidx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_npy_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">buf_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">cidx</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
      <span class="n">buf_idx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">buf_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">[</span><span class="n">buf_idx</span><span class="p">]</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">ShapedArray</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">device_array</span><span class="o">.</span><span class="n">make_device_array</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_sda__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d array&quot;</span><span class="p">)</span>  <span class="c1"># same as numpy error</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">_sda__reversed__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d array&quot;</span><span class="p">)</span>  <span class="c1"># same as numpy error</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_sda_sharding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="n">has_unstacked</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_spec</span><span class="o">.</span><span class="n">sharding</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">has_unstacked</span><span class="p">:</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">d</span><span class="o">.</span><span class="n">device</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">PmapSharding</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding_spec</span><span class="p">)</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;SDAs that are the output of pjit/xmap do not &#39;</span>
                            <span class="s1">&#39;have the sharding attribute implemented. Please &#39;</span>
                            <span class="s1">&#39;use the new `jax.Array` type instead.&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_sda_addressable_shards</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span>
  <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">db</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_buffers</span><span class="p">:</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">_set_aval</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">array</span><span class="o">.</span><span class="n">Shard</span><span class="p">(</span><span class="n">db</span><span class="o">.</span><span class="n">device</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">db</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">out</span>


<span class="k">for</span> <span class="n">sda</span> <span class="ow">in</span> <span class="p">[</span><span class="n">_ShardedDeviceArray</span><span class="p">,</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArray</span><span class="p">]:</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;one_replica_buffer_indices&quot;</span><span class="p">,</span>
          <span class="nb">property</span><span class="p">(</span><span class="n">_sda_one_replica_buffer_indices</span><span class="p">))</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;copy_to_host_async&quot;</span><span class="p">,</span> <span class="n">_sda_copy_to_host_async</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;_check_if_deleted&quot;</span><span class="p">,</span> <span class="n">_sda_check_if_deleted</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;block_until_ready&quot;</span><span class="p">,</span> <span class="n">_sda_block_until_ready</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;_value&quot;</span><span class="p">,</span> <span class="nb">property</span><span class="p">(</span><span class="n">_sda_value</span><span class="p">))</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;__getitem__&quot;</span><span class="p">,</span> <span class="n">_sda__getitem__</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;__iter__&quot;</span><span class="p">,</span> <span class="n">_sda__iter__</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;__reversed__&quot;</span><span class="p">,</span> <span class="n">_sda__reversed__</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;sharding&quot;</span><span class="p">,</span> <span class="nb">property</span><span class="p">(</span><span class="n">_sda_sharding</span><span class="p">))</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span> <span class="s2">&quot;addressable_shards&quot;</span><span class="p">,</span> <span class="nb">property</span><span class="p">(</span><span class="n">_sda_addressable_shards</span><span class="p">))</span>

<span class="k">del</span> <span class="p">(</span><span class="n">_sda_one_replica_buffer_indices</span><span class="p">,</span> <span class="n">_sda_copy_to_host_async</span><span class="p">,</span>
     <span class="n">_sda_check_if_deleted</span><span class="p">,</span> <span class="n">_sda_block_until_ready</span><span class="p">,</span> <span class="n">_sda_value</span><span class="p">,</span> <span class="n">_sda__getitem__</span><span class="p">,</span>
     <span class="n">_sda_sharding</span><span class="p">,</span> <span class="n">_sda_addressable_shards</span><span class="p">)</span>


<span class="n">ShardedDeviceArray</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="nb">object</span><span class="p">]</span>
<span class="k">if</span> <span class="n">_USE_CPP_SDA</span><span class="p">:</span>
  <span class="n">ShardedDeviceArray</span> <span class="o">=</span> <span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArrayBase</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">ShardedDeviceArray</span> <span class="o">=</span> <span class="n">_ShardedDeviceArray</span>


<span class="k">def</span> <span class="nf">_hashable_index</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">stop</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nb">slice</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

<span class="c1"># The fast path is handled directly in shard_args().</span>
<span class="c1"># TODO(skye): is there a simpler way to rewrite this using sharding_spec?</span>
<span class="k">def</span> <span class="nf">_shard_sharded_device_array_slow_path</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax._src.array</span> <span class="kn">import</span> <span class="n">ArrayImpl</span>

  <span class="n">candidates</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ArrayImpl</span><span class="p">):</span>
    <span class="n">bufs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_arrays</span>
    <span class="n">arr_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">devices_indices_map</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">bufs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device_buffers</span>
    <span class="n">arr_indices</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">indices</span>
  <span class="k">for</span> <span class="n">buf</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">bufs</span><span class="p">,</span> <span class="n">arr_indices</span><span class="p">):</span>
    <span class="n">candidates</span><span class="p">[</span><span class="n">_hashable_index</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>

  <span class="n">bufs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">devices</span><span class="p">):</span>
    <span class="c1"># Look up all buffers that contain the correct slice of the logical array.</span>
    <span class="n">candidates_list</span> <span class="o">=</span> <span class="n">candidates</span><span class="p">[</span><span class="n">_hashable_index</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">candidates_list</span><span class="p">:</span>
      <span class="c1"># This array isn&#39;t sharded correctly. Reshard it via host roundtrip.</span>
      <span class="c1"># TODO(skye): more efficient reshard?</span>
      <span class="k">return</span> <span class="n">shard_arg_handlers</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">_value</span><span class="p">)](</span><span class="n">x</span><span class="o">.</span><span class="n">_value</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
    <span class="c1"># Try to find a candidate buffer already on the correct device,</span>
    <span class="c1"># otherwise copy one of them.</span>
    <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">candidates_list</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">buf</span><span class="o">.</span><span class="n">device</span><span class="p">()</span> <span class="o">==</span> <span class="n">device</span><span class="p">:</span>
        <span class="n">bufs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">bufs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">copy_to_device</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">bufs</span>


<span class="k">def</span> <span class="nf">_sharded_device_array_mlir_constant_handler</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">canonicalize_types</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ir_constants</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">val</span><span class="p">),</span>
                           <span class="n">canonicalize_types</span><span class="o">=</span><span class="n">canonicalize_types</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_register_handlers_for_sharded_device_array</span><span class="p">(</span><span class="n">sda</span><span class="p">):</span>
  <span class="n">shard_arg_handlers</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">_shard_sharded_device_array_slow_path</span>
  <span class="n">mlir</span><span class="o">.</span><span class="n">register_constant_handler</span><span class="p">(</span><span class="n">sda</span><span class="p">,</span>
                                 <span class="n">_sharded_device_array_mlir_constant_handler</span><span class="p">)</span>

  <span class="n">core</span><span class="o">.</span><span class="n">pytype_aval_mappings</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">abstract_arrays</span><span class="o">.</span><span class="n">canonical_concrete_aval</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">device_put_handlers</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">_device_put_array</span>
  <span class="n">xla</span><span class="o">.</span><span class="n">pytype_aval_mappings</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">attrgetter</span><span class="p">(</span><span class="s2">&quot;aval&quot;</span><span class="p">)</span>
  <span class="n">xla</span><span class="o">.</span><span class="n">canonicalize_dtype_handlers</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">identity</span>
  <span class="n">api_util</span><span class="o">.</span><span class="n">_shaped_abstractify_handlers</span><span class="p">[</span><span class="n">sda</span><span class="p">]</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">attrgetter</span><span class="p">(</span><span class="s2">&quot;aval&quot;</span><span class="p">)</span>

<span class="n">_register_handlers_for_sharded_device_array</span><span class="p">(</span><span class="n">_ShardedDeviceArray</span><span class="p">)</span>
<span class="n">_register_handlers_for_sharded_device_array</span><span class="p">(</span><span class="n">pmap_lib</span><span class="o">.</span><span class="n">ShardedDeviceArray</span><span class="p">)</span>

<span class="c1">### the xla_pmap primitive and its rules are comparable to xla_call in xla.py</span>

<span class="k">def</span> <span class="nf">xla_pmap_impl</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                  <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span>
                  <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">global_axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
                  <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                  <span class="n">in_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                  <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
                  <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                  <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]):</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">jax_disable_jit</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_eager_pmap</span> <span class="ow">and</span>
      <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">donated_invars</span><span class="p">)</span> <span class="ow">and</span>
      <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">global_arg_shapes</span><span class="p">)):</span>
    <span class="k">return</span> <span class="n">_emap_impl</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="n">axis_name</span><span class="p">,</span>
                      <span class="n">axis_size</span><span class="o">=</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="o">=</span><span class="n">global_axis_size</span><span class="p">,</span>
                      <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
                      <span class="n">out_axes_thunk</span><span class="o">=</span><span class="n">out_axes_thunk</span><span class="p">,</span>
                      <span class="n">donated_invars</span><span class="o">=</span><span class="n">donated_invars</span><span class="p">,</span>
                      <span class="n">global_arg_shapes</span><span class="o">=</span><span class="n">global_arg_shapes</span><span class="p">)</span>
  <span class="n">abstract_args</span> <span class="o">=</span> <span class="n">unsafe_map</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
  <span class="n">compiled_fun</span><span class="p">,</span> <span class="n">fingerprint</span> <span class="o">=</span> <span class="n">parallel_callable</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
      <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span>
      <span class="o">*</span><span class="n">abstract_args</span><span class="p">)</span>

  <span class="c1"># Don&#39;t re-abstractify args unless logging is enabled for performance.</span>
  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_distributed_debug</span><span class="p">:</span>
    <span class="n">distributed_debug_log</span><span class="p">((</span><span class="s2">&quot;Running pmapped function&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span>
                          <span class="p">(</span><span class="s2">&quot;python function&quot;</span><span class="p">,</span> <span class="n">fun</span><span class="o">.</span><span class="n">f</span><span class="p">),</span>
                          <span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="p">),</span>
                          <span class="p">(</span><span class="s2">&quot;abstract args&quot;</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">,</span> <span class="n">args</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s2">&quot;fingerprint&quot;</span><span class="p">,</span> <span class="n">fingerprint</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">compiled_fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">EmapInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
  <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">_emap_impl</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span>
               <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
               <span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span>
               <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">global_axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
               <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
               <span class="n">in_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
               <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
               <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
               <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]):</span>
  <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span>
  <span class="c1"># TODO(sharadmv,mattjj): implement these cases</span>
  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">donated_invars</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Buffer donation not supported in eager pmap.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">global_arg_shapes</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Global arg shapes not supported in eager pmap.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Non-default global_axis_size not supported in &quot;</span>
                              <span class="s2">&quot;eager pmap.&quot;</span><span class="p">)</span>

  <span class="n">emap_info</span> <span class="o">=</span> <span class="n">EmapInfo</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
  <span class="n">shard_axes</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">if</span> <span class="n">in_axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="n">axis_name</span><span class="p">:</span> <span class="n">in_axis</span><span class="p">}</span> <span class="k">for</span> <span class="n">in_axis</span> <span class="ow">in</span> <span class="n">in_axes</span><span class="p">]</span>
  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">new_base_main</span><span class="p">(</span><span class="n">MapTrace</span><span class="p">,</span> <span class="n">emap_info</span><span class="o">=</span><span class="n">emap_info</span><span class="p">)</span> <span class="k">as</span> <span class="n">main</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">new_sublevel</span><span class="p">(),</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">main</span><span class="p">):</span>
      <span class="n">t</span> <span class="o">=</span> <span class="n">main</span><span class="o">.</span><span class="n">with_cur_sublevel</span><span class="p">()</span>
      <span class="n">tracers</span> <span class="o">=</span> <span class="p">[</span>
          <span class="n">MapTracer</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">shard_axes</span><span class="p">)]</span>
      <span class="n">ans</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">tracers</span><span class="p">)</span>
      <span class="n">out_tracers</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">full_raise</span><span class="p">,</span> <span class="n">ans</span><span class="p">)</span>
      <span class="n">outvals</span><span class="p">,</span> <span class="n">out_axes_src</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">((</span><span class="n">t</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">out_tracers</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">main</span>
  <span class="n">out_axes</span> <span class="o">=</span> <span class="n">out_axes_thunk</span><span class="p">()</span>

  <span class="n">platform</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span><span class="o">.</span><span class="n">platform</span>
  <span class="n">donate_argnums</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="k">if</span> <span class="n">platform</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;rocm&quot;</span><span class="p">,</span> <span class="s2">&quot;tpu&quot;</span><span class="p">}</span> <span class="k">else</span> <span class="p">()</span>
  <span class="n">new_outvals</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">out_axis_src</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">,</span> <span class="n">outval</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_axes_src</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">outvals</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">jax</span><span class="o">.</span><span class="n">disable_jit</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
      <span class="n">donate_argnums_</span> <span class="o">=</span> <span class="n">donate_argnums</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outval</span><span class="p">,</span> <span class="p">(</span><span class="n">ShardedDeviceArray</span><span class="p">,</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">)):</span>
        <span class="c1"># We don&#39;t want to donate if it&#39;s already sharded.</span>
        <span class="n">donate_argnums_</span> <span class="o">=</span> <span class="p">()</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
          <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_axis_src</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">axis_name</span><span class="p">)),</span>
          <span class="n">out_axes</span><span class="o">=</span><span class="n">out_axis</span><span class="p">,</span>
          <span class="n">devices</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">devices</span><span class="p">)),</span>
          <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
          <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums_</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">axis_size</span><span class="p">),</span> <span class="n">outval</span><span class="p">)</span>
      <span class="n">new_outvals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_outvals</span>

<span class="k">def</span> <span class="nf">_map_schedule</span><span class="p">(</span><span class="n">idx</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
  <span class="c1"># In order to do a multi-map (a simultaneous map over several axes), we will</span>
  <span class="c1"># nest several maps. Each time we do a map, we &quot;remove&quot; an input axis so we</span>
  <span class="c1"># need to update the remaining map axes. For example, if we are to map over</span>
  <span class="c1"># the axes 0, 3, and 4, we make three calls to pmap with in_axes as 0, 2, 2.</span>
  <span class="k">return</span> <span class="p">[</span><span class="kc">None</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span>
          <span class="n">i</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">j</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">i</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">[:</span><span class="n">l</span><span class="p">])</span>
          <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">_multi_pmap</span><span class="p">(</span><span class="n">f</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">EmapInfo</span><span class="p">,</span> <span class="n">names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">],</span>
                <span class="n">all_axes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="o">...</span><span class="p">]]</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
  <span class="n">used_names</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">names</span><span class="p">))):</span>
    <span class="n">in_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">arg_axis</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">arg_axis</span> <span class="ow">in</span> <span class="n">all_axes</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">in_axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">in_axis</span> <span class="ow">in</span> <span class="n">in_axes</span><span class="p">):</span>
      <span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">pmap</span><span class="p">(</span>
          <span class="n">f</span><span class="p">,</span>
          <span class="n">in_axes</span><span class="o">=</span><span class="n">in_axes</span><span class="p">,</span>
          <span class="n">axis_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">out_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">backend</span><span class="o">=</span><span class="n">info</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
          <span class="n">devices</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span> <span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">devices</span><span class="p">)))</span>
      <span class="n">used_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="n">out_shard_axes</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">used_names</span><span class="p">))}</span>
  <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">out_shard_axes</span>

<span class="k">class</span> <span class="nc">MapTrace</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">Trace</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">emap_info</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">emap_info</span> <span class="o">=</span> <span class="n">emap_info</span>

  <span class="k">def</span> <span class="nf">pure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MapTracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="p">{})</span>

  <span class="k">def</span> <span class="nf">sublift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracer</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MapTracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">process_primitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">primitive</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">payload</span><span class="p">[</span><span class="s2">&quot;emap_info&quot;</span><span class="p">]</span>
    <span class="n">vals</span><span class="p">,</span> <span class="n">shard_axes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">([(</span><span class="n">t</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tracers</span><span class="p">])</span>
    <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">thread_local_state</span><span class="o">.</span><span class="n">trace_state</span><span class="o">.</span><span class="n">axis_env</span>
             <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">main_trace</span> <span class="ow">is</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">]</span>
    <span class="n">all_axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">_map_schedule</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">names</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shard_axes</span><span class="p">]</span>
    <span class="n">f_mapped</span><span class="p">,</span> <span class="n">out_shard_axes</span> <span class="o">=</span> <span class="n">_multi_pmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">),</span>
                                           <span class="n">info</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">all_axes</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">eval_context</span><span class="p">(),</span> <span class="n">jax</span><span class="o">.</span><span class="n">disable_jit</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
      <span class="n">outvals</span> <span class="o">=</span> <span class="n">f_mapped</span><span class="p">(</span><span class="o">*</span><span class="n">vals</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">primitive</span><span class="o">.</span><span class="n">multiple_results</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">MapTracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">out_shard_axes</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">outvals</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">MapTracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outvals</span><span class="p">,</span> <span class="n">out_shard_axes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">process_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_primitive</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">call_primitive</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">xla</span><span class="o">.</span><span class="n">xla_call_p</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span>
    <span class="n">fake_primitive</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">SimpleNamespace</span><span class="p">(</span>
        <span class="n">multiple_results</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bind</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">call_primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">,</span> <span class="n">fun</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="n">fake_primitive</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">process_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_primitive</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;devices&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Nested pmap with explicit devices argument.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_disable_jit</span><span class="p">:</span>
      <span class="n">fake_primitive</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">SimpleNamespace</span><span class="p">(</span>
          <span class="n">multiple_results</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bind</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">call_primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">,</span> <span class="n">fun</span><span class="p">))</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="n">fake_primitive</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">axis_name</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="p">,</span> <span class="n">axis_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;axis_name&quot;</span><span class="p">],</span>
        <span class="n">params</span><span class="p">[</span><span class="s2">&quot;in_axes&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;out_axes_thunk&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;axis_size&quot;</span><span class="p">])</span>
    <span class="n">vals</span><span class="p">,</span> <span class="n">shard_axes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">([(</span><span class="n">t</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tracers</span><span class="p">])</span>
    <span class="n">shard_axes</span> <span class="o">=</span> <span class="p">[{</span><span class="n">axis_name</span><span class="p">:</span> <span class="n">_annot_to_flat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">s</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">ax</span><span class="p">),</span> <span class="o">**</span><span class="n">s</span><span class="p">}</span>
                  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">s</span>
                  <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">shard_axes</span><span class="p">)]</span>
    <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">new_sublevel</span><span class="p">(),</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">):</span>
      <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">with_cur_sublevel</span><span class="p">()</span>
      <span class="n">in_tracers</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">MapTracer</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">vals</span><span class="p">,</span> <span class="n">shard_axes</span><span class="p">)</span>
      <span class="n">ans</span> <span class="o">=</span> <span class="n">fun</span><span class="o">.</span><span class="n">call_wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">in_tracers</span><span class="p">)</span>
      <span class="n">out_tracers</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">full_raise</span><span class="p">,</span> <span class="n">ans</span><span class="p">)</span>
      <span class="n">out</span><span class="p">,</span> <span class="n">outaxes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">((</span><span class="n">t</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">out_tracers</span><span class="p">)</span>
      <span class="k">del</span> <span class="n">t</span><span class="p">,</span> <span class="n">in_tracers</span><span class="p">,</span> <span class="n">ans</span><span class="p">,</span> <span class="n">out_tracers</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">outaxes</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">_match_annot</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dst</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">outaxes</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="p">()))</span>
    <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">MapTracer</span><span class="p">,</span> <span class="bp">self</span><span class="p">),</span> <span class="n">out</span><span class="p">,</span> <span class="n">outaxes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">process_custom_jvp_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">primitive</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">jvp</span><span class="p">,</span> <span class="n">tracers</span><span class="p">):</span>
    <span class="n">fake_primitive</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">SimpleNamespace</span><span class="p">(</span>
        <span class="n">multiple_results</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bind</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">jvp</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="n">fake_primitive</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="p">{})</span>

  <span class="k">def</span> <span class="nf">process_custom_vjp_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">primitive</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">fwd</span><span class="p">,</span> <span class="n">bwd</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span>
                              <span class="n">out_trees</span><span class="p">):</span>
    <span class="n">fake_primitive</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">SimpleNamespace</span><span class="p">(</span>
        <span class="n">multiple_results</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bind</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">primitive</span><span class="o">.</span><span class="n">bind</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">fwd</span><span class="p">,</span> <span class="n">bwd</span><span class="p">,</span>
                                            <span class="n">out_trees</span><span class="o">=</span><span class="n">out_trees</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="n">fake_primitive</span><span class="p">,</span> <span class="n">tracers</span><span class="p">,</span> <span class="p">{})</span>

  <span class="k">def</span> <span class="nf">process_axis_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
    <span class="n">fake_primitive</span> <span class="o">=</span> <span class="n">types</span><span class="o">.</span><span class="n">SimpleNamespace</span><span class="p">(</span>
        <span class="n">multiple_results</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bind</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">axis_index</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">eval_context</span><span class="p">():</span>
      <span class="nb">range</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">iota</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">frame</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">dummy_tracer</span> <span class="o">=</span> <span class="n">MapTracer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="p">{</span><span class="n">frame</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_primitive</span><span class="p">(</span><span class="n">fake_primitive</span><span class="p">,</span> <span class="p">(</span><span class="n">dummy_tracer</span><span class="p">,),</span> <span class="p">{})</span>

<span class="k">def</span> <span class="nf">_annot_to_flat</span><span class="p">(</span><span class="n">ndim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mapped_axes</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">annotation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
  <span class="k">if</span> <span class="n">annotation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="kc">None</span>
  <span class="n">mapped_axes_</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">mapped_axes</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mapped_axes_</span><span class="p">][</span><span class="n">annotation</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">_match_annot</span><span class="p">(</span><span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">val</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
                 <span class="n">shard_axis_src</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                 <span class="n">dst_annotation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
  <span class="n">shard_axis_out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">shard_axis_src</span><span class="p">)</span>
  <span class="n">src</span> <span class="o">=</span> <span class="n">shard_axis_out</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">dst</span> <span class="o">=</span> <span class="n">_annot_to_flat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span> <span class="n">shard_axis_out</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                       <span class="n">dst_annotation</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">eval_context</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">src</span> <span class="o">==</span> <span class="n">dst</span><span class="p">:</span>
      <span class="n">outval</span> <span class="o">=</span> <span class="n">val</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">dst</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
      <span class="n">outval</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
      <span class="n">shard_axis_out</span> <span class="o">=</span> <span class="n">_moveaxis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="n">shard_axis_src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dst</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">outval</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
      <span class="n">shard_axis_out</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="n">d</span> <span class="o">+</span> <span class="p">(</span><span class="n">dst</span> <span class="o">&lt;=</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">shard_axis_out</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span>
  <span class="k">return</span> <span class="n">outval</span><span class="p">,</span> <span class="n">shard_axis_out</span>

<span class="k">def</span> <span class="nf">_moveaxis</span><span class="p">(</span><span class="n">ndim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">shard_axes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
              <span class="n">src</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dst</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
  <span class="n">lst</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span>
  <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">shard_axes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">lst</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">lst</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
  <span class="n">lst</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dst</span> <span class="o">-</span> <span class="p">(</span><span class="n">src</span> <span class="o">&lt;</span> <span class="n">dst</span><span class="p">),</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>

<span class="k">class</span> <span class="nc">MapTracer</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">Tracer</span><span class="p">):</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;val&quot;</span><span class="p">,</span> <span class="s2">&quot;shard_axes&quot;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trace</span><span class="p">:</span> <span class="n">MapTrace</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shard_axes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trace</span> <span class="o">=</span> <span class="n">trace</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">val</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shard_axes</span> <span class="o">=</span> <span class="n">shard_axes</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">val</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="o">.</span><span class="n">ndim</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_axes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">aval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">aval</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
    <span class="n">shard_axes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shard_axes</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">axis_idx</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">shard_axes</span><span class="o">.</span><span class="n">values</span><span class="p">())[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">mapped_aval</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis_idx</span><span class="p">],</span> <span class="n">axis_idx</span><span class="p">,</span> <span class="n">aval</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">aval</span>

  <span class="k">def</span> <span class="nf">full_lower</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">named_axes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_axes</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="si">}</span><span class="se">{{</span><span class="si">{</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">named_axes</span><span class="p">)</span><span class="si">}</span><span class="se">}}</span><span class="s2">&quot;</span>

<span class="nd">@lu</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">parallel_callable</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
                      <span class="n">backend_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                      <span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span>
                      <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">global_axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
                      <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                      <span class="n">in_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                      <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
                      <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                      <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]],</span>
                      <span class="o">*</span><span class="n">avals</span><span class="p">):</span>
  <span class="n">pmap_computation</span> <span class="o">=</span> <span class="n">lower_parallel_callable</span><span class="p">(</span>
      <span class="n">fun</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
      <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">avals</span><span class="p">)</span>
  <span class="n">pmap_executable</span> <span class="o">=</span> <span class="n">pmap_computation</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">WeakRefList</span><span class="p">([</span><span class="n">pmap_executable</span><span class="o">.</span><span class="n">unsafe_call</span><span class="p">,</span> <span class="n">pmap_executable</span><span class="o">.</span><span class="n">fingerprint</span><span class="p">])</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ParallelCallableInfo</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
  <span class="n">backend</span><span class="p">:</span> <span class="n">xla</span><span class="o">.</span><span class="n">Backend</span>
  <span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span>
  <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">global_axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
  <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span>
  <span class="n">in_axes</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
  <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span>
  <span class="n">avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">]</span>

  <span class="nd">@maybe_cached_property</span>
  <span class="k">def</span> <span class="nf">local_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span>
             <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="p">)]</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">out</span>

  <span class="nd">@maybe_cached_property</span>
  <span class="k">def</span> <span class="nf">out_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_axes_thunk</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ShardInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">sharded_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">]</span>
  <span class="n">out_sharded_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">]</span>
  <span class="n">global_sharded_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">]</span>
  <span class="n">num_local_shards</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">num_global_shards</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">class</span> <span class="nc">ReplicaInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">jaxpr_replicas</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">num_local_replicas</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">num_global_replicas</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">def</span> <span class="nf">find_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">):</span>
  <span class="c1"># TODO(skyewm): replace this with a chain of pmaps and/or sharded_jits</span>
  <span class="n">jaxpr_replicas</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">jaxpr_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
  <span class="n">num_local_replicas</span> <span class="o">=</span> <span class="n">axis_size</span> <span class="o">*</span> <span class="n">jaxpr_replicas</span>
  <span class="n">num_global_replicas</span> <span class="o">=</span> <span class="n">global_axis_size</span> <span class="o">*</span> <span class="n">jaxpr_replicas</span>
  <span class="k">return</span> <span class="n">ReplicaInfo</span><span class="p">(</span><span class="n">jaxpr_replicas</span><span class="p">,</span> <span class="n">num_local_replicas</span><span class="p">,</span> <span class="n">num_global_replicas</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">stage_parallel_callable</span><span class="p">(</span>
    <span class="n">pci</span><span class="p">:</span> <span class="n">ParallelCallableInfo</span><span class="p">,</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]):</span>
  <span class="n">sharded_avals</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
      <span class="n">shard_aval</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">aval</span><span class="p">)</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">aval</span>
      <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">aval</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">in_axes</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">))</span>
  <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">global_arg_shapes</span><span class="p">):</span>
    <span class="c1"># TODO(skye): we could take this branch unconditionally if we handled</span>
    <span class="c1"># grad of global_arg_shapes correctly.</span>
    <span class="n">global_sharded_avals</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">aval</span>
        <span class="k">for</span> <span class="n">shape</span><span class="p">,</span> <span class="n">aval</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_arg_shapes</span><span class="p">,</span> <span class="n">sharded_avals</span><span class="p">)]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">global_sharded_avals</span> <span class="o">=</span> <span class="n">sharded_avals</span>  <span class="c1"># type: ignore</span>

  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">global_axis_size</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
    <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished tracing + transforming </span><span class="si">{</span><span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> &quot;</span>
                                   <span class="s2">&quot;for pmap in </span><span class="si">{elapsed_time}</span><span class="s2"> sec&quot;</span><span class="p">):</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_sharded_avals</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_final</span><span class="p">(</span>
          <span class="n">fun</span><span class="p">,</span> <span class="n">global_sharded_avals</span><span class="p">,</span> <span class="n">pe</span><span class="o">.</span><span class="n">debug_info_final</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;pmap&quot;</span><span class="p">))</span>
  <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">apply_outfeed_rewriter</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_sharded_avals</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">out_axes</span><span class="p">),</span> <span class="p">(</span>
      <span class="nb">len</span><span class="p">(</span><span class="n">out_sharded_avals</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">out_axes</span><span class="p">))</span>

  <span class="c1"># TODO(skye,mattjj): allow more collectives on multi-host as we test them, but</span>
  <span class="c1"># for now raise an error</span>
  <span class="k">if</span> <span class="n">pci</span><span class="o">.</span><span class="n">devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">is_multi_host_pmap</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">is_multi_host_pmap</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">is_multi_host_pmap</span><span class="p">:</span>
    <span class="n">check_multihost_collective_allowlist</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="n">replicas</span> <span class="o">=</span> <span class="n">find_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">global_axis_size</span><span class="p">)</span>
  <span class="n">parts</span> <span class="o">=</span> <span class="n">find_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="n">num_local_shards</span> <span class="o">=</span> <span class="n">replicas</span><span class="o">.</span><span class="n">num_local_replicas</span> <span class="o">*</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_num_partitions</span>
  <span class="n">num_global_shards</span> <span class="o">=</span> <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span> <span class="o">*</span> <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span>

  <span class="n">shards</span> <span class="o">=</span> <span class="n">ShardInfo</span><span class="p">(</span>
      <span class="n">sharded_avals</span><span class="p">,</span> <span class="n">out_sharded_avals</span><span class="p">,</span> <span class="n">global_sharded_avals</span><span class="p">,</span>
      <span class="n">num_local_shards</span><span class="p">,</span> <span class="n">num_global_shards</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">replicas</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">shards</span>


<span class="k">def</span> <span class="nf">_shardings_to_mlir_shardings</span><span class="p">(</span>
    <span class="n">shardings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">PartitionsOrReplicated</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">]]]:</span>
  <span class="k">if</span> <span class="n">shardings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">sharding_to_proto</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">]</span>

<span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
<span class="k">def</span> <span class="nf">lower_parallel_callable</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
    <span class="n">backend_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">axis_name</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span><span class="p">,</span>
    <span class="n">axis_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">global_axis_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">Device</span><span class="p">]],</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">in_axes</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">out_axes_thunk</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
    <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">global_arg_shapes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]],</span>
    <span class="n">avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">AbstractValue</span><span class="p">]):</span>
  <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;devices&#39; argument to pmap must be non-empty, or None.&quot;</span><span class="p">)</span>

  <span class="c1"># Determine global_axis_size for use in AxisEnv.</span>
  <span class="c1"># TODO(mattjj,skyewm): revive this check (inner_pmap always False now)</span>
  <span class="c1"># if xb.process_count() &gt; 1 and global_axis_size is None and inner_pmap:</span>
  <span class="c1">#   raise ValueError(&quot;&#39;axis_size&#39; must be specified for nested multi-host pmaps&quot;)</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
      <span class="n">global_axis_size</span> <span class="o">!=</span> <span class="n">axis_size</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Specified axis_size </span><span class="si">{</span><span class="n">global_axis_size</span><span class="si">}</span><span class="s2"> doesn&#39;t match received &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;axis_size </span><span class="si">{</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">backend_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_device_backend</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">backend_name</span><span class="p">)</span>

  <span class="n">must_run_on_all_devices</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">no_nested_sharding</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">if</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">axis_size</span>
    <span class="k">elif</span> <span class="n">devices</span><span class="p">:</span>
      <span class="c1"># This allows each host in a multi-host pmap to run on a different number</span>
      <span class="c1"># of devices, but precludes nested sharding (i.e. inner pmaps or</span>
      <span class="c1"># sharded_jits).</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
      <span class="n">no_nested_sharding</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># This assumes all hosts run on the same number of devices. We make sure</span>
      <span class="c1"># this assumption is true by requiring that the pmap is run on all devices</span>
      <span class="c1"># (and making the further assumption that each host has the same number of</span>
      <span class="c1"># devices). Nested sharding is ok in this case.</span>
      <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">axis_size</span> <span class="o">*</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
          <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(</span><span class="n">process_index</span><span class="p">,</span> <span class="n">backend</span><span class="p">))</span> <span class="o">==</span> <span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">process_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)))</span>
      <span class="n">must_run_on_all_devices</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="n">pci</span> <span class="o">=</span> <span class="n">ParallelCallableInfo</span><span class="p">(</span>
      <span class="n">name</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span>
      <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes_thunk</span><span class="p">,</span> <span class="n">avals</span><span class="p">)</span>
  <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">replicas</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">shards</span> <span class="o">=</span> <span class="n">stage_parallel_callable</span><span class="p">(</span>
      <span class="n">pci</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">logging</span><span class="o">.</span><span class="n">vlog_is_on</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;sharded_avals: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">shards</span><span class="o">.</span><span class="n">sharded_avals</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;global_sharded_avals: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">shards</span><span class="o">.</span><span class="n">global_sharded_avals</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;num_replicas: </span><span class="si">%d</span><span class="s2">  num_local_replicas: </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span>
                 <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span> <span class="n">replicas</span><span class="o">.</span><span class="n">num_local_replicas</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;num_partitions: </span><span class="si">%d</span><span class="s2">  local_num_partitions: </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span>
                 <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_num_partitions</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;arg_parts: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">arg_parts</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;local_arg_parts: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_arg_parts</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;out_parts: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">out_parts</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;local_out_parts: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_out_parts</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;devices: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">vlog</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;local_devices: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">)</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">must_run_on_all_devices</span> <span class="ow">and</span>
      <span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span> <span class="o">!=</span> <span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span> <span class="o">==</span> <span class="n">axis_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
         <span class="sa">f</span><span class="s2">&quot;On multi-host platforms, the input to pmapped functions must have &quot;</span>
         <span class="sa">f</span><span class="s2">&quot;leading axis size equal to the number of local devices if no &quot;</span>
         <span class="sa">f</span><span class="s2">&quot;`devices` argument is specified. Got axis_size=</span><span class="si">{</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">, &quot;</span>
         <span class="sa">f</span><span class="s2">&quot;num_local_devices=</span><span class="si">{</span><span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;On multi-host platforms, pmapped functions must run across all &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;devices, i.e. num_replicas * num_partitions should equal the &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;number of local devices. Got &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;num_replicas=</span><span class="si">{</span><span class="n">replicas</span><span class="o">.</span><span class="n">num_local_replicas</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;num_partitions=</span><span class="si">{</span><span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="si">}</span><span class="s2">, and &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;num_local_devices=</span><span class="si">{</span><span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">no_nested_sharding</span> <span class="ow">and</span> <span class="p">(</span>
      <span class="n">replicas</span><span class="o">.</span><span class="n">jaxpr_replicas</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
      <span class="sa">f</span><span class="s2">&quot;On multi-host platforms, pmapped functions that both have `devices` &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;specified and contain an inner_pmap or sharded_jit must specify an &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;`axis_size` (or remove the `devices` argument). Got nested_replicas=&quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">replicas</span><span class="o">.</span><span class="n">jaxpr_replicas</span><span class="si">}</span><span class="s2"> and nested_partitions=</span><span class="si">{</span><span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="n">log_priority</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_log_compiles</span> <span class="k">else</span> <span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log_priority</span><span class="p">,</span>
              <span class="s2">&quot;Compiling </span><span class="si">%s</span><span class="s2"> (</span><span class="si">%d</span><span class="s2">) for </span><span class="si">%d</span><span class="s2"> devices with args </span><span class="si">%s</span><span class="s2">. (num_replicas=</span><span class="si">%d</span><span class="s2">&quot;</span>
              <span class="s2">&quot; num_partitions=</span><span class="si">%d</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">id</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span>
              <span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span><span class="p">,</span> <span class="n">avals</span><span class="p">,</span> <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span>
              <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">)</span>

  <span class="n">axis_env</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span>
      <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span> <span class="p">(</span><span class="n">axis_name</span><span class="p">,),</span> <span class="p">(</span><span class="n">global_axis_size</span><span class="p">,))</span>
  <span class="n">name_stack</span> <span class="o">=</span> <span class="n">new_name_stack</span><span class="p">(</span><span class="n">wrap_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;pmap&#39;</span><span class="p">))</span>
  <span class="n">closed_jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
  <span class="n">replicated_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">in_axes</span><span class="p">]</span>
  <span class="n">module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">]</span>
  <span class="n">tuple_args</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">should_tuple_args</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="o">.</span><span class="n">global_sharded_avals</span><span class="p">),</span>
                                          <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>
  <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;pmap_</span><span class="si">{</span><span class="n">fun</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
  <span class="k">with</span> <span class="n">maybe_extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ordered effects not supported in `pmap`.&quot;</span><span class="p">)</span>
    <span class="n">unordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                         <span class="k">if</span> <span class="n">eff</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
    <span class="n">ordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                       <span class="k">if</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
    <span class="n">lowering_result</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">lower_jaxpr_to_module</span><span class="p">(</span>
        <span class="n">module_name</span><span class="p">,</span>
        <span class="n">closed_jaxpr</span><span class="p">,</span>
        <span class="n">unordered_effects</span><span class="p">,</span>
        <span class="n">ordered_effects</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">,</span>
        <span class="n">mlir</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">axis_env</span><span class="p">),</span>
        <span class="n">name_stack</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">replicated_args</span><span class="o">=</span><span class="n">replicated_args</span><span class="p">,</span>
        <span class="n">arg_shardings</span><span class="o">=</span><span class="n">_shardings_to_mlir_shardings</span><span class="p">(</span><span class="n">parts</span><span class="o">.</span><span class="n">arg_parts</span><span class="p">),</span>
        <span class="n">result_shardings</span><span class="o">=</span><span class="n">_shardings_to_mlir_shardings</span><span class="p">(</span><span class="n">parts</span><span class="o">.</span><span class="n">out_parts</span><span class="p">))</span>
    <span class="n">module</span><span class="p">,</span> <span class="n">keepalive</span><span class="p">,</span> <span class="n">host_callbacks</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">lowering_result</span><span class="o">.</span><span class="n">keepalive</span><span class="p">,</span>
        <span class="n">lowering_result</span><span class="o">.</span><span class="n">host_callbacks</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">PmapComputation</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">pci</span><span class="o">=</span><span class="n">pci</span><span class="p">,</span> <span class="n">replicas</span><span class="o">=</span><span class="n">replicas</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">parts</span><span class="p">,</span>
                         <span class="n">shards</span><span class="o">=</span><span class="n">shards</span><span class="p">,</span> <span class="n">tuple_args</span><span class="o">=</span><span class="n">tuple_args</span><span class="p">,</span>
                         <span class="n">unordered_effects</span><span class="o">=</span><span class="n">unordered_effects</span><span class="p">,</span>
                         <span class="n">ordered_effects</span><span class="o">=</span><span class="n">ordered_effects</span><span class="p">,</span>
                         <span class="n">keepalive</span><span class="o">=</span><span class="n">keepalive</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="o">=</span><span class="n">host_callbacks</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PmapComputation</span><span class="p">(</span><span class="n">stages</span><span class="o">.</span><span class="n">XlaLowering</span><span class="p">):</span>
  <span class="n">_hlo</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">]</span>
  <span class="n">_executable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PmapExecutable</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hlo</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">],</span> <span class="o">**</span><span class="n">compile_args</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span> <span class="o">=</span> <span class="n">hlo</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span> <span class="o">=</span> <span class="n">compile_args</span>

  <span class="c1"># -- stages.XlaLowering overrides</span>

  <span class="k">def</span> <span class="nf">hlo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">:</span>
    <span class="c1"># this is a method for api consistency with dispatch.XlaComputation</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">xe</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">mlir_module_to_xla_computation</span><span class="p">(</span>
          <span class="n">mlir</span><span class="o">.</span><span class="n">module_to_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">),</span>
          <span class="n">use_tuple_args</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">[</span><span class="s2">&quot;tuple_args&quot;</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">mhlo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">):</span>
      <span class="n">module_str</span> <span class="o">=</span> <span class="n">xe</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">xla_computation_to_mlir_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">mlir</span><span class="o">.</span><span class="n">make_ir_context</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">module_str</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span>

  <span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
  <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PmapExecutable</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="n">PmapExecutable</span><span class="o">.</span><span class="n">from_hlo</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span>


<span class="k">class</span> <span class="nc">PmapExecutable</span><span class="p">(</span><span class="n">stages</span><span class="o">.</span><span class="n">XlaExecutable</span><span class="p">):</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;xla_executable&#39;</span><span class="p">,</span> <span class="s1">&#39;unsafe_call&#39;</span><span class="p">,</span> <span class="s1">&#39;fingerprint&#39;</span><span class="p">,</span> <span class="s1">&#39;in_avals&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xla_executable</span><span class="p">,</span> <span class="n">unsafe_call</span><span class="p">,</span> <span class="n">fingerprint</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span> <span class="o">=</span> <span class="n">xla_executable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">unsafe_call</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fingerprint</span> <span class="o">=</span> <span class="n">fingerprint</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_avals</span> <span class="o">=</span> <span class="n">in_avals</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_hlo</span><span class="p">(</span><span class="n">xla_computation</span><span class="p">,</span>
               <span class="n">pci</span><span class="p">:</span> <span class="n">ParallelCallableInfo</span><span class="p">,</span>
               <span class="n">replicas</span><span class="p">:</span> <span class="n">ReplicaInfo</span><span class="p">,</span>
               <span class="n">parts</span><span class="p">:</span> <span class="n">PartitionInfo</span><span class="p">,</span>
               <span class="n">shards</span><span class="p">:</span> <span class="n">ShardInfo</span><span class="p">,</span>
               <span class="n">tuple_args</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">unordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span>
               <span class="n">ordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span>
               <span class="n">host_callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
               <span class="n">keepalive</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">pci</span><span class="o">.</span><span class="n">devices</span>
    <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span> <span class="o">&gt;</span> <span class="n">xb</span><span class="o">.</span><span class="n">device_count</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;compiling computation that requires </span><span class="si">{}</span><span class="s2"> logical devices, but only </span><span class="si">{}</span><span class="s2"> XLA &quot;</span>
               <span class="s2">&quot;devices are available (num_replicas=</span><span class="si">{}</span><span class="s2">, num_partitions=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span><span class="p">,</span>
                                    <span class="n">xb</span><span class="o">.</span><span class="n">device_count</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">),</span>
                                    <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span>
                                    <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">))</span>
      <span class="c1"># On a single host, we use the platform&#39;s default device assignment to</span>
      <span class="c1"># potentially take advantage of device locality. On multiple hosts, the</span>
      <span class="c1"># default device assignment may interleave different hosts&#39; replicas,</span>
      <span class="c1"># violating pmap&#39;s semantics where data is sharded across replicas in</span>
      <span class="c1"># row-major order. Instead, manually create a device assignment that ensures</span>
      <span class="c1"># each host is responsible for a continguous set of replicas.</span>
      <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span> <span class="o">&gt;</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span><span class="p">:</span>
        <span class="c1"># TODO(skye): use a locality-aware assignment that satisfies the above</span>
        <span class="c1"># constraint.</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">process_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_count</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">))</span>
                  <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">xb</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(</span><span class="n">process_index</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">)]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">)</span><span class="o">.</span><span class="n">get_default_device_assignment</span><span class="p">(</span>
            <span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">):</span>
        <span class="n">local_devices_str</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span> <span class="o">==</span> <span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="sa">f</span><span class="s2">&quot;Leading axis size of input to pmapped function must equal the &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;number of local devices passed to pmap. Got axis_size=&quot;</span>
              <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">, num_local_devices=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">)</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="sa">f</span><span class="s2">&quot;(Local devices available to pmap: </span><span class="si">{</span><span class="n">local_devices_str</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="sa">f</span><span class="s2">&quot;pmapped function requires </span><span class="si">{</span><span class="n">shards</span><span class="o">.</span><span class="n">num_local_shards</span><span class="si">}</span><span class="s2"> local &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;devices to run due to nested pmapped or other parallel &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;functions, but only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">local_devices</span><span class="p">)</span><span class="si">}</span><span class="s2"> are available.</span><span class="se">\n</span><span class="s2">&quot;</span>
              <span class="sa">f</span><span class="s2">&quot;(outer axis size: </span><span class="si">{</span><span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">, local devices available to &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;pmap: </span><span class="si">{</span><span class="n">local_devices_str</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;compiling computation that creates </span><span class="si">%s</span><span class="s2"> shards, &quot;</span>
                        <span class="s2">&quot;but </span><span class="si">%s</span><span class="s2"> devices were specified&quot;</span> <span class="o">%</span>
                        <span class="p">(</span><span class="n">shards</span><span class="o">.</span><span class="n">num_global_shards</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)))</span>

    <span class="c1"># &#39;devices&#39; may be 1D or 2D at this point (e.g.</span>
    <span class="c1"># get_default_device_assignment() returns 2D assignment, caller may have</span>
    <span class="c1"># provided 1D list of devices).</span>
    <span class="c1"># Convert to 2D in case it&#39;s 1D and we have &gt; 1 partitions.</span>
    <span class="n">device_assignment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="p">(</span><span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">))</span>
    <span class="c1"># TODO(b/162356737): Enabling SPMD partitioning causes issues with some</span>
    <span class="c1"># non-partitioned workloads, so disable unless needed.</span>
    <span class="n">use_spmd_partitioning</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="n">compile_options</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_compile_options</span><span class="p">(</span>
        <span class="n">num_replicas</span><span class="o">=</span><span class="n">replicas</span><span class="o">.</span><span class="n">num_global_replicas</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="o">=</span><span class="n">parts</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">,</span>
        <span class="n">device_assignment</span><span class="o">=</span><span class="n">device_assignment</span><span class="p">,</span>
        <span class="n">use_spmd_partitioning</span><span class="o">=</span><span class="n">use_spmd_partitioning</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">compile_options</span><span class="o">.</span><span class="n">parameter_is_tupled_arguments</span> <span class="o">=</span> <span class="n">tuple_args</span>

    <span class="n">process_index</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">local_device_assignment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_assignment</span><span class="o">.</span><span class="n">flat</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">process_index</span>
    <span class="p">])</span>

    <span class="n">local_arg_parts_</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_arg_parts</span> <span class="ow">or</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">)</span>
    <span class="n">input_sharding_specs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_pmap_sharding_spec</span><span class="p">(</span><span class="n">replicas</span><span class="o">.</span><span class="n">num_local_replicas</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">,</span>
                            <span class="n">parts</span><span class="o">.</span><span class="n">local_num_partitions</span><span class="p">,</span> <span class="n">arg_parts</span><span class="p">,</span> <span class="n">aval</span><span class="p">,</span> <span class="n">in_axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">arg_parts</span><span class="p">,</span> <span class="n">in_axis</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span>
            <span class="n">shards</span><span class="o">.</span><span class="n">sharded_avals</span><span class="p">,</span> <span class="n">local_arg_parts_</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">in_axes</span><span class="p">)]</span>
    <span class="n">input_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">spec_to_indices</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">,</span> <span class="n">input_sharding_specs</span><span class="p">)]</span>
    <span class="n">in_shardings</span> <span class="o">=</span> <span class="n">_get_pmap_sharding</span><span class="p">(</span><span class="n">local_device_assignment</span><span class="p">,</span> <span class="n">input_sharding_specs</span><span class="p">)</span>
    <span class="n">nouts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="o">.</span><span class="n">out_sharded_avals</span><span class="p">)</span>

    <span class="n">out_parts</span><span class="p">,</span> <span class="n">local_out_parts</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">out_parts</span><span class="p">,</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_out_parts</span>
    <span class="k">if</span> <span class="n">parts</span><span class="o">.</span><span class="n">out_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">out_parts</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">nouts</span>
    <span class="k">if</span> <span class="n">parts</span><span class="o">.</span><span class="n">local_out_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">local_out_parts</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">nouts</span>

    <span class="n">local_out_avals</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">get_local_aval</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">lparts</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">lparts</span>
      <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">shards</span><span class="o">.</span><span class="n">out_sharded_avals</span><span class="p">,</span> <span class="n">out_parts</span><span class="p">,</span> <span class="n">local_out_parts</span><span class="p">)]</span>
    <span class="n">local_unmapped_avals</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">core</span><span class="o">.</span><span class="n">unmapped_aval</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">,</span> <span class="n">aval</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">aval</span>
        <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">out_axis</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">local_out_avals</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">out_axes</span><span class="p">)]</span>
    <span class="n">out_specs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">_pmap_sharding_spec</span><span class="p">(</span><span class="n">replicas</span><span class="o">.</span><span class="n">num_local_replicas</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">axis_size</span><span class="p">,</span>
                            <span class="n">parts</span><span class="o">.</span><span class="n">local_num_partitions</span><span class="p">,</span> <span class="n">out_parts</span><span class="p">,</span> <span class="n">aval</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">out_parts</span><span class="p">,</span> <span class="n">aval</span><span class="p">,</span> <span class="n">out_axis</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span>
            <span class="n">local_out_parts</span><span class="p">,</span> <span class="n">local_out_avals</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">out_axes</span><span class="p">)]</span>
    <span class="n">out_shardings</span> <span class="o">=</span> <span class="n">_get_pmap_sharding</span><span class="p">(</span><span class="n">local_device_assignment</span><span class="p">,</span> <span class="n">out_specs</span><span class="p">)</span>
    <span class="n">handle_outs</span> <span class="o">=</span> <span class="n">local_avals_to_results_handler</span><span class="p">(</span><span class="n">local_unmapped_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span> <span class="s2">&quot;compile_replicated&quot;</span><span class="p">):</span>
      <span class="n">execute_fun</span> <span class="o">=</span> <span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">compile_replicated</span><span class="p">(</span>
          <span class="n">xla_computation</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">,</span>
          <span class="n">input_indices</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pmap</span><span class="p">,</span> <span class="n">handle_outs</span><span class="p">)</span>
      <span class="c1"># TODO(frostig): need `compile_replicated` to give us the XLA executable</span>
      <span class="k">return</span> <span class="n">PmapExecutable</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">execute_fun</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Finished XLA compilation of </span><span class="si">{</span><span class="n">pci</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> in </span><span class="se">{{</span><span class="s2">elapsed_time</span><span class="se">}}</span><span class="s2"> sec&quot;</span><span class="p">):</span>
      <span class="n">compiled</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">compile_or_get_cached</span><span class="p">(</span>
          <span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span> <span class="n">xla_computation</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="p">)</span>
    <span class="n">handle_args</span> <span class="o">=</span> <span class="n">InputsHandler</span><span class="p">(</span>
        <span class="n">compiled</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(),</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pmap</span><span class="p">)</span>
    <span class="n">execute_fun</span> <span class="o">=</span> <span class="n">ExecuteReplicated</span><span class="p">(</span><span class="n">compiled</span><span class="p">,</span> <span class="s2">&quot;parallel computation&quot;</span><span class="p">,</span>
                                    <span class="n">pci</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span> <span class="n">handle_args</span><span class="p">,</span> <span class="n">handle_outs</span><span class="p">,</span>
                                    <span class="n">unordered_effects</span><span class="p">,</span> <span class="n">ordered_effects</span><span class="p">,</span>
                                    <span class="n">keepalive</span><span class="p">,</span> <span class="nb">bool</span><span class="p">(</span><span class="n">host_callbacks</span><span class="p">),</span>
                                    <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_indices</span><span class="p">))))</span>
    <span class="n">fingerprint</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">compiled</span><span class="p">,</span> <span class="s2">&quot;fingerprint&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">PmapExecutable</span><span class="p">(</span><span class="n">compiled</span><span class="p">,</span> <span class="n">execute_fun</span><span class="p">,</span> <span class="n">fingerprint</span><span class="p">,</span> <span class="n">pci</span><span class="o">.</span><span class="n">avals</span><span class="p">)</span>

  <span class="c1"># -- stages.XlaExecutable overrides</span>

  <span class="k">def</span> <span class="nf">xla_extension_executable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span>

  <span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># TODO(frostig): do we need to check sharding and sharded avals?</span>
    <span class="n">arg_avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg_avals_for_call</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_avals</span><span class="p">,</span> <span class="n">arg_avals</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsafe_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_pmap_sharding</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">specs</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">PmapSharding</span>

  <span class="k">return</span> <span class="p">[</span><span class="n">PmapSharding</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">specs</span><span class="p">]</span>


<span class="n">multi_host_supported_collectives</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">check_multihost_collective_allowlist</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">):</span>
  <span class="n">used_collectives</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">jaxpr_collectives</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">))</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">used_collectives</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">multi_host_supported_collectives</span><span class="p">):</span>
    <span class="n">bad_collectives</span> <span class="o">=</span> <span class="n">used_collectives</span> <span class="o">-</span> <span class="n">multi_host_supported_collectives</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;using collectives that aren&#39;t supported for multi-host: </span><span class="si">{}</span><span class="s2">&quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">bad_collectives</span><span class="p">))))</span>


<span class="n">PartitionsOrReplicated</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>

<span class="k">class</span> <span class="nc">PartitionInfo</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">arg_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">PartitionsOrReplicated</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
  <span class="n">out_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">PartitionsOrReplicated</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
  <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">local_arg_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">PartitionsOrReplicated</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
  <span class="n">local_out_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">PartitionsOrReplicated</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
  <span class="n">local_num_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">_find_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns (in_partitions, out_partitions, num_partitions, local_in_parts,</span>
<span class="sd">              local_out_parts, local_num_partitions).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">eqn</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;sharded_call&quot;</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;pmap of sharded_jit + non-sharded operations not yet implemented.&quot;</span><span class="p">)</span>
      <span class="n">num_partitions</span> <span class="o">=</span> <span class="n">reconcile_num_partitions</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;call_jaxpr&quot;</span><span class="p">],</span>
                                                <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;nparts&quot;</span><span class="p">])</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;in_parts&quot;</span><span class="p">],</span>
              <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;out_parts_thunk&quot;</span><span class="p">](),</span>
              <span class="n">num_partitions</span><span class="p">,</span>
              <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;local_in_parts&quot;</span><span class="p">],</span>
              <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;local_out_parts_thunk&quot;</span><span class="p">](),</span>
              <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;local_nparts&quot;</span><span class="p">])</span>
  <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">find_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PartitionInfo</span><span class="p">:</span>
  <span class="p">(</span><span class="n">arg_parts</span><span class="p">,</span> <span class="n">out_parts</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">,</span> <span class="n">local_arg_parts</span><span class="p">,</span> <span class="n">local_out_parts</span><span class="p">,</span>
   <span class="n">local_num_partitions</span><span class="p">)</span> <span class="o">=</span> <span class="n">_find_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">local_num_partitions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">local_num_partitions</span> <span class="o">=</span> <span class="n">num_partitions</span>
  <span class="k">if</span> <span class="n">local_arg_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">local_arg_parts</span> <span class="o">=</span> <span class="n">arg_parts</span>
  <span class="k">if</span> <span class="n">local_out_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">local_out_parts</span> <span class="o">=</span> <span class="n">out_parts</span>

  <span class="k">return</span> <span class="n">PartitionInfo</span><span class="p">(</span><span class="n">arg_parts</span><span class="p">,</span> <span class="n">out_parts</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">,</span>
                       <span class="n">local_arg_parts</span><span class="p">,</span> <span class="n">local_out_parts</span><span class="p">,</span> <span class="n">local_num_partitions</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reconcile_num_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">outer_num_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Returns the total number of partitions to use.</span>

<span class="sd">  Validates that any inner partitioning matches outer_num_parts if provided, and</span>
<span class="sd">  returns the number of partitions to use based on outer_num_parts and any inner</span>
<span class="sd">  partitioning.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inner_num_parts</span> <span class="o">=</span> <span class="n">_inner_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">outer_num_parts</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">outer_num_parts</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inner_num_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># No partitions specified anywhere, everything is replicated.</span>
    <span class="k">return</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">outer_num_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">inner_num_parts</span>
  <span class="k">return</span> <span class="n">outer_num_parts</span>


<span class="k">def</span> <span class="nf">_inner_partitions</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">expected_num_parts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Returns the total number of partitions from PartitionSpecs inside `jaxpr`.</span>

<span class="sd">  Also validates that this number matches `expected_num_parts` if provided.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">eqn</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sharding_constraint&quot;</span><span class="p">,</span> <span class="s2">&quot;infeed&quot;</span><span class="p">]:</span>
      <span class="n">parts</span> <span class="o">=</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;partitions&quot;</span><span class="p">]</span>
      <span class="n">nparts</span> <span class="o">=</span> <span class="n">get_num_partitions</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">expected_num_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">expected_num_parts</span> <span class="o">=</span> <span class="n">nparts</span>
      <span class="k">elif</span> <span class="n">nparts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">nparts</span> <span class="o">!=</span> <span class="n">expected_num_parts</span><span class="p">:</span>
        <span class="c1"># TODO(skye): raise this error as we trace the jaxpr</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;with_sharding_constraint with partitions=</span><span class="si">{</span><span class="n">parts</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;(total partitions: </span><span class="si">{</span><span class="n">nparts</span><span class="si">}</span><span class="s2">) doesn&#39;t match expected number of &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;partitions: </span><span class="si">{</span><span class="n">expected_num_parts</span><span class="si">}</span><span class="s2">. If these partitions look &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;right, check outer sharded_jit and/or other &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;with_sharding_constraint calls.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">subjaxpr</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">jaxprs_in_params</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
        <span class="n">expected_num_parts</span> <span class="o">=</span> <span class="n">_inner_partitions</span><span class="p">(</span><span class="n">subjaxpr</span><span class="p">,</span> <span class="n">expected_num_parts</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">expected_num_parts</span>


<span class="k">def</span> <span class="nf">get_num_partitions</span><span class="p">(</span><span class="o">*</span><span class="n">partitions</span><span class="p">):</span>
  <span class="n">partition_specs</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">partitions</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">partition_specs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># Everything is specified as replicated (all Nones).</span>
    <span class="k">return</span> <span class="kc">None</span>
  <span class="n">num_partitions_set</span> <span class="o">=</span> <span class="p">{</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">partition_specs</span><span class="p">}</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_partitions_set</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;All partition specs must use the same number of total partitions, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">partitions</span><span class="si">}</span><span class="s2">, with distinct number of partitions &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">num_partitions_set</span><span class="si">}</span><span class="s2"> (the total number of partitions is the product &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;of a partition spec)&quot;</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_partitions_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">num_partitions_set</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_global_aval</span><span class="p">(</span><span class="n">local_aval</span><span class="p">,</span> <span class="n">global_parts</span><span class="p">:</span> <span class="n">PartitionsOrReplicated</span><span class="p">,</span>
                    <span class="n">local_parts</span><span class="p">:</span> <span class="n">PartitionsOrReplicated</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">global_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">local_aval</span>
  <span class="k">assert</span> <span class="n">local_parts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">global_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="o">*</span> <span class="n">_safe_div</span><span class="p">(</span><span class="n">ngparts</span><span class="p">,</span> <span class="n">nlparts</span><span class="p">)</span>
                  <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">ngparts</span><span class="p">,</span> <span class="n">nlparts</span>
                  <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">local_aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">global_parts</span><span class="p">,</span> <span class="n">local_parts</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">local_aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">global_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_local_aval</span><span class="p">(</span><span class="n">global_aval</span><span class="p">,</span> <span class="n">global_parts</span><span class="p">:</span> <span class="n">PartitionsOrReplicated</span><span class="p">,</span>
                   <span class="n">local_parts</span><span class="p">:</span> <span class="n">PartitionsOrReplicated</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">global_parts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">global_aval</span>
  <span class="k">assert</span> <span class="n">local_parts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
  <span class="n">local_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">_safe_div</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">_safe_div</span><span class="p">(</span><span class="n">ngparts</span><span class="p">,</span> <span class="n">nlparts</span><span class="p">))</span>
                 <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">ngparts</span><span class="p">,</span> <span class="n">nlparts</span>
                 <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_aval</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">global_parts</span><span class="p">,</span> <span class="n">local_parts</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">global_aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">local_shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_safe_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="n">result</span><span class="p">,</span> <span class="n">ragged</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="k">assert</span> <span class="ow">not</span> <span class="n">ragged</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> % </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2"> != 0&quot;</span>
  <span class="k">return</span> <span class="n">result</span>


<span class="k">class</span> <span class="nc">InputsHandlerMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
  <span class="n">pmap</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">pjit_or_xmap</span> <span class="o">=</span> <span class="mi">1</span>


<span class="k">class</span> <span class="nc">InputsHandler</span><span class="p">:</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;handler&quot;</span><span class="p">,</span> <span class="s2">&quot;local_devices&quot;</span><span class="p">,</span> <span class="s2">&quot;in_shardings&quot;</span><span class="p">,</span> <span class="s2">&quot;input_indices&quot;</span><span class="p">,</span>
               <span class="s2">&quot;mode&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">local_devices</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">handler</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">shard_args</span><span class="p">,</span> <span class="n">local_devices</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">local_devices</span> <span class="o">=</span> <span class="n">local_devices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_shardings</span> <span class="o">=</span> <span class="n">in_shardings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_indices</span> <span class="o">=</span> <span class="n">input_indices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_buffers</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">handler</span><span class="p">(</span><span class="n">input_buffers</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;InputsHandler(</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;local_devices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">local_devices</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;in_shardings=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_shardings</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;input_indices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_indices</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;mode=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResultsHandler</span><span class="p">:</span>
  <span class="c1"># `out_avals` is the `GlobalDeviceArray` global avals when using pjit or xmap</span>
  <span class="c1"># with `config.parallel_functions_output_gda=True`. It is the local one</span>
  <span class="c1"># otherwise, and also when using `pmap`.</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;handlers&quot;</span><span class="p">,</span> <span class="s2">&quot;out_shardings&quot;</span><span class="p">,</span> <span class="s2">&quot;out_avals&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">out_avals</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">handlers</span> <span class="o">=</span> <span class="n">handlers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_shardings</span> <span class="o">=</span> <span class="n">out_shardings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_avals</span> <span class="o">=</span> <span class="n">out_avals</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_bufs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">h</span><span class="p">(</span><span class="n">bufs</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span><span class="p">,</span> <span class="n">bufs</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handlers</span><span class="p">,</span> <span class="n">out_bufs</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">_get_sharding_specs</span><span class="p">(</span>
    <span class="n">shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span> <span class="n">avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShardingSpec</span><span class="p">]:</span>
  <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">sharding</span>

  <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">PmapSharding</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">sharding_spec</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
  <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">MeshPspecSharding</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">new_mesh_sharding_specs</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)(</span>
              <span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">spec</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">avals</span><span class="p">,</span> <span class="n">shardings</span><span class="p">)]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Getting sharding spec is only supported for &#39;</span>
                     <span class="s1">&#39;PmapSharding and MeshPspecSharding.&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">local_avals_to_results_handler</span><span class="p">(</span>
    <span class="n">unmapped_local_out_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">]],</span>
    <span class="n">local_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ResultsHandler</span><span class="p">:</span>
  <span class="n">out_indices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">devices_indices_map</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
                 <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">aval</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">local_shardings</span><span class="p">,</span> <span class="n">unmapped_local_out_avals</span><span class="p">)]</span>
  <span class="n">handlers</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">local_aval_to_result_handler</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">idcs</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">idcs</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">unmapped_local_out_avals</span><span class="p">,</span> <span class="n">local_shardings</span><span class="p">,</span> <span class="n">out_indices</span><span class="p">)</span>
  <span class="p">]</span>
  <span class="k">return</span> <span class="n">ResultsHandler</span><span class="p">(</span><span class="n">handlers</span><span class="p">,</span> <span class="n">local_shardings</span><span class="p">,</span> <span class="n">unmapped_local_out_avals</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">global_avals_to_results_handler</span><span class="p">(</span>
    <span class="n">global_out_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">],</span>
    <span class="n">shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span>
    <span class="n">committed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">are_out_shardings_from_xla</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ResultsHandler</span><span class="p">:</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">MeshPspecSharding</span>

  <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_parallel_functions_output_gda</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span><span class="p">:</span>
    <span class="n">handlers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">global_aval_to_result_handler</span><span class="p">(</span><span class="n">global_aval</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">committed</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">global_aval</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span>
                                          <span class="n">are_out_shardings_from_xla</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">ResultsHandler</span><span class="p">(</span><span class="n">handlers</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">global_out_avals</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># This path is taken when the outputs are SDAs.</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">MeshPspecSharding</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">)</span>
    <span class="n">local_out_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">_global_to_local</span><span class="p">(</span><span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">spec</span><span class="p">),</span> <span class="n">aval</span><span class="p">)</span>
                       <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">,</span> <span class="n">shardings</span><span class="p">)]</span>
    <span class="n">local_shardings</span> <span class="o">=</span> <span class="p">[</span><span class="n">MeshPspecSharding</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">local_mesh</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">local_avals_to_results_handler</span><span class="p">(</span><span class="n">local_out_avals</span><span class="p">,</span> <span class="n">local_shardings</span><span class="p">)</span>


<span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
<span class="k">def</span> <span class="nf">replicate</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">nrep</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">in_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replicates ``val`` across multiple devices.</span>

<span class="sd">  Args:</span>
<span class="sd">    val: the value to be replicated.</span>
<span class="sd">    axis_size: the length of the output, i.e. the logical number of replicas to</span>
<span class="sd">    create. Usually equal to `nrep`, but in the case of nested pmaps, `nrep` may</span>
<span class="sd">    be a multiple of `axis_size`.</span>
<span class="sd">    nrep: the number of replicas to create. If ``devices`` is set, must be equal</span>
<span class="sd">      to ``len(devices)``.</span>
<span class="sd">    devices: the devices to replicate across. If None, ``nrep`` will be used to</span>
<span class="sd">      generate a default device assignment.</span>
<span class="sd">    backend: string specifying which backend to use.</span>
<span class="sd">    in_axis: axis along which the value is to be replciated.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A ShardedDeviceArray of length `axis_size` where each shard is equal to</span>
<span class="sd">    ``val``.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">device_count</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="k">if</span> <span class="n">devices</span> <span class="k">else</span> <span class="n">xb</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">(</span><span class="n">backend</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">nrep</span> <span class="o">&gt;</span> <span class="n">device_count</span><span class="p">:</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Cannot replicate across </span><span class="si">%d</span><span class="s2"> replicas because only </span><span class="si">%d</span><span class="s2"> local devices &quot;</span>
           <span class="s2">&quot;are available.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">nrep</span><span class="p">,</span> <span class="n">device_count</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">devices</span><span class="p">:</span>
      <span class="n">msg</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot; (local devices = </span><span class="si">%s</span><span class="s2">)&quot;</span>
              <span class="o">%</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="n">devices</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="kc">None</span><span class="p">))</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">nrep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="c1"># TODO(skye): use different device assignment on multihost</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span><span class="o">.</span><span class="n">get_default_device_assignment</span><span class="p">(</span><span class="n">nrep</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">nrep</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>

  <span class="n">aval</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>  <span class="c1"># type: ShapedArray</span>
  <span class="k">if</span> <span class="n">in_axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">replicated_aval</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">axis_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">replicated_aval</span> <span class="o">=</span> <span class="n">aval</span>
  <span class="c1"># TODO(skye): figure out how partitioning should work here</span>
  <span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">_pmap_sharding_spec</span><span class="p">(</span><span class="n">nrep</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">aval</span><span class="p">,</span> <span class="n">in_axis</span><span class="p">)</span>
  <span class="n">device_buffers</span> <span class="o">=</span> <span class="n">device_put</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">replicate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">make_sharded_device_array</span><span class="p">(</span><span class="n">replicated_aval</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span>
                                   <span class="n">device_buffers</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_pmap_sharding_spec</span><span class="p">(</span><span class="n">nrep</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">,</span> <span class="n">npart</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">sharded_aval</span><span class="p">,</span>
                        <span class="n">map_axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ShardingSpec</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Sharding spec for arguments or results of a pmap.</span>
<span class="sd">  Args:</span>
<span class="sd">    nrep: number of local XLA replicas (product of local axis sizes)</span>
<span class="sd">    axis_size: local axis size for outer pmap</span>
<span class="sd">    npart: total number of XLA partitions (required by sharded_jit calls)</span>
<span class="sd">    parts: the partitioning of the value or None</span>
<span class="sd">    sharded_aval: the aval of the value inside the outer pmap, an instance of</span>
<span class="sd">      a ShapedArray.</span>
<span class="sd">    map_axis: the axis along which the value is mapped in the outer pmap</span>
<span class="sd">  Returns:</span>
<span class="sd">    A ShardingSpec.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharded_aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">),</span> <span class="n">sharded_aval</span>
  <span class="n">replication_factor</span><span class="p">,</span> <span class="n">ragged</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">nrep</span><span class="p">,</span> <span class="n">axis_size</span><span class="p">)</span>
  <span class="k">assert</span> <span class="ow">not</span> <span class="n">ragged</span>
  <span class="c1"># get the sharding spec from inner sharded_jits as if we weren&#39;t in a pmap</span>
  <span class="n">pspec</span> <span class="o">=</span> <span class="n">partitioned_sharding_spec</span><span class="p">(</span><span class="n">npart</span><span class="p">,</span> <span class="n">parts</span><span class="p">,</span> <span class="n">sharded_aval</span><span class="p">)</span>
  <span class="n">maybe_replicate</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">replication_factor</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">(</span><span class="n">Replicated</span><span class="p">(</span><span class="n">replication_factor</span><span class="p">),)</span>
  <span class="k">if</span> <span class="n">map_axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">sharded_in_axis</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">pspec</span><span class="o">.</span><span class="n">sharding</span><span class="p">[:</span><span class="n">map_axis</span><span class="p">])</span>
    <span class="k">def</span> <span class="nf">shift_sharded_axis</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">MeshDimAssignment</span><span class="p">):</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ShardedAxis</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">sharded_in_axis</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ShardedAxis</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">a</span>
    <span class="c1"># replication_factor represents the product of inner pmaps, so it goes</span>
    <span class="c1"># after the outer pmapped axis at index 0</span>
    <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">(</span>
      <span class="n">sharding</span><span class="o">=</span><span class="n">tuple_insert</span><span class="p">(</span><span class="n">pspec</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="n">map_axis</span><span class="p">,</span> <span class="n">Unstacked</span><span class="p">(</span><span class="n">axis_size</span><span class="p">)),</span>
      <span class="n">mesh_mapping</span><span class="o">=</span><span class="n">it</span><span class="o">.</span><span class="n">chain</span><span class="p">([</span><span class="n">ShardedAxis</span><span class="p">(</span><span class="n">sharded_in_axis</span><span class="p">)],</span>
                            <span class="n">maybe_replicate</span><span class="p">,</span>
                            <span class="nb">map</span><span class="p">(</span><span class="n">shift_sharded_axis</span><span class="p">,</span> <span class="n">pspec</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">)))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">(</span>
      <span class="n">sharding</span><span class="o">=</span><span class="n">pspec</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span>
      <span class="n">mesh_mapping</span><span class="o">=</span><span class="p">(</span><span class="n">Replicated</span><span class="p">(</span><span class="n">axis_size</span><span class="p">),)</span> <span class="o">+</span> <span class="n">maybe_replicate</span> <span class="o">+</span> <span class="n">pspec</span><span class="o">.</span><span class="n">mesh_mapping</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">partitioned_sharding_spec</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                              <span class="n">partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                              <span class="n">aval</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ShardingSpec</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">partitions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">maybe_replicate</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">(</span><span class="n">Replicated</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">),)</span>
    <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">(</span>
        <span class="n">sharding</span><span class="o">=</span><span class="p">[</span><span class="n">_UNSHARDED_INSTANCE</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
        <span class="n">mesh_mapping</span><span class="o">=</span><span class="n">maybe_replicate</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">partitions</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">(</span>
        <span class="c1"># Chunked expects a list of integers</span>
        <span class="n">sharding</span><span class="o">=</span><span class="nb">map</span><span class="p">(</span><span class="n">Chunked</span><span class="p">,</span> <span class="p">[[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">partitions</span><span class="p">]),</span>
        <span class="n">mesh_mapping</span><span class="o">=</span><span class="nb">map</span><span class="p">(</span><span class="n">ShardedAxis</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">partitions</span><span class="p">))))</span>


<span class="k">class</span> <span class="nc">ExecuteReplicated</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;The logic to shard inputs, execute a replicated model, returning outputs.&quot;&quot;&quot;</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;xla_executable&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;backend&#39;</span><span class="p">,</span> <span class="s1">&#39;in_handler&#39;</span><span class="p">,</span> <span class="s1">&#39;out_handler&#39;</span><span class="p">,</span>
               <span class="s1">&#39;has_unordered_effects&#39;</span><span class="p">,</span> <span class="s1">&#39;ordered_effects&#39;</span><span class="p">,</span> <span class="s1">&#39;keepalive&#39;</span><span class="p">,</span>
               <span class="s1">&#39;has_host_callbacks&#39;</span><span class="p">,</span> <span class="s1">&#39;_local_devices&#39;</span><span class="p">,</span> <span class="s1">&#39;kept_var_idx&#39;</span><span class="p">,</span>
               <span class="s1">&#39;__weakref__&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xla_executable</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">in_handler</span><span class="p">:</span> <span class="n">InputsHandler</span><span class="p">,</span>
               <span class="n">out_handler</span><span class="p">:</span> <span class="n">ResultsHandler</span><span class="p">,</span>
               <span class="n">unordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span>
               <span class="n">ordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span> <span class="n">keepalive</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
               <span class="n">has_host_callbacks</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span> <span class="o">=</span> <span class="n">xla_executable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_handler</span> <span class="o">=</span> <span class="n">in_handler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_handler</span> <span class="o">=</span> <span class="n">out_handler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">has_unordered_effects</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">unordered_effects</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="o">=</span> <span class="n">ordered_effects</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_local_devices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span><span class="o">.</span><span class="n">local_devices</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ordered_effects</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keepalive</span> <span class="o">=</span> <span class="n">keepalive</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">has_host_callbacks</span> <span class="o">=</span> <span class="n">has_host_callbacks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kept_var_idx</span> <span class="o">=</span> <span class="n">kept_var_idx</span>

  <span class="k">def</span> <span class="nf">_call_with_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_bufs</span><span class="p">):</span>
    <span class="c1"># TODO(sharadmv): simplify this logic when minimum jaxlib version is</span>
    <span class="c1"># bumped</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">:</span>
      <span class="n">device</span><span class="p">,</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_devices</span>
      <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">get_token</span><span class="p">(</span><span class="n">eff</span><span class="p">,</span> <span class="n">device</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
      <span class="n">input_bufs</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">tokens</span><span class="p">,</span> <span class="o">*</span><span class="n">input_bufs</span><span class="p">]</span>
    <span class="n">num_output_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="n">can_execute_with_token</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_unordered_effects</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">can_execute_with_token</span><span class="p">:</span>
      <span class="n">out_bufs</span><span class="p">,</span> <span class="n">sharded_token</span> <span class="o">=</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span><span class="o">.</span><span class="n">execute_sharded_on_local_devices_with_tokens</span><span class="p">(</span>
            <span class="n">input_bufs</span><span class="p">))</span>
      <span class="n">token_bufs</span><span class="p">,</span> <span class="n">out_bufs</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">split_list</span><span class="p">(</span><span class="n">out_bufs</span><span class="p">,</span> <span class="p">[</span><span class="n">num_output_tokens</span><span class="p">])</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_devices</span><span class="p">):</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">set_output_runtime_token</span><span class="p">(</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">sharded_token</span><span class="o">.</span><span class="n">get_token</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">eff</span><span class="p">,</span> <span class="n">token_buf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">,</span> <span class="n">token_bufs</span><span class="p">):</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">update_token</span><span class="p">(</span><span class="n">eff</span><span class="p">,</span> <span class="n">token_buf</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out_bufs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span><span class="o">.</span><span class="n">execute_sharded_on_local_devices</span><span class="p">(</span>
          <span class="n">input_bufs</span><span class="p">)</span>
      <span class="n">token_bufs</span><span class="p">,</span> <span class="n">out_bufs</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">split_list</span><span class="p">(</span><span class="n">out_bufs</span><span class="p">,</span> <span class="p">[</span><span class="n">num_output_tokens</span><span class="p">])</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_unordered_effects</span><span class="p">:</span>
        <span class="n">unordered_token_buf</span><span class="p">,</span> <span class="o">*</span><span class="n">token_bufs</span> <span class="o">=</span> <span class="n">token_bufs</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_devices</span><span class="p">):</span>
          <span class="n">token</span> <span class="o">=</span> <span class="p">(</span><span class="n">unordered_token_buf</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
          <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">set_output_token</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">eff</span><span class="p">,</span> <span class="n">token_buf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">,</span> <span class="n">token_bufs</span><span class="p">):</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">runtime_tokens</span><span class="o">.</span><span class="n">update_token</span><span class="p">(</span><span class="n">eff</span><span class="p">,</span> <span class="n">token_buf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_bufs</span>

  <span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kept_var_idx</span><span class="p">]</span>
    <span class="n">input_bufs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_handler</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_unordered_effects</span> <span class="ow">or</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_host_callbacks</span><span class="p">):</span>
      <span class="n">out_bufs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_with_tokens</span><span class="p">(</span><span class="n">input_bufs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out_bufs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span><span class="o">.</span><span class="n">execute_sharded_on_local_devices</span><span class="p">(</span>
          <span class="n">input_bufs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">needs_check_special</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">bufs</span> <span class="ow">in</span> <span class="n">out_bufs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">xb</span><span class="o">.</span><span class="n">use_sharded_buffer</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bufs</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">):</span>
          <span class="n">bufs</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">xc</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">,</span> <span class="n">bufs</span><span class="p">)</span><span class="o">.</span><span class="n">get_device_buffers</span><span class="p">()</span>
        <span class="n">dispatch</span><span class="o">.</span><span class="n">check_special</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">bufs</span><span class="p">)</span>
    <span class="c1"># TODO(yashkatariya): Remove once migration to Array is completed.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">jax_array</span> <span class="ow">and</span> <span class="n">out_bufs</span> <span class="ow">and</span> <span class="n">xb</span><span class="o">.</span><span class="n">use_sharded_buffer</span> <span class="ow">and</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">out_bufs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xc</span><span class="o">.</span><span class="n">ShardedBuffer</span><span class="p">)):</span>
      <span class="n">out_bufs</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">get_device_buffers</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_bufs</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_handler</span><span class="p">(</span><span class="n">out_bufs</span><span class="p">)</span>


<span class="n">xla_pmap_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">MapPrimitive</span><span class="p">(</span><span class="s1">&#39;xla_pmap&#39;</span><span class="p">)</span>
<span class="n">xla_pmap</span> <span class="o">=</span> <span class="n">xla_pmap_p</span><span class="o">.</span><span class="n">bind</span>
<span class="n">xla_pmap_p</span><span class="o">.</span><span class="n">def_impl</span><span class="p">(</span><span class="n">xla_pmap_impl</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_pmap_partial_eval_custom_params_updater</span><span class="p">(</span>
    <span class="n">unks_in</span><span class="p">,</span> <span class="n">inst_in</span><span class="p">,</span> <span class="n">kept_outs_known</span><span class="p">,</span> <span class="n">kept_outs_staged</span><span class="p">,</span> <span class="n">num_res</span><span class="p">,</span> <span class="n">params_known</span><span class="p">,</span>
    <span class="n">params_staged</span><span class="p">):</span>
  <span class="c1"># prune inputs to jaxpr_known according to unks_in</span>
  <span class="n">donated_invars_known</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">unks_in</span><span class="p">,</span> <span class="n">params_known</span><span class="p">[</span><span class="s1">&#39;donated_invars&#39;</span><span class="p">])</span>
  <span class="n">in_axes_known</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">unks_in</span><span class="p">,</span> <span class="n">params_known</span><span class="p">[</span><span class="s1">&#39;in_axes&#39;</span><span class="p">])</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">out_axes_known</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">kept_outs_known</span><span class="p">,</span> <span class="n">params_known</span><span class="p">[</span><span class="s1">&#39;out_axes&#39;</span><span class="p">])</span>
  <span class="n">out_axes_known</span> <span class="o">=</span> <span class="n">out_axes_known</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_res</span>
  <span class="n">new_params_known</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">params_known</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes_known</span><span class="p">),</span>
                          <span class="n">out_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">out_axes_known</span><span class="p">),</span>
                          <span class="n">donated_invars</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">donated_invars_known</span><span class="p">))</span>

  <span class="c1"># added num_res new inputs to jaxpr_staged, pruning according to inst_in</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">donated_invars_staged</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">inst_in</span><span class="p">,</span> <span class="n">params_staged</span><span class="p">[</span><span class="s1">&#39;donated_invars&#39;</span><span class="p">])</span>
  <span class="n">donated_invars_staged</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_res</span> <span class="o">+</span> <span class="n">donated_invars_staged</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">in_axes_staged</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">inst_in</span><span class="p">,</span> <span class="n">params_staged</span><span class="p">[</span><span class="s1">&#39;in_axes&#39;</span><span class="p">])</span>
  <span class="n">in_axes_staged</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_res</span> <span class="o">+</span> <span class="n">in_axes_staged</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">out_axes_staged</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">kept_outs_staged</span><span class="p">,</span> <span class="n">params_staged</span><span class="p">[</span><span class="s1">&#39;out_axes&#39;</span><span class="p">])</span>
  <span class="n">new_params_staged</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">params_staged</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes_staged</span><span class="p">),</span>
                           <span class="n">out_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">out_axes_staged</span><span class="p">),</span>
                           <span class="n">donated_invars</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">donated_invars_staged</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">new_params_known</span><span class="p">,</span> <span class="n">new_params_staged</span>

<span class="k">def</span> <span class="nf">_pmap_partial_eval_custom_res_maker</span><span class="p">(</span><span class="n">params_known</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">core</span><span class="o">.</span><span class="n">unmapped_aval</span><span class="p">(</span><span class="n">params_known</span><span class="p">[</span><span class="s1">&#39;axis_size&#39;</span><span class="p">],</span> <span class="n">core</span><span class="o">.</span><span class="n">no_axis_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">aval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_pmap_dce_rule</span><span class="p">(</span><span class="n">used_outputs</span><span class="p">,</span> <span class="n">eqn</span><span class="p">):</span>
  <span class="c1"># just like pe.dce_jaxpr_call_rule, except handles in_axes / out_axes</span>
  <span class="n">new_jaxpr</span><span class="p">,</span> <span class="n">used_inputs</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">dce_jaxpr</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;call_jaxpr&#39;</span><span class="p">],</span> <span class="n">used_outputs</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">donated_invars</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">used_inputs</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;donated_invars&#39;</span><span class="p">])</span>
  <span class="c1"># TODO(yashkatariya,mattjj): Handle global_arg_shapes here too.</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">in_axes</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">used_inputs</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;in_axes&#39;</span><span class="p">])</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">out_axes</span> <span class="o">=</span> <span class="n">partition_list</span><span class="p">(</span><span class="n">used_outputs</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;out_axes&#39;</span><span class="p">])</span>
  <span class="n">new_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">call_jaxpr</span><span class="o">=</span><span class="n">new_jaxpr</span><span class="p">,</span>
                    <span class="n">donated_invars</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">donated_invars</span><span class="p">),</span>
                    <span class="n">in_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">in_axes</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">out_axes</span><span class="p">))</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">used_inputs</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">used_outputs</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">new_jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">used_inputs</span><span class="p">,</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">new_eqn</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">new_jaxpr_eqn</span><span class="p">(</span>
        <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">used</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">invars</span><span class="p">,</span> <span class="n">used_inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">used</span><span class="p">],</span>
        <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">used</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">outvars</span><span class="p">,</span> <span class="n">used_outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">used</span><span class="p">],</span>
        <span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">source_info</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">used_inputs</span><span class="p">,</span> <span class="n">new_eqn</span>


<span class="c1"># Set param update handlers to update `donated_invars` just like xla_call_p</span>
<span class="n">pe</span><span class="o">.</span><span class="n">call_param_updaters</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">call_param_updaters</span><span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">xla_call_p</span><span class="p">]</span>
<span class="n">pe</span><span class="o">.</span><span class="n">partial_eval_jaxpr_custom_rules</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> \
    <span class="n">partial</span><span class="p">(</span><span class="n">pe</span><span class="o">.</span><span class="n">call_partial_eval_custom_rule</span><span class="p">,</span>
            <span class="s1">&#39;call_jaxpr&#39;</span><span class="p">,</span> <span class="n">_pmap_partial_eval_custom_params_updater</span><span class="p">,</span>
            <span class="n">res_aval</span><span class="o">=</span><span class="n">_pmap_partial_eval_custom_res_maker</span><span class="p">)</span>
<span class="n">pe</span><span class="o">.</span><span class="n">dce_rules</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">_pmap_dce_rule</span>
<span class="n">ad</span><span class="o">.</span><span class="n">call_param_updaters</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">call_param_updaters</span><span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">xla_call_p</span><span class="p">]</span>
<span class="n">ad</span><span class="o">.</span><span class="n">call_transpose_param_updaters</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> \
    <span class="n">ad</span><span class="o">.</span><span class="n">call_transpose_param_updaters</span><span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">xla_call_p</span><span class="p">]</span>

<span class="n">ad</span><span class="o">.</span><span class="n">primitive_transposes</span><span class="p">[</span><span class="n">xla_pmap_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ad</span><span class="o">.</span><span class="n">map_transpose</span><span class="p">,</span> <span class="n">xla_pmap_p</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_unravel_index_mhlo</span><span class="p">(</span><span class="n">axis_env</span><span class="p">):</span>
  <span class="n">div</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ir_constant</span><span class="p">(</span>
      <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">axis_env</span><span class="o">.</span><span class="n">nreps</span> <span class="o">//</span> <span class="n">util</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">axis_env</span><span class="o">.</span><span class="n">sizes</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
  <span class="n">mod</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ir_constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">axis_env</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">RemOp</span><span class="p">(</span>
      <span class="n">mhlo</span><span class="o">.</span><span class="n">DivOp</span><span class="p">(</span><span class="n">mhlo</span><span class="o">.</span><span class="n">ReplicaIdOp</span><span class="p">()</span><span class="o">.</span><span class="n">result</span><span class="p">,</span> <span class="n">div</span><span class="p">)</span><span class="o">.</span><span class="n">result</span><span class="p">,</span> <span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">result</span>

<span class="k">def</span> <span class="nf">_mhlo_shard</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">axis_env</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">in_axis</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">xs</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">xs</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ir_constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">zero</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">idxs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">in_axis</span><span class="p">,</span> <span class="n">_unravel_index_mhlo</span><span class="p">(</span><span class="n">axis_env</span><span class="p">))</span>
    <span class="n">dims_unsqueezed</span> <span class="o">=</span> <span class="n">dims</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dims_unsqueezed</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">in_axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">dynamic_slice_result</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">DynamicSliceOp</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">idxs</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">dense_int_elements</span><span class="p">(</span><span class="n">dims_unsqueezed</span><span class="p">))</span><span class="o">.</span><span class="n">result</span>
    <span class="k">return</span> <span class="p">[</span>
      <span class="n">mhlo</span><span class="o">.</span><span class="n">ReshapeOp</span><span class="p">(</span><span class="n">mlir</span><span class="o">.</span><span class="n">aval_to_ir_type</span><span class="p">(</span><span class="n">aval</span><span class="p">),</span> <span class="n">dynamic_slice_result</span><span class="p">)</span><span class="o">.</span><span class="n">result</span>
    <span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>


<span class="c1"># TODO(b/110096942): more efficient gather</span>
<span class="k">def</span> <span class="nf">_mhlo_unshard</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">axis_env</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">platform</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">xs</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">xs</span>
    <span class="c1"># TODO(mattjj): remove this logic when AllReduce PRED supported on CPU / GPU</span>
    <span class="n">convert_bool</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">platform</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">convert_bool</span><span class="p">:</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">ConvertOp</span><span class="p">(</span><span class="n">mlir</span><span class="o">.</span><span class="n">aval_to_ir_type</span><span class="p">(</span><span class="n">aval</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">result</span>

    <span class="n">dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">padded_aval</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">axis_env</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">padded</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">full_like_aval</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_aval</span><span class="p">)</span>
    <span class="n">zero</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ir_constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_unravel_index_mhlo</span><span class="p">(</span><span class="n">axis_env</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">zero</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
    <span class="n">broadcast_result</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">BroadcastOp</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">dense_int_elements</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">result</span>
    <span class="n">padded</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">DynamicUpdateSliceOp</span><span class="p">(</span>
        <span class="n">padded</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">padded</span><span class="p">,</span> <span class="n">broadcast_result</span><span class="p">,</span> <span class="n">idxs</span><span class="p">)</span><span class="o">.</span><span class="n">result</span>
    <span class="n">replica_groups</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">dense_int_elements</span><span class="p">(</span>
      <span class="n">xla</span><span class="o">.</span><span class="n">axis_groups</span><span class="p">(</span><span class="n">axis_env</span><span class="p">,</span> <span class="n">axis_env</span><span class="o">.</span><span class="n">names</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">CrossReplicaSumOp</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">replica_groups</span><span class="p">)</span><span class="o">.</span><span class="n">result</span>
    <span class="k">if</span> <span class="n">out_axis</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="c1"># TODO(apaszke,mattjj): Change the indices to DynamicUpdateSlice instead</span>
      <span class="n">perm</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)))</span>
      <span class="n">perm</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">out_axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">transposed_dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
      <span class="n">transposed_dims</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">out_axis</span><span class="p">,</span> <span class="n">axis_env</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">transposed_dims</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">TransposeOp</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">dense_int_elements</span><span class="p">(</span><span class="n">perm</span><span class="p">))</span><span class="o">.</span><span class="n">result</span>

    <span class="c1"># TODO(mattjj): remove this logic when AllReduce PRED supported on CPU / GPU</span>
    <span class="k">if</span> <span class="n">convert_bool</span><span class="p">:</span>
      <span class="n">float_zero</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">full_like_aval</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_aval</span><span class="p">)</span>
      <span class="n">out</span> <span class="o">=</span> <span class="n">mhlo</span><span class="o">.</span><span class="n">CompareOp</span><span class="p">(</span>
          <span class="n">out</span><span class="p">,</span>
          <span class="n">float_zero</span><span class="p">,</span>
          <span class="n">mhlo</span><span class="o">.</span><span class="n">ComparisonDirectionAttr</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;NE&quot;</span><span class="p">),</span>
          <span class="n">compare_type</span><span class="o">=</span><span class="n">mhlo</span><span class="o">.</span><span class="n">ComparisonTypeAttr</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;FLOAT&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">result</span>
    <span class="k">return</span> <span class="n">out</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_pmap_lowering</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">in_nodes</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span>
                   <span class="n">axis_size</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                   <span class="n">call_jaxpr</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span>
                   <span class="n">donated_invars</span><span class="p">,</span> <span class="n">global_arg_shapes</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">donated_invars</span>  <span class="c1"># Unused.</span>
  <span class="n">xla</span><span class="o">.</span><span class="n">check_backend_matches</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>
  <span class="c1"># We in-line here rather than generating a Call HLO as in the xla_call</span>
  <span class="c1"># translation rule just because the extra tuple stuff is a pain.</span>
  <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">axis_env</span><span class="o">.</span><span class="n">names</span> <span class="ow">and</span> <span class="n">devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Nested pmap with explicit devices argument.&quot;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">global_axis_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">global_axis_size</span> <span class="o">=</span> <span class="n">axis_size</span>
  <span class="n">new_env</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">axis_env</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">,</span>
                                <span class="n">global_axis_size</span><span class="p">)</span>
  <span class="c1"># Shard the in_nodes that are mapped</span>
  <span class="n">in_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">call_jaxpr</span><span class="o">.</span><span class="n">invars</span><span class="p">]</span>
  <span class="n">in_nodes_sharded</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">_mhlo_shard</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">new_env</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_singleton_ir_values</span><span class="p">(</span><span class="n">in_node</span><span class="p">),</span> <span class="n">in_axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">in_axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_singleton_ir_values</span><span class="p">(</span><span class="n">in_node</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">in_node</span><span class="p">,</span> <span class="n">in_axis</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">in_avals</span><span class="p">,</span> <span class="n">in_nodes</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">))</span>

  <span class="k">with</span> <span class="n">maybe_extend_axis_env</span><span class="p">(</span><span class="n">axis_name</span><span class="p">,</span> <span class="n">global_axis_size</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
    <span class="n">sub_ctx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
        <span class="n">axis_context</span><span class="o">=</span><span class="n">mlir</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">new_env</span><span class="p">),</span>
        <span class="n">name_stack</span><span class="o">=</span><span class="n">xla</span><span class="o">.</span><span class="n">extend_name_stack</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">name_stack</span><span class="p">,</span>
                                         <span class="n">util</span><span class="o">.</span><span class="n">wrap_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;pmap&#39;</span><span class="p">)))</span>
    <span class="n">sharded_outs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">jaxpr_subcomp</span><span class="p">(</span><span class="n">sub_ctx</span><span class="p">,</span> <span class="n">call_jaxpr</span><span class="p">,</span> <span class="n">mlir</span><span class="o">.</span><span class="n">TokenSet</span><span class="p">(),</span> <span class="p">(),</span>
                                         <span class="o">*</span><span class="n">in_nodes_sharded</span><span class="p">)</span>
  <span class="n">out_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">call_jaxpr</span><span class="o">.</span><span class="n">outvars</span><span class="p">]</span>
  <span class="n">outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">_mhlo_unshard</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">new_env</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">,</span> <span class="n">shard</span><span class="p">,</span>
                        <span class="n">platform</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">module_context</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">out_axis</span><span class="p">,</span> <span class="n">shard</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">out_avals</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">,</span> <span class="n">sharded_outs</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">outs</span>

<span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">(</span><span class="n">xla_pmap_p</span><span class="p">,</span> <span class="n">_pmap_lowering</span><span class="p">)</span>


<span class="c1"># ------------------- xmap -------------------</span>

<div class="viewcode-block" id="Mesh"><a class="viewcode-back" href="../../../_autosummary/jax.experimental.maps.Mesh.html#jax.experimental.maps.Mesh">[docs]</a><span class="k">class</span> <span class="nc">Mesh</span><span class="p">(</span><span class="n">ContextDecorator</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Declare the hardware resources available in the scope of this manager.</span>

<span class="sd">  In particular, all ``axis_names`` become valid resource names inside the</span>
<span class="sd">  managed block and can be used e.g. in the ``in_axis_resources`` argument of</span>
<span class="sd">  :py:func:`jax.experimental.pjit.pjit`. Also see JAX&#39;s multi-process programming model (https://jax.readthedocs.io/en/latest/multi_process.html)</span>
<span class="sd">  and pjit tutorial (https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html).</span>

<span class="sd">  If you are compiling in multiple threads, make sure that the</span>
<span class="sd">  ``with Mesh`` context manager is inside the function that the threads will</span>
<span class="sd">  execute.</span>

<span class="sd">  Args:</span>
<span class="sd">    devices: A NumPy ndarray object containing JAX device objects (as</span>
<span class="sd">      obtained e.g. from :py:func:`jax.devices`).</span>
<span class="sd">    axis_names: A sequence of resource axis names to be assigned to the</span>
<span class="sd">      dimensions of the ``devices`` argument. Its length should match the</span>
<span class="sd">      rank of ``devices``.</span>

<span class="sd">  Example:</span>

<span class="sd">    &gt;&gt;&gt; from jax.experimental.maps import Mesh</span>
<span class="sd">    &gt;&gt;&gt; from jax.experimental.pjit import pjit</span>
<span class="sd">    &gt;&gt;&gt; from jax.experimental import PartitionSpec as P</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; inp = np.arange(16).reshape((8, 2))</span>
<span class="sd">    &gt;&gt;&gt; devices = np.array(jax.devices()).reshape(4, 2)</span>
<span class="sd">    ...</span>
<span class="sd">    &gt;&gt;&gt; # Declare a 2D mesh with axes `x` and `y`.</span>
<span class="sd">    &gt;&gt;&gt; global_mesh = Mesh(devices, (&#39;x&#39;, &#39;y&#39;))</span>
<span class="sd">    &gt;&gt;&gt; # Use the mesh object directly as a context manager.</span>
<span class="sd">    &gt;&gt;&gt; with global_mesh:</span>
<span class="sd">    ...   out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)</span>

<span class="sd">    &gt;&gt;&gt; # Initialize the Mesh and use the mesh as the context manager.</span>
<span class="sd">    &gt;&gt;&gt; with Mesh(devices, (&#39;x&#39;, &#39;y&#39;)) as global_mesh:</span>
<span class="sd">    ...   out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)</span>

<span class="sd">    &gt;&gt;&gt; # Also you can use it as `with ... as ...`.</span>
<span class="sd">    &gt;&gt;&gt; global_mesh = Mesh(devices, (&#39;x&#39;, &#39;y&#39;))</span>
<span class="sd">    &gt;&gt;&gt; with global_mesh as m:</span>
<span class="sd">    ...   out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)</span>

<span class="sd">    &gt;&gt;&gt; # You can also use it as `with Mesh(...)`.</span>
<span class="sd">    &gt;&gt;&gt; with Mesh(devices, (&#39;x&#39;, &#39;y&#39;)):</span>
<span class="sd">    ...   out = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=None)(inp)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">devices</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
  <span class="n">axis_names</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<div class="viewcode-block" id="Mesh.__init__"><a class="viewcode-back" href="../../../_autosummary/jax.experimental.maps.Mesh.html#jax.experimental.maps.Mesh.__init__">[docs]</a>  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]],</span>
               <span class="n">axis_names</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">]):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
      <span class="n">devices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">devices</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis_names</span><span class="p">)</span>
    <span class="c1"># TODO: Make sure that devices are unique? At least with the quick and</span>
    <span class="c1">#       dirty check that the array size is not larger than the number of</span>
    <span class="c1">#       available devices?</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">writeable</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">axis_names</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Mesh</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">False</span>
    <span class="c1"># This is a performance optimization. Comparing thousands of devices</span>
    <span class="c1"># can be expensive.</span>
    <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">other</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">axis_names</span> <span class="ow">and</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">devices</span><span class="p">))</span>

  <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_hash&#39;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_hash</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">(</span>
          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">flat</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hash</span>

  <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot reassign attributes of immutable mesh objects&quot;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">new_env</span> <span class="o">=</span> <span class="n">_old_env</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">with_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">_old_env</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_env</span><span class="p">)</span>
    <span class="n">thread_resources</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">new_env</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
    <span class="n">_old_env</span><span class="o">.</span><span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="n">thread_resources</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">_old_env</span><span class="o">.</span><span class="n">stack</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="kc">False</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">((</span><span class="n">name</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">is_multi_process</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_devices</span><span class="p">)</span>

  <span class="nd">@maybe_cached_property</span>
  <span class="k">def</span> <span class="nf">local_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_mesh</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">_local_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">process_index</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span>
    <span class="n">is_local_device</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">process_index</span><span class="p">,</span> <span class="n">otypes</span><span class="o">=</span><span class="p">[</span><span class="nb">bool</span><span class="p">])(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
    <span class="n">subcube_indices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># We take the smallest slice of each dimension that doesn&#39;t skip any local device.</span>
    <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
      <span class="n">other_axes</span> <span class="o">=</span> <span class="n">tuple_delete</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">ndim</span><span class="p">)),</span> <span class="n">axis</span><span class="p">)</span>
      <span class="c1"># NOTE: This re-reduces over many axes multiple times, so we could definitely</span>
      <span class="c1">#       optimize it, but I hope it won&#39;t be a bottleneck anytime soon.</span>
      <span class="n">local_slices</span> <span class="o">=</span> <span class="n">is_local_device</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">other_axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">nonzero_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">local_slices</span><span class="p">)</span>
      <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">nonzero_indices</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">nonzero_indices</span><span class="p">))</span>
      <span class="n">subcube_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">subcube_indices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">subcube_indices</span><span class="p">)</span>
    <span class="c1"># We only end up with all conditions being true if the local devices formed a</span>
    <span class="c1"># subcube of the full array. This is because we were biased towards taking a</span>
    <span class="c1"># &quot;hull&quot; spanned by the devices, and in case the local devices don&#39;t form a</span>
    <span class="c1"># subcube that hull will contain non-local devices.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_local_device</span><span class="p">[</span><span class="n">subcube_indices</span><span class="p">]</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;When passing non-GlobalDeviceArray inputs to pjit or xmap, devices &quot;</span>
          <span class="s2">&quot;connected to a single host must form a contiguous subcube of the &quot;</span>
          <span class="s2">&quot;global device mesh&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Mesh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="n">subcube_indices</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">empty</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">d</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">otypes</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">])(</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;Mesh([], ())&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Mesh(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="si">!r}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">axis_names</span><span class="si">!r}</span><span class="s2">)&quot;</span>

  <span class="nd">@maybe_cached_property</span>
  <span class="k">def</span> <span class="nf">local_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">process_index</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">flat</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">process_index</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_local_to_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">untile_aval_nd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span>
                          <span class="n">tile_aval_nd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">aval</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_global_to_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">untile_aval_nd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span>
                          <span class="n">tile_aval_nd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">aval</span><span class="p">))</span></div>


<span class="n">ResourceAxisName</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">AxisName</span>

<span class="k">class</span> <span class="nc">_Loop</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">ResourceAxisName</span>
  <span class="n">length</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">def</span> <span class="nf">show_axes</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
  <span class="k">return</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">`&quot;</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">ResourceEnv</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
  <span class="n">physical_mesh</span><span class="p">:</span> <span class="n">Mesh</span>
  <span class="n">loops</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">_Loop</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">with_mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">):</span>
    <span class="n">overlap</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">resource_axes</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">physical_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">overlap</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cannot update the mesh of the current resource &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;environment. The new mesh shadows already defined axes &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">show_axes</span><span class="p">(</span><span class="n">overlap</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">physical_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">with_extra_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loop</span><span class="p">:</span> <span class="n">_Loop</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">loop</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">resource_axes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cannot extend the resource environment with loop named &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">loop</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">`. An axis of this name is already defined!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">loops</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loops</span> <span class="o">+</span> <span class="p">(</span><span class="n">loop</span><span class="p">,))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">physical_resource_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">ResourceAxisName</span><span class="p">]:</span>
    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">physical_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">loop_resource_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">ResourceAxisName</span><span class="p">]:</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">loop</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">loop</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loops</span><span class="p">}</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">resource_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="n">ResourceAxisName</span><span class="p">]:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">physical_resource_axes</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">loop_resource_axes</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">physical_mesh</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">shape</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">local_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">physical_mesh</span><span class="o">.</span><span class="n">local_mesh</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">shape</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shape</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;ResourceEnv(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">physical_mesh</span><span class="si">!r}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loops</span><span class="si">!r}</span><span class="s2">)&quot;</span>

<span class="n">EMPTY_ENV</span> <span class="o">=</span> <span class="n">ResourceEnv</span><span class="p">(</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span> <span class="p">()),</span> <span class="p">())</span>

<span class="k">class</span> <span class="nc">_ThreadResourcesLocalState</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">EMPTY_ENV</span>

<span class="n">thread_resources</span> <span class="o">=</span> <span class="n">_ThreadResourcesLocalState</span><span class="p">()</span>

<span class="c1"># TODO(yashkatariya): Merge this into `_ThreadResourcesLocalState` by</span>
<span class="c1"># maintaining a stack there and pointing `self.env` to `self.stack[-1]`.</span>
<span class="c1"># Do this after the old `mesh` context manager is deprecated.</span>
<span class="k">class</span> <span class="nc">_ThreadLocalOldEnv</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stack</span> <span class="o">=</span> <span class="p">[</span><span class="n">EMPTY_ENV</span><span class="p">]</span>

<span class="n">_old_env</span> <span class="o">=</span> <span class="n">_ThreadLocalOldEnv</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">tile_aval_nd</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">named_shape</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">in_axes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">%</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">named_shape</span>
    <span class="n">named_shape</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">//=</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">named_shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">untile_aval_nd</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">aval</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">ShapedArray</span><span class="p">)</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">named_shape</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">named_shape</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">out_axes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*=</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">named_shape</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># The name might be missing --- it&#39;s a broadcast.</span>
  <span class="k">return</span> <span class="n">aval</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">named_shape</span><span class="o">=</span><span class="n">named_shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SPMDBatchTrace</span><span class="p">(</span><span class="n">batching</span><span class="o">.</span><span class="n">BatchTrace</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">get_axis_primitive_batcher</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">primitive</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">primitive</span> <span class="ow">in</span> <span class="n">spmd_primitive_batchers</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">spmd_primitive_batchers</span><span class="p">[</span><span class="n">primitive</span><span class="p">],</span>
          <span class="n">frame</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">frame</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">frame</span><span class="o">.</span><span class="n">main_trace</span><span class="o">.</span><span class="n">trace_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_axis_primitive_batcher</span><span class="p">(</span><span class="n">primitive</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>


<span class="n">spmd_primitive_batchers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">vtile_by_mesh</span><span class="p">(</span><span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
                  <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span>
                  <span class="n">in_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ArrayMapping</span><span class="p">],</span>
                  <span class="n">out_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ArrayMapping</span><span class="p">]):</span>
  <span class="c1"># We vectorize in reversed order, because vmap is often biased towards</span>
  <span class="c1"># moving the batch axis to the front, and this way of stacking transforms</span>
  <span class="c1"># will order the batch axes according to the mesh axis order.</span>
  <span class="c1"># Not strictly necessary, but seems nicer than reversing it?</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">fun</span> <span class="o">=</span> <span class="n">batching</span><span class="o">.</span><span class="n">vtile</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span>
                         <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">in_axes</span><span class="p">),</span>
                         <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">out_axes</span><span class="p">),</span>
                         <span class="n">tile_size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span>
                         <span class="n">axis_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                         <span class="n">main_type</span><span class="o">=</span><span class="n">SPMDBatchTrace</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">fun</span>

<span class="n">full_to_shard_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="s1">&#39;full_to_shard&#39;</span><span class="p">)</span>

<span class="nd">@full_to_shard_p</span><span class="o">.</span><span class="n">def_abstract_eval</span>
<span class="k">def</span> <span class="nf">_full_to_shard_abstract_eval</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="o">**</span><span class="n">_</span><span class="p">):</span>
  <span class="c1"># TODO: Assert x is a global aval! Or ideally check that it&#39;s global in dims from axes!</span>
  <span class="k">return</span> <span class="n">tile_aval_nd</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_manual_proto</span><span class="p">(</span><span class="n">aval</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">,</span> <span class="n">manual_axes_set</span><span class="p">:</span> <span class="n">FrozenSet</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">],</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create an OpSharding proto that declares all mesh axes from `axes` as manual</span>
<span class="sd">  and all others as replicated.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">named_mesh_shape</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">mesh_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">named_mesh_shape</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
  <span class="n">axis_order</span> <span class="o">=</span> <span class="p">{</span><span class="n">axis</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)}</span>

  <span class="n">manual_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">manual_axes_set</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">str</span><span class="p">))</span>
  <span class="n">replicated_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">axis</span> <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">manual_axes_set</span><span class="p">)</span>

  <span class="n">tad_perm</span> <span class="o">=</span> <span class="p">([</span><span class="n">axis_order</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">replicated_axes</span><span class="p">]</span> <span class="o">+</span>
              <span class="p">[</span><span class="n">axis_order</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">manual_axes</span><span class="p">])</span>
  <span class="n">tad_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">aval</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">tad_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">named_mesh_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">replicated_axes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)))</span>
  <span class="n">tad_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="n">named_mesh_shape</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">manual_axes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)))</span>

  <span class="n">raw_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">)</span>
  <span class="n">proto</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">()</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">OTHER</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">tile_assignment_dimensions</span> <span class="o">=</span> <span class="n">tad_shape</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">tile_assignment_devices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">raw_mesh</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tad_perm</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tad_shape</span><span class="p">)</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>
  <span class="n">proto</span><span class="o">.</span><span class="n">last_tile_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">REPLICATED</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">MANUAL</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">proto</span>

<span class="nd">@partial</span><span class="p">(</span><span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">,</span> <span class="n">full_to_shard_p</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_full_to_shard_lowering</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">manual_axes</span><span class="p">:</span> <span class="n">FrozenSet</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">]):</span>
  <span class="c1"># TODO: Can we short-circuit for replicated values? Probably not.</span>
  <span class="n">aval_in</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">avals_in</span>
  <span class="n">aval_out</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">avals_out</span>
  <span class="n">sharding_proto</span> <span class="o">=</span> <span class="n">mesh_sharding_specs</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)(</span><span class="n">aval_in</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span><span class="o">.</span><span class="n">sharding_proto</span><span class="p">()</span>
  <span class="n">unspecified_dims</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">aval_in</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
  <span class="n">sx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_with_sharding_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sharding_proto</span><span class="p">,</span> <span class="n">unspecified_dims</span><span class="o">=</span><span class="n">unspecified_dims</span><span class="p">)</span>
  <span class="n">manual_proto</span> <span class="o">=</span> <span class="n">_manual_proto</span><span class="p">(</span><span class="n">aval_in</span><span class="p">,</span> <span class="n">manual_axes</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
  <span class="n">result_type</span><span class="p">,</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">aval_to_ir_types</span><span class="p">(</span><span class="n">aval_out</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_with_full_to_shard_op</span><span class="p">(</span><span class="n">result_type</span><span class="p">,</span> <span class="n">sx</span><span class="p">,</span> <span class="n">manual_proto</span><span class="p">,</span> <span class="n">unspecified_dims</span><span class="o">=</span><span class="n">unspecified_dims</span><span class="p">),</span>

<span class="n">shard_to_full_p</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">(</span><span class="s1">&#39;shard_to_full&#39;</span><span class="p">)</span>

<span class="nd">@shard_to_full_p</span><span class="o">.</span><span class="n">def_abstract_eval</span>
<span class="k">def</span> <span class="nf">_shard_to_full_abstract_eval</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="o">**</span><span class="n">_</span><span class="p">):</span>
  <span class="c1"># TODO: Assert x is a global aval! Or ideally check that it&#39;s global in dims from axes!</span>
  <span class="k">return</span> <span class="n">untile_aval_nd</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nd">@partial</span><span class="p">(</span><span class="n">mlir</span><span class="o">.</span><span class="n">register_lowering</span><span class="p">,</span> <span class="n">shard_to_full_p</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_shard_to_full_lowering</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">manual_axes</span><span class="p">:</span> <span class="n">FrozenSet</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">]):</span>
  <span class="n">aval_in</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">avals_in</span>
  <span class="n">aval_out</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">avals_out</span>
  <span class="n">manual_proto</span> <span class="o">=</span> <span class="n">_manual_proto</span><span class="p">(</span><span class="n">aval_in</span><span class="p">,</span> <span class="n">manual_axes</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
  <span class="n">result_type</span><span class="p">,</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">aval_to_ir_types</span><span class="p">(</span><span class="n">aval_out</span><span class="p">)</span>
  <span class="n">unspecified_dims</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">aval_in</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
  <span class="n">sx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_with_sharding_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">manual_proto</span><span class="p">,</span> <span class="n">unspecified_dims</span><span class="o">=</span><span class="n">unspecified_dims</span><span class="p">)</span>
  <span class="n">sharding_proto</span> <span class="o">=</span> <span class="n">mesh_sharding_specs</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)(</span><span class="n">aval_out</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span><span class="o">.</span><span class="n">sharding_proto</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">mlir</span><span class="o">.</span><span class="n">wrap_with_shard_to_full_op</span><span class="p">(</span><span class="n">result_type</span><span class="p">,</span> <span class="n">sx</span><span class="p">,</span> <span class="n">sharding_proto</span><span class="p">,</span> <span class="n">unspecified_dims</span><span class="p">),</span>

<span class="nd">@lu</span><span class="o">.</span><span class="n">transformation</span>
<span class="k">def</span> <span class="nf">vtile_manual</span><span class="p">(</span><span class="n">manual_axes</span><span class="p">:</span> <span class="n">FrozenSet</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">],</span>
                 <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span>
                 <span class="n">in_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ArrayMapping</span><span class="p">],</span>
                 <span class="n">out_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ArrayMapping</span><span class="p">],</span>
                 <span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="n">tiled_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">full_to_shard_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span> <span class="n">manual_axes</span><span class="o">=</span><span class="n">manual_axes</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">axes</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">in_axes</span><span class="p">)]</span>
  <span class="n">tiled_outs</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tiled_args</span><span class="p">,</span> <span class="p">{}</span>
  <span class="n">outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">shard_to_full_p</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span> <span class="n">manual_axes</span><span class="o">=</span><span class="n">manual_axes</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">axes</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tiled_outs</span><span class="p">,</span> <span class="n">out_axes</span><span class="p">)]</span>
  <span class="k">yield</span> <span class="n">outs</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TileVectorize</span><span class="p">:</span>
  <span class="k">pass</span>

<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TileManual</span><span class="p">:</span>
  <span class="n">manual_axes</span><span class="p">:</span> <span class="n">FrozenSet</span><span class="p">[</span><span class="n">MeshAxisName</span><span class="p">]</span>

<span class="n">TilingMethod</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">TileVectorize</span><span class="p">,</span> <span class="n">TileManual</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_check_if_any_auto</span><span class="p">(</span>
    <span class="n">shardings</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">,</span>
                              <span class="n">_UnspecifiedValue</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_is_auto</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
  <span class="k">return</span> <span class="kc">False</span>


<span class="k">class</span> <span class="nc">_UnconstrainedPartitionSingleton</span><span class="p">:</span>

  <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;UNCONSTRAINED&quot;</span>


<span class="c1"># Unconstrained sentinel value for PartitionSpec, representing a dimension for</span>
<span class="c1"># which the user wants XLA to assign the best partitioning.</span>
<span class="c1"># TODO(yashkatariya): May rename to AUTO.</span>
<span class="n">_UNCONSTRAINED_PARTITION</span> <span class="o">=</span> <span class="n">_UnconstrainedPartitionSingleton</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PartitionSpec</span><span class="p">(</span><span class="nb">tuple</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Tuple of integer specifying how a value should be partitioned.</span>

<span class="sd">  Each integer corresponds to how many ways a dimension is partitioned. We</span>
<span class="sd">  create a separate class for this so JAX&#39;s pytree utilities can distinguish it</span>
<span class="sd">  from a tuple that should be treated as a pytree.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">partitions</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">partitions</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">PartitionSpec</span><span class="p">,</span> <span class="n">partitions</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;PartitionSpec</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">PartitionSpec</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

  <span class="sd">&quot;&quot;&quot;A sentinel value representing a dim is unconstrained.&quot;&quot;&quot;</span>
  <span class="n">UNCONSTRAINED</span> <span class="o">=</span> <span class="n">_UNCONSTRAINED_PARTITION</span>


<span class="k">def</span> <span class="nf">_get_backend_from_shardings</span><span class="p">(</span>
    <span class="n">shardings</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">,</span> <span class="n">_UnspecifiedValue</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">XlaBackend</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]:</span>
  <span class="n">da</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shardings</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
      <span class="k">continue</span>
    <span class="c1"># pytype does not understand that _UNSPECIFIED is being skipped above.</span>
    <span class="n">da</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">_device_assignment</span>  <span class="c1"># type: ignore</span>
    <span class="k">break</span>
  <span class="n">da</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span> <span class="n">da</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">da</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_device_backend</span><span class="p">(</span><span class="n">da</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">da</span>


<span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
<span class="k">def</span> <span class="nf">lower_sharding_computation</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
    <span class="n">api_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fun_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">in_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span>
    <span class="n">out_shardings</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">,</span> <span class="n">_UnspecifiedValue</span><span class="p">]],</span> <span class="n">_UnspecifiedValue</span><span class="p">],</span>
    <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">global_in_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">],</span>
    <span class="n">in_is_global</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">keep_unused</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">committed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">always_lower</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">inp_device_assignment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Lowers a computation to XLA. It can take arbitrary shardings as input.</span>

<span class="sd">  The caller of this code can pass in a singleton _UNSPECIFIED because the</span>
<span class="sd">  number of out_avals might not be known at that time and</span>
<span class="sd">  lower_sharding_computation calculates the number of out_avals so it can apply</span>
<span class="sd">  the singleton _UNSPECIFIED to all out_avals.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Device assignment across all inputs and outputs should be the same. This</span>
  <span class="c1"># is checked in pjit.</span>
  <span class="k">if</span> <span class="n">inp_device_assignment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="s2">&quot;if device_assignment given, no in_shardings&quot;</span>
    <span class="c1"># TODO(yashkatariya): Look into allowing more than 1 device here.</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp_device_assignment</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">device_assignment</span> <span class="o">=</span> <span class="n">inp_device_assignment</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_device_backend</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">):</span>
      <span class="n">backend</span><span class="p">,</span> <span class="n">device_assignment</span> <span class="o">=</span> <span class="n">_get_backend_from_shardings</span><span class="p">(</span><span class="n">in_shardings</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># type ignore because mypy can&#39;t understand that out_shardings that are</span>
      <span class="c1"># UNSPECIFIED singleton are filtered above.</span>
      <span class="n">backend</span><span class="p">,</span> <span class="n">device_assignment</span> <span class="o">=</span> <span class="n">_get_backend_from_shardings</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
          <span class="n">it</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">in_shardings</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

  <span class="n">name_stack</span> <span class="o">=</span> <span class="n">new_name_stack</span><span class="p">(</span><span class="n">wrap_name</span><span class="p">(</span><span class="n">fun_name</span><span class="p">,</span> <span class="n">api_name</span><span class="p">))</span>

  <span class="c1"># 1. Trace to jaxpr and preprocess/verify it</span>
  <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished tracing + transforming </span><span class="si">{</span><span class="n">name_stack</span><span class="si">}</span><span class="s2"> &quot;</span>
                                 <span class="s2">&quot;in </span><span class="si">{elapsed_time}</span><span class="s2"> sec&quot;</span><span class="p">):</span>
    <span class="n">jaxpr</span><span class="p">,</span> <span class="n">global_out_avals</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_final</span><span class="p">(</span>
        <span class="n">fun</span><span class="p">,</span> <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">debug_info</span><span class="o">=</span><span class="n">pe</span><span class="o">.</span><span class="n">debug_info_final</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">api_name</span><span class="p">))</span>
  <span class="n">kept_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>

  <span class="n">log_priority</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_log_compiles</span> <span class="k">else</span> <span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log_priority</span><span class="p">,</span>
              <span class="s2">&quot;Compiling </span><span class="si">%s</span><span class="s2"> (</span><span class="si">%d</span><span class="s2">) for with global shapes and types </span><span class="si">%s</span><span class="s2">. &quot;</span>
              <span class="s2">&quot;Argument mapping: </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
              <span class="nb">getattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;unnamed function&gt;&#39;</span><span class="p">),</span> <span class="nb">id</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span>
              <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">):</span>
    <span class="n">out_shardings</span> <span class="o">=</span> <span class="p">(</span><span class="n">_UNSPECIFIED</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>

  <span class="c1"># mypy doesn&#39;t understand that out_sharding here is always a sequence.</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">),</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
      <span class="nb">len</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

  <span class="k">if</span> <span class="n">keep_unused</span><span class="p">:</span>
    <span class="n">kept_var_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">)))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">jaxpr</span><span class="p">,</span> <span class="n">kept_const_idx</span><span class="p">,</span> <span class="n">kept_var_idx</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">_prune_unused_inputs</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
    <span class="n">consts</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">consts</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_const_idx</span><span class="p">]</span>
    <span class="n">global_in_avals</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">a</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span><span class="p">)</span>
    <span class="n">in_shardings</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">s</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">in_shardings</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span><span class="p">)</span>
    <span class="n">in_is_global</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">g</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">in_is_global</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span><span class="p">)</span>
    <span class="n">donated_invars</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">donated_invars</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">kept_const_idx</span>

  <span class="n">process_index</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span>
  <span class="n">local_device_assignment</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_assignment</span>
                             <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">process_index</span><span class="p">]</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">local_device_assignment</span><span class="p">):</span>
    <span class="n">check_multihost_collective_allowlist</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="n">has_outfeed</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">jaxpr_uses_outfeed</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
  <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">apply_outfeed_rewriter</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="c1"># Computations that only produce constants and/or only rearrange their inputs,</span>
  <span class="c1"># which are often produced from partial evaluation, don&#39;t need compilation,</span>
  <span class="c1"># and don&#39;t need to evaluate their arguments.</span>
  <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">always_lower</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="n">jaxpr</span><span class="o">.</span><span class="n">effects</span> <span class="ow">or</span> <span class="n">has_outfeed</span><span class="p">)</span> <span class="ow">and</span>
      <span class="p">(</span><span class="ow">not</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">kept_outputs</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">outvars</span><span class="p">)</span> <span class="ow">and</span>
      <span class="nb">all</span><span class="p">(</span><span class="n">_is_unspecified</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_shardings</span><span class="p">)</span> <span class="ow">and</span>  <span class="c1"># type: ignore</span>
      <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="s2">&quot;compile_replicated&quot;</span><span class="p">)):</span>  <span class="c1"># this means &#39;not pathways&#39;</span>
    <span class="k">return</span> <span class="n">MeshComputation</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">name_stack</span><span class="p">),</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">=</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="o">=</span><span class="n">consts</span><span class="p">,</span>
        <span class="n">global_in_avals</span><span class="o">=</span><span class="n">global_in_avals</span><span class="p">,</span> <span class="n">global_out_avals</span><span class="o">=</span><span class="n">global_out_avals</span><span class="p">,</span>
        <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_shardings</span><span class="p">,</span>
        <span class="n">device_assignment</span><span class="o">=</span><span class="n">device_assignment</span><span class="p">,</span> <span class="n">committed</span><span class="o">=</span><span class="n">committed</span><span class="p">,</span>
        <span class="n">kept_var_idx</span><span class="o">=</span><span class="n">kept_var_idx</span><span class="p">,</span> <span class="n">keepalive</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

  <span class="c1"># Look at the number of replcas present in the jaxpr. In</span>
  <span class="c1"># lower_sharding_computation, nreps &gt; 1 during `jit(pmap)` cases. This is</span>
  <span class="c1"># handled here so as to deprecate the lower_xla_callable codepath when</span>
  <span class="c1"># `jax.Array` is turned on by default.</span>
  <span class="c1"># TODO(yashkatariya): Remove this when `jit(pmap)` is removed.</span>
  <span class="n">nreps</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">jaxpr_replicas</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
  <span class="n">dispatch</span><span class="o">.</span><span class="n">raise_warnings_or_errors_for_jit_of_pmap</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">fun_name</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">)</span>

  <span class="c1"># 2. Build up the HLO</span>
  <span class="n">tuple_args</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">should_tuple_args</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">),</span> <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>

  <span class="n">in_op_shardings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">]]]</span>
  <span class="n">out_op_shardings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">]]]</span>
  <span class="n">axis_ctx</span><span class="p">:</span> <span class="n">mlir</span><span class="o">.</span><span class="n">AxisContext</span>

  <span class="k">if</span> <span class="n">nreps</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">in_op_shardings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
        <span class="n">in_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">in_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">physical_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">in_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="c1"># TODO(yashkatariya): Fix the HLO produced if out_partitions is</span>
    <span class="c1"># [None, OpShardingProto] has the sharding annotations.</span>
    <span class="n">out_op_shardings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">or</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
        <span class="n">out_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">out_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">physical_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">o</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out_op_shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
    <span class="n">replicated_args</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">)</span>
    <span class="n">axis_ctx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ShardingContext</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># This path is triggered for `jit(pmap)` cases.</span>
    <span class="n">replicated_args</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">in_op_shardings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">out_op_shardings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">axis_env</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="p">,</span> <span class="p">(),</span> <span class="p">())</span>
    <span class="n">axis_ctx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">axis_env</span><span class="p">)</span>

  <span class="n">closed_jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
  <span class="n">module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">]</span>
  <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">api_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">fun_name</span><span class="si">}</span><span class="s2">&quot;</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ordered effects are not supported for more than 1 device.&quot;</span><span class="p">)</span>
  <span class="n">unordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                       <span class="k">if</span> <span class="n">eff</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
  <span class="n">ordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                     <span class="k">if</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
  <span class="n">lowering_result</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">lower_jaxpr_to_module</span><span class="p">(</span>
      <span class="n">module_name</span><span class="p">,</span>
      <span class="n">closed_jaxpr</span><span class="p">,</span>
      <span class="n">unordered_effects</span><span class="p">,</span>
      <span class="n">ordered_effects</span><span class="p">,</span>
      <span class="n">backend</span><span class="p">,</span>
      <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">,</span>
      <span class="n">axis_ctx</span><span class="p">,</span>
      <span class="n">name_stack</span><span class="p">,</span>
      <span class="n">donated_invars</span><span class="p">,</span>
      <span class="n">replicated_args</span><span class="o">=</span><span class="n">replicated_args</span><span class="p">,</span>
      <span class="n">arg_shardings</span><span class="o">=</span><span class="n">in_op_shardings</span><span class="p">,</span>
      <span class="n">result_shardings</span><span class="o">=</span><span class="n">out_op_shardings</span><span class="p">)</span>

  <span class="n">module</span><span class="p">,</span> <span class="n">keepalive</span><span class="p">,</span> <span class="n">host_callbacks</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">lowering_result</span><span class="o">.</span><span class="n">keepalive</span><span class="p">,</span>
      <span class="n">lowering_result</span><span class="o">.</span><span class="n">host_callbacks</span><span class="p">)</span>

  <span class="c1"># backend and device_assignment is passed through to MeshExecutable because</span>
  <span class="c1"># if keep_unused=False and all in_shardings are pruned, then there is no way</span>
  <span class="c1"># to get the device_assignment and backend. So pass it to MeshExecutable</span>
  <span class="c1"># because we calculate the device_assignment and backend before in_shardings,</span>
  <span class="c1"># etc are pruned.</span>
  <span class="k">return</span> <span class="n">MeshComputation</span><span class="p">(</span>
      <span class="nb">str</span><span class="p">(</span><span class="n">name_stack</span><span class="p">),</span>
      <span class="n">module</span><span class="p">,</span>
      <span class="kc">False</span><span class="p">,</span>
      <span class="n">donated_invars</span><span class="p">,</span>
      <span class="n">mesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">global_in_avals</span><span class="o">=</span><span class="n">global_in_avals</span><span class="p">,</span>
      <span class="n">global_out_avals</span><span class="o">=</span><span class="n">global_out_avals</span><span class="p">,</span>
      <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_shardings</span><span class="p">,</span>
      <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_shardings</span><span class="p">,</span>
      <span class="n">spmd_lowering</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">tuple_args</span><span class="o">=</span><span class="n">tuple_args</span><span class="p">,</span>
      <span class="n">in_is_global</span><span class="o">=</span><span class="n">in_is_global</span><span class="p">,</span>
      <span class="n">auto_spmd_lowering</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">unordered_effects</span><span class="o">=</span><span class="n">unordered_effects</span><span class="p">,</span>
      <span class="n">ordered_effects</span><span class="o">=</span><span class="n">ordered_effects</span><span class="p">,</span>
      <span class="n">host_callbacks</span><span class="o">=</span><span class="n">host_callbacks</span><span class="p">,</span>
      <span class="n">keepalive</span><span class="o">=</span><span class="n">keepalive</span><span class="p">,</span>
      <span class="n">kept_var_idx</span><span class="o">=</span><span class="n">kept_var_idx</span><span class="p">,</span>
      <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
      <span class="n">device_assignment</span><span class="o">=</span><span class="n">device_assignment</span><span class="p">,</span>
      <span class="n">committed</span><span class="o">=</span><span class="n">committed</span><span class="p">,</span>
      <span class="n">pmap_nreps</span><span class="o">=</span><span class="n">nreps</span><span class="p">)</span>


<span class="nd">@profiler</span><span class="o">.</span><span class="n">annotate_function</span>
<span class="k">def</span> <span class="nf">lower_mesh_computation</span><span class="p">(</span>
    <span class="n">fun</span><span class="p">:</span> <span class="n">lu</span><span class="o">.</span><span class="n">WrappedFun</span><span class="p">,</span>
    <span class="n">api_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">fun_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span>
    <span class="n">in_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">MeshPspecSharding</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">]],</span>
    <span class="n">out_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">MeshPspecSharding</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">,</span>
                            <span class="n">_UnspecifiedValue</span><span class="p">]],</span>
    <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">spmd_lowering</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">global_in_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">],</span>
    <span class="n">tiling_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TilingMethod</span><span class="p">],</span>
    <span class="n">in_is_global</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]):</span>
  <span class="k">assert</span> <span class="ow">not</span> <span class="n">mesh</span><span class="o">.</span><span class="n">empty</span>
  <span class="n">backend</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_device_backend</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">name_stack</span> <span class="o">=</span> <span class="n">new_name_stack</span><span class="p">(</span><span class="n">wrap_name</span><span class="p">(</span><span class="n">fun_name</span><span class="p">,</span> <span class="n">api_name</span><span class="p">))</span>

  <span class="n">auto_spmd_lowering</span> <span class="o">=</span> <span class="n">_check_if_any_auto</span><span class="p">(</span><span class="n">in_shardings</span> <span class="o">+</span> <span class="n">out_shardings</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

  <span class="k">if</span> <span class="n">auto_spmd_lowering</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">spmd_lowering</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Enable spmd_lowering to use auto spmd lowering.&#39;</span><span class="p">)</span>

  <span class="n">global_axis_sizes</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">shape</span>

  <span class="n">log_priority</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_log_compiles</span> <span class="k">else</span> <span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span>
  <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log_priority</span><span class="p">,</span>
              <span class="s2">&quot;Compiling </span><span class="si">%s</span><span class="s2"> (</span><span class="si">%d</span><span class="s2">) for </span><span class="si">%s</span><span class="s2"> mesh with global shapes and types </span><span class="si">%s</span><span class="s2">. &quot;</span>
              <span class="s2">&quot;Argument mapping: </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
              <span class="nb">getattr</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;unnamed function&gt;&#39;</span><span class="p">),</span> <span class="nb">id</span><span class="p">(</span><span class="n">fun</span><span class="p">),</span>
              <span class="nb">tuple</span><span class="p">(</span><span class="n">global_axis_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">global_in_avals</span><span class="p">,</span>
              <span class="n">in_shardings</span><span class="p">)</span>

  <span class="c1"># 1. Trace to jaxpr and preprocess/verify it</span>
  <span class="k">if</span> <span class="n">spmd_lowering</span><span class="p">:</span>
    <span class="c1"># TODO: Consider handling xmap&#39;s &#39;vectorize&#39; in here. We can vmap once instead of vtile twice!</span>
    <span class="k">if</span> <span class="n">tiling_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tiling_method</span><span class="p">,</span> <span class="n">TileVectorize</span><span class="p">):</span>
        <span class="n">tiling_transform</span> <span class="o">=</span> <span class="n">vtile_by_mesh</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tiling_method</span><span class="p">,</span> <span class="n">TileManual</span><span class="p">):</span>
        <span class="n">tiling_transform</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">vtile_manual</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">tiling_method</span><span class="o">.</span><span class="n">manual_axes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unrecognized tiling method: </span><span class="si">{</span><span class="n">tiling_method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">)</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">auto_spmd_lowering</span>
      <span class="c1"># This is the xmap path where there is no `AUTO` or `UNSPECIFIED`, which</span>
      <span class="c1"># is why `.spec` can be accessed.</span>
      <span class="n">fun</span> <span class="o">=</span> <span class="n">tiling_transform</span><span class="p">(</span>
          <span class="n">fun</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">[</span><span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">in_shardings</span><span class="p">],</span>  <span class="c1"># type: ignore</span>
          <span class="p">[</span><span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_shardings</span><span class="p">])</span>  <span class="c1"># type: ignore</span>
    <span class="n">in_jaxpr_avals</span> <span class="o">=</span> <span class="n">global_in_avals</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tiling_method</span><span class="p">,</span> <span class="n">TileVectorize</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">auto_spmd_lowering</span>
    <span class="c1"># In non-spmd lowering path, there is no `AUTO` or `UNSPECIFIED`, which is</span>
    <span class="c1"># why `.spec` can be accessed.</span>
    <span class="n">in_tiled_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">tile_aval_nd</span><span class="p">(</span><span class="n">global_axis_sizes</span><span class="p">,</span> <span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">spec</span><span class="p">),</span> <span class="n">aval</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                      <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">)]</span>
    <span class="n">in_jaxpr_avals</span> <span class="o">=</span> <span class="n">in_tiled_avals</span>
  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env_nd</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished tracing + transforming </span><span class="si">{</span><span class="n">name_stack</span><span class="si">}</span><span class="s2"> &quot;</span>
                                   <span class="s2">&quot;in </span><span class="si">{elapsed_time}</span><span class="s2"> sec&quot;</span><span class="p">):</span>
      <span class="n">jaxpr</span><span class="p">,</span> <span class="n">out_jaxpr_avals</span><span class="p">,</span> <span class="n">consts</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">trace_to_jaxpr_final</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">in_jaxpr_avals</span><span class="p">)</span>
  <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_shardings</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_jaxpr_avals</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">spmd_lowering</span><span class="p">:</span>
    <span class="n">global_out_avals</span> <span class="o">=</span> <span class="n">out_jaxpr_avals</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># In non-spmd lowering path, there is no `AUTO` or `UNSPECIFIED`, which is</span>
    <span class="c1"># why `.spec` can be accessed.</span>
    <span class="n">global_out_avals</span> <span class="o">=</span> <span class="p">[</span><span class="n">untile_aval_nd</span><span class="p">(</span><span class="n">global_axis_sizes</span><span class="p">,</span> <span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">spec</span><span class="p">),</span> <span class="n">aval</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">out_jaxpr_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">)]</span>
  <span class="n">_sanitize_mesh_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">mesh</span><span class="o">.</span><span class="n">is_multi_process</span><span class="p">:</span>
    <span class="n">check_multihost_collective_allowlist</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>
  <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">apply_outfeed_rewriter</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">)</span>

  <span class="c1"># 2. Build up the HLO</span>
  <span class="n">tuple_args</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">should_tuple_args</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">in_jaxpr_avals</span><span class="p">),</span> <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">)</span>

  <span class="n">in_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">]]]</span>
  <span class="n">out_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">]]]</span>
  <span class="n">axis_ctx</span><span class="p">:</span> <span class="n">mlir</span><span class="o">.</span><span class="n">AxisContext</span>
  <span class="k">if</span> <span class="n">spmd_lowering</span><span class="p">:</span>
    <span class="n">in_partitions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">_is_auto</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="n">in_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">in_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">physical_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">in_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>

    <span class="c1"># TODO(yashkatariya): Fix the HLO produced if out_partitions is</span>
    <span class="c1"># [None, OpShardingProto] has the sharding annotations.</span>
    <span class="n">out_partitions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">aval</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">_is_auto</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
        <span class="n">out_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">core</span><span class="o">.</span><span class="n">is_opaque_dtype</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">out_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">_rules</span><span class="o">.</span><span class="n">physical_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">o</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">out_partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
    <span class="n">replicated_args</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_jaxpr_avals</span><span class="p">)</span>
    <span class="n">axis_ctx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">SPMDAxisContext</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">replicated_args</span> <span class="o">=</span> <span class="p">[</span><span class="ow">not</span> <span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">in_shardings</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
    <span class="n">in_partitions</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">out_partitions</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">axis_env</span> <span class="o">=</span> <span class="n">xla</span><span class="o">.</span><span class="n">AxisEnv</span><span class="p">(</span><span class="n">nreps</span><span class="o">=</span><span class="n">mesh</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
                           <span class="n">names</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">global_axis_sizes</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                           <span class="n">sizes</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">global_axis_sizes</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
    <span class="n">axis_ctx</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">ReplicaAxisContext</span><span class="p">(</span><span class="n">axis_env</span><span class="p">)</span>
  <span class="n">closed_jaxpr</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
  <span class="n">module</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">]</span>
  <span class="n">module_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">api_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">fun_name</span><span class="si">}</span><span class="s2">&quot;</span>
  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env_nd</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ordered effects not supported in mesh computations.&quot;</span><span class="p">)</span>
    <span class="n">unordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                         <span class="k">if</span> <span class="n">eff</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
    <span class="n">ordered_effects</span> <span class="o">=</span> <span class="p">[</span><span class="n">eff</span> <span class="k">for</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">closed_jaxpr</span><span class="o">.</span><span class="n">effects</span>
                       <span class="k">if</span> <span class="n">eff</span> <span class="ow">in</span> <span class="n">core</span><span class="o">.</span><span class="n">ordered_effects</span><span class="p">]</span>
    <span class="n">lowering_result</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">.</span><span class="n">lower_jaxpr_to_module</span><span class="p">(</span>
        <span class="n">module_name</span><span class="p">,</span>
        <span class="n">closed_jaxpr</span><span class="p">,</span>
        <span class="n">unordered_effects</span><span class="p">,</span>
        <span class="n">ordered_effects</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">.</span><span class="n">platform</span><span class="p">,</span>
        <span class="n">axis_ctx</span><span class="p">,</span>
        <span class="n">name_stack</span><span class="p">,</span>
        <span class="n">donated_invars</span><span class="p">,</span>
        <span class="n">replicated_args</span><span class="o">=</span><span class="n">replicated_args</span><span class="p">,</span>
        <span class="n">arg_shardings</span><span class="o">=</span><span class="n">in_partitions</span><span class="p">,</span>
        <span class="n">result_shardings</span><span class="o">=</span><span class="n">out_partitions</span><span class="p">)</span>
    <span class="n">module</span><span class="p">,</span> <span class="n">keepalive</span><span class="p">,</span> <span class="n">host_callbacks</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">lowering_result</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">lowering_result</span><span class="o">.</span><span class="n">keepalive</span><span class="p">,</span>
        <span class="n">lowering_result</span><span class="o">.</span><span class="n">host_callbacks</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">MeshComputation</span><span class="p">(</span>
      <span class="nb">str</span><span class="p">(</span><span class="n">name_stack</span><span class="p">),</span>
      <span class="n">module</span><span class="p">,</span>
      <span class="kc">False</span><span class="p">,</span>
      <span class="n">donated_invars</span><span class="p">,</span>
      <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
      <span class="n">global_in_avals</span><span class="o">=</span><span class="n">global_in_avals</span><span class="p">,</span>
      <span class="n">global_out_avals</span><span class="o">=</span><span class="n">global_out_avals</span><span class="p">,</span>
      <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_shardings</span><span class="p">,</span>
      <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_shardings</span><span class="p">,</span>
      <span class="n">spmd_lowering</span><span class="o">=</span><span class="n">spmd_lowering</span><span class="p">,</span>
      <span class="n">tuple_args</span><span class="o">=</span><span class="n">tuple_args</span><span class="p">,</span>
      <span class="n">in_is_global</span><span class="o">=</span><span class="n">in_is_global</span><span class="p">,</span>
      <span class="n">auto_spmd_lowering</span><span class="o">=</span><span class="n">auto_spmd_lowering</span><span class="p">,</span>
      <span class="n">unordered_effects</span><span class="o">=</span><span class="n">unordered_effects</span><span class="p">,</span>
      <span class="n">ordered_effects</span><span class="o">=</span><span class="n">ordered_effects</span><span class="p">,</span>
      <span class="n">host_callbacks</span><span class="o">=</span><span class="n">host_callbacks</span><span class="p">,</span>
      <span class="n">keepalive</span><span class="o">=</span><span class="n">keepalive</span><span class="p">,</span>
      <span class="n">kept_var_idx</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">))),</span>
      <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
      <span class="n">device_assignment</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">flat</span><span class="p">),</span>
      <span class="n">committed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MeshComputation</span><span class="p">(</span><span class="n">stages</span><span class="o">.</span><span class="n">XlaLowering</span><span class="p">):</span>
  <span class="n">_hlo</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">]</span>
  <span class="n">_executable</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MeshExecutable</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">hlo</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">],</span>
               <span class="n">is_trivial</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">donated_invars</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="o">**</span><span class="n">compile_args</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span> <span class="o">=</span> <span class="n">hlo</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_trivial</span> <span class="o">=</span> <span class="n">is_trivial</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_donated_invars</span> <span class="o">=</span> <span class="n">donated_invars</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span> <span class="o">=</span> <span class="n">compile_args</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="c1"># -- stages.XlaLowering overrides</span>

  <span class="k">def</span> <span class="nf">hlo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_trivial</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A trivial computation has no HLO&quot;</span><span class="p">)</span>
    <span class="c1"># this is a method for api consistency with dispatch.XlaComputation</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span>
    <span class="k">return</span> <span class="n">xe</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">mlir_module_to_xla_computation</span><span class="p">(</span>
        <span class="n">mlir</span><span class="o">.</span><span class="n">module_to_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">),</span>
        <span class="n">use_tuple_args</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">[</span><span class="s2">&quot;tuple_args&quot;</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">mhlo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_trivial</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;A trivial computation has no MHLO&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">):</span>
      <span class="n">module_str</span> <span class="o">=</span> <span class="n">xe</span><span class="o">.</span><span class="n">mlir</span><span class="o">.</span><span class="n">xla_computation_to_mlir_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">mlir</span><span class="o">.</span><span class="n">make_ir_context</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">module_str</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span>

  <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">_allow_propagation_to_outputs</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
              <span class="n">_allow_compile_replicated</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MeshExecutable</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_trivial</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="n">MeshExecutable</span><span class="o">.</span><span class="n">from_trivial_jaxpr</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span> <span class="o">=</span> <span class="n">MeshExecutable</span><span class="o">.</span><span class="n">from_hlo</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hlo</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">compile_args</span><span class="p">,</span>
            <span class="n">_allow_propagation_to_outputs</span><span class="o">=</span><span class="n">_allow_propagation_to_outputs</span><span class="p">,</span>
            <span class="n">_allow_compile_replicated</span><span class="o">=</span><span class="n">_allow_compile_replicated</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_executable</span>


<span class="k">def</span> <span class="nf">_get_input_metadata</span><span class="p">(</span>
    <span class="n">global_in_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">],</span>
    <span class="n">in_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span> <span class="n">in_is_global</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Index</span><span class="p">],</span> <span class="o">...</span><span class="p">]],</span>
           <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">]]:</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">MeshPspecSharding</span>

  <span class="n">shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">input_avals</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">gaval</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">is_global</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">in_is_global</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_global</span><span class="p">:</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">gaval</span>
      <span class="n">sharding</span> <span class="o">=</span> <span class="n">i</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">MeshPspecSharding</span><span class="p">)</span>
      <span class="n">aval</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">_global_to_local</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">ArrayMapping</span><span class="p">,</span> <span class="n">_get_array_mapping</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">spec</span><span class="p">)),</span> <span class="n">gaval</span><span class="p">)</span>
      <span class="n">sharding</span> <span class="o">=</span> <span class="n">MeshPspecSharding</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">local_mesh</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">spec</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">addressable_devices</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># We special case this logic to support fully replicated values because</span>
      <span class="c1"># the mesh is global mesh and the indices returned by `spec_to_indices` will</span>
      <span class="c1"># represent index for each device in the global mesh. But here we want</span>
      <span class="c1"># indices for the local devices of the global mesh.</span>
      <span class="n">proto</span> <span class="o">=</span> <span class="n">sharding</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">is_op_sharding_replicated</span><span class="p">(</span><span class="n">proto</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="n">aval</span><span class="o">.</span><span class="n">ndim</span>
                      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">addressable_devices</span><span class="p">)))</span>  <span class="c1"># type: ignore</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sharding</span><span class="o">.</span><span class="n">addressable_devices_indices_map</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>  <span class="c1"># type: ignore</span>

    <span class="n">shardings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sharding</span><span class="p">)</span>
    <span class="n">input_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">input_avals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aval</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">input_avals</span>


<span class="k">def</span> <span class="nf">_get_op_sharding_shardings_from_executable</span><span class="p">(</span>
    <span class="n">xla_executable</span><span class="p">,</span> <span class="n">device_assignment</span><span class="p">,</span> <span class="n">num_in_avals</span><span class="p">,</span> <span class="n">num_out_avals</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">pjit</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">OpShardingSharding</span><span class="p">,</span> <span class="n">SingleDeviceSharding</span>

  <span class="c1"># When the device assignment only has 1 device, SPMD partitioner will not run.</span>
  <span class="c1"># Hence the op shardings will not be set on the `hlo_module`. In that case,</span>
  <span class="c1"># just return SingleDeviceShardings since we know the computation is running</span>
  <span class="c1"># only on 1 device.</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">([</span><span class="n">SingleDeviceSharding</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_in_avals</span><span class="p">)],</span>
            <span class="p">[</span><span class="n">SingleDeviceSharding</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_out_avals</span><span class="p">)])</span>

  <span class="n">in_op_shardings</span><span class="p">,</span> <span class="n">out_op_shardings</span> <span class="o">=</span> <span class="n">pjit</span><span class="o">.</span><span class="n">_get_op_sharding_from_executable</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">)</span>

  <span class="n">in_shardings_xla</span> <span class="o">=</span> <span class="p">[</span><span class="n">OpShardingSharding</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">in_op_shardings</span><span class="p">]</span>
  <span class="n">out_shardings_xla</span> <span class="o">=</span> <span class="p">[</span><span class="n">OpShardingSharding</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_op_shardings</span><span class="p">]</span>
  <span class="c1"># This condition happens when all the elements in the output tuple have the</span>
  <span class="c1"># same sharding, so XLA decides to run the `FusionTupleDeduplicator` to</span>
  <span class="c1"># put the sharding on ROOT instead of the tuple.</span>
  <span class="c1"># TODO(b/245667823): Remove this when XLA fixes this.</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_shardings_xla</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_shardings_xla</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_out_avals</span><span class="p">:</span>
    <span class="n">out_shardings_xla</span> <span class="o">=</span> <span class="n">out_shardings_xla</span> <span class="o">*</span> <span class="n">num_out_avals</span>
  <span class="k">return</span> <span class="n">in_shardings_xla</span><span class="p">,</span> <span class="n">out_shardings_xla</span>



<span class="c1"># TODO(yashkatariya): Remove this function after `AUTO` can return shardings</span>
<span class="c1"># without mesh.</span>
<span class="k">def</span> <span class="nf">_get_mesh_pspec_shardings_from_executable</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">pjit</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">MeshPspecSharding</span>

  <span class="n">in_pspec</span><span class="p">,</span> <span class="n">out_pspec</span> <span class="o">=</span> <span class="n">pjit</span><span class="o">.</span><span class="n">_get_pspec_from_executable</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">([</span><span class="n">MeshPspecSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">in_pspec</span><span class="p">],</span>
          <span class="p">[</span><span class="n">MeshPspecSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_pspec</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">MeshExecutable</span><span class="p">(</span><span class="n">stages</span><span class="o">.</span><span class="n">XlaExecutable</span><span class="p">):</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;xla_executable&#39;</span><span class="p">,</span> <span class="s1">&#39;unsafe_call&#39;</span><span class="p">,</span> <span class="s1">&#39;in_avals&#39;</span><span class="p">,</span>
               <span class="s1">&#39;_in_shardings&#39;</span><span class="p">,</span> <span class="s1">&#39;_out_shardings&#39;</span><span class="p">,</span> <span class="s1">&#39;_auto_spmd_lowering&#39;</span><span class="p">,</span>
               <span class="s1">&#39;_kept_var_idx&#39;</span><span class="p">]</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xla_executable</span><span class="p">,</span> <span class="n">unsafe_call</span><span class="p">,</span> <span class="n">in_avals</span><span class="p">,</span>
               <span class="n">in_shardings</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">auto_spmd_lowering</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span> <span class="o">=</span> <span class="n">xla_executable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">unsafe_call</span>
    <span class="c1"># in_avals is a list of global and local avals. Aval is global if input</span>
    <span class="c1"># is a GDA or jax.Array else local.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_avals</span> <span class="o">=</span> <span class="n">in_avals</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_shardings</span> <span class="o">=</span> <span class="n">in_shardings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_out_shardings</span> <span class="o">=</span> <span class="n">out_shardings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_auto_spmd_lowering</span> <span class="o">=</span> <span class="n">auto_spmd_lowering</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kept_var_idx</span> <span class="o">=</span> <span class="n">kept_var_idx</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_hlo</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
               <span class="n">computation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ir</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">xc</span><span class="o">.</span><span class="n">XlaComputation</span><span class="p">],</span>
               <span class="c1"># TODO(yashkatariya): Remove `mesh` from here once AUTO can work</span>
               <span class="c1"># without mesh.</span>
               <span class="n">mesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mesh</span><span class="p">],</span>
               <span class="n">global_in_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">],</span>
               <span class="n">global_out_avals</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ShapedArray</span><span class="p">],</span>
               <span class="n">in_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">]],</span>
               <span class="n">out_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">,</span> <span class="n">_AUTOAxisResource</span><span class="p">,</span>
                                       <span class="n">_UnspecifiedValue</span><span class="p">]],</span>
               <span class="n">spmd_lowering</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">tuple_args</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">in_is_global</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
               <span class="n">auto_spmd_lowering</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">_allow_propagation_to_outputs</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">_allow_compile_replicated</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">unordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span>
               <span class="n">ordered_effects</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Effect</span><span class="p">],</span>
               <span class="n">host_callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
               <span class="n">keepalive</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
               <span class="n">kept_var_idx</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">backend</span><span class="p">:</span> <span class="n">xb</span><span class="o">.</span><span class="n">XlaBackend</span><span class="p">,</span>
               <span class="n">device_assignment</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span>
               <span class="n">committed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">pmap_nreps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MeshExecutable</span><span class="p">:</span>
    <span class="n">dev</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="k">if</span> <span class="n">auto_spmd_lowering</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">spmd_lowering</span>
      <span class="n">dev</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">devices</span>
      <span class="n">num_replicas</span><span class="p">,</span> <span class="n">num_partitions</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mesh</span><span class="o">.</span><span class="n">size</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">device_assignment</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">pmap_nreps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">num_replicas</span><span class="p">,</span> <span class="n">num_partitions</span> <span class="o">=</span> <span class="n">pmap_nreps</span><span class="p">,</span> <span class="mi">1</span>
      <span class="k">elif</span> <span class="n">spmd_lowering</span><span class="p">:</span>
        <span class="n">num_replicas</span><span class="p">,</span> <span class="n">num_partitions</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dev</span><span class="o">.</span><span class="n">size</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">num_replicas</span><span class="p">,</span> <span class="n">num_partitions</span> <span class="o">=</span> <span class="n">dev</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">pmap_nreps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># In `jit` device_assignment is set to None when num_replicas &gt; 1. Do</span>
      <span class="c1"># the same thing here too.</span>
      <span class="n">xla_device_assignment</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">xla_device_assignment</span> <span class="o">=</span> <span class="n">dev</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_replicas</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">))</span>

    <span class="n">compile_options</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">get_compile_options</span><span class="p">(</span>
        <span class="n">num_replicas</span><span class="o">=</span><span class="n">num_replicas</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
        <span class="n">device_assignment</span><span class="o">=</span><span class="n">xla_device_assignment</span><span class="p">,</span>
        <span class="n">use_spmd_partitioning</span><span class="o">=</span><span class="n">spmd_lowering</span><span class="p">,</span>
        <span class="n">use_auto_spmd_partitioning</span><span class="o">=</span><span class="n">auto_spmd_lowering</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">auto_spmd_lowering</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">compile_options</span><span class="o">.</span><span class="n">executable_build_options</span><span class="o">.</span><span class="n">auto_spmd_partitioning_mesh_shape</span> <span class="o">=</span> \
          <span class="nb">list</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
      <span class="n">compile_options</span><span class="o">.</span><span class="n">executable_build_options</span><span class="o">.</span><span class="n">auto_spmd_partitioning_mesh_ids</span> <span class="o">=</span> \
          <span class="n">_get_logical_mesh_ids</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">compile_options</span><span class="o">.</span><span class="n">parameter_is_tupled_arguments</span> <span class="o">=</span> <span class="n">tuple_args</span>
    <span class="n">compile_options</span><span class="o">.</span><span class="n">executable_build_options</span><span class="o">.</span><span class="n">allow_spmd_sharding_propagation_to_output</span> <span class="o">=</span> \
        <span class="n">_allow_propagation_to_outputs</span>

    <span class="k">if</span> <span class="n">_allow_compile_replicated</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="s2">&quot;compile_replicated&quot;</span><span class="p">):</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">auto_spmd_lowering</span>
      <span class="n">in_shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">input_avals</span> <span class="o">=</span> <span class="n">_get_input_metadata</span><span class="p">(</span>
          <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">in_is_global</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
      <span class="n">are_out_shardings_from_xla</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>
      <span class="n">handle_outs</span> <span class="o">=</span> <span class="n">global_avals_to_results_handler</span><span class="p">(</span>
          <span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">committed</span><span class="p">,</span> <span class="n">are_out_shardings_from_xla</span><span class="p">)</span>  <span class="c1"># type: ignore  # arg-type</span>
      <span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">compile_replicated</span><span class="p">(</span><span class="n">computation</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span>
                                               <span class="n">host_callbacks</span><span class="p">,</span> <span class="n">input_avals</span><span class="p">,</span>
                                               <span class="n">input_indices</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span>
                                               <span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pjit_or_xmap</span><span class="p">,</span>
                                               <span class="n">handle_outs</span><span class="p">)</span>
      <span class="n">xla_executable</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">log_elapsed_time</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished XLA compilation of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> &quot;</span>
                                     <span class="s2">&quot;in </span><span class="si">{elapsed_time}</span><span class="s2"> sec&quot;</span><span class="p">):</span>
        <span class="n">xla_executable</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">compile_or_get_cached</span><span class="p">(</span>
            <span class="n">backend</span><span class="p">,</span> <span class="n">computation</span><span class="p">,</span> <span class="n">compile_options</span><span class="p">,</span> <span class="n">host_callbacks</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">auto_spmd_lowering</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">in_shardings_xla</span><span class="p">,</span> <span class="n">out_shardings_xla</span> <span class="o">=</span> <span class="n">_get_mesh_pspec_shardings_from_executable</span><span class="p">(</span>
            <span class="n">xla_executable</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
        <span class="n">in_shardings</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">_is_auto</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">else</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">in_shardings_xla</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">)]</span>
        <span class="n">out_shardings_tuple</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">_is_auto</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">out_shardings_xla</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">out_shardings</span><span class="p">,</span> <span class="n">are_out_shardings_from_xla</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_shardings_tuple</span><span class="p">)</span>
      <span class="k">elif</span> <span class="n">out_shardings</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">_is_unspecified</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_shardings</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">mesh</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">out_shardings_xla</span> <span class="o">=</span> <span class="n">_get_op_sharding_shardings_from_executable</span><span class="p">(</span>
            <span class="n">xla_executable</span><span class="p">,</span> <span class="n">device_assignment</span><span class="p">,</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">global_in_avals</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">))</span>
        <span class="n">out_shardings_tuple</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">_is_unspecified</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">out_shardings_xla</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">out_shardings</span><span class="p">,</span> <span class="n">are_out_shardings_from_xla</span> <span class="o">=</span> <span class="n">unzip2</span><span class="p">(</span><span class="n">out_shardings_tuple</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">are_out_shardings_from_xla</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>

      <span class="n">in_shardings</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">input_avals</span> <span class="o">=</span> <span class="n">_get_input_metadata</span><span class="p">(</span>
          <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">in_is_global</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
      <span class="n">handle_args</span> <span class="o">=</span> <span class="n">InputsHandler</span><span class="p">(</span><span class="n">xla_executable</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(),</span> <span class="n">in_shardings</span><span class="p">,</span>
                                  <span class="n">input_indices</span><span class="p">,</span> <span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pjit_or_xmap</span><span class="p">)</span>
      <span class="n">handle_outs</span> <span class="o">=</span> <span class="n">global_avals_to_results_handler</span><span class="p">(</span>
          <span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">committed</span><span class="p">,</span> <span class="n">are_out_shardings_from_xla</span><span class="p">)</span>  <span class="c1"># type: ignore  # arg-type</span>

      <span class="c1"># This path is taken for `jit(pmap)` cases. Nothing else should flow</span>
      <span class="c1"># through this path. This is exactly same to what happens in `jit`.</span>
      <span class="k">if</span> <span class="n">pmap_nreps</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">has_unordered_effects</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">unordered_effects</span><span class="p">)</span>
        <span class="n">buffer_counts</span> <span class="o">=</span> <span class="n">dispatch</span><span class="o">.</span><span class="n">get_buffer_counts</span><span class="p">(</span>
            <span class="n">global_out_avals</span><span class="p">,</span> <span class="n">ordered_effects</span><span class="p">,</span> <span class="n">has_unordered_effects</span><span class="p">)</span>
        <span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">dispatch</span><span class="o">.</span><span class="n">_execute_replicated</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">xla_executable</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">buffer_counts</span><span class="p">,</span> <span class="n">handle_outs</span><span class="p">,</span> <span class="n">has_unordered_effects</span><span class="p">,</span> <span class="n">ordered_effects</span><span class="p">,</span>
            <span class="n">kept_var_idx</span><span class="p">,</span> <span class="nb">bool</span><span class="p">(</span><span class="n">host_callbacks</span><span class="p">),</span>
            <span class="n">from_lower_sharding_computation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">ExecuteReplicated</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">handle_args</span><span class="p">,</span>
                                        <span class="n">handle_outs</span><span class="p">,</span> <span class="n">unordered_effects</span><span class="p">,</span>
                                        <span class="n">ordered_effects</span><span class="p">,</span> <span class="n">keepalive</span><span class="p">,</span>
                                        <span class="nb">bool</span><span class="p">(</span><span class="n">host_callbacks</span><span class="p">),</span> <span class="n">kept_var_idx</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">MeshExecutable</span><span class="p">(</span><span class="n">xla_executable</span><span class="p">,</span> <span class="n">unsafe_call</span><span class="p">,</span> <span class="n">input_avals</span><span class="p">,</span>
                          <span class="n">in_shardings</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">auto_spmd_lowering</span><span class="p">,</span>
                          <span class="n">kept_var_idx</span><span class="p">)</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_trivial_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">global_out_avals</span><span class="p">,</span>
                         <span class="n">in_shardings</span><span class="p">,</span> <span class="n">device_assignment</span><span class="p">,</span>
                         <span class="n">committed</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">,</span> <span class="n">keepalive</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MeshExecutable</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">keepalive</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="n">out_shardings</span> <span class="o">=</span> <span class="n">_out_shardings_for_trivial</span><span class="p">(</span>
        <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span> <span class="n">device_assignment</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_array</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">jax_parallel_functions_output_gda</span><span class="p">:</span>
      <span class="n">are_global</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">are_global</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_get_input_metadata</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span>
                                        <span class="n">are_global</span><span class="p">)</span>
    <span class="n">process_index</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span>
    <span class="n">local_device_assignment</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_assignment</span>
                               <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">process_index</span> <span class="o">==</span> <span class="n">process_index</span><span class="p">]</span>
    <span class="n">handle_ins</span> <span class="o">=</span> <span class="n">InputsHandler</span><span class="p">(</span><span class="n">local_device_assignment</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span>
                               <span class="n">InputsHandlerMode</span><span class="o">.</span><span class="n">pjit_or_xmap</span><span class="p">)</span>
    <span class="n">handle_outs</span> <span class="o">=</span> <span class="n">global_avals_to_results_handler</span><span class="p">(</span>
          <span class="n">global_out_avals</span><span class="p">,</span> <span class="n">out_shardings</span><span class="p">,</span> <span class="n">committed</span><span class="p">,</span>
          <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_out_avals</span><span class="p">))</span>
    <span class="n">unsafe_call</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">_execute_trivial</span><span class="p">,</span> <span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">handle_ins</span><span class="p">,</span>
                          <span class="n">handle_outs</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MeshExecutable</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">unsafe_call</span><span class="p">,</span> <span class="n">global_in_avals</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">,</span>
                          <span class="n">out_shardings</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">)</span>

  <span class="c1"># -- stages.XlaExecutable overrides</span>

  <span class="k">def</span> <span class="nf">xla_extension_executable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">xla_executable</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">kept_args</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kept_var_idx</span><span class="p">]</span>
    <span class="n">arg_avals</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">xla</span><span class="o">.</span><span class="n">abstractify</span><span class="p">,</span> <span class="n">kept_args</span><span class="p">)</span>
    <span class="n">ref_avals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_avals</span>
    <span class="n">dispatch</span><span class="o">.</span><span class="n">check_arg_avals_for_call</span><span class="p">(</span><span class="n">ref_avals</span><span class="p">,</span> <span class="n">arg_avals</span><span class="p">)</span>
    <span class="c1"># Check the GDA sharding and the input sharding.</span>
    <span class="n">_check_gda_or_array_xla_sharding_match</span><span class="p">(</span><span class="n">kept_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_shardings</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsafe_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">input_shardings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_shardings</span>

  <span class="k">def</span> <span class="nf">output_shardings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_shardings</span>


<span class="k">def</span> <span class="nf">_out_shardings_for_trivial</span><span class="p">(</span>
    <span class="n">jaxpr</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">Jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">in_shardings</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">],</span>
    <span class="n">device_assignment</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xc</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">XLACompatibleSharding</span><span class="p">]:</span>
  <span class="c1"># For each jaxpr output, compute a Sharding by:</span>
  <span class="c1">#   * if the output is a forwarded input, get the corresponding in_sharding;</span>
  <span class="c1">#   * if the output is a constant Array, get its .sharding attribute;</span>
  <span class="c1">#   * otherwise, the output is a literal or numpy.ndarray constant, so give it</span>
  <span class="c1">#     a replicated sharding</span>
  <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">array</span>
  <span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">sharding</span>
  <span class="n">rep</span> <span class="o">=</span> <span class="n">sharding</span><span class="o">.</span><span class="n">OpShardingSharding</span><span class="p">(</span>
      <span class="n">device_assignment</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">_get_replicated_op_sharding</span><span class="p">())</span>
  <span class="n">shardings</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">XLACompatibleSharding</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">constvar</span><span class="p">,</span> <span class="n">constval</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jaxpr</span><span class="o">.</span><span class="n">constvars</span><span class="p">,</span> <span class="n">consts</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constval</span><span class="p">,</span> <span class="n">array</span><span class="o">.</span><span class="n">ArrayImpl</span><span class="p">):</span>
      <span class="n">shardings</span><span class="p">[</span><span class="n">constvar</span><span class="p">]</span> <span class="o">=</span> <span class="n">constval</span><span class="o">.</span><span class="n">sharding</span>
  <span class="nb">map</span><span class="p">(</span><span class="n">shardings</span><span class="o">.</span><span class="n">setdefault</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">invars</span><span class="p">,</span> <span class="n">in_shardings</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">rep</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Literal</span><span class="p">)</span> <span class="k">else</span> <span class="n">shardings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rep</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">outvars</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_execute_trivial</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">consts</span><span class="p">,</span> <span class="n">in_handler</span><span class="p">,</span> <span class="n">out_handler</span><span class="p">,</span> <span class="n">kept_var_idx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
  <span class="n">env</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>  <span class="o">=</span> <span class="p">{}</span>
  <span class="n">pruned_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kept_var_idx</span><span class="p">)</span>
  <span class="nb">map</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">setdefault</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">invars</span><span class="p">,</span> <span class="n">pruned_args</span><span class="p">)</span>
  <span class="nb">map</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">setdefault</span><span class="p">,</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">constvars</span><span class="p">,</span> <span class="n">consts</span><span class="p">)</span>
  <span class="n">outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">xla</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">val</span><span class="p">)</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">Literal</span> <span class="k">else</span> <span class="n">env</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>
          <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">outvars</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">out_handler</span><span class="p">(</span><span class="n">in_handler</span><span class="p">(</span><span class="n">outs</span><span class="p">))</span>


<span class="nd">@lru_cache</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_create_mesh_pspec_sharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">pspec</span><span class="p">,</span> <span class="n">parsed_pspec</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax._src.sharding</span> <span class="kn">import</span> <span class="n">MeshPspecSharding</span>
  <span class="k">return</span> <span class="n">MeshPspecSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">pspec</span><span class="p">,</span> <span class="n">parsed_pspec</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_gda_or_array_xla_sharding_match</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">in_xla_shardings</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">jax.experimental.global_device_array</span> <span class="kn">import</span> <span class="n">GlobalDeviceArray</span>
  <span class="kn">from</span> <span class="nn">jax._src.array</span> <span class="kn">import</span> <span class="n">ArrayImpl</span>

  <span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">4096</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_cached_check</span><span class="p">(</span><span class="n">arg_sharding</span><span class="p">,</span> <span class="n">in_xla_sharding</span><span class="p">,</span> <span class="n">arg_type</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">committed</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">committed</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">are_op_shardings_equal</span><span class="p">(</span>
        <span class="n">arg_sharding</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">ndim</span><span class="p">),</span>
        <span class="n">in_xla_sharding</span><span class="o">.</span><span class="n">_to_xla_op_sharding</span><span class="p">(</span><span class="n">ndim</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arg_type</span><span class="si">}</span><span class="s2"> sharding does not match the input sharding. &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">arg_type</span><span class="si">}</span><span class="s2"> sharding: </span><span class="si">{</span><span class="n">arg_sharding</span><span class="si">}</span><span class="s2"> and &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;xla sharding: </span><span class="si">{</span><span class="n">in_xla_sharding</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">arg</span><span class="p">,</span> <span class="n">xs</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">in_xla_shardings</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="p">(</span><span class="n">GlobalDeviceArray</span><span class="p">,</span> <span class="n">ArrayImpl</span><span class="p">)):</span>
      <span class="k">continue</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">GlobalDeviceArray</span><span class="p">):</span>
      <span class="n">_cached_check</span><span class="p">(</span><span class="n">_create_mesh_pspec_sharding</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">mesh_axes</span><span class="p">),</span> <span class="n">xs</span><span class="p">,</span>
                    <span class="s1">&#39;GDA&#39;</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">_cached_check</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">sharding</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="s1">&#39;Array&#39;</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">arg</span><span class="o">.</span><span class="n">_committed</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_array_mapping</span><span class="p">(</span><span class="n">pspec</span><span class="p">:</span> <span class="n">PartitionSpec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ArrayMappingOrAutoOrUnspecified</span><span class="p">:</span>
  <span class="c1"># Import here to avoid cyclic import error when importing gda in pjit.py.</span>
  <span class="kn">from</span> <span class="nn">jax.experimental.pjit</span> <span class="kn">import</span> <span class="n">get_array_mapping</span><span class="p">,</span> <span class="n">_prepare_axis_resources</span>

  <span class="n">parsed_pspec</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_prepare_axis_resources</span><span class="p">(</span><span class="n">pspec</span><span class="p">,</span> <span class="s2">&quot;pspec to array_mapping&quot;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">get_array_mapping</span><span class="p">(</span><span class="n">parsed_pspec</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">are_op_shardings_equal</span><span class="p">(</span><span class="n">op1</span><span class="p">,</span> <span class="n">op2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">op1</span><span class="p">)</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">op2</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>
  <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&gt;=</span> <span class="mi">81</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">is_op_sharding_replicated</span><span class="p">(</span><span class="n">op1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_op_sharding_replicated</span><span class="p">(</span><span class="n">op2</span><span class="p">):</span>
      <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">xc</span><span class="o">.</span><span class="n">HloSharding</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">op1</span><span class="p">)</span> <span class="o">==</span> <span class="n">xc</span><span class="o">.</span><span class="n">HloSharding</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">op2</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">op1</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">TUPLE</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">are_op_shardings_equal</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
                 <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">op1</span><span class="o">.</span><span class="n">tuple_shardings</span><span class="p">,</span> <span class="n">op2</span><span class="o">.</span><span class="n">tuple_shardings</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">op1</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">op2</span><span class="o">.</span><span class="n">type</span> <span class="ow">and</span>
            <span class="n">op1</span><span class="o">.</span><span class="n">tile_assignment_dimensions</span> <span class="o">==</span> <span class="n">op2</span><span class="o">.</span><span class="n">tile_assignment_dimensions</span> <span class="ow">and</span>
            <span class="n">op1</span><span class="o">.</span><span class="n">tile_assignment_devices</span> <span class="o">==</span> <span class="n">op2</span><span class="o">.</span><span class="n">tile_assignment_devices</span> <span class="ow">and</span>
            <span class="n">op1</span><span class="o">.</span><span class="n">last_tile_dims</span> <span class="o">==</span> <span class="n">op2</span><span class="o">.</span><span class="n">last_tile_dims</span> <span class="ow">and</span>
            <span class="n">op1</span><span class="o">.</span><span class="n">replicate_on_last_tile_dim</span> <span class="o">==</span> <span class="n">op2</span><span class="o">.</span><span class="n">replicate_on_last_tile_dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_op_sharding_replicated</span><span class="p">(</span><span class="n">op</span><span class="p">:</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">xla_extension_version</span> <span class="o">&gt;=</span> <span class="mi">82</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">tile_assignment_devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">xc</span><span class="o">.</span><span class="n">HloSharding</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">op</span><span class="p">)</span><span class="o">.</span><span class="n">is_replicated</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">xc</span><span class="o">.</span><span class="n">OpSharding</span><span class="o">.</span><span class="n">Type</span><span class="o">.</span><span class="n">REPLICATED</span>


<span class="n">_forbidden_primitives</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s1">&#39;xla_pmap&#39;</span><span class="p">:</span> <span class="s1">&#39;pmap&#39;</span><span class="p">,</span>
  <span class="s1">&#39;sharded_call&#39;</span><span class="p">:</span> <span class="s1">&#39;sharded_jit&#39;</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">def</span> <span class="nf">_sanitize_mesh_jaxpr</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">):</span>
    <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">jaxpr</span>
  <span class="k">for</span> <span class="n">eqn</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">_forbidden_primitives</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nesting </span><span class="si">{</span><span class="n">_forbidden_primitives</span><span class="p">[</span><span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;inside xmaps not supported!&quot;</span><span class="p">)</span>
    <span class="n">core</span><span class="o">.</span><span class="n">traverse_jaxpr_params</span><span class="p">(</span><span class="n">_sanitize_mesh_jaxpr</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>


<span class="n">custom_resource_typing_rules</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">core</span><span class="o">.</span><span class="n">Primitive</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">resource_typecheck</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">resource_env</span><span class="p">,</span> <span class="n">axis_resources</span><span class="p">,</span> <span class="n">what_jaxpr_thunk</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">jaxpr</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">ClosedJaxpr</span><span class="p">):</span>
    <span class="n">jaxpr</span> <span class="o">=</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">jaxpr</span>
  <span class="k">def</span> <span class="nf">_check_aval</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">what_thunk</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="s1">&#39;named_shape&#39;</span><span class="p">):</span>
      <span class="k">return</span>
    <span class="n">resource_to_axis</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="n">aval</span><span class="o">.</span><span class="n">named_shape</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">axis_resources</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">resource_to_axis</span><span class="p">:</span>
          <span class="n">other_axis</span> <span class="o">=</span> <span class="n">resource_to_axis</span><span class="p">[</span><span class="n">resource</span><span class="p">]</span>
          <span class="n">axis</span><span class="p">,</span> <span class="n">other_axis</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">other_axis</span><span class="p">)])</span>
          <span class="k">raise</span> <span class="n">JAXTypeError</span><span class="p">(</span>
              <span class="sa">f</span><span class="s2">&quot;Axes `</span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">` and `</span><span class="si">{</span><span class="n">other_axis</span><span class="si">}</span><span class="s2">` are both mapped to the &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;resource `</span><span class="si">{</span><span class="n">resource</span><span class="si">}</span><span class="s2">`, but they coincide in the named_shape &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;of </span><span class="si">{</span><span class="n">what_thunk</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">resource_to_axis</span><span class="p">[</span><span class="n">resource</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span>

  <span class="n">what_thunk</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;an input to </span><span class="si">{</span><span class="n">what_jaxpr_thunk</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">constvars</span><span class="p">:</span>
    <span class="n">_check_aval</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span><span class="p">,</span> <span class="n">what_thunk</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">invars</span><span class="p">:</span>
    <span class="n">_check_aval</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span><span class="p">,</span> <span class="n">what_thunk</span><span class="p">)</span>
  <span class="n">what_thunk</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a value returned from a primitive </span><span class="si">{</span><span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="si">}</span><span class="s2"> created &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;at </span><span class="si">{</span><span class="n">source_info_util</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">source_info</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="n">rec_what_jaxpr_thunk</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a primitive </span><span class="si">{</span><span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="si">}</span><span class="s2"> created at&quot;</span>
                                  <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">source_info_util</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">source_info</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">eqn</span> <span class="ow">in</span> <span class="n">jaxpr</span><span class="o">.</span><span class="n">eqns</span><span class="p">:</span>
    <span class="n">typing_rule</span> <span class="o">=</span> <span class="n">custom_resource_typing_rules</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">eqn</span><span class="o">.</span><span class="n">primitive</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">typing_rule</span><span class="p">:</span>
      <span class="n">typing_rule</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">eqn</span><span class="o">.</span><span class="n">invars</span><span class="p">],</span> <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">eqn</span><span class="o">.</span><span class="n">source_info</span><span class="p">,</span>
                  <span class="n">resource_env</span><span class="p">,</span> <span class="n">axis_resources</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">core</span><span class="o">.</span><span class="n">traverse_jaxpr_params</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">resource_typecheck</span><span class="p">,</span>
                                         <span class="n">resource_env</span><span class="o">=</span><span class="n">resource_env</span><span class="p">,</span>
                                         <span class="n">axis_resources</span><span class="o">=</span><span class="n">axis_resources</span><span class="p">,</span>
                                         <span class="n">what_jaxpr_thunk</span><span class="o">=</span><span class="n">rec_what_jaxpr_thunk</span><span class="p">),</span>
                                 <span class="n">eqn</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">eqn</span><span class="o">.</span><span class="n">outvars</span><span class="p">:</span>
      <span class="n">_check_aval</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">aval</span><span class="p">,</span> <span class="n">what_thunk</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_make_sharding_spec</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">mesh_axis_pos</span><span class="p">,</span> <span class="n">num_dimensions</span><span class="p">,</span> <span class="n">aval_axes</span><span class="p">):</span>
  <span class="n">mesh_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="n">Replicated</span><span class="p">(</span><span class="n">axis_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">axis_size</span> <span class="ow">in</span> <span class="n">axis_sizes</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
  <span class="n">sharding</span> <span class="o">=</span> <span class="p">[</span><span class="n">_UNSHARDED_INSTANCE</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_dimensions</span>
  <span class="n">next_sharded_axis</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="c1"># NOTE: sorted is stable, which is important when multiple resources</span>
  <span class="c1">#       map to the same axis.</span>
  <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">aval_axes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">chunked</span> <span class="o">=</span> <span class="n">sharding</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunked</span><span class="p">,</span> <span class="n">NoSharding</span><span class="p">):</span>
      <span class="n">chunked</span> <span class="o">=</span> <span class="n">Chunked</span><span class="p">([])</span>
    <span class="n">sharding</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">Chunked</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">chunked</span><span class="o">.</span><span class="n">chunks</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]])</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mesh_mapping</span><span class="p">[</span><span class="n">mesh_axis_pos</span><span class="p">[</span><span class="n">name</span><span class="p">]],</span> <span class="n">Replicated</span><span class="p">),</span> \
        <span class="s2">&quot;Value mapped to the same mesh axis twice&quot;</span>
    <span class="n">mesh_mapping</span><span class="p">[</span><span class="n">mesh_axis_pos</span><span class="p">[</span><span class="n">name</span><span class="p">]]</span> <span class="o">=</span> <span class="n">ShardedAxis</span><span class="p">(</span><span class="n">next_sharded_axis</span><span class="p">)</span>
    <span class="n">next_sharded_axis</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">(</span><span class="n">sharding</span><span class="p">,</span> <span class="n">mesh_mapping</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">new_mesh_sharding_specs</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">axis_names</span><span class="p">):</span>
  <span class="n">mesh_axis_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis_names</span><span class="p">)}</span>
  <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_make_sharding_spec</span><span class="p">,</span> <span class="n">axis_sizes</span><span class="p">,</span> <span class="n">mesh_axis_pos</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mesh_sharding_specs</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">axis_names</span><span class="p">,</span> <span class="n">allow_uneven_axes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="n">mesh_axis_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis_names</span><span class="p">)}</span>
  <span class="c1"># NOTE: This takes in the non-sharded avals!</span>
  <span class="k">def</span> <span class="nf">mk_sharding_spec</span><span class="p">(</span><span class="n">aval</span><span class="p">,</span> <span class="n">aval_axes</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">aval</span> <span class="ow">is</span> <span class="n">core</span><span class="o">.</span><span class="n">abstract_token</span><span class="p">:</span>
      <span class="k">assert</span> <span class="ow">not</span> <span class="n">aval_axes</span>
      <span class="k">return</span> <span class="n">ShardingSpec</span><span class="p">([],</span> <span class="p">[</span><span class="n">Replicated</span><span class="p">(</span><span class="n">axis_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">axis_size</span> <span class="ow">in</span> <span class="n">axis_sizes</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
    <span class="n">aval_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># NOTE: sorted is stable, which is important when multiple resources</span>
    <span class="c1">#       map to the same axis.</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">aval_axes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_uneven_axes</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">aval_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">%</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;The aval shape on dimension </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">aval_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="si">}</span><span class="s1"> and &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;the size of axis </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1"> is </span><span class="si">{</span><span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="si">}</span><span class="s1">. The aval shape % &#39;</span>
            <span class="s1">&#39;axis size should be zero but got &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">aval_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">%</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="n">aval_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">//=</span> <span class="n">axis_sizes</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_make_sharding_spec</span><span class="p">(</span><span class="n">axis_sizes</span><span class="p">,</span> <span class="n">mesh_axis_pos</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">aval</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">aval_axes</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">mk_sharding_spec</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">maybe_extend_axis_env</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">core</span><span class="o">.</span><span class="n">extend_axis_env</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">yield</span>

<span class="k">class</span> <span class="nc">DynamicAxisEnvFrame</span><span class="p">:</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;pmap_trace&quot;</span><span class="p">,</span> <span class="s2">&quot;hard_size&quot;</span><span class="p">]</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">pmap_trace</span><span class="p">,</span> <span class="n">hard_size</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pmap_trace</span> <span class="o">=</span> <span class="n">pmap_trace</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hard_size</span> <span class="o">=</span> <span class="n">hard_size</span>

<span class="k">class</span> <span class="nc">DynamicAxisEnv</span><span class="p">(</span><span class="nb">list</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">axis_name</span> <span class="ow">in</span> <span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;unbound axis name: </span><span class="si">{</span><span class="n">axis_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">frame</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">axis_name</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">frame</span>

    <span class="k">raise</span> <span class="ne">AssertionError</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">sizes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">hard_size</span> <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">nreps</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">prod</span><span class="p">(</span><span class="n">frame</span><span class="o">.</span><span class="n">hard_size</span> <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">_ThreadLocalState</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dynamic_axis_env</span> <span class="o">=</span> <span class="n">DynamicAxisEnv</span><span class="p">()</span>

<span class="n">_thread_local_state</span> <span class="o">=</span> <span class="n">_ThreadLocalState</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Device</span><span class="p">],</span> <span class="n">replicate</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">xb</span><span class="o">.</span><span class="n">xla_client</span><span class="o">.</span><span class="n">Buffer</span><span class="p">]:</span>
  <span class="sd">&quot;&quot;&quot;Call device_put on a sequence of devices and return a flat sequence of buffers.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">replicate</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">dispatch</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">dispatch</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span><span class="p">,</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">safe_zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">devices</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">_set_aval</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">aval</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">val</span><span class="o">.</span><span class="n">aval</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">ShapedArray</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">val</span>

<span class="k">def</span> <span class="nf">_original_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="nb">property</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="nb">property</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">fget</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">maybe_cached_property</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="n">func</span>
  <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">use_cpp_class</span><span class="p">(</span><span class="n">cpp_cls</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper decorator to replace a python class with its C++ version&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">TYPE_CHECKING</span> <span class="ow">or</span> <span class="n">cpp_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">cls</span>

    <span class="n">exclude_methods</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;__module__&#39;</span><span class="p">,</span> <span class="s1">&#39;__dict__&#39;</span><span class="p">,</span> <span class="s1">&#39;__doc__&#39;</span><span class="p">}</span>

    <span class="k">for</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">attr_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">exclude_methods</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
          <span class="n">_original_func</span><span class="p">(</span><span class="n">attr</span><span class="p">),</span> <span class="s2">&quot;_use_cpp&quot;</span><span class="p">):</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">cpp_cls</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cpp_cls</span>

  <span class="k">return</span> <span class="n">wrapper</span>

<span class="k">def</span> <span class="nf">use_cpp_method</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A helper decorator to exclude methods from the set that are forwarded to C++ class&quot;&quot;&quot;</span>
  <span class="n">original_func</span> <span class="o">=</span> <span class="n">_original_func</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
  <span class="n">original_func</span><span class="o">.</span><span class="n">_use_cpp</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">return</span> <span class="n">f</span>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The JAX authors<br/>
  
      &copy; Copyright 2020, The JAX Authors. NumPy and SciPy documentation are copyright the respective authors..<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>