<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="http://ogp.me/ns/fb#" charset="utf-8"><head><meta charset="UTF-8"><title></title>
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" type="text/css" href="css/main.css">
<link rel="stylesheet" href="css/style.css">
<style>
html,body {
margin: 0px;
padding: 10px;
width: 210mm;
max-width: 210mm;
overflow-x: hidden;
}
pre {
	width: 100%;
	overflow-x: hidden;
}
</style>
    <script src="js/prefixfree.min.js"></script>
</head><body><h1>Welcome to knowledge management</h1><a name="whatissplunkknowledge"></a><div class="all-questions"><h2> <a name="whatissplunkknowledge_what_is_splunk_enterprise_knowledge.3f"><span class="mw-headline" id="What_is_Splunk_Enterprise_knowledge.3F"> What is Splunk Enterprise knowledge?</span></a></h2>

<p>Splunk Enterprise is a powerful search and analysis engine that helps you see both the details and the larger patterns in your IT data. When you use Splunk Enterprise you do more than just look at individual entries in your log files; you leverage the information they hold collectively to find out more about your IT environment.
</p><p>Splunk Enterprise automatically extracts different kinds of knowledge from your IT data--events, fields, timestamps, and so on--to help you harness that information in a better, smarter, more focused way. Some of this information is extracted at index time, as Splunk Enterprise indexes your IT data. But the bulk of this information is created at "search time," both by Splunk Enterprise and its users. Unlike databases or schema-based analytical tools that decide what information to pull out or analyze beforehand, Splunk Enterprise enables you to dynamically extract knowledge from raw data as you need it.
</p><p>As your organization uses Splunk Enterprise, additional categories of Splunk Enterprise knowledge objects are created, including event types, tags, lookups, field extractions, workflow actions, and saved searches.
</p><p>You can think of Splunk Enterprise knowledge as a multitool that you use to discover and analyze various aspects of your IT data. For example, event types enable you to quickly and easily classify and group together similar events; you can then use them to perform analytical searches on precisely-defined subgroups of events.
</p><p>The Knowledge Manager manual shows you how to maintain sets of knowledge objects for your organization through Splunk Web and configuration files, and it demonstrates ways that Splunk Enterprise knowledge can be used to solve your organization's real-world problems.
</p><p>Splunk Enterprise knowledge is grouped into five categories: 
</p>
<ul><li> <b><a href="#aboutfields" class="external text">Data interpretation: Fields and field extractions</a></b> - Fields and field extractions make up the first order of Splunk Enterprise knowledge. The fields that Splunk Enterprise automatically extracts from your IT data help bring meaning to your raw data, clarifying what can at first glance seem incomprehensible. The fields that you extract manually expand and improve upon this layer of meaning. 
</li><li> <b><a href="#abouteventtypes" class="external text">Data classification: Event types and transactions</a></b> - You use event types and transactions to group together interesting sets of similar events. Event types group together sets of events discovered through searches, while transactions are collections of conceptually-related events that span time. 
</li><li> <b><a href="#aboutlookupsandfieldactions" class="external text">Data enrichment: Lookups and workflow actions</a></b> - Lookups and workflow actions are categories of knowledge objects that extend the usefulness of your data in various ways. Field lookups enable you to add fields to your data from external data sources such as static tables (CSV files) or Python-based commands. Workflow actions enable interactions between fields in your data and other applications or web resources, such as a WHOIS lookup on a field containing an IP address.
</li><li> <b><a href="#abouttagsandaliases" class="external text">Data normalization: Tags and aliases</a></b> - Tags and aliases are used to manage and normalize sets of field information. You can use tags and aliases to group sets of related field values together, and to give extracted fields tags that reflect different aspects of their identity. For example, you can group events from set of hosts in a particular location (such as a building or city) together--just give each host the same tag. Or maybe you have two different sources using different field names to refer to same data--you can normalize your data by using aliases (by aliasing <code><font size="2">clientip</font></code> to <code><font size="2">ipaddress</font></code>, for example). 
</li><li> <b><a href="#aboutdatamodels" class="external text">Data models</a></b> - Data models are representations of one or more datasets, and they drive the Pivot tool, enabling Pivot users to quickly generate useful tables, complex visualizations, and robust reports without needing to interact with the Splunk Enterprise search language. Data models are designed by knowledge managers who fully understand the format and semantics of their indexed data. A typical data model makes use of other knowledge object types discussed in this manual, including lookups, transactions, search-time field extractions, and calculated fields.
</li></ul><p>The Knowledge Manager manual also includes chapters on:
</p>
<ul><li> <b>Search and pivot jobs</b> - Search and pivot jobs are essentially the artifacts of an individual run of a search or a pivot. They are automatically deleted within 10 minutes unless they are saved or shared with others. Knowledge managers can review and manage recently-run and saved jobs through the Jobs page. 
</li><li> <b><a href="#aboutsummaryindexing" class="external text">Summary-based report and data model acceleration</a></b> - When searches and pivots are slow to complete, knowledge managers can utilize the summary-based acceleration strategies offered by Splunk Enterprise to speed things up. This chapter discusses report acceleration (for searches), data model acceleration (for pivots) and summary indexing (for special case searches). 
</li></ul><p>At this point you may be asking the question "Why does Splunk Enterprise knowledge need to be 'managed' anyway?" For answers, see <a href="#whymanagesplunkknowledge" class="external text">"Why manage Splunk Enterprise knowledge?"</a>, the next topic in this chapter.
</p><p>Knowledge managers should have at least a basic understanding of data input setup, event processing, and indexing concepts. For more information, see <a href="#prerequisitesforknowledgemanagement" class="external text">"Prerequisites for knowledge management"</a>, the third topic in this chapter.
</p>
<h4><font size="3"><b><i> <a name="whatissplunkknowledge_make_a_pdf"><span class="mw-headline" id="Make_a_PDF">Make a PDF</span></a></i></b></font></h4>
<p>If you'd like a PDF of this manual, click the red <b>Download the Knowledge Manager Manual as PDF</b> link below the table of contents on the left side of this page. A PDF version of the manual is generated on the fly for you, and you can save it or print it out to read later.
</p>
<a name="whymanagesplunkknowledge"></a><h2> <a name="whymanagesplunkknowledge_why_manage_splunk_enterprise_knowledge.3f"><span class="mw-headline" id="Why_manage_Splunk_Enterprise_knowledge.3F"> Why manage Splunk Enterprise knowledge?</span></a></h2>
<p>If you have to maintain a fairly large number of knowledge objects across your Splunk Enterprise deployment, you know that management of that knowledge is important. This is especially true of organizations that have a large number of Splunk Enterprise users, and even more so if you have several teams of users working with Splunk Enterprise. This is simply because a greater proliferation of users leads to a greater proliferation of additional Splunk Enterprise knowledge.
</p><p>When you leave a situation like this unchecked, your users may find themselves sorting through large sets of objects with misleading or conflicting names, struggling to find and use objects that have unevenly applied app assignments and permissions, and wasting precious time creating objects such as reports and field extractions that already exist elsewhere in the system.
</p><p>Splunk Enterprise managers provide centralized oversight of Splunk Enterprise knowledge. The benefits that knowledge managers can provide include:
</p>
<ul><li> <b>Oversight of knowledge object creation and usage across teams, departments, and deployments.</b> If you have a large Splunk Enterprise deployment spread across several teams of users, you'll eventually find teams "reinventing the wheel" by designing objects that were already developed by other teams. Knowledge managers can mitigate these situations by monitoring object creation and ensuring that useful "general purpose" objects are shared on a global basis across deployments.
</li></ul><dl><dd>For more information, see <a href="#monitorandorganizeknowledgeobjects" class="external text">"Monitor and organize knowledge objects"</a> in this manual.
</dd></dl><ul><li> <b>Normalization of event data.</b> To put it plainly: knowledge objects proliferate. Although Splunk Enterprise is based on data indexes, not databases, the basic principles of normalization still apply. It's easy for any robust, well-used Splunk Enterprise implementation to end up with a dozen tags that all have been to the same field, but as these redundant knowledge objects stack up, the end result is confusion and inefficiency on the part of its users. We'll provide you with some tips about normalizing your knowledge object libraries by applying uniform naming standards and using the Splunk Enterprise Common Information Model.
</li></ul><dl><dd>For more information, see <a href="#developnamingconventionsforknowledgeobjecttitles" class="external text">"Develop naming conventions for knowledge objects"</a> in this manual.
</dd></dl><ul><li> <b>Management of knowledge objects through configuration files.</b> True knowledge management experts know how and when to leverage the power of configuration files when it comes to the administration of Splunk Enterprise knowledge. There are certain aspects of knowledge object setup that are best handled through configuration files. This manual will show you how to work with knowledge objects this way.
</li></ul><dl><dd>See <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time field extractions through index files"</a> in this manual as an example of how you can manage Splunk Enterprise knowledge through configuration files.
</dd></dl><ul><li> <b>Creation of data models for Pivot users.</b> Splunk Enterprise offers the Pivot tool for users who want to quickly create tables, charts, and dashboards without having to write search strings that can sometimes be long and complicated. The Pivot tool is driven by <b>data models</b>--without a data model Pivot has nothing to report on. Data models are designed by Splunk Enterprise knowledge managers: people who understand the format and semantics of their indexed data, and who are familiar with the Splunk Enterprise search language. 
</li></ul><dl><dd>See <a href="#aboutdatamodels" class="external text">"About data models"</a> in this manual for a conceptual overview of data model architecture and usage.
</dd></dl><ul><li> <b>Manage setup and usage of summary-based search and pivot acceleration tools.</b> Large volumes of data can result in slow performance for Splunk Enterprise, whether you're launching a search, running a report, or trying to use Pivot. To speed things up the knowledge manager can make use of <b>report acceleration</b>, data model acceleration, and <b>summary indexing</b> to help ensure that the teams in your deployment can get results quickly and efficiently. This manual shows you how to provide centralized oversight of these acceleration strategies so you can ensure that they are being used responsibly and effectively.
</li></ul><dl><dd>For more information, see <a href="#aboutsummaryindexing" class="external text">"Overview of summary-based search and pivot acceleration"</a> in this manual.
</dd></dl><a name="prerequisitesforknowledgemanagement"></a><h2> <a name="prerequisitesforknowledgemanagement_prerequisites_for_knowledge_management"><span class="mw-headline" id="Prerequisites_for_knowledge_management"> Prerequisites for knowledge management</span></a></h2>
<p>Most knowledge management tasks are centered around "search time" event manipulation. In other words, a typical knowledge manager usually doesn't focus their attention on work that takes place before events are indexed, such as setting up data inputs, adjusting event processing activities, correcting default field extraction issues, creating and maintaining indexes, setting up forwarding and receiving, and so on. 
</p><p>However, we do recommend that all knowledge managers have a good understanding of these "Splunk Enterprise admin" concepts. A solid grounding in these subjects enables knowledge managers to better plan out their approach towards management of knowledge objects for their deployment...and it helps them troubleshoot issues that will inevitably come up over time. 
</p><p>Here are some of the "admin" topics that knowledge managers should be familiar with, with links to get you started:
</p>
<ul><li> <b>Working with Splunk apps:</b> If your deployment uses more than one Splunk Enterprise app, you should get some background on how they're organized and how app object management works within multi-app deployments. See "What's an app?", "App architecture and object ownership", and "Manage app objects" in the Admin manual.
</li></ul><ul><li> <b>Configuration file management:</b> Where are the configuration files? How are they organized? How do configuration files take precedence over each other? See "About configuration files" and "Configuration file precedence" in the Admin manual.
</li></ul><ul><li> <b>Indexing with Splunk:</b> What is an index and how does it work? What is the difference between "index time" and "search time" and why is this distinction significant? Start with "About indexes and indexers" in the Managing Indexers and Clusters manual and read the rest of the chapter. Pay special attention to "Index time vs search time".
</li></ul><ul><li> <b>Getting event data into Splunk:</b> It's important to have at least a baseline understanding of Splunk Enterprise data inputs. Check out "What Splunk can index" and read the other topics in the Getting Data In manual as necessary. 
</li></ul><ul><li> <b>Understand your forwarding and receiving setup:</b> If your Splunk Enterprise deployment utilizes forwarders and receivers, it's a good idea to get a handle on how they've been implemented, as this can affect your knowledge management strategy. Get an overview of the subject at "About forwarding and receiving" in the Forwarding Data manual. 
</li></ul><ul><li> <b>Understand event processing:</b> It's a good idea to get a good grounding in the steps that Splunk Enterprise goes through to "parse" data before it indexes it. This knowledge can help you troubleshoot problems with your event data and recognize "index time" event processing issues. Start with "Overview of event processing" in the Getting Data In manual and read the entire chapter.
</li></ul><ul><li> <b>Default field extraction:</b> Most field extraction takes place at search time, with the exception of certain default fields, which get extracted at index-time. As a knowledge manager, most of the time you'll concern yourself with search-time field extraction, but it's a good idea to know how default field extraction can be managed when it's absolutely necessary to do so. This can help you troubleshoot issues with the <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, and <code><font size="2">sourcetype</font></code> fields that Splunk Enterprise applies to each event. Start with "About default fields" in the Getting Data In manual.
</li></ul><ul><li> <b>Managing users and roles:</b> Knowledge managers typically <i>do not</i> directly set up users and roles. However, it's a good idea to understand how they're set up within your deployment, as this directly affects your efforts to share and promote knowledge objects between groups of users. For more information, start with "About users and roles" in the Admin manual, and read the rest of the chapter as necessary.
</li></ul><h1>Organize and administrate knowledge objects</h1><a name="curatesplunkknowledgewithmanager"></a><h2> <a name="curatesplunkknowledgewithmanager_manage_knowledge_objects_through_settings_pages"><span class="mw-headline" id="Manage_knowledge_objects_through_Settings_pages"> Manage knowledge objects through Settings pages</span></a></h2>
<p>As your organization uses Splunk Enterprise, people add <b>knowledge</b> to the base set of <b>event data</b> indexed within it. You and your colleagues might:
</p>
<ul><li> Save and schedule <b>searches</b>.
</li><li> Add <b>tags</b> to fields.
</li><li> Define <b>event types</b> and <b>transactions</b> that group together sets of events.
</li><li> Create <b>lookups</b> and <b>workflow actions</b>. 
</li></ul><p>The process of creating <b>knowledge objects</b> starts  slowly, but it can become complicated as people use Splunk Enterprise for longer periods. It is easy to reach a point where users are creating searches that already exist, adding unnecessary tags, designing redundant event types, and so on. These issues may not be significant if your user base is small. But if they accumulate over time, they can cause unnecessary confusion and repetition of effort.
</p><p>This chapter discusses how knowledge managers can use the <b>Knowledge</b> pages in <b>Settings</b> to control the knowledge objects in their Splunk Enterprise implementation. Settings can give an attentive knowledge manager insight into what knowledge objects people are creating, who is creating them, and (to some degree) how people are using them. 
</p><p>With Settings, you can easily:
</p>
<ul><li> Create knowledge objects when you need to, either "from scratch" or through object cloning.
</li><li> Review knowledge objects as others create them, in order to reduce redundancy and ensure that people are following naming standards.
</li><li> Delete unwanted or poorly-defined knowledge objects before they develop downstream dependencies. 
</li><li> Ensure that knowledge objects worth sharing beyond a particular working group, role, or app are made available to other groups, roles, and users of other apps. 
</li></ul><p><b>Note:</b> This chapter assumes that as a knowledge manager you have an <i>admin</i> role or a role with an equivalent permission set.
</p><p>This chapter contains topics that will explain how to:
</p>
<ul><li> <a href="#monitorandorganizeknowledgeobjects" class="external text">Keep your knowledge object collections normalized and orderly</a>.
</li><li> <a href="#developnamingconventionsforknowledgeobjecttitles" class="external text">Develop naming conventions for your knowledge objects</a> that will make them easier to understand and use.
</li><li> <a href="#understandandusethecommoninformationmodel" class="external text">Use the Common Information Model Add-on to normalize your event data.</a>  
</li><li> <a href="#manageknowledgeobjectpermissions" class="external text">Manage your knowledge object permissions.</a> Make a knowledge object available to users of a specific app, users with a specific role, or users of all apps ("global" permissions).
</li><li> <a href="#disableordeleteknowledgeobjects" class="external text">Disable or delete knowledge objects.</a> Understand the restrictions on deleting knowledge objects, and know the risks of deleting knowledge objects that have downstream dependencies.
</li></ul><h3> <a name="curatesplunkknowledgewithmanager_managing_knowledge_using_configuration_files_instead_of_settings"><span class="mw-headline" id="Managing_knowledge_using_configuration_files_instead_of_Settings">Managing knowledge using configuration files instead of Settings</span></a></h3>
<p>In previous releases, Splunk Enterprise users edited configuration files directly to add, update, or delete knowledge objects. Now they can use the <b>Knowledge</b> pages in Settings, which provide a graphical interface for updating those configuration files.
</p><p>Splunk recommends that Splunk Enterprise administrators learn how to modify configuration files. Understanding configuration files is beneficial for the following reasons: 
</p>
<ul><li> Some Splunk Web features make more sense if you understand how things work at the configuration file level. This is especially true for the <a href="#managesearch-timefieldextractions" class="external text">Field extractions</a> and <a href="#managefieldtransforms" class="external text">Field transformations</a> pages in Splunk Web. 
</li><li> Managing certain knowledge object types requires changes to configuration files. 
</li><li> Bulk deletion of obsolete, redundant, or improperly-defined knowledge objects is only possible with configuration files.
</li><li> You might find that you prefer to work directly with configuration files. For example, if you are a long-time Splunk Enterprise administrator who is already familiar with the configuration file system, you might already be familiar with managing Splunk Enterprise knowledge using configuration files. Other users rely on the level of granularity and control that configuration files can provide.
</li></ul><p>The Knowledge Manager manual includes instructions for handling various knowledge object types via configuration files. For more information, see the documentation of those types.
</p><p>For general information about configuration files in Splunk Enterprise, see the following topics in the Admin manual:
</p>
<ul><li> About configuration files
</li><li> Configuration file precedence
</li></ul><p>The Admin Manual also contains a configuration file reference, which includes <code><font size="2">.spec</font></code> and <code><font size="2">.example</font></code> files for all the configuration files in Splunk Enterprise.
</p>
<a name="monitorandorganizeknowledgeobjects"></a><h2> <a name="monitorandorganizeknowledgeobjects_monitor_and_organize_knowledge_objects"><span class="mw-headline" id="Monitor_and_organize_knowledge_objects"> Monitor and organize knowledge objects</span></a></h2>
<p>As a knowledge manager, you should periodically check up on the knowledge object collections in your Splunk Enterprise implementation. You should be on the lookout for knowledge objects that:
</p>
<ul><li> Fail to adhere to naming standards
</li><li> Are duplicates/redundant
</li><li> Are worthy of being shared with wider audiences
</li><li> Should be disabled or deleted due to obsolescence or poor design
</li></ul><p>Regular inspection of the knowledge objects in your system will help you detect anomalies that could become problems later on. 
</p><p><b>Note:</b> This topic assumes that as a knowledge manager you have an <i>admin</i> role or a role with an equivalent permission set.
</p>
<h3> <a name="monitorandorganizeknowledgeobjects_example_-_keeping_tags_straight"><span class="mw-headline" id="Example_-_Keeping_tags_straight">Example - Keeping tags straight</span></a></h3>
<p>Most healthy Splunk Enterprise implementations end up with a lot of <b>tags</b>, which are used  to perform searches on clusters of field/value pairings. Over time, however, it's easy to end up with tags that have similar names but which produce surprisingly dissimilar results. This can lead to considerable confusion and frustration. 
</p><p>Here's a procedure you can follow for curating tags. It can easily be adapted for other types of knowledge objects handled through Splunk Web.
</p><p><b>1.</b> Go to <b>Settings &gt; Tags &gt; List by tag name.</b> 
</p><p><b>2.</b> Look for tags with similar or duplicate names that belong to the same app (or which have been promoted to global availability for all users). For example, you might find a set of tags like <code><font size="2">authentication</font></code> and <code><font size="2">authentications</font></code> in the same app, where one tag is linked to an entirely different set of field/value pairs than the other. 
</p><p>Alternatively, you may encounter tags with identical names except for the use of capital letters, as in <code><font size="2">crash</font></code> and <code><font size="2">Crash</font></code>. Tags are case-sensitive, so Splunk Enterprise sees them as two separate knowledge objects.
</p><p>Keep in mind that you may find legitimate tag duplications if you have the <b>App context</b> set to <i>All</i>, where tags belonging to different apps have the same name. This is often permissible--after all, an <code><font size="2">authentication</font></code> tag for the Windows app will have to be associated with an entirely different set of field/value pairs than an <code><font size="2">authentication</font></code> for the UNIX app, for example.
</p><p><b>3.</b> Try to disable or delete the duplicate or obsolete tags you find, if your permissions enable you to do so. <b>However, be aware that there may be objects dependent on it that will be affected.</b> If the tag is used in reports, dashboard searches, other event types, or transactions, those objects will cease to function once the tag is removed or disabled. This can also happen if the object belongs to one app context, and you attempt to move it to another app context.
</p><p>For more information, see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects,"</a> in this manual.
</p><p><b>4.</b> If you create a replacement tag with a new, more unique name, ensure that it is connected to the same field/value pairs as the tag that you are replacing.
</p>
<h3> <a name="monitorandorganizeknowledgeobjects_using_naming_conventions_to_head_off_object_nomenclature_issues"><span class="mw-headline" id="Using_naming_conventions_to_head_off_object_nomenclature_issues"> Using naming conventions to head off object nomenclature issues </span></a></h3>
<p>If you set up naming conventions for your knowledge objects early in your implementation of Splunk Enterprise you can avoid some of the thornier object naming issues. For more information, see <a href="#developnamingconventionsforknowledgeobjecttitles" class="external text">"Develop naming conventions for knowledge objects"</a> in this manual.
</p>
<a name="developnamingconventionsforknowledgeobjecttitles"></a><h2> <a name="developnamingconventionsforknowledgeobjecttitles_develop_naming_conventions_for_knowledge_objects"><span class="mw-headline" id="Develop_naming_conventions_for_knowledge_objects"> Develop naming conventions for knowledge objects</span></a></h2>
<p>We suggest you develop naming conventions for your knowledge objects when it makes sense to do so. If the naming conventions you develop are followed consistently by all of the Splunk Enterprise users in your organization, you'll find that they become easier to use and that their purpose is much easier to discern at a glance. 
</p><p>You can develop naming conventions for just about every kind of knowledge object in Splunk Enterprise. Naming conventions can help with object organization, but they can also help users differentiate between groups of reports, event types, and tags that have similar uses. And they can help identify a variety of things about the object that may not even be in the object definition, such as what teams or locations use the object, what technology it involves, and what it's designed to do.  
</p><p>Early development of naming conventions for your Splunk Enterprise implementation will help you avoid confusion and chaos later on down the road.
</p>
<h5> <a name="developnamingconventionsforknowledgeobjecttitles_use_the_common_information_model_add-on"><span class="mw-headline" id="Use_the_Common_Information_Model_Add-on"> Use the Common Information Model Add-on</span></a></h5>
<p>The Common Information Model Add-on provides a standard method of parsing, categorizing, and normalizing your event data. It includes various categories of custom fields and tags that are implemented in the add-on as data models.  
</p><p>You can use the CIM Add-on to help ensure that your fields and tags are standardized and normalized. The data model implementation of the CIM will help you verify that your fields are named or aliased correctly and also enable you quickly report on various categories of data. 
</p><p>You can download the Common Information Model Add-on from Splunkbase here. For a more in-depth overview of the CIM Add-on, see the Common Information Model Add-on product documentation.
</p>
<h3> <a name="developnamingconventionsforknowledgeobjecttitles_example_-_set_up_a_naming_convention_for_reports"><span class="mw-headline" id="Example_-_Set_up_a_naming_convention_for_reports"> Example - Set up a naming convention for reports </span></a></h3>
<p>You work in the systems engineering group of your company, and as the knowledge manager for your Splunk Enterprise implementation, it's up to you to come up with a naming convention for the reports produced by your team. 
</p><p>In the end you develop a naming convention that pulls together:
</p>
<ul><li> <b>Group</b>: Corresponds to the working group(s) of the user saving the search.
</li><li> <b>Search type</b>: Indicates the type of search (alert, report, summary-index-populating)
</li><li> <b>Platform</b>: Corresponds to the platform subjected to the search
</li><li> <b>Category</b>: Corresponds to the concern areas for the prevailing platforms. 
</li><li> <b>Time interval</b>: The interval over which the search runs (or on which the search runs, if it is a scheduled search). 
</li><li> <b>Description</b>: A meaningful description of the context and intent of the search, limited to one or two words if possible. Ensures the search name is unique.
</li></ul><table cellpadding="5" cellspacing="0" border="1" width="600px"><tr><th bgcolor="#C0C0C0"> Group
</th><th bgcolor="#C0C0C0"> Search type
</th><th bgcolor="#C0C0C0"> Platform
</th><th bgcolor="#C0C0C0"> Category
</th><th bgcolor="#C0C0C0"> Time interval
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr valign="top"><td valign="center" align="left"> SEG<br>NEG<br>OPS<br>NOC<br></td><td valign="center" align="left"> Alert<br>Report<br>Summary<br></td><td valign="center" align="left"> Windows<br>iSeries<br>Network<br></td><td valign="center" align="left"> Disk<br>Exchange<br>SQL<br>Event log<br>CPU<br>Jobs<br>Subsystems<br>Services<br>Security
</td><td valign="center" align="left"> &lt;arbitrary&gt;
</td><td valign="center" align="left"> &lt;arbitrary&gt;
</td></tr></table><p><br><b>Possible reports using this naming convention:</b>
</p>
<ul><li> SEG_Alert_Windows_Eventlog_15m_Failures
</li><li> SEG_Report_iSeries_Jobs_12hr_Failed_Batch
</li><li> NOC_Summary_Network_Security_24hr_Top_src_ip
</li></ul><a name="understandandusethecommoninformationmodel"></a><h2> <a name="understandandusethecommoninformationmodel_understand_and_use_the_common_information_model_add-on"><span class="mw-headline" id="Understand_and_use_the_Common_Information_Model_Add-on"> Understand and use the Common Information Model Add-on</span></a></h2>
<p>The Common Information Model Add-on is based on the idea that you can break down most log files into two components: 
</p>
<ul><li> fields
</li><li> event category tags
</li></ul><p>With these two components a knowledge manager can set up their log files in a way that makes them easily processable by Splunk and which normalizes noncompliant log files and forces them to follow a similar schema. The Common Information Model details the standard fields and event category tags that Splunk uses when it processes most IT data. 
</p><p>In the past, the Common Information Model was represented here as a set of tables that one could use to normalize their data by ensuring that they were using the same field names and event tags for equivalent events from different sources or vendors. 
</p><p>We've since updated the Common Information Model. It's now set up as an Add-on that implements the CIM tables as <b>data models</b>. You can use these data models in two ways: 
</p>
<ul><li> Initially, you can use them to test whether your fields and tags have been normalized correctly.
</li><li> After you've verified that your data is normalized you can use the models to generate reports and dashboard panels via Pivot.
</li></ul><p>You can download the Common Information Model Add-on from Splunkbase here. For a more in-depth overview of the CIM Add-on, see the Common Information Model Add-on product documentation.
</p>
<a name="manageknowledgeobjectpermissions"></a><h2> <a name="manageknowledgeobjectpermissions_manage_knowledge_object_permissions"><span class="mw-headline" id="Manage_knowledge_object_permissions"> Manage knowledge object permissions</span></a></h2>
<p><b>Note:</b> This topic assumes that as a knowledge manager you have an <i>admin</i> role or a role with an equivalent permission set.
</p><p>As a Knowledge Manager, you can set knowledge object permissions to restrict or expand access to the variety of knowledge objects within your Splunk Enterprise implementation. 
</p><p>In some cases you'll determine that certain specialized knowledge objects should only be used by people in a particular role, within a specific app. And in others you'll move to the other side of the scale and make universally useful knowledge objects globally available to all users in all apps. As with all aspects of knowledge management you'll want to carefully consider the implications of these access restrictions and expansions. 
</p><p>When a Splunk Enterprise user first creates a new report, event type, transaction, or similar knowledge object, it is only available to that user. To make that object available to more people, Splunk Web provides the following options, which you can take advantage of if your permissions enable you to do so. You can: 
</p>
<ul><li> Make the knowledge object available globally to users of all apps (also referred to as "promoting" an object).
</li><li> Make the knowledge object available to all users of an app.
</li><li> Restrict (or expand) access to global or app-specific objects by user or role. 
</li><li> Set read/write permissions at the app level for roles, to enable users to share or delete objects they do not own.
</li></ul><p><b>By default, only users with a</b> <i><b>power</b></i> <b>or</b> <i><b>admin</b></i> <b>role can share and promote knowledge objects.</b> This makes you and your fellow knowledge managers gatekeepers with approval capability over the sharing of new knowledge objects. 
</p><p>For more information about extending the ability to set permissions to other roles, see the subtopic <a href="#manageknowledgeobjectpermissions_enabling_roles_other_than_power_and_admin_to_set_permissions_and_share_objects" class="external text">"Enabling roles other than <i>power</i> and <i>admin</i> to set permissions and share objects,"</a> below. 
</p>
<h3> <a name="manageknowledgeobjectpermissions_how_do_permissions_affect_knowledge_object_usage.3f"><span class="mw-headline" id="How_do_permissions_affect_knowledge_object_usage.3F"> How do permissions affect knowledge object usage? </span></a></h3>
<p>To illustrate how these choices can affect usage of a knowledge object, imagine that Bob, a user of a (fictional) Network Security app with an <i>admin</i>-level "Firewall Manager" role, creates a new event type named <code><font size="2">firewallbreach</font></code>, which finds events that indicate firewall breaches. Here's a series of permissions-related issues that could come up, and the actions and results that would follow:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><td valign="center" align="left"><b>Issue</b></td><td valign="center" align="left"><b>Action</b></td><td valign="center" align="left"><b>Result</b>
</td></tr><tr valign="top"><td valign="center" align="left">When Bob first creates <code><font size="2">firewallbreach</font></code>, it is only available to him. Other users cannot see it or work with it. Bob decides he wants to share it with his fellow Network Security app users.</td><td valign="center" align="left">Bob updates the permissions of the <code><font size="2">firewallbreach</font></code> event type so that it is available to all users of the Network Security app, regardless of role. He also sets up the new event type so that all Network Security users can edit its definition.</td><td valign="center" align="left">Anyone using Splunk in the Network Security app context can see, work with, and edit the <code><font size="2">firewallbreach</font></code> event type. Users of other Splunk apps in the same Splunk Enterprise implementation have no idea it exists.
</td></tr><tr valign="top"><td valign="center" align="left">A bit later on, Mary, the knowledge manager, realizes that only users in the Firewall Manager role should have the ability to edit or update the <code><font size="2">firewallbreach</font></code> event type.</td><td valign="center" align="left">Mary restricts the ability to edit the event type to the Firewall Manager role.</td><td valign="center" align="left">Users of the Network Security app can use the <code><font size="2">firewallbreach</font></code> event type in transactions, searches, dashboards, and so on, but now the only people that can edit the knowledge object are those with the Firewall Manager role and people with <i>admin</i>-level permissions (such as the knowledge manager). People using Splunk in other app contexts remain blissfully ignorant of the event type.
</td></tr><tr valign="top"><td valign="center" align="left">At some point a few people who have grown used to using the very handy <code><font size="2">firewallbreach</font></code> event type in the Network Security app decide they'd like to use it in the context of the Windows app as well.</td><td valign="center" align="left">They make their case to the knowledge manager, who promptly promotes the <code><font size="2">firewallbreach</font></code> event type to global availability.</td><td valign="center" align="left">Now, everyone that uses this implementation of Splunk Enterprise can use the <code><font size="2">firewallbreach</font></code> event type, no matter what app context they happen to be in. But the ability to update the event type definition is still confined to <i>admin</i>-level users and users with the Firewall Manager role.
</td></tr></table><h3> <a name="manageknowledgeobjectpermissions_permissions_-_getting_started"><span class="mw-headline" id="Permissions_-_Getting_started">Permissions - Getting started</span></a></h3>
<p>To change the permissions for a knowledge object, follow these steps:
</p><p><b>1.</b> In Splunk Web, navigate to the page for the type of knowledge object that you want to update permissions for (such as Searches and reports or Event types). 
</p><p><b>2.</b> Find the knowledge object that you created (use the filtering fields at the top of the page if necessary) and click its <b>Permissions</b> link. 
</p><p><b>3.</b> On the Permissions page for the knowledge object in question, perform the actions in the following subsections depending on how you'd like to change the object's permissions. 
</p><p><img alt="6.1 permissions lookup.png" src="images/7/71/6.1_permissions_lookup.png" width="509" height="419"></p>
<h3> <a name="manageknowledgeobjectpermissions_make_an_object_available_to_users_of_all_apps"><span class="mw-headline" id="Make_an_object_available_to_users_of_all_apps"> Make an object available to users of all apps  </span></a></h3>
<p>To make an object globally available to users of all apps in your Splunk Enterprise implementation:
</p><p><b>1.</b> Navigate to the Permissions page for the knowledge object (following the instructions above). 
</p><p><b>2.</b> Under <b>Object should appear in</b>, select <i>All apps</i>. 
</p><p><b>3.</b> In the Permissions section, for <b>Everyone</b>, select a permission of either <i>Read</i> or <i>Write</i>:
</p>
<ul><li> <i>Read</i> enables users to see and use the object, but not update its definition. In other words, when users only have <i>Read</i> permission for a particular report, they can see it in the top level navigation and they can run it. But they can't update the search string, change its time range, and save their changes. 
</li><li> <i>Write</i> enables users to view, use, <i>and</i> update the defining details of an object as necessary.
</li><li> If neither <i>Read</i> or <i>Write</i> is selected then users cannot see or use the knowledge object.
</li></ul><p><b>4.</b> Save the permission change.
</p>
<h3> <a name="manageknowledgeobjectpermissions_make_an_object_available_to_all_users_of_its_app"><span class="mw-headline" id="Make_an_object_available_to_all_users_of_its_app"> Make an object available to all users of its app </span></a></h3>
<p>All knowledge objects are associated with an app. When you create a new knowledge object, it is associated with the app context that you are in at the time. In other words, if you are using the Search &amp; Reporting app when you create the object, the object will be listed in Settings with <i>Search &amp; Reporting</i> as its <b>App</b> column value. This means that if you restrict its sharing permissions to the app level it will only be available to users of the Search &amp; Reporting app. 
</p><p>When you create a new object, you are given the option of keeping it private, sharing it with users of the app that you're currently using, or sharing it globally with all users. Opt to make the app available to "this app only" to restrict its usage to users of that app, when they are in that app context. 
</p><p>If you have write permissions for an object that already exists, you can change its permissions so that it is only available to users of its app by following these steps.
</p><p><b>1.</b> Navigate to the Permissions page for the knowledge object (following the instructions in "Permissions - Getting Started," above). 
</p><p><b>2.</b> Under <b>Object should appear in</b>, select <i>This app only</i>. 
</p><p><b>3.</b> In the Permissions section, for <b>Everyone</b>, select a permission of either <i>Read</i> or <i>Write</i>:
</p>
<ul><li> <i>Read</i> enables users to see and use the object, but not update its definition. In other words, when users only have <i>Read</i> permission for a particular report, they can see it in the top level navigation and they can run it. But they can't update the search string, change its time range, and save their changes. 
</li><li> <i>Write</i> enables users to view, use, and update the defining details of an object as necessary.
</li><li> If neither <i>Read</i> or <i>Write</i> is selected then users cannot see or use the knowledge object.
</li></ul><p><b>4.</b> Save the permission change.
</p>
<h4><font size="3"><b><i> <a name="manageknowledgeobjectpermissions_moving_or_cloning_a_knowledge_object"><span class="mw-headline" id="Moving_or_cloning_a_knowledge_object">Moving or cloning a knowledge object</span></a></i></b></font></h4>
<p>You may run into situations where you want users of an app to be able to access a particular knowledge object that belongs to a different app, but you do not want to share that object globally with all apps. There are two ways you can do this: by cloning the object, or by moving it.
</p>
<ul><li> <b>Clone</b> - Make a copy of a knowledge object. The copy has all of the same settings as the original object, which you can keep or modify. You can keep it in the same app as the object you're cloning, or you can put it in a new app. If you add the cloned object to the same app as the original, give it a different name. You can keep the original name if you add the object to an app that doesn't have a knowledge object of the same type with that name. You can clone any object, even if your role does not have write permissions for it.
</li><li> <b>Move</b> - Move an existing knowledge object to another app. Removes the object from its current app and places it in an app that you determine. Once there, you can set its permissions so that it is private, globally available, or only available to users of that app. The ability to move an app is connected to the same permissions that determine whether you can delete an app: You can only move a knowledge object if you have created that object and have write permissions for the app to which it belongs. 
</li></ul><p><b>Note:</b> Switching the app context of an knowledge object by moving it can have downstream consequences for objects that have been associated with it. For more information see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects,"</a> in this manual.
</p><p>You can find the <b>Clone</b> and <b>Move</b> controls on the Settings pages for various knowledge object types. To clone or move an object, find the object in its list and click <b>Clone</b> or <b>Move</b>.
</p>
<h3> <a name="manageknowledgeobjectpermissions_restrict_knowledge_object_access_by_app_and_role"><span class="mw-headline" id="Restrict_knowledge_object_access_by_app_and_role"> Restrict knowledge object access by app and role </span></a></h3>
<p>You can use this method to lock down various knowledge objects from alteration by specific roles. You can arrange things so users in a particular role can use the knowledge object but not update it--or you can set it up so those users cannot see the object at all. In the latter case, the object will not show up for them in Splunk Web, and they will not find any results when they search on it.
</p><p>If you want restrict the ability to see or update a knowledge object by role, simply navigate to the Permissions page for the object. If you want members of a role to:
</p>
<ul><li> <b>Be able to use the object and update its definition</b>, give that role <i>Read</i> and <i>Write</i> access.
</li><li> <b>Be able to use the object but be</b> <i><b>unable</b></i> <b>to update it</b>, give that role <i>Read</i> access only (and make sure that <i>Write</i> is unchecked for the <b>Everyone</b> role).
</li><li> <b>Be unable to see or use the knowledge object at all</b>, leave <i>Read</i> and <i>Write</i> unchecked for that role (and unchecked for the <b>Everyone</b> role as well).
</li></ul><p>For more information about role-based permissions in Splunk Enterprise see "About role-based user access" in the Security manual.
</p>
<h3> <a name="manageknowledgeobjectpermissions_enabling_roles_other_than_power_and_admin_to_set_permissions_and_share_objects"><span class="mw-headline" id="Enabling_roles_other_than_power_and_admin_to_set_permissions_and_share_objects"> Enabling roles other than <i>power</i> and <i>admin</i> to set permissions and share objects </span></a></h3>
<p>When you use Splunk Enterprise as delivered the only roles that can set permissions for knowledge objects are <i>power</i> and <i>admin</i>. If you want to grant other roles the ability to set permissions for knowledge objects, select <b>Apps</b> and click <b>Permissions</b> for a specific app to go to its Permissions page. On this page you can determine which roles have read or write access to the knowledge objects that the app contains. Grant a role write access if you wish for it to have full ability to set knowledge object permissions, including the ability to share searches, alerts, and dashboards when these things are created via Splunk Web.
</p><p>The ability to set permissions for knowledge objects share knowledge objects is controlled at the app level--it is not connected to a <b>capability</b> like other actions such as scheduling searches or changing default input settings.
</p><p>For detailed information about enabling roles to set permissions for knowledge objects within apps, see "Step 5: Set Permissions" in the Developing Dashboards, Forms, and Views for Splunk Web Manual.
</p>
<h3> <a name="manageknowledgeobjectpermissions_about_deleting_users_and_roles_with_unshared_objects"><span class="mw-headline" id="About_deleting_users_and_roles_with_unshared_objects"> About deleting users and roles with unshared objects </span></a></h3>
<p>If a user leaves your team and you need to delete that user or role from the Splunk Enterprise system, be aware that you will lose any knowledge objects belonging to them that have a sharing status of <i>private</i>. If you want to keep those knowledge objects, share them at the app or global level before deleting the user or role.
</p>
<a name="disableordeleteknowledgeobjects"></a><h2> <a name="disableordeleteknowledgeobjects_disable_or_delete_knowledge_objects"><span class="mw-headline" id="Disable_or_delete_knowledge_objects"> Disable or delete knowledge objects</span></a></h2>
<p>In Splunk Enterprise your ability to delete knowledge objects in Splunk Web depends on a set of factors: 
</p>
<ul><li> You cannot delete default knowledge objects that were delivered with Splunk Enterprise (or with an app). 
</li></ul><dl><dd> If the knowledge object definition resides in a default directory, it can't be removed through Splunk Web. It can only be disabled (by clicking <b>Disable</b> for the object in Settings). Only objects that exist in an app's local directory are eligible for deletion.
</dd></dl><ul><li> You can always delete knowledge objects that you have created, and which haven't been shared (by you or someone with a dmin-level permissions). 
</li></ul><dl><dd> Once you share a knowledge object you've created with other users, your ability to delete it is revoked, unless you have write permissions for the app to which they belong (see the next point).
</dd></dl><ul><li> To delete any other knowledge object, your role must have write permissions for: 
<ul><li> The app to which the knowledge object belongs.
</li><li> The knowledge object itself. 
</li></ul></li></ul><dl><dd> This applies to knowledge objects that are shared globally as well as those that are only shared within an app. All knowledge objects belong to a specific app, no matter how they are shared.
</dd></dl><p>App-level write permissions are usually only granted to users with admin-equivalent roles. 
</p><p><b>Note:</b> If a role does not have write permissions for an app but does have write permissions for knowledge objects belonging to that app, it can disable those knowledge objects.  Clicking <b>Disable</b> for a knowledge object has the same function as knowledge object deletion, with the exception that Splunk Enterprise does not remove disabled knowledge objects from the system. A role with write permissions for a disabled knowledge object can reenable it at any time. 
</p><p>There are similar rules for data models. To enable a role to create data models and share them with others, the role must be given write access to an app. This means that users who can create and share data models can potentially also delete knowledge objects. For more information about this see <a href="#managedatamodels" class="external text">"Manage data models"</a> in this manual,  (and look for the subtopic "Enable roles to create data models").
</p>
<h3> <a name="disableordeleteknowledgeobjects_grant_a_role_write_permissions_for_an_app"><span class="mw-headline" id="Grant_a_role_write_permissions_for_an_app">Grant a role write permissions for an app</span></a></h3>
<p>If your role has admin-level permissions, you can grant a role write permissions for an app in Splunk Web.
</p><p><b>1.</b> Click the <b>App</b> dropdown at the top of the page and select <i>Manage Apps</i> to go to the Apps page. 
</p><p><b>2.</b> On the Apps page, find the app that you want to grant write permissions for and click <b>Permissions</b>.
</p><p><b>3.</b> On the Permissions page for the app, select <b>Write</b> for the roles that should be able to delete knowledge objects for the app.
</p><p><b>4.</b> Click <b>Save</b> to save your changes.
</p><p>You can also manage role-based permissions for apps by updating the app's <code><font size="2">local.meta</font></code> file. For more information see "Setting access to manager consoles and apps" in the Securing Splunk Enterprise manual.
</p>
<h3> <a name="disableordeleteknowledgeobjects_grant_a_role_with_app_write_permissions_the_ability_to_delete_a_knowledge_object_that_belongs_to_that_app"><span class="mw-headline" id="Grant_a_role_with_app_write_permissions_the_ability_to_delete_a_knowledge_object_that_belongs_to_that_app"> Grant a role with app write permissions the ability to delete a knowledge object that belongs to that app</span></a></h3>
<p>Once a role has write permissions for an app, users with that role can delete any knowledge object belonging to that app as long as they also have write permissions for those objects. Users can do this whether the knowledge object is shared just to the app, or globally to all apps. Even when knowledge objects are shared globally they belong to a specific app.
</p><p>This procedure assumes that: 
</p>
<ul><li> Your role has admin-level permissions.
</li><li> The role that you are setting object-level permissions for has the ability to write to the app that the object belongs to. 
</li></ul><p><b>1.</b> Navigate to the listing page for the knowledge object in <b>Settings</b>.
</p><p><b>2.</b> To ensure that you are viewing objects that belong to the app for which the role has write permissions , select the app name or <i>All</i> for the <b>App context</b>. 
</p><p><b>3.</b> On the listing page, locate the knowledge object that the role needs to be able to delete and click its <b>Permissions</b> link. 
</p><p><b>4.</b> On the permissions page for the knowledge object, select <b>Write</b> for the role. 
</p><p>If you have followed this procedure and the procedure that came before it you the role should be able to delete the knowledge object.
</p>
<h3> <a name="disableordeleteknowledgeobjects_deleting_knowledge_objects_with_downstream_dependencies"><span class="mw-headline" id="Deleting_knowledge_objects_with_downstream_dependencies"> Deleting knowledge objects with downstream dependencies </span></a></h3>
<p>You have to be careful about deleting knowledge objects with downstream dependencies, as this can have negative impacts. 
</p><p>For example, you could have a tag that looks like the duplicate of another, far more common tag. On the surface it would seem to be harmless to delete the dup tag. But what you may not realize is that this duplicate tag also happens to be part of a search that a very popular event type is based upon. And that popular event type is used in <i>two</i> important reports--the first is the basis for a well-used dashboard panel, and the other is used to populate a <b>summary index</b> that is used by searches that run several other dashboard panels. So if you delete that tag, the event type breaks, and everything downstream of that event type breaks. 
</p><p><b>This is why it is important to nip poorly named or defined knowledge objects in the bud, before they become inadvertently hard-wired into the workings of your deployment.</b> The only way to identify the downstream dependencies of a particular knowledge object is to search on it, find out where it is used, and then search on those things to see where they are used--it can take a bit of detective work. There is no "one click" way to bring up a list of knowledge object downstream dependencies at this point. 
</p><p>If you really feel that you have to delete an knowledge object, and you're not sure if you've tracked down and fixed all of its downstream dependencies, you could try <i>disabling</i> it first to see what impact that has. If nothing seems to go seriously awry after a day or so, delete it. 
</p>
<h3> <a name="disableordeleteknowledgeobjects_deleting_knowledge_objects_in_configuration_files"><span class="mw-headline" id="Deleting_knowledge_objects_in_configuration_files"> Deleting knowledge objects in configuration files </span></a></h3>
<p>Note that in Splunk Web, you can only disable or delete one knowledge object at a time. If you need to remove large numbers of objects, the most efficient way to do it is by removing the knowledge object stanzas directly through the configuration files. Keep in mind that several versions of a particular configuration file can exist within your system. In most cases you should only edit the configuration files in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, to make local changes on a site-wide basis, or <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;App_name&gt;/local/</font></code>, if you need to make changes that apply only to a specific app.
</p><p>Do not try to edit configuration files until you have read and understood the following topics in the Admin manual:
</p>
<ul><li> About configuration files
</li><li> Configuration file precedence
</li></ul><h1>Data interpretation: Fields and field extractions</h1><a name="aboutfields"></a><h2> <a name="aboutfields_about_fields"><span class="mw-headline" id="About_fields"> About fields</span></a></h2>
<table cellpadding="10" cellspacing="0" border="1" width="100%"><tr><td valign="center" align="left"> This page is currently a work in progress. Expect frequent updates.
</td></tr></table><p><b>Fields</b> appear in <b>event data</b> as searchable name/value pairings such as <code><font size="2">user_name=fred</font></code> or <code><font size="2">ip_address=192.168.1.1</font></code>. They are the building blocks of searches, reports, and data models in Splunk Enterprise. When you run a search on your event data, Splunk Enterprise looks for fields in that data. 
</p><p><b>Note:</b> Field names are often referred to as keys. The acronym kv is short for key/value.
</p><p>Look at the following example search.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">status=404</font></code><br></div>
<p>This search finds events with <code><font size="2">status</font></code> fields that have a value of <code><font size="2">404</font></code>. When you run this search, Splunk Enterprise does not look for events with any other <code><font size="2">status</font></code> value. It also does not look for events containing other fields that share <code><font size="2">404</font></code> as a value. As a result, this search returns a set of results that are more focused than you get if you used <code><font size="2">404</font></code> in the search string.
</p><p>Fields often appear in events as <code><font size="2">key=value</font></code> pairs such as <code><font size="2">user_name=Fred</font></code>. But in many events, field values appear in fixed, delimited positions without identifying keys. For example, you might have events where the <code><font size="2">user_name</font></code> value always appears by itself after the timestamp and the <code><font size="2">user_id</font></code> value.
</p>
<div class="samplecode">
<code><font size="2"><br>Nov 15 09:32:22 00224 johnz<br>Nov 15 09:39:12 01671 dmehta<br>Nov 15 09:45:23 00043 sting<br>Nov 15 10:02:54 00676 lscott<br></font></code></div>
<p>Splunk Enterprise can identify these fields using a custom field extraction. 
</p>
<h3> <a name="aboutfields_about_field_extraction"><span class="mw-headline" id="About_field_extraction">About field extraction</span></a></h3>
<p>As Splunk Enterprise processes events, it extracts fields from them. This process is called <b>field extraction</b>. 
</p>
<h4><font size="3"><b><i> <a name="aboutfields_splunk_enterprise_automatically_extracts_some_fields"><span class="mw-headline" id="Splunk_Enterprise_automatically_extracts_some_fields">Splunk Enterprise automatically extracts some fields</span></a></i></b></font></h4>
<p>Splunk Enterprise extracts some fields from your events without assistance. It automatically extracts <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, and <code><font size="2">sourcetype</font></code> values, timestamps, and several other <b>default fields</b> when it indexes incoming events. 
</p><p>It also extracts fields that appear in your event data as <code><font size="2">key=value</font></code> pairs. This process of recognizing and extracting k/v pairs is called <b>field discovery</b>. You can disable field discovery to improve search performance. 
</p><p>When fields appear in events without their keys, Splunk Enterprise uses pattern-matching rules called regular expressions to extract those fields as complete k/v pairs. With a properly configured regular expression, Splunk Enterprise can extract <code><font size="2">user_id=johnz</font></code> from the previous sample event. Splunk Enterprise comes with several field extraction configurations that use regular expressions to identify and extract fields from event data. 
</p><p>For more information about field discovery and an example of automatic field extraction, see <a href="#whensplunkenterpriseaddsfields" class="external text">"When Splunk Enterprise extracts fields,"</a> in this manual.
</p><p>For more information on how Splunk Enterprise uses regular expressions to extract fields, see <a href="#aboutsplunkregularexpressions" class="external text">"About Splunk Enterprise regular expressions,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_to_get_all_of_the_fields_in_your_data.2c_create_custom_field_extractions"><span class="mw-headline" id="To_get_all_of_the_fields_in_your_data.2C_create_custom_field_extractions">To get all of the fields in your data, create custom field extractions</span></a></i></b></font></h4>
<p>To use the power of Splunk Enterprise search, create additional field extractions. Custom field extractions allow you to capture and track information that is important to your needs, but which is not automatically discovered and extracted by Splunk Enterprise. Any field extraction configuration you provide must include a regular expression that tells Splunk Enterprise how to find the field that you want to extract. 
</p><p>All field extractions, including custom field extractions, are tied to a specific <code><font size="2">source</font></code>, <code><font size="2">sourcetype</font></code>, or <code><font size="2">host</font></code> value. For example, if you create an <code><font size="2">ip</font></code> field extraction, you might tie the extraction configuration for <code><font size="2">ip</font></code> to <code><font size="2">sourcetype=access_combined</font></code>.
</p><p>Custom field extractions should take place at <b>search time</b>, but in certain rare circumstances you can arrange for some custom field extractions to take place at <b>index time</b>. See <a href="#whensplunkenterpriseaddsfields" class="external text">"When Splunk Enterprise extracts fields,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_before_you_create_custom_field_extractions.2c_get_to_know_your_data"><span class="mw-headline" id="Before_you_create_custom_field_extractions.2C_get_to_know_your_data">Before you create custom field extractions, get to know your data</span></a></i></b></font></h4>
<p>Before you begin to create field extractions, ensure that you are familiar with the formats and patterns of the event data associated with the <code><font size="2">source</font></code>, <code><font size="2">sourcetype</font></code>, or <code><font size="2">host</font></code> that you are working with. One way is to investigate the predominant event patterns in your data with the <b>Patterns</b> tab. See "Identify event patterns with the Patterns tab" in the <i>Search Manual</i>.
</p><p>Here are two events from the same source type, an apache server web access log.
</p>
<div class="samplecode">
<code><font size="2"><br>131.253.24.135 - - [03/Jun/2014:20:49:53 -0700] "GET /wp-content/themes/aurora/style.css HTTP/1.1" 200 7464 "http://www.splunk.com/download" "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0; &nbsp;Trident/5.0)&acirc;&#128;&#157;<br><br>10.1.10.14 - - [03/Jun/2014:20:49:33 -0700] "GET / HTTP/1.1" 200 75017 "-" "Mozilla/5.0 (compatible; Nmap Scripting Engine; http://nmap.org/book/nse.html)"<br></font></code></div>
<p>While these events contain different strings and characters, they are formatted in a consistent manner. They both present values for fields such as <code><font size="2">clientIP</font></code>, <code><font size="2">status</font></code>, <code><font size="2">bytes</font></code>, <code><font size="2">method</font></code>, and so on in a reliable order. 
</p><p>Reliable means that the <code><font size="2">method</font></code> value is always followed by the <code><font size="2">URI</font></code> value, the <code><font size="2">URI</font></code> value is always followed by the <code><font size="2">status</font></code> value, the <code><font size="2">status</font></code> value is always followed by the <code><font size="2">bytes</font></code> value, and so on. When your events have consistent and reliable formats, you can create a field extraction that accurately captures multiple field values from them.
</p><p>For contrast, look at this set of Cisco ASA firewall log events:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><td valign="center" align="left"> <b>1</b>
</td><td valign="center" align="left"> <code><font size="2">Jul 15 20:10:27 10.11.36.31&nbsp;%ASA-6-113003: AAA group policy for user AmorAubrey is being set to Acme_techoutbound</font></code>
</td></tr><tr><td valign="center" align="left"> <b>2</b>
</td><td valign="center" align="left"> <code><font size="2">Jul 15 20:12:42 10.11.36.11&nbsp;%ASA-7-710006: IGMP request discarded from 10.11.36.36 to outside:87.194.216.51 </font></code>
</td></tr><tr><td valign="center" align="left"> <b>3</b>
</td><td valign="center" align="left"> <code><font size="2">Jul 15 20:13:52 10.11.36.28&nbsp;%ASA-6-302014: Teardown TCP connection 517934 for Outside:128.241.220.82/1561 to Inside:10.123.124.28/8443 duration 0:05:02 bytes 297 Tunnel has been torn down (AMOSORTILEGIO)</font></code>
</td></tr><tr><td valign="center" align="left"> <b>4</b>
</td><td valign="center" align="left"> <code><font size="2">Apr 19 11:24:32 PROD-MFS-002&nbsp;%ASA-4-106103: access-list fmVPN-1300 denied udp for user 'sdewilde7' outside/12.130.60.4(137) -&gt; inside1/10.157.200.154(137) hit-cnt 1 first hit [0x286364c7, 0x0] "</font></code>
</td></tr></table><p>While these events contain field values that are always space-delimited, they do not share a reliable format like the preceding two events. In order, these events represent: 
</p>
<ol><li> A group policy change
</li><li> An IGMP request
</li><li> A TCP connection
</li><li> A firewall access denial for a request from a specific IP
</li></ol><p>Because these events differ so widely, it is difficult to create a single field extraction that can apply to each of these event patterns and extract relevant field values. 
</p><p>In situations like this, where a specific host, source type, or source contains multiple event patterns, you may want to define field extractions that match each pattern, rather than designing a single extraction that can apply to all of the patterns. Inspect the events to identify text that is common and reliable for each pattern.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_using_required_text_in_field_extractions"><span class="mw-headline" id="Using_required_text_in_field_extractions">Using required text in field extractions</span></a></i></b></font></h4>
<p>In the last four events, the string of numbers that follows <code><font size="2">%ASA-#-</font></code> have specific meanings. You can find their definitions in the Cisco documentation. When you have unique event identifiers like these in your data, specify them as required text in your field extraction. Required text strings limit the events that can match the regular expression in your field extraction. 
</p><p>Specifying required text is optional, but it offers multiple benefits. Because required text reduces the set of events that it scans, it improves field extraction efficiency and decreases the number of false-positive field extractions.
</p><p>The Field Extractor utility enables you to highlight text in a sample event and specify that it is required text.
</p>
<h3> <a name="aboutfields_methods_of_custom_field_extraction_in_splunk_enterprise"><span class="mw-headline" id="Methods_of_custom_field_extraction_in_Splunk_Enterprise">Methods of custom field extraction in Splunk Enterprise</span></a></h3>
<p>As a knowledge manager you oversee the set of custom field extractions created by users of your Splunk Enterprise implementation, and you might define specialized groups of custom field extractions yourself. The ways that you can do this include:
</p>
<ul><li> The Field Extractor utility, which generates regular expressions for your field extractions.
</li><li> Adding field extractions through pages in Settings. You must provide a regular expression.
</li><li> Manual addition of field extraction configurations at the <code><font size="2">.conf</font></code> file level. Provides the most flexibility for field extraction. 
</li></ul><p>The field extraction methods that are available to Splunk Enterprise users are described in the following sections. These methods enable you to create search-time field extractions. To create an index-time field extraction, choose the third option: Direct edits to the configuration files.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_let_the_field_extractor_build_extractions_for_you"><span class="mw-headline" id="Let_the_field_extractor_build_extractions_for_you">Let the field extractor build extractions for you</span></a></i></b></font></h4>
<p>The field extractor utility leads you step-by-step through the field extraction design process. It is useful if you are unfamiliar with regular expression syntax and usage, because it generates regular expressions and lets you validate them. However, you can always manually create or edit regular expressions while using the field extractor.
</p><p>With the field extractor you can:
</p>
<ul><li> Set up a field extraction by selecting a sample event and highlighting fields to extract from that event.
</li><li> Create individual extractions that capture multiple fields.
</li><li> Improve extraction accuracy by detecting and removing false positive matches.
</li><li> Validate extraction results by using search filters to ensure specific values are being extracted.
</li><li> Specify that fields only be extracted from events that have a specific string of required text.
</li><li> Review stats tables of the field values discovered by your extraction.
</li><li> Manually configure regular expression for the field expression yourself.
</li></ul><p>The field extractor can only build <b>search time</b> field extractions that are associated with specific source types in your data (no hosts or sources).
</p><p>For more information about using the field extractor, see <a href="#extractfieldsinteractivelywithifx" class="external text">"Build field extractions with the field extractor"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_define_field_extractions_with_the_field_extractions_and_field_transformations_pages"><span class="mw-headline" id="Define_field_extractions_with_the_Field_Extractions_and_Field_Transformations_pages">Define field extractions with the Field Extractions and Field Transformations pages</span></a></i></b></font></h4>
<p>You can use the Field Extractions and Field Transformations pages in Settings to define and maintain complex extracted fields in Splunk Web. 
</p><p>This method of field extraction creation lets you create a wider range of field extractions than you can generate with the Field Extractor. It requires that you have the following knowledge.
</p>
<ul><li> Understand how to design regular expressions.
</li><li> Have a basic understanding of how field extractions are configured in <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>.
</li></ul><p>If you create a custom field extraction that extracts its fields from <code><font size="2">_raw</font></code> and does not require a field transform, use the Field Extractor utility. The Field Extractor can generate regular expressions, and it can give you feedback about the accuracy of your field extractions as you define them. 
</p><p>Use the Field Extractions page to create basic field extractions, or use it in conjunction with the Field Transformations page to define field extraction configurations that can do the following things.
</p>
<ul><li> Reuse the same regular expression across multiple sources, source types, or hosts.
</li><li> Apply multiple regular expressions to the same source, source type, or host.
</li><li> Use a regular expression to extract fields from the values of another field. 
</li></ul><p>The Field Extractions and Field Transformations pages define only <b>search time</b> field extractions.
</p><p>See the following topics in this manual:
</p>
<ul><li> <a href="#managesearch-timefieldextractions" class="external text">Use the Field extractions page in Splunk Web</a>
</li><li> <a href="#managefieldtransforms" class="external text">Use the Field transformations page in Splunk Web</a>.
</li></ul><h4><font size="3"><b><i> <a name="aboutfields_configure_field_extractions_directly_in_.conf_files"><span class="mw-headline" id="Configure_field_extractions_directly_in_.conf_files">Configure field extractions directly in .conf files</span></a></i></b></font></h4>
<p>To get complete control over your field extractions, add the configurations directly into <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. This method lets you create field extractions with capabilites that extend beyond what you can create with Splunk Web methods such as the Field Extractor utility or the Settings pages. For example, with the configuration files, you can set up:
</p>
<ul><li> Delimiter-based field extractions.
</li><li> Extractions for multivalue fields.
</li><li> Extractions of fields with names that begin with numbers or underscores. This action is typically not allowed unless key cleaning is disabled.
</li><li> Formatting of extracted fields. 
</li></ul><p>See <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time extractions through configuration files,"</a> in this manual.
</p><p>You can create <b>index-time</b> field extractions only by configuring them in <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. Adding to the default set of indexed fields can result in search performance and indexing problems. But if you must create additional index-time field extractions, see "Create custom fields at index time" in the <i>Getting Data In</i> manual.
</p>
<h5> <a name="aboutfields_create_custom_calculated_fields_and_multivalue_fields"><span class="mw-headline" id="Create_custom_calculated_fields_and_multivalue_fields">Create custom calculated fields and multivalue fields</span></a></h5>
<p>Two kinds of custom fields can be persistently configured with the help of <code><font size="2">.conf</font></code> files: calculated fields and multivalue fields. 
</p><p><b>Multivalue fields</b> can appear multiple times in a single event, each time with a different value. To configure custom multivalue fields, make changes to <code><font size="2">fields.conf</font></code> as well as to <code><font size="2">props.conf</font></code>. See <a href="#configuresplunktoparsemulti-valuefields" class="external text">"Configure multivalue fields"</a> in this manual.
</p><p><b>Calculated fields</b> provide values that are calculated from the values of other fields present in the event, with the help of <code><font size="2">eval</font></code> expressions. Configure them in <code><font size="2">props.conf</font></code>. See <a href="#definecalcfields" class="external text">"Define calculated fields"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutfields_build_field_extractions_into_search_strings"><span class="mw-headline" id="Build_field_extractions_into_search_strings">Build field extractions into search strings</span></a></i></b></font></h4>
<p>Splunk Enterprise provides search commands that facilitate the search-time extraction of fields in different ways. These commands include:
</p>
<ul><li> <code><font size="2">rex</font></code>
</li><li> <code><font size="2">extract</font></code>
</li><li> <code><font size="2">multikv</font></code>
</li><li> <code><font size="2">spath</font></code>
</li><li> <code><font size="2">xmlkv</font></code>
</li><li> <code><font size="2">xpath</font></code>
</li><li> <code><font size="2">kvform</font></code>
</li></ul><p>See "Extract fields with search commands," in the <i>Search Manual</i>. Alternatively you can look up each of these commands in the <i>Search Reference</i>.
</p><p>Field extractions facilitated by search commands apply only to the results returned by the searches in which you use these commands. You cannot use these search commands to create reusable extractions that persist after the search is completed. For that, use the Field Extractor utility, configure extractions with the Settings pages, or set up configurations directly in the <code><font size="2">.conf</font></code> files.
</p>
<a name="whensplunkenterpriseaddsfields"></a><h2> <a name="whensplunkenterpriseaddsfields_when_splunk_enterprise_extracts_fields"><span class="mw-headline" id="When_Splunk_Enterprise_extracts_fields"> When Splunk Enterprise extracts fields</span></a></h2>
<p>Splunk Enterprise extracts fields first at <b>index time</b>, and again at <b>search time</b>. After you run a search, Splunk Enterprise lists the fields that it extracted for that search in the fields sidebar.
</p>
<h3> <a name="whensplunkenterpriseaddsfields_field_extraction_at_index_time"><span class="mw-headline" id="Field_extraction_at_index_time">Field extraction at index time</span></a></h3>
<p>At index time Splunk Enterprise extracts a small set of <b>default fields</b> for each event, including <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, and <code><font size="2">sourcetype</font></code>. Default fields are common to all events. See <a href="#usedefaultfields" class="external text">"Use default fields,"</a> in this manual.
</p><p>Splunk Enterprise can also extract custom <b>indexed fields</b> at index time. These are fields that you have explicitly configured for index-time extraction. 
</p><p><b>Caution:</b> Do not add custom fields to the set of default fields that Splunk extracts and indexes at <b>index time</b>. Adding to this list of fields can slow indexing performance and search times, because each indexed field increases the size of the searchable index. Indexed fields are also less flexible, because whenever you make changes to your set of indexed fields, you must re-index your entire dataset. See "Index time versus search time" in the <i>Managing Indexers and Clusters</i> manual.
</p>
<h3> <a name="whensplunkenterpriseaddsfields_field_extraction_at_search_time"><span class="mw-headline" id="Field_extraction_at_search_time">Field extraction at search time</span></a></h3>
<p>At search time, Splunk Enterprise can extract additional fields, depending on its <b>Search Mode</b> setting and whether that setting enables <b>field discovery</b> given the type of search being run. 
</p><p>When field discovery is enabled, Splunk Enterprise:
</p>
<ul><li> Identifies and extracts the first 50 fields that it finds in the event data that match obvious <code><font size="2">key=value</font></code> pairs. This 50 field limit is a default that you can modify by editing the <code><font size="2">[kv]</font></code> stanza in <code><font size="2">limits.conf</font></code>.
</li><li> Extracts any field explicitly mentioned in the search that it might otherwise have found though automatic extraction, but is not among the first 50 fields identified.
</li><li> Performs custom field extractions that you have defined, either through the Field Extractor, the Extracted Fields page in Settings, configuration file edits, or search commands such as <code><font size="2">rex</font></code>. 
</li></ul><p>When field discovery is disabled, Splunk Enterprise extracts:
</p>
<ul><li> Any field explicitly mentioned in the search.
</li><li> The default and indexed fields mentioned above. 
</li><li> Any custom field extraction that has the <code><font size="2">CAN_OPTIMIZE</font></code> parameter <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_create_advanced_search-time_field_extractions_with_field_transforms" class="external text">set to true</a> in <code><font size="2">transforms.conf</font></code>.
</li></ul><p>Splunk Enterprise discovers fields other than default fields and fields explicitly mentioned in the search string only when you:
</p>
<ul><li> run a <b>non-transforming</b> search in the <i>Smart</i> search mode.
</li><li> run any search in the <i>Verbose</i> search mode.
</li></ul><p>See "Set search mode to adjust your search experience" in the Search Manual.
</p><p>For an explanation of search time and index time, see "Index time versus search time" in the Managing Indexers and Clusters manual.
</p>
<h3> <a name="whensplunkenterpriseaddsfields_example_of_automatic_field_extraction"><span class="mw-headline" id="Example_of_automatic_field_extraction"> Example of automatic field extraction </span></a></h3>
<p>This is an example of how Splunk Enterprise automatically extracts fields without user help, as opposed to custom field extractions, which follow event-extraction rules that you define.
</p><p>Say you search on <code><font size="2">sourcetype</font></code>, a default field that Splunk Enterprise extracts for every event at index time. If your search is
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=veeblefetzer</font></code><br></div>
<p>for the past 24 hours, Splunk Enterprise returns every event with a sourcetype of <code><font size="2">veeblefetzer</font></code> in that time range. From this set of events, Splunk Enterprise extracts the first 50 fields that it can identify on its own. And it performs extractions of custom fields, based on configuration files. All of these fields appear in the fields sidebar when the search is complete.
</p><p>Now, if a name/value combination like <code><font size="2">userlogin=fail</font></code> appears for the first time 25,000 events into the search, and <code><font size="2">userlogin</font></code> isn't among the set of custom fields that you've preconfigured, it likely is not among the first 50 fields that Splunk Enterprise finds on its own.
</p><p>However, if you change your search to
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=veeblefetzer userlogin=*</font></code><br></div>
<p>then Splunk Enterprise finds and returns all events including both the <code><font size="2">userlogin</font></code> field and a <code><font size="2">sourcetype</font></code> value of <code><font size="2">veeblefetzer</font></code>. It will be available in the field sidebar along with the other fields that Splunk Enterprise has extracted for this search.
</p>
<a name="extractfieldsinteractivelywithifx"></a><h2> <a name="extractfieldsinteractivelywithifx_build_field_extractions_with_the_field_extractor"><span class="mw-headline" id="Build_field_extractions_with_the_field_extractor"> Build field extractions with the field extractor </span></a></h2>
<p>Use the <b>field extractor</b> utility to create custom fields dynamically on your Splunk Enterprise instance. The field extractor enables you to define field extractions by selecting a sample event and highlighting fields to extract from that event. It also provides several tools to help you test and refine the accuracy of your field extraction.
</p><p>The field extractor is useful if you are not familiar with regular expression syntax and usage, because it generates field-extracting regular expressions and allows you to test them. Regular expressions form the foundation of field extractions. Splunk Enterprise uses regular expressions to find fields in events and extract them. 
</p><p>You can manually create or edit these regular expressions. However, doing this pulls you out of the field extractor workflow. When you save your changes to a regular expression, you skip to the final Save step of the field extractor, where you save the field extraction you just created.
</p>
<h3> <a name="extractfieldsinteractivelywithifx_overview_of_the_field_extractor"><span class="mw-headline" id="Overview_of_the_field_extractor">Overview of the field extractor</span></a></h3>
<p>To help you create a new field, the field extractor takes you through a set of steps. This table gives you an overview of the required steps. Each step is described in detail after the table.
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0"> Step title
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr><td valign="center" align="left"> <a href="#fxselectsourcetypestep" class="external text">Select sourcetype</a>
</td><td valign="center" align="left"> Define the source type that the new field is tied to.
</td></tr><tr><td valign="center" align="left"> <a href="#fxselectsampleeventstep" class="external text">Select sample</a>
</td><td valign="center" align="left"> Select an event that has the field or fields that you want to extract.
</td></tr><tr><td valign="center" align="left"> <a href="#fxselectfieldsstep" class="external text">Select fields</a>
</td><td valign="center" align="left"> Highlight one or more values in the event to identify them as fields for the field extractor to extract from similar events. Optionally, you can:
<ul><li> Provide event examples to improve extraction accuracy.
</li><li> Identify required text to focus the field extraction on events that contain this text.
</li><li> Examine field extraction results.
</li><li> Update the underlying regular expression manually. If you do this, you are out of the field extractor workflow.
</li></ul></td></tr><tr><td valign="center" align="left"> <a href="#fxvalidatestep" class="external text">Validate fields</a>
</td><td valign="center" align="left">
<ul><li> Examine the field extraction results. 
</li><li> Identify incorrectly extracted fields as counterexamples to improve the accuracy of the field extraction.
</li></ul></td></tr><tr><td valign="center" align="left"> <a href="#fxsavestep" class="external text">Save</a>
</td><td valign="center" align="left"> Name your new field extraction, set its permissions, and save it.
</td></tr></table><h3> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor"><span class="mw-headline" id="Access_the_field_extractor">Access the field extractor</span></a></h3>
<p>There are several ways to access the field extractor utility. The access method you use can determine which step of the field extractor workflow you start at.
</p><p>All users can access the field extractor after running a search that returns events. You have three post-search entry points to the field extractor:
</p>
<ul><li> Bottom of the fields sidebar
</li><li> All Fields dialog box
</li><li> Any event in the search results
</li></ul><p>You can also enter the field extractor: 
</p>
<ul><li> from the Splunk Enterprise Home page
</li><li> from the Field Extractions page in Settings.
</li><li> when you add data with a fixed source type.
</li></ul><h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_from_the_bottom_of_the_fields_sidebar"><span class="mw-headline" id="Access_the_field_extractor_from_the_bottom_of_the_fields_sidebar"> Access the field extractor from the bottom of the fields sidebar</span></a></i></b></font></h4>
<p>When you use this method to access the field extractor it runs only against the set of events returned by the search that you have run. To get the full set of source types in your Splunk Enterprise instance, <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_field_extractions_page_in_settings" class="external text">go to the Field Extractions page in Settings</a>.
</p><p><b>1.</b> Run a search that returns events.
</p><p><b>2.</b> Scroll down to the bottom of the fields sidebar and click <b>Extract new fields</b>.
</p>
<dl><dd> If your search string does not identify a <code><font size="2">sourcetype</font></code> value, the field extractor starts you <a href="#fxselectsourcetypestep" class="external text">at the Select Sourcetype step</a>.
</dd></dl><dl><dd> If your search string identifies a <code><font size="2">sourcetype</font></code> value, such as <code><font size="2">sourcetype=access_combined</font></code>, the field extractor starts you <a href="#fxselectsampleeventstep" class="external text">at the Select Sample step</a>. 
</dd></dl><p><img alt="Dsh FX access search sidebar1.png" src="images/f/f3/Dsh_FX_access_search_sidebar1.png" width="323" height="229"></p>
<h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_from_the_all_fields_dialog_box"><span class="mw-headline" id="Access_the_field_extractor_from_the_All_Fields_dialog_box"> Access the field extractor from the All Fields dialog box </span></a></i></b></font></h4>
<p>When you use this method to access the field extractor it runs only against the set of events returned by the search that you have run. To get the full set of source types in your Splunk Enterprise instance, <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_field_extractions_page_in_settings" class="external text">go to the Field Extractions page in Settings</a>.
</p><p><b>1.</b> Run a search that returns events.
</p><p><b>2.</b> At the top of the fields sidebar, click <b>All Fields</b>.
</p><p><b>3.</b> In the All Fields dialog box, click <b>Extract new fields</b>.
</p>
<dl><dd> If your search string does not identify a <code><font size="2">sourcetype</font></code> value, the field extractor starts you <a href="#fxselectsourcetypestep" class="external text">at the Select Sourcetype step</a>.
</dd></dl><dl><dd> If your search string identifies a <code><font size="2">sourcetype</font></code> value, such as <code><font size="2">sourcetype=access_combined</font></code>, the field extractor starts you <a href="#fxselectsampleeventstep" class="external text">at the Select Sample step</a>. 
</dd></dl><p><img alt="Dsh FX access search selectfieldsdialog2.png" src="images/f/fa/Dsh_FX_access_search_selectfieldsdialog2.png" width="297" height="236"></p>
<h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_from_a_specific_event"><span class="mw-headline" id="Access_the_field_extractor_from_a_specific_event"> Access the field extractor from a specific event </span></a></i></b></font></h4>
<p>Use this method to select an event in your search results, and create a field extraction that:
</p>
<ul><li> Extracts one or more fields found in that event.
</li><li> Is tied to the source type of that event.
</li></ul><p>When you use this method to access the field extractor it runs only against the set of events returned by the search that you have run. To get the full set of source types in your Splunk Enterprise instance, <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_field_extractions_page_in_settings" class="external text">go to the Field Extractions page in Settings</a>.
</p><p><b>1.</b> Run a search that returns events.
</p><p><b>2.</b> Find an event that you want to extract fields from, and click the arrow symbol to the left of the timestamp to open it. 
</p><p><b>3.</b> Click <b>Event Actions</b>, and select <b>Extract Fields.</b>
</p>
<dl><dd> The field extractor starts you at <a href="#fxselectfieldsstep" class="external text">the Select Fields step</a>. You have already defined the source type and sample event.
</dd></dl><p><img alt="Dsh FX access search eventactions3.png" src="images/4/46/Dsh_FX_access_search_eventactions3.png" width="493" height="255"></p>
<h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_field_extractions_page_in_settings"><span class="mw-headline" id="Access_the_field_extractor_through_the_Field_Extractions_page_in_Settings">Access the field extractor through the Field Extractions page in Settings</span></a></i></b></font></h4>
<p>This entry method is available to all users.
</p><p><b>1.</b> Select <b>Settings &gt; Fields &gt; Field extractions</b>.
</p><p><b>2.</b> Click the <b>Open field extractor</b> button.
</p>
<dl><dd> The field extractor starts you <a href="#fxselectsourcetypestep" class="external text">at the Select Sourcetype step</a>.
</dd></dl><h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_home_page"><span class="mw-headline" id="Access_the_field_extractor_through_the_Home_page">Access the field extractor through the Home page</span></a></i></b></font></h4>
<p>This entry method is available only to users whose roles have the <code><font size="2">edit_monitor</font></code> capability, such as Admin. 
</p><p>On the Home page, click the <b>extract fields</b> link under the <b>Add Data</b> icon.
</p><p>The field extractor starts you <a href="#fxselectsourcetypestep" class="external text">at the Select Sourcetype step</a>.
</p><p><img alt="Dsh FX access home.png" src="images/b/b3/Dsh_FX_access_home.png" width="265" height="252"></p>
<h4><font size="3"><b><i> <a name="extractfieldsinteractivelywithifx_access_the_field_extractor_after_you_add_data"><span class="mw-headline" id="Access_the_field_extractor_after_you_add_data">Access the field extractor after you add data</span></a></i></b></font></h4>
<p>This entry method is available only to users whose roles have the <code><font size="2">edit_monitor</font></code> capability, such as Admin. 
</p><p>After you add data to Splunk Enterprise, use the field extractor to extract fields from that data, as long as it has a fixed source type. 
</p><p>For example: You add a file named <code><font size="2">vendors.csv</font></code> to your Splunk Enterprise instance and give it the custom source type <code><font size="2">vendors</font></code>. After you save this input, you can enter the field extractor and extract fields from the events associated with the <code><font size="2">vendors</font></code> source type. 
</p><p>On the other hand, here is another example: You create a monitor input for the <code><font size="2">/var/log</font></code> directory and select <b>Automatic</b> for the source type, meaning that Splunk Enterprise automatically determines the source type values of the data from that input on an event by event basis. When you save this input you do not get a prompt to extract fields from this new data input, because the events indexed from that directory can have a variety of source type values.
</p><p><b>1.</b> Enter the Add Data page. 
</p>
<dl><dd> See "How do you want to add data?" in the <i>Getting Data In</i> manual. 
</dd></dl><p><b>2.</b> Define a data input with a fixed source type. 
</p>
<dl><dd>This can be an existing source type or a custom source type that you define. See "View and set source types for event data" in the <i>Getting Data In</i> manual.
</dd></dl><p><b>3.</b> Save the new data input.
</p><p><b>4.</b> In the "File has been uploaded successfully" dialog box, click <b>Extract Fields</b>.
</p>
<dl><dd> The field extractor starts you <a href="#fxselectsampleeventstep" class="external text">at the Select Sample step</a>.
</dd></dl><a name="fxselectsourcetypestep"></a><h2> <a name="fxselectsourcetypestep_field_extractor:_select_sourcetype_step"><span class="mw-headline" id="Field_Extractor:_Select_Sourcetype_step"> Field Extractor: Select Sourcetype step</span></a></h2>
<p>In the Select Sourcetype step of the field extractor, select a <b>source type</b> for the field extraction. All field extractions defined by the field extractor utility are tied to a source type. 
</p><p><b>Note:</b> When you enter the field extractor after you run a search, the set of source types that you can choose from is limited to those discovered in the results returned by the search. To get the full set of source types in your Splunk Enterprise instance, <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_through_the_field_extractions_page_in_settings" class="external text">go to the Field Extractions page in Settings</a>.
</p><p><b>1.</b> Choose a <b>Source type</b> for your field extraction.
</p><p><b>2.</b> Click <b>Next</b> to go to <a href="#fxselectsampleeventstep" class="external text">the Select Sample step</a>.
</p><p>This screenshot is an example of the source type listing you see when you enter the field extractor from the Field extractions page in Settings.
</p><p><img alt="Dsh FX select sourcetype.png" src="images/e/ee/Dsh_FX_select_sourcetype.png" width="584" height="661"></p>
<h3> <a name="fxselectsourcetypestep_select_sourcetype_step_omission_conditions"><span class="mw-headline" id="Select_Sourcetype_step_omission_conditions">Select Sourcetype step omission conditions</span></a></h3>
<p>The field extractor bypasses the Select Sourcetype step when you define a source type before you enter the field extractor. This can happen when you enter the field extractor:
</p>
<ul><li> After running a search that identifies a source type, through the <b>Extract Fields</b> links <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_from_the_bottom_of_the_fields_sidebar" class="external text">in the fields sidebar</a> or <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_from_the_all_fields_dialog_box" class="external text">the All Fields dialog box</a>.
</li><li>  <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_from_a_specific_event" class="external text">From an event in search results</a>.
</li><li>  After you <a href="#extractfieldsinteractivelywithifx_access_the_field_extractor_after_you_add_data" class="external text">add a data input with a fixed source type.</a>
</li></ul><a name="fxselectsampleeventstep"></a><h2> <a name="fxselectsampleeventstep_field_extractor:_select_sample_event_step"><span class="mw-headline" id="Field_Extractor:_Select_Sample_Event_step"> Field Extractor: Select Sample Event step</span></a></h2>
<p>In the Select Sample step of the field extractor, select a sample event that contains values for the fields that you want to extract. 
</p><p><b>Note:</b> The field extractor bypasses this step and the preceding Select Sourcetype step when you enter the field extractor from an event in the results of a search. The field extractor uses the event you entered through as the sample event, and it uses the source type of the event as the source type for the field extraction.
</p><p><b>1.</b> Browse through the list of events to find an event that you want to select.
</p><p><b>2.</b> (Optional) If you do not see the event you are looking for, try to find it by filtering the event list with keywords.
</p>
<dl><dd> Enter keywords into the filter and click <b>Apply</b> to filter the event listing on those keywords. Remove filters by deleting keywords from the filter field and clicking <b>Apply</b>.
</dd></dl><p><b>3.</b> (Optional) Change the <b>Sample</b> value to a larger event set, such as <b>First 10,000</b> events or events from the <b>Last 24 hours</b>, to capture especially rare events.
</p>
<dl><dd> By default the event list displays the first 1000 events. This dataset may not be large enough to capture some rare events.
</dd></dl><p><b>4.</b> (Optional) Switch <b>All events</b> to <b>Rare events</b> or <b>Diverse events</b> to view events that fit into those categories.
</p><p><b>5.</b> Click on an event to select it.
</p><p><b>6.</b> Click <b>Next</b> to go to <a href="#fxselectfieldsstep" class="external text">the Select Fields step</a>.
</p><p>In this example, the filter finds events that contain the string <code><font size="2">POST</font></code> for the <code><font size="2">access_combined</font></code> source type. The selected event appears above the field list as white text on a blue background.
</p><p><img alt="Dsh FX select sample.png" src="images/8/87/Dsh_FX_select_sample.png" width="720" height="586"></p>
<a name="fxselectfieldsstep"></a><h2> <a name="fxselectfieldsstep_field_extractor:_select_fields_step"><span class="mw-headline" id="Field_Extractor:_Select_Fields_step"> Field Extractor: Select Fields step</span></a></h2>
<p>In the Select Fields step of the field extractor, highlight values in the sample event that you want the field extractor to extract as fields. 
</p><p>To improve the accuracy of your field extraction, you can optionally:
</p>
<ul><li> <a href="#fxselectfieldsstep_add_sample_events_to_expand_the_range_of_the_regular_expression" class="external text">Identify sample events</a> to expand the range of the regular expression. 
</li><li> <a href="#fxselectfieldsstep_identify_required_text_to_create_extractions_that_match_specific_event_patterns" class="external text">Identify a string of required text</a> to focus the field extraction on events that contain this text.
</li><li> <a href="#fxselectfieldsstep_preview_the_results_of_the_field_extraction" class="external text">Preview the results</a> returned by the regular expression.
</li><li> <a href="#fxselectfieldsstep_manually_edit_the_regular_expression" class="external text">Manually edit the regular expression</a>. 
</li></ul><h3> <a name="fxselectfieldsstep_identify_one_or_more_field_values"><span class="mw-headline" id="Identify_one_or_more_field_values"> Identify one or more field values </span></a></h3>
<p>Before you identify fields, make sure that they are not being extracted for this source type. See <a href="#fxselectfieldsstep_review_existing_fields" class="external text">"Review existing fields"</a>.
</p><p>You must identify at least one field value that you want to extract as a field. 
</p><p>If you identify two or more field values, the field extractor marks each value with a different highlight color. 
</p><p>Each time you highlight a field value and add it as a field extraction, the field extractor generates a regular expression. This regular expression matches events like the event you have selected and extracts values for the fields that you have identified. 
</p><p>When the field extractor generates a regular expression it runs the fields that belong to the selected source type against it and displays the results in the event listing. Events that appear to match the regular expression have a green check mark in the leftmost column while non-matching events get a red "x". Matching field values are highlit with the same colors in the sample event.
</p><p><b>1.</b> In the sample event, highlight a value that you want to extract as a field. 
</p>
<dl><dd> A dialog box with fields appears underneath the highlighted value.
</dd></dl><p><b>2.</b> Enter a name for the <b>Field Name</b> field. 
</p>
<dl><dd> Field names must start with a letter and contain only letters, numbers, and underscores.
</dd></dl><p><b>3.</b> Click <b>Add Extraction</b> to save the extraction.
</p><p><b>4.</b> (Optional) Repeat steps 1 through 3 until you identify all the values that you want to extract. 
</p>
<dl><dd> As you select more fields in an event for extraction there is a greater chance that the field extractor will be unable to generate a regular expression that can reliably extract all of the fields. You can improve the reliability of multifield extractions by <a href="#fxselectfieldsstep_add_sample_events_to_expand_the_range_of_the_regular_expression" class="external text">adding sample events</a> and <a href="#fxselectfieldsstep_identify_required_text_to_create_extractions_that_match_specific_event_patterns" class="external text">identifying required text</a>. You can also improve the regular expression by <a href="#fxselectfieldsstep_manually_edit_the_regular_expression" class="external text">editing it manually</a>.
</dd></dl><p><b>5.</b> (Optional) Remove or rename field extractions in the sample event by clicking on them and selecting an action of <b>Remove</b> or <b>Rename</b>.
<b>6.</b> Click <b>Next</b> to go to the <a href="#fxvalidatestep" class="external text">Validate Fields step</a>.
</p>
<h3> <a name="fxselectfieldsstep_add_sample_events_to_expand_the_range_of_the_regular_expression"><span class="mw-headline" id="Add_sample_events_to_expand_the_range_of_the_regular_expression">Add sample events to expand the range of the regular expression</span></a></h3>
<p>This action is optional for the Select Fields step.
</p><p>Sometimes you select a set of fields in your sample event and in the event list you find that events with those fields are not matched. This happens when the regular expression generated by the field extractor matches events with patterns similar to your sample event, but misses others that have slightly different patterns. 
</p><p>Try to expand the range of the regular expression by adding one of the missed events as an additional sample event. After you highlight the missed fields, the field extractor attempts to generate a new field extraction that encompasses both event patterns. 
</p><p><b>1.</b> In the field listing table, click an event that is not matched by the regular expression but which has values for all of the fields that you are extracting from your first sample event.
</p>
<dl><dd> Additional sample events have the greatest chance of improving the accuracy of the field extraction when their format or pattern closely matches that of the original sample event.
</dd><dd> The sample event you select appears under the original sample event. 
</dd></dl><p><b>2.</b> In the additional sample event, highlight the value for a field that you are extracting from the first sample event. 
</p><p><b>3.</b> Select the correct <b>Field Name</b>. 
</p>
<dl><dd> You see values only for fields that you identified in the first sample event.
</dd></dl><p><b>4.</b> Click <b>Add Extraction</b>.
</p>
<dl><dd> The field extractor attempts to expand the range of the regular expression so that it can find the field value in both event patterns. It matches the new regular expression against the event sample and displays the results in the event table.
</dd></dl><p><b>5.</b> (Optional) If you are extracting multiple fields, repeat steps 2 through 4 for each field.
</p>
<dl><dd> You do not need to highlight all of the fields that are highlighted in the first sample event. For example you may find that a more reliable field extraction results when the additional sample event only highlights one of the two fields highlighted in the original sample event.
</dd></dl><p><b>6.</b> (Optional) Add sample events.
</p><p><b>7.</b> (Optional) Remove sample events by clicking the "X" next to the event.
</p><p><img alt="Dsh FX select field add sample event.png" src="images/7/7a/Dsh_FX_select_field_add_sample_event.png" width="720" height="364"></p><p>The field extractor sometimes cannot build a regular expression that matches the sample events as well as the original sample event. You can address the situation by using one of these methods.
</p>
<ul><li> <b>Remove some of the fields you are trying to extract, if you are extracting multiple fields.</b> This action can result in a field extraction that works across all of your selected events. The first field values you should remove are those that are embedded within longer text strings. You can set up separate field extractions for the fields that you remove.
</li><li> <b>Define a  field extraction for each event pattern that contains the field values that you want to extract, using required text to set the extractions apart.</b> For information about required text, see the next topic.
</li></ul><h3> <a name="fxselectfieldsstep_identify_required_text_to_create_extractions_that_match_specific_event_patterns"><span class="mw-headline" id="Identify_required_text_to_create_extractions_that_match_specific_event_patterns">Identify required text to create extractions that match specific event patterns</span></a></h3>
<p>This action is optional for the Select Fields step.
</p><p>Sometimes a source type contains different kinds of events that contain the same field or fields that you want to extract. It can be difficult to design a single field extraction that matches multiple event patterns. One way to deal with this is to define a different field extraction for each event pattern. 
</p><p>You can focus the extraction to specific event patterns with required text. Required text behaves like a search filter. It is a string of text that must be present in the event for Splunk Enterprise to match it with the extraction. 
</p><p>For example, you might have event patterns for the <code><font size="2">access_combined</font></code> source type that are differentiated by the strings <code><font size="2">action=addtocart</font></code>, <code><font size="2">action=changequantity</font></code>, <code><font size="2">action=purchase</font></code>, and <code><font size="2">action=remove</font></code>. You can create four extractions, one for each string, that each extract the same fields, but which have a different string for required text. 
</p><p>You can also use required text to make sure that a value is extracted only from specific events. 
</p><p>There are two limits to required text definition: 
</p>
<ul><li> You can define only one string of required text for a single field extraction. 
</li><li> You cannot apply a required text string to a string of text that you highlighted as a field value, nor can you do the reverse.
</li></ul><p><b>1.</b> In the sample event, highlight the text you want to require.
</p><p><b>2.</b> Select <b>Require</b>.
</p><p><img alt="Dsh FX select field required text.png" src="images/1/12/Dsh_FX_select_field_required_text.png" width="283" height="221"></p><p><b>3.</b> Click <b>Add Required Text</b> to add the required text to the field extraction. 
</p><p><b>4.</b> (Optional) Remove required text in the sample event by clicking it and selecting <b>Remove Required Text</b>.
</p><p><img alt="Dsh FX select field fields defined overview.png" src="images/c/c1/Dsh_FX_select_field_fields_defined_overview.png" width="720" height="464"></p><p>This example shows a field extraction that extracts fields named <code><font size="2">http_method</font></code> (green) and <code><font size="2">status</font></code> (yellow) and which has <code><font size="2">action=purchase</font></code> defined as required text. In the field listing table, the first two events do not match the extraction, because they do not have the required text. The third event matches the regular expression and has the required text. It has highlighting that shows the extracted fields.
</p>
<h3> <a name="fxselectfieldsstep_preview_the_results_of_the_field_extraction"><span class="mw-headline" id="Preview_the_results_of_the_field_extraction">Preview the results of the field extraction</span></a></h3>
<p>This action is optional for the Select Fields and Validate Fields steps.
</p><p>The event list has features that you can use to inspect the accuracy of the field extraction. The list displays all of the events in the sample for the source type, by default. 
</p>
<ul><li> Use the left-most column to identify which events match the regular expression and which events do not. 
</li><li> If the regular expression matches a small percentage of the sample events, toggle the view to <b>Matches</b> to review the nonmatching events from the list. You can also select <b>Non-Matches</b> to see only the events that fail to match the regular expression.
</li><li> Click a field tab to see statistics for that field. Each field tab displays a bar chart showing the count of each value found for the field in the event sample, organized from highest to lowest. 
</li></ul><p><img alt="Dsh FX select field preview of status field.png" src="images/5/52/Dsh_FX_select_field_preview_of_status_field.png" width="720" height="319"></p>
<ul><li> Click a value in the chart to filter the field listing table on that value. For example, in the <code><font size="2">status</font></code> chart, a click on the <code><font size="2">503</font></code> value causes the field extractor to return to the main Preview field list view, with the filter set to <code><font size="2">status=503</font></code>. It lists only events with that <code><font size="2">status</font></code> value.
</li></ul><p><b>Note</b> If you find field values that have been incorrectly extracted, you can submit them as counterexamples in See <a href="#fxvalidatestep" class="external text">Validate Fields</a>, the next field extractor step.
</p>
<h3> <a name="fxselectfieldsstep_manually_edit_the_regular_expression"><span class="mw-headline" id="Manually_edit_the_regular_expression">Manually edit the regular expression</span></a></h3>
<p>This action is optional for the Select Fields and Validate Fields steps.
</p><p>You can manually edit the regular expression. However, doing this takes you out of the field extractor workflow. When you save your changes you go to the final Save step of the field extractor, where you name the field extraction, set its permissions, and save it. You cannot validate or fine-tune the extraction before you save it. 
</p><p><b>1.</b> Click <b>Show Regular Expression</b>. 
</p><p><b>2.</b> Click <b>Edit the Regular Expression</b>.
</p>
<dl><dd> If you do not want to exit the field extractor workflow, click the <b>Back</b> button at the top left of the page to return to the workflow. This button disappears after you begin editing the regular expression.
</dd></dl><p><b>3.</b> Edit the regular expression. 
</p><p><b>4.</b> Click <b>Preview</b> to match your edited extraction against the sample events. 
</p>
<dl><dd> Repeat steps 3 and 4 until the regular expression is matching events and extracting fields appropriately.
</dd></dl><p><b>5.</b> Click <b>Save</b> to save your new field extraction. 
</p>
<dl><dd> When you enter the Save step, click <b>Back</b> to continue editing the regular expression. The <b>Back</b> button disappears after you enter a name for the extraction or make permissions choices.
</dd></dl><p><img alt="Dsh FX select field manual regex.png" src="images/8/8e/Dsh_FX_select_field_manual_regex.png" width="720" height="410"></p><p>See <a href="#aboutsplunkregularexpressions" class="external text">"About Splunk Enterprise regular expressions,"</a> in this manual.
</p>
<h3> <a name="fxselectfieldsstep_review_existing_fields"><span class="mw-headline" id="Review_existing_fields">Review existing fields</span></a></h3>
<p>This is an optional action for the Select Fields and Validate Fields steps. 
</p><p>The field you want to extract may already be extracted for the source type that you have selected. You can determine whether this is so, and if so, whether it is being extracted from the event pattern that you want to extract the field from. 
</p><p><b>1.</b> Click <b>Existing fields</b> in the upper right of the screen. 
</p>
<dl><dd> The <b>Existing fields</b> button appears in the Select Fields, Validate Fields, and Save steps of the field extractor.
</dd></dl><dl><dd> The <b>Fields</b> sidebar opens. If any fields are extracted for the source type, they appear in a table. If you do not see the field that you have extracted, click the <b>X</b> in the corner to close the sidebar.
</dd></dl><p><img alt="Dsh FX existing fields.png" src="images/c/c4/Dsh_FX_existing_fields.png" width="425" height="286"></p>
<dl><dd> The field name may appear multiple times with different <b>Pattern Name</b> values. 
</dd></dl><p><b>2.</b> If the field that you want to extract appears in the table, click <b>open</b> to view detail information about its field extraction.
</p>
<dl><dd> This opens a page in a new tab. In this page you can inspect the regular expression for the field extraction, the events that it matches, and the field values that it extracts.
</dd></dl><p><b>3.</b> Review the <b>Regular Expression</b>, the events it matches, and the field values it extracts.
</p>
<dl><dd> If the events match the type of event event pattern that you are hoping to extract the field from and you can see that the field values are extracted correctly, you do not need to create a new field extraction.
</dd></dl><dl><dd> If the field extraction matches a different event pattern than the one you want to extract the field from, you can create a new extraction for the field as long as it has a unique <b>Pattern Name</b>.
</dd></dl><a name="fxvalidatestep"></a><h2> <a name="fxvalidatestep_field_extractor:_validate_fields_step"><span class="mw-headline" id="Field_Extractor:_Validate_Fields_step"> Field Extractor: Validate Fields step</span></a></h2>
<p>Validate your field extraction in the Validate Fields step of the field extractor. The field extractor provides the following validation methods:
</p>
<ul><li> <b>Review the event list table to see which events match or fail to match the field extraction.</b> See <a href="#fxselectfieldsstep_preview_the_results_of_the_field_extraction" class="external text">"Preview the results of the field extraction"</a>.
</li><li> <b>Report incorrect extractions to the field extractor by providing counterexamples.</b> In response, the field extractor attempts to improve the accuracy of the regular expression. 
</li><li> Manually edit the regular expression. See <a href="#fxselectfieldsstep_manually_edit_the_regular_expression" class="external text">"Manually edit the regular expression"</a>.
</li></ul><p>When you are done validating your field extractions, click <b>Save</b> to <a href="#fxsavestep_save" class="external text">save the extraction</a>.
</p>
<h3> <a name="fxvalidatestep_provide_counterexample_feedback"><span class="mw-headline" id="Provide_counterexample_feedback">Provide counterexample feedback</span></a></h3>
<p>This is an optional action for the Validate Fields step.
</p><p>If you find events that contain incorrectly extracted fields, submit those events as counterexample feedback. 
</p><p><b>1.</b> Find an event with a field value that has been incorrectly extracted. 
</p>
<dl><dd> The highlighted text is not a correct value for the field that the highlighter represents.
</dd></dl><p><b>2.</b> Click the "X" next to the incorrect field value. 
</p>
<dl><dd> The field extractor displays the counterexample event above the table, marking the incorrect value with red strikethrough. It also updates the regular expression and its preview results. 
</dd></dl><p><img alt="Dsh FX validate field counterexamples.png" src="images/f/ff/Dsh_FX_validate_field_counterexamples.png" width="720" height="406"></p><p><b>3.</b> If a counterexample does not help, remove it by clicking the "X" to the left of the counterexample event.
</p>
<a name="fxsavestep"></a><h2> <a name="fxsavestep_field_extractor:_save_step"><span class="mw-headline" id="Field_Extractor:_Save_step"> Field Extractor: Save step</span></a></h2>
<p>In the Save step of the field extractor you define the name of the new field extraction, set its permissions, and save the extraction. 
</p><p><b>1.</b> (Optional) Change the <b>Name</b> of the field extraction if you do not accept the default name given by the field extractor.
</p>
<dl><dd> The field extractor provides a default name for the field extraction made up of the field names that the field extraction extracts. You can keep this name if you want.
</dd></dl><p><b>2.</b> (Optional) Change the <b>Permissions</b> of the field extraction to either <b>App</b> or <b>All apps</b> and update the role-based read/write permissions.
</p>
<dl><dd> The field extraction is set to <b>Owner</b>, meaning that it only extracts fields in searches run by the person who created the extraction. 
</dd></dl><dl><dd> Set <b>Permissions</b> to <b>App</b> to make this extraction available only to users of the app that the field extraction belongs to. 
</dd></dl><dl><dd> Set <b>Permissions</b> to <i>All apps</i> to enable all users of all apps to benefit from this field extraction when they run searches. 
</dd></dl><dl><dd> When you change the app permissions to <b>App</b> or <b>All apps</b> you can set read and write permissions per role. See <a href="#manageknowledgeobjectpermissions" class="external text">"Manage knowledge object permissions,"</a> in this manual.
</dd></dl><p><b>3.</b> Click <b>Finish</b> to save the extraction.
</p><p>You can manage the field extractions that you create. They are listed on the Field Extractions page in Settings. See <a href="#managesearch-timefieldextractions" class="external text">"Use the Field Extractions page in Splunk Web,"</a> in this manual.
</p>
<a name="managesearch-timefieldextractions"></a><h2> <a name="managesearch-timefieldextractions_use_the_field_extractions_page_in_splunk_web"><span class="mw-headline" id="Use_the_Field_Extractions_page_in_Splunk_Web"> Use the Field Extractions page in Splunk Web </span></a></h2>
<p>Use the Field Extractions page to manage search-time <b>field extractions</b> that have been added to <code><font size="2">props.conf</font></code>. There are three methods by which you can add search-time field extractions to <code><font size="2">props.conf</font></code>. You can: 
</p>
<ul><li> <a href="#extractfieldsinteractivelywithifx" class="external text">Use the field extractor</a> to create extractions. This method is relatively easy and does not require you to understand how regular expressions work.
</li><li> <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">Make direct edits to <code><font size="2">props.conf</font></code></a>.
</li><li> Add new field extractions with the Field Extractions page (see below).
</li></ul><p>The Field Extractions page enables you to:
</p>
<ul><li> Review the overall set of search-time extractions that you have created or which your permissions enable you to see, for all Apps in your Splunk Enterprise instance. 
</li><li> Create new search-time field extractions.
</li><li> Update permissions for field extractions. Field extractions created through the field extractor and the Field extractions page are initially only available to their creators until they are shared with others.
</li><li> Delete field extractions, if your app-level permissions enable you to do so, and if they are not default extractions that were delivered with the product. Default knowledge objects cannot be deleted. For more information about deleting knowledge objects, see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects"</a> in this manual.
</li></ul><p>If you have "write" <b>permissions</b> for a particular search-time field extraction, the Field extractions page enables you to:
</p>
<ul><li> Update its regular expression, if it is an inline transaction.
</li><li> Add or delete named extractions that have been defined in <code><font size="2">transforms.conf</font></code> or the <a href="#managefieldtransforms" class="external text">Field transactions page in Splunk Web</a>, if it uses transactions.
</li></ul><p><b>Note:</b> You cannot manage index-time field extractions in Splunk Web. We don't recommend that you change your set of index-time field extractions, but if you find that you must do so, you have to modify your <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> configuration files manually. For more information about index-time field extraction configuration, see "Configure index-time field extractions" in the Getting Data In Manual.
</p><p>Navigate to the Field extractions page by selecting <b>Settings &gt; Fields &gt; Field extractions</b>.
</p>
<h3> <a name="managesearch-timefieldextractions_review_search-time_field_extractions_in_splunk_web"><span class="mw-headline" id="Review_search-time_field_extractions_in_Splunk_Web"> Review search-time field extractions in Splunk Web </span></a></h3>
<p>To better understand how the Field extractions page displays your field extraction, it helps to understand how field extractions are set up in your <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> files. 
</p><p>Field extractions can be set up entirely in <code><font size="2">props.conf</font></code>, in which case they are identified on the Field extractions page as <i>inline</i> field extractions. But some field extractions include a <code><font size="2">transforms.conf</font></code> component called a <b>field transform</b>. To create/edit that component of the field extraction via Splunk Web, you use the Field Transforms page in Splunk Web.
</p><p>For more information about transforms and the Field Transforms page, see <a href="#managefieldtransforms" class="external text">"Manage field transforms"</a> in this manual. 
</p><p>For more information about field extraction setup directly in the props.conf and transforms.conf files see <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time field extractions through configuration files"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="managesearch-timefieldextractions_name_column"><span class="mw-headline" id="Name_column"> Name column </span></a></i></b></font></h4>
<p>The <b>Name</b> column in the Field extractions page displays the overall name (or "class") of the field extraction, as it appears in <code><font size="2">props.conf</font></code>. In <code><font size="2">props.conf</font></code>, the field extraction format is:
</p><p><code><font size="2">&lt;spec&gt;&nbsp;: [EXTRACT-&lt;class&gt; | REPORT-&lt;class&gt;]</font></code>
</p>
<ul><li> <code><font size="2">&lt;spec&gt;</font></code> can be:
<ul><li> <code><font size="2">&lt;sourcetype&gt;</font></code>, the source type of an event.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event.
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event.
</li></ul></li></ul><p><code><font size="2">EXTRACT-&lt;class&gt;</font></code> field extractions are extractions that are <i>wholly defined</i> in <code><font size="2">props.conf</font></code> (in other words, they do <i>not</i> reference a transform in transforms.conf). They are created automatically by field extractions made through IFX and certain search commands. You can also add them <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">by making direct updates</a> to the <code><font size="2">props.conf</font></code> file. This kind of extraction is always associated with a field-extracting regular expression. On the Field Extractions page, this regex appears in the <b>Extraction/Transform</b> column.
</p><p><code><font size="2">REPORT-&lt;class&gt;</font></code> field extractions reference field transform stanzas in <code><font size="2">transforms.conf</font></code>. This is where their field-extracting regular expressions are located. On the Field Extractions page, the referenced field transform stanza is indicated in the "Extraction/Transform" column. 
</p><p>You can work with transforms in Splunk Web through the Field Transformations page. For more information see <a href="#managefieldtransforms" class="external text">"Use the Field Transformations page in Splunk Web"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="managesearch-timefieldextractions_type_column"><span class="mw-headline" id="Type_column"> Type column </span></a></i></b></font></h4>
<p>There are two field extraction types: <i>inline</i> and <i>transforms.conf</i>. 
</p>
<ul><li> <i>Inline</i> extractions always have <code><font size="2">EXTRACT-&lt;class&gt;</font></code> configurations. They are identified as such because they are entirely defined within <code><font size="2">props.conf</font></code>; they do not reference external field transforms. 
</li><li> <i>Uses transform</i> extractions always have <code><font size="2">REPORT-&lt;class&gt;</font></code> name configurations. As such they reference field transforms in <code><font size="2">transforms.conf</font></code>. You can define field transforms directly in <code><font size="2">transforms.conf</font></code> or via Splunk Web using the Field transformations page.
</li></ul><h4><font size="3"><b><i> <a name="managesearch-timefieldextractions_extraction_transform_column"><span class="mw-headline" id="Extraction_Transform_column"> Extraction Transform column </span></a></i></b></font></h4>
<p>In the <b>Extraction/Transform</b> column, Splunk Web displays different things depending on the field extraction <b>Type</b>. 
</p>
<ul><li> For <i>inline</i> extraction types, Splunk Web displays the regular expression that Splunk Enterprise uses to extract the field. The named group (or groups) within the regex show you what field(s) it extracts. 
</li></ul><dl><dd> For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test your regex by using it in a search with the rex search command.  Splunk Enterprise also maintains a list of useful third-party tools for writing and testing regular expressions.   
</dd></dl><ul><li> In the case of <i>Uses transform</i> extraction types, Splunk Web displays the name of the <code><font size="2">transforms.conf</font></code> field transform stanza (or stanzas) that the field extraction is linked to through <code><font size="2">props.conf</font></code>. A field extraction can reference multiple field transforms if you want to apply more than one field-extracting regex to the same source, source type, or host. This can be necessary in cases where the field or fields that you want to extract appear in two or more very different event patterns.
</li></ul><p>For example, the Expression column could display two values for a <i>Uses transform</i> extraction: <i>access-extractions</i> and <i>ip-extractions</i>. These may appear in <code><font size="2">props.conf</font></code> as:
</p>
<dl><dd> <code><font size="2">[access_combined]</font></code>
</dd><dd> <code><font size="2">REPORT-access = access-extractions, ip-extractions</font></code>
</dd></dl><dl><dd> In this example, <code><font size="2">access-extractions</font></code> and <code><font size="2">ip-extractions</font></code> are both names of field transform stanzas in <code><font size="2">transforms.conf</font></code>. To work with those field transforms through Splunk Web, <a href="#managefieldtransforms" class="external text">go to the Field transforms page</a>.
</dd></dl><h3> <a name="managesearch-timefieldextractions_add_new_field_extractions"><span class="mw-headline" id="Add_new_field_extractions"> Add new field extractions </span></a></h3>
<p>Click the <b>New</b> button at the top of the Field extractions page to add a new field extraction. The Add New page appears. 
</p><p>If you know how field extractions are set up in <code><font size="2">props.conf</font></code>, you should find this to be pretty simple. 
</p><p>All of the fields described below are required.
</p><p><b>1.</b> Define a <b>Destination app</b> context for the field extraction. By default it will be the app context you are currently in. 
</p><p><b>2.</b> Give the field extraction a <b>Name</b>, using underscores for spaces between words. In <code><font size="2">props.conf</font></code> this is the <code><font size="2">&lt;class&gt;</font></code> value for an EXTRACT or REPORT field extraction type. <b>Note:</b> <code><font size="2">&lt;class&gt;</font></code> values do not have to follow field name syntax restrictions (see the "<b>Important</b>" note below). You <i>can</i> use characters other than a-z, A-Z, and 0-9, and spaces are allowed. In addition <code><font size="2">&lt;class&gt;</font></code> values are not subject to "key cleaning."
</p><p><b>3.</b> Define the sourcetype, source, or host to which the extraction applies. Select <i>sourcetype</i>, <i>source</i>, or <i>host</i> and enter the value. This maps to the <code><font size="2">&lt;spec&gt;</font></code> value in <code><font size="2">props.conf</font></code>.
</p><p><b>4.</b> Define the extraction type. 
</p>
<dl><dd> If you select <i>Uses transform,</i> enter the transform(s) involved in the <b>Extraction/Transform</b> field, separated by commas. The transforms can then be created or updated <a href="#managefieldtransforms" class="external text">via the Field transforms page</a>.
</dd></dl><dl><dd> If you select <i>Inline,</i> enter the regular expression used to extract the field (or fields) in the <b>Extraction/Transform</b> field. For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test your regex by using it in a search with the rex search command.  Splunk also maintains a list of useful third-party tools for writing and testing regular expressions.   
</dd></dl><p><b>Important:</b> The capturing groups in your regex must identify field names that only contain alpha-numeric characters or underscores. 
</p>
<ul><li> Valid characters for field names are <b>a-z, A-Z, 0-9,</b> or <b>_</b> .
</li><li> Field names cannot begin with <b>0-9</b> or <b>_</b> . Leading underscores are reserved for Splunk Enterprise's internal variables.  
</li><li> International characters are not allowed.
</li></ul><p>Splunk Enterprise applies the following "key cleaning" rules to all extracted fields, either by default or through a custom configuration: 
</p>
<ul><li> All characters that are <b>not</b> in a-z, A-Z, and 0-9 ranges are replaced with an underscore (_). 
</li><li> All leading underscores and 0-9 characters are removed from extracted field names.
</li></ul><p>To disable this behavior for a specific field extraction, you have to manually modify both <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. For more information, see <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time field extractions through configuration files"</a> in this manual. 
</p><p><b>Note:</b> You cannot turn off key cleaning for inline field extractions (field extractions that do not require a field transform component) without editing the extraction stanza in <code><font size="2">props.conf</font></code>. 
</p>
<h4><font size="3"><b><i> <a name="managesearch-timefieldextractions_example_-_add_a_new_error_code_field"><span class="mw-headline" id="Example_-_Add_a_new_error_code_field"> Example - Add a new error code field </span></a></i></b></font></h4>
<p>This shows how you would define an extraction for a new <code><font size="2">err_code</font></code> field. The field can be identified by the occurrence of <code><font size="2">device_id=</font></code> followed by a word within brackets and a text string terminating with a colon. The field should be extracted from events related to the <code><font size="2">testlog</font></code> source type.
</p><p>In <code><font size="2">props.conf</font></code> this extraction would look like:
</p>
<code><font size="2"><br>[testlog]<br>EXTRACT-errors = device_id=\[w+\](?&lt;err_code&gt;[^:]+)<br></font></code>
<p>Here's how you would set that up through the Add new field extractions page:
</p><p><img alt="300p" src="images/f/f4/60NewFieldExtraction.png" width="772" height="456"></p><p><b>Note:</b> You can find a version of this example in <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time field extractions"</a> topic in this manual, which shows you how to set up field extractions using the <code><font size="2">props.conf</font></code> file.
</p>
<h3> <a name="managesearch-timefieldextractions_update_existing_field_extractions"><span class="mw-headline" id="Update_existing_field_extractions"> Update existing field extractions </span></a></h3>
<p>To edit an existing field extraction, click its name in the <b>Name</b> column. 
</p><p><img alt="EditFieldExtractions.png" src="images/2/2d/EditFieldExtractions.png" width="782" height="236"></p><p>This takes you to a details page for that field extraction. In the <b>Extraction/Transform</b> field what you can do depends on the type of extraction that you are working with. 
</p>
<ul><li> If the field extraction is an inline extraction, you can edit the regular expression it uses to extract fields. 
</li><li> If the field extraction uses one or more transforms, you can update the transform or transforms involved (put them in a comma-separated list if there is more than one.) The transforms can then be created or updated <a href="#managefieldtransforms" class="external text">via the Field transforms page</a>.
</li></ul><p>The field extraction depicted above is uses three transforms named <code><font size="2">wel-message</font></code>, <code><font size="2">wel-eq-kv</font></code>, and <code><font size="2">wel-col-kv</font></code>. To find out more about how these transforms are set up, navigate to <b>Settings &gt; Fields &gt; Field Transformations</b> or go straight to <code><font size="2">transforms.conf</font></code>.
</p><p><b>Note:</b> <i>Uses transform</i> field extractions must include at least one valid <code><font size="2">transforms.conf</font></code> field extraction stanza name.
</p>
<h3> <a name="managesearch-timefieldextractions_update_field_extraction_permissions"><span class="mw-headline" id="Update_field_extraction_permissions"> Update field extraction permissions </span></a></h3>
<p>When a field extraction is created through an inline method (such as IFX or a search command) it is initially only available to its creator. To make it so that other users can use the field extraction, you need to update its <b>permissions</b>. To do this, locate the field extraction on the Field extractions page and select its <b>Permissions</b> link. This opens the standard permission management page used in Splunk Web for <b>knowledge objects</b>. 
</p><p>On this page you can set up <b>role-based</b> permissions for the field extraction, and determine whether it is available to users of one specific App, or globally to users of all Apps. For more information about managing permissions with Splunk Web, see <a href="#manageknowledgeobjectpermissions" class="external text">"Manage knowledge object permissions,"</a> in this manual.
</p>
<h3> <a name="managesearch-timefieldextractions_delete_field_extractions"><span class="mw-headline" id="Delete_field_extractions"> Delete field extractions </span></a></h3>
<p>On the Field extractions page in Splunk Web, you can delete field extractions if your permissions enable you to do so. You won't be able to delete default field extractions (extractions that were delivered with the product and which are stored in the "default" directory of an app). 
</p><p>Click <b>Delete</b> for the field extraction that you want to remove.
</p><p><b>Note:</b> Take care when deleting objects that have downstream dependencies. For example, if your field extraction is used in a search that in turn is the basis for an event type that is used by five other saved searches (two of which are the foundation of dashboard panels), all of those other knowledge objects will be negatively impacted by the removal of that extraction from the system. For more information about deleting knowledge objects, see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects"</a> in this manual.
</p>
<a name="managefieldtransforms"></a><h2> <a name="managefieldtransforms_use_the_field_transformations_page_in_splunk_web"><span class="mw-headline" id="Use_the_Field_transformations_page_in_Splunk_Web"> Use the Field transformations page in Splunk Web</span></a></h2>
<p>The Field transformations page in Splunk Web lets you manage the <b>field transform</b> components of search-time field extractions, which reside in <code><font size="2">transforms.conf</font></code>. Field transforms can be created either through direct edits to <code><font size="2">transforms.conf</font></code> or by addition through the Field transformations page. 
</p><p><b>Note:</b> Every field transform has at least one field extraction component. But "inline" field extractions <i>do not</i> need to have a field transform component.
</p><p>The Field transformations page enables you to:
</p>
<ul><li> Review the overall set of field transforms that you have created or which your permissions enable you to see, for all Apps in your Splunk Enterprise instance. 
</li><li> Create new search-time field transforms. For more information about situations that call for the use of field transforms, see "When to use the Field transformations page," below.
</li><li> Update permissions for field transforms. Field transforms created through the Field transformations page are initially only available to their creators until they are shared with others. You can only update field transform permissions if you own the transform, or if your role's permissions enable you to do so.
</li><li> Delete field transforms, if your app-level permissions enable you to do so, and if they are not default field transforms that were delivered with the product. Default knowledge objects cannot be deleted. For more information about deleting knowledge objects, see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects"</a> in this manual.
</li></ul><p>If you have "write" permissions for a particular field transform, the Field transformations page enables you to:
</p>
<ul><li> Update its regular expression and change the key the regular expression applies to. 
</li><li> Define or update the field transform format. 
</li></ul><p>Navigate to the Field transformations page by selecting <b>Settings &gt; Fields &gt; Field transformations</b>.
</p>
<h3> <a name="managefieldtransforms_why_set_up_a_field_transform_for_a_field_extraction.3f"><span class="mw-headline" id="Why_set_up_a_field_transform_for_a_field_extraction.3F"> Why set up a field transform for a field extraction? </span></a></h3>
<p>While you can define most search-time field extractions entirely within <code><font size="2">props.conf</font></code> (or the Field extractions page in Splunk Web), some advanced search-time field extractions require a <code><font size="2">transforms.conf</font></code> component called a <b>field transform</b>. This component can be defined and managed through the Field transforms page. 
</p><p>You set up search-time field extractions with a field transform component when you need to:
</p>
<ul><li> <b>Reuse the same field-extracting regular expression across multiple sources, source types, or hosts</b> (in other words, configure one field transform that is referenced by multiple field extractions). If you find yourself using the same regex to extract fields for different sources, source types, and hosts, you may want to set it up as a transform. Then, if you find that you need to update the regex, you only have to do so once, even though it is used by more than one field extraction.
</li><li> <b>Apply more than one field-extracting regular expression to the same source, source type, or host</b> (in other words, apply multiple field transforms to the same field extraction). This is sometimes necessary in cases where the field or fields that you want to extract from a particular source/source type/host appear in two or more very different event patterns.
</li><li> <b>Use a regular expression to extract fields from the values of another field</b> (also referred to as a "source key"). For example, you might pull a string out of a <code><font size="2">url</font></code> field value, and have that be a value of a new field.
</li></ul><p>You can do more things with search-time field transforms (such as setting up delimeter based field extractions and configuring extractions for multivalue fields) if you configure them directly within <code><font size="2">transforms.conf</font></code>. See the section on field transform setup in <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">"Create and maintain search-time field extractions through configuration files"</a> in this manual for more information.
</p><p><b>Note:</b> <i>All</i> index-time field extractions are coupled with one or more field transforms. You cannot manage index-time field extractions in Splunk Web, however--you have to use the <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> configuration files. We don't recommend that you change your set of index-time field extractions under normal circumstances, but if you find that you must do so, see "Create custom fields at index-time" in the Getting Data In manual.
</p>
<h3> <a name="managefieldtransforms_review_and_update_search-time_field_transforms_in_splunk_web"><span class="mw-headline" id="Review_and_update_search-time_field_transforms_in_Splunk_Web"> Review and update search-time field transforms in Splunk Web </span></a></h3>
<p>To better understand how the Field transformations page in Splunk Web displays your field transforms, it helps to understand how search-time field extractions are set up in your <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> files. 
</p><p>A typical field transform looks like this in <code><font size="2">transforms.conf</font></code>:
</p>
<code><font size="2"><br>[banner]<br>REGEX = /js/(?&lt;license_type&gt;[^/]*)/(?&lt;version&gt;[^/]*)/login/(?&lt;login&gt;[^/]*)<br>SOURCE_KEY = uri<br></font></code>
<p>This transform matches its regex against <code><font size="2">uri</font></code> field values, and extracts three fields as named groups: <code><font size="2">license_type</font></code>, <code><font size="2">version</font></code>, and <code><font size="2">login</font></code>. 
</p><p>In <code><font size="2">props.conf</font></code>, that transform is matched to the source <code><font size="2">.../banner_access_log*</font></code> like so: 
</p>
<code><font size="2"><br>[source::.../banner_access_log*]<br>REPORT-banner = banner<br></font></code>
<p>This means the regex is only matched to <code><font size="2">uri</font></code> fields in events coming from the <code><font size="2">.../banner_access_log</font></code> source.
</p><p>But you can match it to other sources, sourcetypes, and hosts if necessary. This is something you can't do with inline field extractions (field extractions set up entirely within <code><font size="2">props.conf</font></code>). 
</p><p><b>Note:</b> By default, transforms are matched to a <code><font size="2">SOURCE_KEY</font></code> value of <code><font size="2">_raw</font></code>, in which case their regexes are applied to the entire event, not just fields within that event.
</p>
<h4><font size="3"><b><i> <a name="managefieldtransforms_the_name_column"><span class="mw-headline" id="The_Name_column"> The Name column </span></a></i></b></font></h4>
<p>The <b>Name</b> column of the Field transformations page displays the names of the search-time field transforms that your permissions enable you to see. These names are the actual stanza names for field transforms in <code><font size="2">transforms.conf</font></code>. The transform example presented above would appear in the list of transforms as <code><font size="2">banner</font></code>.
</p><p>Click on a transform name to see the detail information for that particular transform. 
</p>
<h4><font size="3"><b><i> <a name="managefieldtransforms_reviewing_and_editing_transform_details"><span class="mw-headline" id="Reviewing_and_editing_transform_details"> Reviewing and editing transform details </span></a></i></b></font></h4>
<p>The details page for a field transform enables you to view and update its regular expression, key, and event format. Here's the details page for the <code><font size="2">banner</font></code> transform that we described at the start of this subtopic:
</p><p><img alt="NewFieldTransformation.png" src="images/3/32/NewFieldTransformation.png" width="668" height="472"></p><p>If you have the permissions to do so, you can edit the regex, key, and event format. Keep in mind that these edits can affect multiple field extractions defined in <code><font size="2">props.conf</font></code> and the Field extractions page, if the transform has been applied to more than one source, sourcetype, or host.
</p>
<h3> <a name="managefieldtransforms_create_a_new_field_transform"><span class="mw-headline" id="Create_a_new_field_transform"> Create a new field transform </span></a></h3>
<p>To create a new field transform:
</p><p><b>1.</b> First, navigate to the Field transformations page and click the <b>New</b> button. 
</p><p><b>2.</b> Identify the <b>Destination app</b> for the field transform, if it is not the app you are currently in. 
</p><p><b>3.</b> Give the field transform a <b>Name</b>. 
</p>
<dl><dd> This equates to the stanza name for the transform on <code><font size="2">transforms.conf</font></code>. When you save this transform this is the name that appears in the <b>Name</b> column on the Field transformations page. (This is a required field.)
</dd></dl><p><b>4.</b> Enter a <b>Regular expression</b> for the transform.
</p>
<dl><dd> See <a href="#managefieldtransforms_regular_expression_syntax_and_usage" class="external text">"Regular expression syntax and usage."</a>
</dd></dl><p><b>5.</b> (Optional) Define a <b>Key</b> for the transform. 
</p>
<dl><dd> This corresponds to the <code><font size="2">SOURCE_KEY</font></code> option in <code><font size="2">transforms.conf</font></code>. By default it is set to <code><font size="2">_raw</font></code>, which means the regular expression is applied to entire events. 
</dd></dl><dl><dd> To have the regular expression be applied to values of a specific field, replace <code><font size="2">_raw</font></code> with the name of that field. You can only use fields that are present when the field transform is executed. 
</dd></dl><p><b>6.</b> (Optional) Specify the <b>Event format.</b> 
</p>
<dl><dd> This corresponds to the <code><font size="2">FORMAT</font></code> option in <code><font size="2">transforms.conf</font></code>. You use <code><font size="2">$n</font></code> to indicate groups captured by the regular expression. For example, if the regular expression you've designed captures two groups, you could have a <b>Format</b> set up like this: <code><font size="2">$1::$2</font></code>, where the first group is the field name, and the second group is the field value. Or you could set <b>Format</b> up as <code><font size="2">username::$1 userid::$2</font></code>, which means the regular expression extracts the values for the <code><font size="2">username</font></code> and <code><font size="2">userid</font></code> fields. The Format field defaults to &lt;transform_stanza_name&gt;::$1. 
</dd></dl><p><b>7.</b> (Optional) Select <b>Create multivalue fields</b> if the same field can be extracted from your events more than once. 
</p>
<dl><dd> This causes Splunk Enterprise to extract the field as a single multivalue field<b>. </b>
</dd></dl><p><b>8.</b> (Optional) Select <b>Automatically clean field names</b> to ensure that the fields that Splunk Enterprise extracts have valid names. 
</p>
<dl><dd> This means that it removes leading underscore characters and 0-9 numerical characters from field names, and that it replaces characters other than those falling within the a-z, A-Z, and 0-9 ranges in field names with underscores. See <a href="#managefieldtransforms_regular_expression_syntax_and_usage" class="external text">"Regular expression syntax and usage."</a>
</dd></dl><h4><font size="3"><b><i> <a name="managefieldtransforms_regular_expression_syntax_and_usage"><span class="mw-headline" id="Regular_expression_syntax_and_usage">Regular expression syntax and usage</span></a></i></b></font></h4>
<p>For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test your regular expression by using it in a search with the rex search command.  Splunk Enterprise also maintains a list of useful third-party tools for writing and testing regular expressions.   
</p><p><b>Important:</b> The capturing groups in your regular expression must identify field names that contain alpha-numeric characters or an underscore. 
</p>
<ul><li> Valid characters for field names are <b>a-z, A-Z, 0-9,</b> or <b>_</b> .
</li><li> Field names cannot begin with <b>0-9</b> or <b>_</b> . Leading underscores are reserved for Splunk Enterprise's internal variables.  
</li><li> International characters are not allowed.
</li></ul><p>When <b>Automatically clean field names</b> is selected for a field transform, Splunk Enterprise applies the following "key cleaning" rules to field names extracted by that transform: 
</p>
<ul><li> All characters that are not in a-z, A-Z, and 0-9 ranges are replaced with an underscore (_). 
</li><li> All leading underscores and 0-9 characters are removed from the name.
</li></ul><p><b>Note:</b> You cannot turn off key cleaning for inline field extractions (field extractions that do not require a field transform component).
</p>
<h4><font size="3"><b><i> <a name="managefieldtransforms_example_-_extract_both_field_names_and_their_corresponding_field_values_from_an_event"><span class="mw-headline" id="Example_-_Extract_both_field_names_and_their_corresponding_field_values_from_an_event"> Example - Extract both field names and their corresponding field values from an event </span></a></i></b></font></h4>
<p>You can use the <b>Event format</b> attribute in conjunction with a properly designed regular expression to set up a field transform that extracts both a field name and its corresponding field value from each matching event.
</p><p>Here's an example, using a transform that is delivered with Splunk Enterprise. 
</p><p>The <code><font size="2">bracket-space</font></code> field transform has a regular expression that finds field name/value pairs within brackets in event data. It will reapply this regular expression until all of the matching field/value pairs in an event are extracted. 
</p><p><img alt="BracketSpaceExample.png" src="images/7/73/BracketSpaceExample.png" width="696" height="482"></p><p>As we stated earlier in this topic, field transforms are <i>always</i> associated with a field extraction. On the Field Extractions page in Splunk Web, you can see that the <code><font size="2">bracket-space</font></code> field transform is associated with the <code><font size="2">osx-asl:REPORT-asl</font></code> extraction.
</p>
<h3> <a name="managefieldtransforms_update_field_transform_permissions"><span class="mw-headline" id="Update_field_transform_permissions"> Update field transform permissions </span></a></h3>
<p>When a field transform is first created, by default it is only available to its creator. To make it so that other users can use the field transform, you need to update its <b>permissions</b>. To do this, locate the field transform on the Field transformations page and select its <b>Permissions</b> link. This opens the standard permission management page used in Splunk Web for <b>knowledge objects</b>. 
</p><p>On this page you can set up <b>role-based</b> permissions for the field transform, and determine whether it is available to users of one specific App, or globally to users of all Apps. For more information about managing permissions with Splunk Web, see <a href="#manageknowledgeobjectpermissions" class="external text">"Manage knowledge object permissions,"</a> in this manual.
</p>
<h3> <a name="managefieldtransforms_delete_field_transforms"><span class="mw-headline" id="Delete_field_transforms"> Delete field transforms </span></a></h3>
<p>On the Field transformations page in Splunk Web, you can delete field transforms if your permissions enable you to do so. 
</p><p>Click <b>Delete</b> for the field extraction that you want to remove.
</p><p><b>Note:</b> Take care when deleting knowledge objects that have downstream dependencies. For example, if the field extracted by your field transform is used in a search that in turn is the basis for an event type that is used by five other reports (two of which are the foundation of dashboard panels), all of those other knowledge objects will be negatively impacted by the removal of that transform from the system. For more information about deleting knowledge objects, see <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects"</a> in this manual.
</p>
<a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles"></a><h2> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_create_and_maintain_search-time_field_extractions_through_configuration_files"><span class="mw-headline" id="Create_and_maintain_search-time_field_extractions_through_configuration_files"> Create and maintain search-time field extractions through configuration files</span></a></h2>
<p>While you can set up and manage search-time <b>field extractions</b> via Splunk Web, it's important to understand how they are handled at the <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> level, because those are the configuration files that the Field extractions and Field transformations pages in Splunk Web read from and write to. 
</p><p>Many knowledge managers, especially those who have been using Splunk Enterprise for some time, find it easier to manage their custom fields through configuration files, which can be used to add, maintain, and review libraries of custom field additions for their teams. The configuration files also enable a wider range of field extraction options than you'll get with the Settings pages for field extraction. 
</p><p>This topic shows you how you can:
</p>
<ul><li> Set up basic "inline" search-time field extractions through edits to <code><font size="2">props.conf</font></code>.
</li><li> Design more complex search-time field extractions through a combination of edits to <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>.
</li></ul><h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_regular_expressions_and_field_name_syntax"><span class="mw-headline" id="Regular_expressions_and_field_name_syntax"> Regular expressions and field name syntax </span></a></h3>
<p>Splunk Enterprise uses regular expressions, or regexes, to extract fields from event data. When you use the interactive field extractor (IFX), Splunk Enterprise attempts to generate field-extracting regexes for you, but it can only create regular expressions that extract one field at a time from the events that match them. 
</p><p>On the other hand, when you set up field extractions manually through configuration files, you have to provide the regex yourself--but you can design them so that they extract two or more fields from the events that match them, if necessary.
</p><p>For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test your regex by using it in a search with the rex search command.  Splunk Enterprise also maintains a list of useful third-party tools for writing and testing regular expressions.   
</p><p><b>Important:</b> The capturing groups in your regex must identify field names that contain alpha-numeric characters or an underscore. See "Use proper field name syntax," below.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_use_proper_field_name_syntax"><span class="mw-headline" id="Use_proper_field_name_syntax"> Use proper field name syntax </span></a></i></b></font></h4>
<p>Splunk Enterprise only accepts field names that contain alpha-numeric characters or an underscore: 
</p>
<ul><li> Valid characters for field names are <b>a-z, A-Z, 0-9,</b> or <b>_</b> .
</li><li> Field names cannot begin with <b>0-9</b> or <b>_</b> . Leading underscores are reserved for Splunk Enterprise's internal variables.  
</li><li> International characters are not allowed.
</li></ul><p>Splunk Enterprise applies the following "key cleaning" rules to all extracted fields when they are extracted at search time, either by default or through a custom configuration: 
</p><p><b>1.</b> All characters that are <b>not</b> in a-z, A-Z, and 0-9 ranges are replaced with an underscore (_). 
</p><p><b>2.</b> When key cleaning is enabled (it is enabled by default), Splunk Enterprise removes all leading underscores and 0-9 characters from extracted fields. 
</p><p>You can disable key cleaning for a particular search-time field extraction by configuring it as an advanced REPORT extraction type, and then having the referenced field transform stanza include the setting <code><font size="2">CLEAN_KEYS=false</font></code>. See below for more information about the REPORT extraction configuration. 
</p><p><b>Note:</b> You cannot turn off key cleaning for basic EXTRACT (<code><font size="2">props.conf</font></code> only) field extraction configurations.
</p>
<h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_create_basic_search-time_field_extractions_with_props.conf_edits"><span class="mw-headline" id="Create_basic_search-time_field_extractions_with_props.conf_edits"> Create basic search-time field extractions with props.conf edits </span></a></h3>
<p>You can create basic search-time field extractions (field extractions that are defined entirely within <code><font size="2">props.conf</font></code>, as opposed to extractions that reference <b>field transforms</b> in <code><font size="2">transforms.conf</font></code>) by editing the <code><font size="2">props.conf</font></code> configuration file. You can find <code><font size="2">props.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>. (We recommend using the latter directory if you want to make it easy to transfer your data customizations to other search servers.)
</p><p><b>Note:</b> <i><b>Do not</b></i> edit files in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>.
</p><p>For more information on configuration files in general, see "About configuration files" in the Admin Manual.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_steps_for_defining_basic_search-time_field_extractions_with_props.conf"><span class="mw-headline" id="Steps_for_defining_basic_search-time_field_extractions_with_props.conf"> Steps for defining basic search-time field extractions with props.conf </span></a></i></b></font></h4>
<p>Basic search-time field extractions use the EXTRACT extraction configuration in <code><font size="2">props.conf</font></code>. Each EXTRACT extraction stanza contains the regular expression that Splunk Enterprise uses to extract one or more fields at search time, as well as other attributes that govern the manner in which those fields are extracted. 
</p><p>Follow these steps when you create a basic search-time field extraction: 
</p><p><b>1.</b> All extraction configurations in <code><font size="2">props.conf</font></code> are restricted by a specific source, source type, or host. Start by identifying the source type, source, or host that provide the events that your field should be extracted from. 
</p><p><b>Note:</b> For information about hosts, sources, and sourcetypes, see "About default fields (host, source, source type, and more)" in the Getting Data In manual.
</p><p><b>2.</b> Create a regular expression that identifies the field in the event. Use named capturing groups to provide the field names for the extracted values. Use the field name syntax as described in the preceding sections.
</p><p><b>3.</b> Follow the format for the EXTRACT field extraction type (defined in the next section) to create a field extraction stanza in <code><font size="2">props.conf</font></code> that includes the host/source/sourcetype and regex that you have identified. Edit the <code><font size="2">props.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.  
</p><p><b>Note:</b> <i><b>Do not</b></i> edit files in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>.
</p><p><b>4.</b> If your field value is a portion of a word, you must also add an entry to <code><font size="2">fields.conf</font></code>. See the example "Create a field from a subtoken" below.
</p><p><b>5.</b> Restart Splunk Enterprise for your changes to take effect.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_add_an_extract_field_extraction_stanza_to_props.conf"><span class="mw-headline" id="Add_an_EXTRACT_field_extraction_stanza_to_props.conf"> Add an EXTRACT field extraction stanza to props.conf </span></a></i></b></font></h4>
<p>Follow this format when adding an EXTRACT field extraction to <code><font size="2">props.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;spec&gt;]<br>EXTRACT-&lt;class&gt; = [&lt;regular_expression&gt;|&lt;regular_expression&gt; in &lt;source_field&gt;]<br></font></code></div>
<ul><li> <code><font size="2">&lt;spec&gt;</font></code> can be:
<ul><li> <code><font size="2">&lt;source type&gt;</font></code>, the source type of an event.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event.
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event.
</li><li> <code><font size="2">rule::&lt;rulename&gt;</font></code>, where <code><font size="2">&lt;rulename&gt;</font></code> is the unique name of a source type classification rule.
</li><li> <code><font size="2">delayedrule::&lt;rulename&gt;</font></code>, where <code><font size="2">&lt;rulename&gt;</font></code> is a unique name of a delayed source type classification rule. 
</li></ul></li></ul><p><b>Note:</b> <code><font size="2">rule</font></code> and <code><font size="2">delayedrule</font></code> are only considered as a last resort before generating a new source type based on the source that Splunk Enterprise sees.
</p>
<ul><li> <code><font size="2">&lt;class&gt;</font></code> is a unique literal string that identifies the namespace of the field (key) you're extracting. 
<ul><li> <b>Note:</b> <code><font size="2">&lt;class&gt;</font></code> values do not have to follow <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_use_proper_field_name_syntax" class="external text">field name syntax restrictions</a> (see above). You can use characters other than a-z, A-Z, and 0-9, and spaces are allowed. <code><font size="2">&lt;class&gt;</font></code> values are not subject to key cleaning.
</li></ul></li><li> The <code><font size="2">&lt;regular_expression&gt;</font></code> is required to have named capturing groups; each group represents a different extracted field. When the &lt;regular_expression&gt; matches an event, the named capturing groups and their values are added to the event. 
</li><li> Use <code><font size="2">&lt;regular_expression&gt; in &lt;source_field&gt;</font></code> to match the regex against the values of a specific field. Otherwise it matches against <code><font size="2">_raw</font></code> (all raw event data). 
<ul><li> <b>Note:</b> <code><font size="2">&lt;src_field&gt;</font></code> is a field name, which means it must follow field name syntax. It can only contain alphanumeric characters (a-z, A-Z, and 0-9).
</li></ul></li><li> If your regex needs to end with <code><font size="2">in &lt;string&gt;</font></code> where <code><font size="2">&lt;string&gt;</font></code> is <b>not</b> a field name, change the regex to end with <code><font size="2">[i]n &lt;string&gt;</font></code> to ensure that Splunk Enterprise doesn't try to match <code><font size="2">&lt;string&gt;</font></code> to a field name.
</li></ul><p>Precedence rules for the EXTRACT field extraction type:
</p>
<ul><li> For each field extraction, Splunk Enterprise takes the configuration from the highest precedence configuration stanza.
</li><li> When there are multiple categories of matching <code><font size="2">[&lt;spec&gt;]</font></code> stanzas, <code><font size="2">[host::&lt;host&gt;]</font></code> settings override <code><font size="2">[&lt;sourcetype&gt;]</font></code> settings.
</li><li> <code><font size="2">[source::&lt;source&gt;]</font></code> settings override both  <code><font size="2">[host::&lt;host&gt;]</font></code> and <code><font size="2">[&lt;sourcetype&gt;]</font></code> settings. 
</li><li> Similarly, if a particular field extraction is specified in <code><font size="2">../local/</font></code> for a <code><font size="2">&lt;spec&gt;</font></code>, it overrides that class  in ../default/.
</li></ul><p>There's more to <code><font size="2">[&lt;spec&gt;]</font></code> stanza precedence; see <code><font size="2">props.conf.spec</font></code> for all the details.
</p><p><b>Note:</b> Unlike the procedure for configuring the default set of fields that Splunk Enterprise extracts at index time, <code><font size="2">transforms.conf</font></code> requires no <code><font size="2">DEST_KEY</font></code> because nothing is being written to the index during search-time field extraction. Fields extracted at search time are not persisted in the index as keys.
</p><p>Splunk Enterprise follows precedence rules when it runs search-time field extractions. It runs inline field extractions (<code><font size="2">EXTRACT-&lt;class&gt;</font></code>) first, and then runs field extractions that reference field transforms (<code><font size="2">REPORT-&lt;class&gt;</font></code>).
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_setting_kv_mode_for_search-time_data"><span class="mw-headline" id="Setting_KV_MODE_for_search-time_data">Setting KV_MODE for search-time data</span></a></i></b></font></h4>
<p>You can use the <code><font size="2">KV_MODE</font></code> attribute to specify the field/value extraction mode for your data. You can add KV_MODE to an <code><font size="2">EXTRACT</font></code> or <code><font size="2">REPORT</font></code> stanza. Its format is:
</p>
<div class="samplecode">
<code><font size="2"><br>KV_MODE = [none|auto|multi|json|xml]<br></font></code></div>
<ul><li> <code><font size="2">none</font></code>: Disables field extraction for the source, source type, or host identified by the stanza name. You can use this setting to ensure that other regexes that you have created are not overridden by automatic field/value extraction for a particular source, source type, or host. You can also use this setting to increase search performance by disabling extraction for common but nonessential fields. We have some field extraction examples at the end of this topic that demonstrate the disabling of field extraction in different circumstances.
</li><li> <code><font size="2">auto</font></code>: Extracts field/value pairs and separates them with equal signs. This is the default field extraction behavior if you do not include this attribute in your field extraction stanza.
</li><li> <code><font size="2">auto_escaped</font></code>: Extracts field/value pairs and separates them with equal signs. In addition, this setting ensures that Splunk Enterprise honors \" and \\ as escaped sequences within quoted values. For example: <code><font size="2">field="value with \"nested\" quotes"</font></code>.
</li><li> <code><font size="2">multi</font></code>: This invokes the <code><font size="2">multikv</font></code> search command, which extracts field values from table-formatted events.
</li><li> <code><font size="2">xml</font></code>: Use this setting if you intend to use the field extraction stanza to extract fields from XML data.
</li><li> <code><font size="2">json</font></code>: Use this setting if you intend to use the field extraction stanza to extract fields from JSON data.
</li><li> The <code><font size="2">xml</font></code> and <code><font size="2">json</font></code> modes will not extract any fields when used on data that isn't of the indicated format (XML or JSON).
</li></ul><h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_inline_.28props.conf_only.29_search-time_field_extraction_examples"><span class="mw-headline" id="Inline_.28props.conf_only.29_search-time_field_extraction_examples">Inline (props.conf only) search-time field extraction examples </span></a></h3>
<p>Here are a set of examples of search-time custom field extraction, set up using <code><font size="2">props.conf</font></code> only. 
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_add_a_new_error_code_field"><span class="mw-headline" id="Add_a_new_error_code_field"> Add a new error code field </span></a></i></b></font></h4>
<p>This example shows how to create a new "error code" field by configuring a field extraction in <code><font size="2">props.conf</font></code>.  The field can be identified by the occurrence of <code><font size="2">device_id=</font></code> followed by a word within brackets and a text string terminating with a colon. The field should be extracted from events related to the <code><font size="2">testlog</font></code> source type. 
</p><p>In <code><font size="2">props.conf</font></code>, add:
</p>
<div class="samplecode">
<code><font size="2"><br>[testlog]<br>EXTRACT-errors = device_id=\[w+\](?&lt;err_code&gt;[^:]+)<br></font></code></div>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_extract_multiple_fields_using_one_regex"><span class="mw-headline" id="Extract_multiple_fields_using_one_regex"> Extract multiple fields using one regex </span></a></i></b></font></h4>
<p>This is an example of a field extraction that pulls out five separate fields. You can then use these fields in concert with some event types to help you find port flapping events and report on them. 
</p><p>Here's a sample of the event data that the fields are being extracted from:
</p>
<div class="samplecode">
<code><font size="2">#%LINEPROTO-5-UPDOWN: Line protocol on Interface GigabitEthernet9/16, changed state to down</font></code></div>
<p>The stanza in <code><font size="2">props.conf</font></code> for the extraction looks like this:
</p>
<div class="samplecode">
<code><font size="2"><br>[syslog]<br>EXTRACT-port_flapping = Interface\s(?&lt;interface&gt;(?&lt;media&gt;[^\d]+)(?&lt;slot&gt;\d+)\/(?&lt;port&gt;\d+))\,\schanged<br>\sstate\sto\s(?&lt;port_status&gt;up|down)<br></font></code></div>
<p>Note that five separate fields are extracted as named groups: interface, media, slot, port, and port_status. 
</p><p>The following two steps aren't required for field extraction--they show you what you might do with the extracted fields to find port flapping events and then report on them.
</p><p>Use tags to define a couple of event types in eventtypes.conf:
</p>
<div class="samplecode">
<code><font size="2"><br>[cisco_ios_port_down]<br>search = "changed state to down"<br><br>[cisco_ios_port_up]<br>search = "changed state to up"<br></font></code></div>
<p>Finally, create a report in savedsearches.conf that ties much of the above together to find port flapping and report on the results:
</p>
<div class="samplecode">
<code><font size="2"><br>[port flapping]<br>search = eventtype=cisco_ios_port_down OR eventtype=cisco_ios_port_up starthoursago=3 | stats count by <br>interface,host,port_status | sort -count<br></font></code></div>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_create_a_field_from_a_subtoken"><span class="mw-headline" id="Create_a_field_from_a_subtoken"> Create a field from a subtoken </span></a></i></b></font></h4>
<p>You may run into problems if you are extracting a field value that is a subtoken--a part of a larger token. Tokens are chunks of event data that have been run through event processing prior to being indexed. During event processing, events are broken up into segments, and this is the point where tokens are created--each segment created is a token. 
</p><p>Tokens are never smaller than a complete word or number. For example, you may have the word <code><font size="2">foo123</font></code> in your event. If it has been run through event processing and indexing, it's a token, and it can be a value of a field. However, if your extraction pulls out the <code><font size="2">foo</font></code> as a field value unto itself, you're extracting a subtoken. The problem is that while <code><font size="2">foo123</font></code> exists in the index, <code><font size="2">foo</font></code> does not, which means that you'll likely get few results if you search on that subtoken, even though it may appear to be extracted correctly in your search results. 
</p><p>Because tokens cannot be smaller than individual "words" within strings, a field extraction of a subtoken (a part of a word) can cause problems because subtokens will not themselves be in the index, only the larger word of which they are a part. 
</p><p>If your field value is a smaller part of a token, you must configure <code><font size="2">props.conf</font></code> as explained above. Then, add an entry to <code><font size="2">fields.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;fieldname&gt;]<br>INDEXED = False<br>INDEXED_VALUE = False<br></font></code></div>
<ul><li> Fill in &lt;fieldname&gt; with the name of your field.
<ul><li> For example, <code><font size="2">[url]</font></code> if you've configured a field named "url."
</li></ul></li><li> Set <code><font size="2">INDEXED</font></code> and <code><font size="2">INDEXED_VALUE</font></code> to false.
<ul><li> This tells Splunk Enterprise that the value you're searching for is not a token in the index.
</li></ul></li></ul><p><b>Note:</b> As of release 4.3, you no longer need add this entry to <code><font size="2">fields.conf</font></code> for cases where you are extracting a field's value from the value of a default field (such as <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, <code><font size="2">sourcetype</font></code>, or <code><font size="2">timestamp</font></code>) that is not indexed (and therefore not tokenized). 
</p><p>For more information on the tokenization of event data, see "About segmentation" in the Getting Data In Manual.
</p>
<h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_create_advanced_search-time_field_extractions_with_field_transforms"><span class="mw-headline" id="Create_advanced_search-time_field_extractions_with_field_transforms"> Create advanced search-time field extractions with field transforms </span></a></h3>
<p>While you can define most search-time field extractions entirely within <code><font size="2">props.conf</font></code>, some advanced search-time field extractions reference an additional component called a <b>field transform</b>. This section shows you how to configure field transforms in <code><font size="2">transforms.conf</font></code>.
</p><p>Field transforms contain a field-extracting regular expression and other attributes that govern the way the transform extracts fields. Field transforms are always created in conjunction with field extraction stanzas in <code><font size="2">props.conf</font></code>--they cannot stand alone.
</p><p>Your search-time field extractions require a field transform component if you need to:
</p>
<ul><li> <b>Reuse the same field-extracting regular expression across multiple sources, source types, or hosts</b> (in other words, configure one field transform for multiple field extractions). If you find yourself using the same regex to extract fields for different sources, source types, and hosts, you may want to set it up as a transform. Then, if you find that you need to update the regex, you only have to do so once, even though it is used more than one field extraction.
</li><li> <b>Apply more than one field-extracting regular expression to the same source, source type, or host</b> (in other words, apply multiple field transforms to the same field extraction). This is sometimes necessary in cases where the field or fields that you want to extract from a particular source/source type/host appear in two or more very different event patterns.
</li><li> <b>Set up delimiter-based field extractions.</b> Delimiter-based extractions come in handy when your event data presents field-value pairs (or just field values) that are separated by delimiters such as commas, colons, bars, line breaks, and tab spaces. 
</li><li> <b>Configure extractions for multivalue fields.</b> When you do this, Splunk Enterprise appends additional field values to the field as it finds them in the event data.
</li><li> <b>Extract fields with names that begin with numbers or underscores.</b> Ordinarily key cleaning removes leading numeric characters and underscores from field names, but you can configure your transform to turn this functionality off if necessary. 
</li></ul><p>You can also configure transforms to:
</p>
<ul><li> Extract fields from the values of another field (other than <code><font size="2">_raw</font></code>) by using the <code><font size="2">SOURCE_KEY</font></code> attribute.
</li><li> Manage the formatting of extracted fields, in cases where you are extracting multiple fields or are extracting both the field name and field value, by using the <code><font size="2">FORMAT</font></code> attribute. 
</li></ul><p>Both of these configurations can now be set up directly in the regular expression as well. See the "Define a field transform" section below for more information about how to do this.
</p><p><b>NOTE:</b> If you need to concatenate a set of regex extractions into a single field value, you can do this with the <code><font size="2">FORMAT</font></code> attribute, but only if you set it up as an index-time extraction. For example, if you have a string like <code><font size="2">192(x)0(y)2(z)1</font></code> in your event data, you can extract it <i>at index time</i> as an <code><font size="2">ip address</font></code> field value in the format <code><font size="2">192.0.2.1</font></code>. For more information, see "Configure index-time field extractions" in the Getting Data In Manual. However we DO NOT RECOMMEND that you make extensive changes to your set of indexed fields--do so sparingly if at all.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_steps_for_defining_custom_search-time_field_extractions_that_reference_field_transforms"><span class="mw-headline" id="Steps_for_defining_custom_search-time_field_extractions_that_reference_field_transforms"> Steps for defining custom search-time field extractions that reference field transforms </span></a></i></b></font></h4>
<p>Advanced search-time field extractions use the REPORT extraction configuration in <code><font size="2">props.conf</font></code>. Each REPORT extraction stanza references a field transform that is defined separately in <code><font size="2">transforms.conf</font></code>. The field transform contains the regular expression that Splunk Enterprise uses to extract fields at search time, as well as other attributes that govern the way that the transform extracts those fields.
</p><p>Follow these steps when you create an advanced search-time field extraction: 
</p><p><b>1.</b> All extraction configurations in <code><font size="2">props.conf</font></code> are restricted by a specific source, source type, or host. Start by identifying the source type, source, or host that provide the events that your field should be extracted from. (Don't update <code><font size="2">props.conf</font></code> yet.)
</p><p><b>Note:</b> For information about hosts, sources, and sourcetypes, see "About default fields (host, source, source type, and more)" in the Getting Data In manual. 
</p><p><b>2.</b> Create a regular expression that identifies the field in the event. Use named capturing groups to provide the field names for the extracted values. Use the field name syntax as described in the preceding sections.
</p><p><b>Note:</b> If your event lists field/value pairs or just field values, you can create a delimiter-based field extraction that won't require a regex; see the information on the <code><font size="2">DELIMS</font></code> attribute, below, for more information.)
</p><p><b>3.</b> Create a field transform in <code><font size="2">transforms.conf</font></code> that utilizes this regex (or delimiter configuration). The transform can also define a source key and/or event value formatting.  
</p><p>Edit the <code><font size="2">transforms.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.
</p><p><b>Note:</b> <i><b>Do not</b></i> edit files in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>.
</p><p><b>4.</b> Follow the format for the REPORT field extraction type (defined two sections down) to create a field extraction stanza in <code><font size="2">props.conf</font></code> that uses the host, source, or source type that you identified in Step 1. If necessary, you can create additional field extraction stanzas for other hosts, sources, and source types that refer to the same field transform.
</p><p>Edit the <code><font size="2">props.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.  
</p><p><b>Note:</b> <i><b>Do not</b></i> edit files in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>.
</p><p><b>5.</b> Restart Splunk Enterprise for your changes to take effect.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_first.2c_define_a_field_transform"><span class="mw-headline" id="First.2C_define_a_field_transform"> First, define a field transform </span></a></i></b></font></h4>
<p>Follow this format when defining a search-time field transform in <code><font size="2">transforms.conf</font></code>: 
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;unique_transform_stanza_name&gt;]<br>REGEX = &lt;regular expression&gt;<br>FORMAT = &lt;string&gt;<br>SOURCE_KEY = &lt;string&gt;<br>DELIMS = &lt;quoted string list&gt;<br>FIELDS = &lt;quoted string list&gt;<br>MV_ADD = [true|false]<br>CLEAN_KEYS = [true|false]<br>KEEP_EMPTY_VALS = [true|false]<br>CAN_OPTIMIZE = [true|false]<br></font></code></div>
<ul><li> The <code><font size="2">&lt;unique_transform_stanza_name&gt;</font></code> is required for all search-time transforms. <b>Note:</b> <code><font size="2">&lt;unique_transform_stanza_name&gt;</font></code> values do not have to follow <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_use_proper_field_name_syntax" class="external text">field name syntax restrictions</a> (see above). You can use characters other than a-z, A-Z, and 0-9, and spaces are allowed. They are not subject to key cleaning.
</li><li> <code><font size="2">REGEX</font></code> is a regular expression that operates on your data to extract fields. It is required for all search-time field transforms unless you are setting up a delimiter-based transaction, in which case you use the <code><font size="2">DELIMS</font></code> attribute instead (see the DELIMS attribute description, below).
<ul><li> Defaults to an empty string.
</li></ul></li><li> <code><font size="2">REGEX</font></code> and the <code><font size="2">FORMAT</font></code> attribute:
<ul><li> Name-capturing groups in the <code><font size="2">REGEX</font></code> are extracted directly to fields, which means that you don't have to specify <code><font size="2">FORMAT</font></code> for simple field extraction cases. 
</li><li> If the <code><font size="2">REGEX</font></code> extracts both the field name <i>and</i> its corresponding value, you can use the following special capturing groups to skip specifying the mapping in <code><font size="2">FORMAT</font></code>: 
</li></ul></li></ul><dl><dd><dl><dd><code><font size="2">&lt;_KEY_&gt;&lt;string&gt;</font></code>, <code><font size="2">&lt;_VAL_&gt;&lt;string&gt;</font></code>. 
</dd></dl></dd></dl><ul><li>For example, the following are equivalent:
</li></ul><dl><dd><dl><dd>Using <code><font size="2">FORMAT</font></code>:
</dd></dl></dd></dl><dl><dd><dl><dd><code><font size="2">REGEX = ([a-z]+)=([a-z]+)</font></code>
</dd><dd><code><font size="2">FORMAT = $1::$2</font></code>
</dd></dl></dd></dl><dl><dd><dl><dd>Not using <code><font size="2">FORMAT</font></code>:
</dd></dl></dd></dl><dl><dd><dl><dd><code><font size="2">REGEX = (?&lt;_KEY_1&gt;[a-z]+)=(?&lt;_VAL_1&gt;[a-z]+)</font></code>
</dd></dl></dd></dl><ul><li> In both of these cases, Splunk Enterprise applies the regular expression against the source text of an event repeatedly to extract all of the field/value combinations that it can identify. 
</li></ul><ul><li> <code><font size="2">FORMAT</font></code> is optional. Use it to specify the format of the field/value pair(s) that you are extracting. You don't need to specify the <code><font size="2">FORMAT</font></code> if you have a simple <code><font size="2">REGEX</font></code> with name-capturing groups.
<ul><li> For search-time extractions, this is the pattern for the <code><font size="2">FORMAT</font></code> field:
</li></ul></li></ul><dl><dd><dl><dd><code><font size="2">FORMAT = &lt;field-name&gt;::&lt;field-value&gt;(&lt;field-name&gt;::&lt;field-value&gt;)*</font></code>
</dd></dl></dd></dl><dl><dd><dl><dd>where:
</dd></dl></dd></dl><dl><dd><dl><dd><code><font size="2">field-name  = [&lt;string&gt;|$&lt;extracting-group-number&gt;]</font></code>
</dd><dd><code><font size="2">field-value = [&lt;string&gt;|$&lt;extracting-group-number&gt;]</font></code>
</dd></dl></dd></dl><dl><dd><dl><dd>Examples of search-time <code><font size="2">FORMAT</font></code> usage:
</dd></dl></dd></dl><dl><dd><dl><dd>1. <code><font size="2">FORMAT = firstfield::$1 secondfield::$2 thirdfield::other-value</font></code>
</dd><dd>2. <code><font size="2">FORMAT = $1::$2</font></code>
</dd></dl></dd></dl><ul><li> If you configure <code><font size="2">FORMAT</font></code> with a variable field name (such as in example #2 just above, where <code><font size="2">$1</font></code> represents the field name), Splunk Enterprise applies the regular expression against the source event text repeatedly to match and extract as many field/value pairs as it can find.
<ul><li> <b>Note:</b> You cannot create concatenated fields with <code><font size="2">FORMAT</font></code> at search time. This functionality is only available for index-time field transforms.
</li><li> <code><font size="2">FORMAT</font></code> defaults to an empty string.
</li></ul></li></ul><ul><li> <code><font size="2">SOURCE_KEY</font></code> is optional. Use it to extract one or more values from the values of another field. You can use any field that is available at the time of the execution of this field extraction. 
<ul><li> To configure <code><font size="2">SOURCE_KEY</font></code>, identify the field to which Splunk Enterprise should apply the transform's <code><font size="2">REGEX</font></code>. 
</li><li> By default, <code><font size="2">SOURCE_KEY</font></code> is set to <code><font size="2">_raw</font></code>, which means it is applied to the raw, unprocessed text of all events. 
</li></ul></li></ul><ul><li> <code><font size="2">DELIMS</font></code> is optional. Use it in place of <code><font size="2">REGEX</font></code> when dealing with delimiter-based field extractions, where field values--or field/value pairs--are separated by delimiters such as commas, colons, spaces, tab spaces, line breaks, and so on.
<ul><li> Delimiters must be quoted with " " . You can use the backwards slash to escape double quotes around a value if necessary (\"). 
</li><li> IMPORTANT: If a value may contain an embedded unescaped double quote character, such as "foo"bar", we recommend that you use REGEX, not DELIMS. 
</li><li> Each character in the delimiter string is used as a delimiter to split the event.
</li><li> If the event contains full delimiter-separated field/value pairs, you enter two sets of quoted delimiters for <code><font size="2">DELIMS</font></code>. The first set of quoted delimiters separates the field/value pairs. The second set of quoted delimiters separates the field name from its corresponding value.
</li><li> If the events <i>only contain delimiter-separated values</i> (no field names), you use <i>one</i> set of quoted delimiters, to separate the values. Then you use the <code><font size="2">FIELDS</font></code> attribute to apply field names to the extracted values (see <code><font size="2">FIELDS</font></code> below). Alternatively, Splunk Enterprise reads even tokens as field names and odd tokens as field values. 
</li><li> Splunk Enterprise consumes consecutive delimiter characters unless you specify a list of field names. 
</li><li> Defaults to empty string.
</li><li>This example of <code><font size="2">DELIMS</font></code> usage applies to an event where field/value pairs are separated by '|' symbols, and the field names are separated from their corresponding values by '=' symbols:
</li></ul></li></ul><dl><dd><dl><dd> <code><font size="2">[pipe_eq]</font></code>
</dd><dd> <code><font size="2">DELIMS = "|", "="</font></code>
</dd></dl></dd></dl><ul><li> <code><font size="2">FIELDS</font></code> is used in conjunction with <code><font size="2">DELIMS</font></code> when you are performing delimiter-based field extraction, but you only have field values to extract. Use <code><font size="2">FIELDS</font></code> to provide field names for the extracted field values, in list format according to the order in which the values are extracted.
<ul><li> <b>Note:</b> If field names contain spaces or commas they must be quoted with " " (to escape, use \). 
</li><li> Defaults to an empty string.
</li><li> Here's an example of a delimiter-based extraction where three field values appear in an event. They are separated by a comma and then a space.
</li></ul></li></ul><dl><dd><dl><dd> <code><font size="2">[commalist]</font></code>
</dd><dd> <code><font size="2">DELIMS = ", "</font></code>
</dd><dd> <code><font size="2">FIELDS = field1, field2, field3</font></code>
</dd></dl></dd></dl><ul><li> <code><font size="2">MV_ADD</font></code> is optional. Use it when you have events that have multiple occurrences of the same field but with different values and you want to keep each of the field's values.
<ul><li> When <code><font size="2">MV_ADD = true</font></code>, Splunk Enterprise transforms fields that appear multiple times in an event with different values into multivalue fields (the field name appears once, the multiple values for the field follow the '=' sign).
</li><li> When <code><font size="2">MV_ADD=false</font></code>, Splunk Enterprise keeps the first value found for a field in an event and discards every subsequent value found for that same field in that same event. 
</li><li> Defaults to <code><font size="2">false</font></code>.
</li></ul></li></ul><ul><li> <code><font size="2">CLEAN_KEYS</font></code> is optional. It controls whether or not the system strips leading underscores and 0-9 characters from the keys (field names) it extracts (see the subtopic <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_use_proper_field_name_syntax" class="external text">"Use proper field name syntax,"</a> above, for more information). "Key cleaning" is the practice of replacing any non-alphanumeric characters (characters other than those falling between the a-z, A-Z, and 0-9 ranges) in field names with underscores, as well as the stripping of leading underscores and 0-9 characters from field names. 
<ul><li> Add <code><font size="2">CLEAN_KEYS = false</font></code> to your transform if you need to keep your field names intact (no removal of leading underscores and/or 0-9 characters).
</li><li> By default, <code><font size="2">CLEAN_KEYS</font></code> is always set to <code><font size="2">true</font></code> for transforms. 
</li></ul></li></ul><ul><li> <code><font size="2">KEEP_EMPTY_VALS</font></code> is optional. It controls whether Splunk Enterprise keeps field/value pairs when the value is an empty string.
<ul><li> This option does not apply to field/value pairs that are generated by Splunk Enterprise's autokv extraction (automatic field extraction) process. Autokv ignores field/value pairs with empty values.
</li><li> Defaults to <code><font size="2">false</font></code>.
</li></ul></li></ul><ul><li> <code><font size="2">CAN_OPTIMIZE</font></code> is optional. It controls whether Splunk Enterprise can optimize the extraction out (or, in other words, disable the extraction). 
<ul><li> You might use this if you're running searches under a <b>search mode</b> setting that disables <b>field discovery</b>--it ensures that Splunk Enterprise <i>always</i> discovers specific fields. 
</li><li> Splunk Enterprise only disables an extraction if it can determine that none of the fields identified by the extraction will ever be needed for the successful evaluation of a search.
</li><li> <b>Note:</b> This attribute should <i>rarely</i> be set to <code><font size="2">false</font></code>.
</li><li> Defaults to <code><font size="2">true</font></code>.
</li></ul></li></ul><h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_second.2c_configure_a_report_field_extraction_stanza_in_props.conf_and_associate_it_with_the_field_transform"><span class="mw-headline" id="Second.2C_configure_a_REPORT_field_extraction_stanza_in_props.conf_and_associate_it_with_the_field_transform"> Second, configure a REPORT field extraction stanza in props.conf and associate it with the field transform </span></a></i></b></font></h4>
<p>When you're setting up a search-time field extraction in <code><font size="2">props.conf</font></code> that is associated with a field transform, you use the <code><font size="2">REPORT</font></code> field extraction class. Follow this format. 
</p><p>You can associate multiple field transform stanzas to a single field extraction by listing them after the initial <code><font size="2">&lt;unique_transform_stanza_name&gt;</font></code>, separated by commas. (For more information, <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_configuring_a_field_extraction_that_utilizes_multiple_field_transforms" class="external text">see the example later in this topic</a>.)
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;spec&gt;]<br>REPORT-&lt;class&gt; = &lt;unique_transform_stanza_name1&gt;, &lt;unique_transform_stanza_name2&gt;,...<br></font></code></div>
<ul><li> <code><font size="2">&lt;spec&gt;</font></code> can be:
<ul><li> <code><font size="2">&lt;sourcetype&gt;</font></code>, the source type of an event.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event.
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event.
</li></ul></li><li> <code><font size="2">&lt;class&gt;</font></code> is a unique literal string that identifies the namespace of the field (key) you're extracting. <b>Note:</b> <code><font size="2">&lt;class&gt;</font></code> values do not have to follow <a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_use_proper_field_name_syntax" class="external text">field name syntax restrictions</a> (see above). You can use characters other than a-z, A-Z, and 0-9, and spaces are allowed. <code><font size="2">&lt;class&gt;</font></code> values are not subject to key cleaning.
</li><li> <code><font size="2">&lt;unique_transform_stanza_name&gt;</font></code> is the name of your field transform stanza from transforms.conf. 
</li></ul><ul><li> Precedence rules for the REPORT field extraction class:
<ul><li> For each class, Splunk Enterprise takes the configuration from the highest precedence configuration block. 
</li><li> If a particular class is specified for a <code><font size="2">source</font></code> and a <code><font size="2">sourcetype</font></code>, the class for <code><font size="2">source</font></code> wins out. 
</li><li> Similarly, if a particular class is specified in <code><font size="2">../local/</font></code> for a <code><font size="2">&lt;spec&gt;</font></code>, it overrides that class  in ../default/.
</li></ul></li></ul><p>If you have a set of transforms that must be run in a specific order and which belong to the same host, source, or source type, you can place them in a comma-separated list within the same props.conf stanza. Splunk Enterprise will apply them in the specified order. For example, this sequence insures that the <code><font size="2">[yellow]</font></code> field transform gets applied first, then <code><font size="2">[blue]</font></code>, and then <code><font size="2">[red]</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[source::color_logs]<br>REPORT-colorchange = yellow, blue, red<br></font></code></div>
<p>If you need to change the order, rearrange the list.
</p>
<h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_examples_of_custom_search-time_field_extractions_using_field_transforms"><span class="mw-headline" id="Examples_of_custom_search-time_field_extractions_using_field_transforms">Examples of custom search-time field extractions using field transforms</span></a></h3>
<p>These examples present custom field extraction use cases that require you to configure one or more field transform stanzas in <code><font size="2">transforms.conf</font></code> and then reference them in a <code><font size="2">props.conf</font></code> field extraction stanza.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_configuring_a_field_extraction_that_utilizes_multiple_field_transforms"><span class="mw-headline" id="Configuring_a_field_extraction_that_utilizes_multiple_field_transforms"> Configuring a field extraction that utilizes multiple field transforms </span></a></i></b></font></h4>
<p>This example of search-time field transform setup demonstrates how:
</p>
<ul><li> you can create transforms that pull varying field name/value pairs from events.
</li><li> you can create a field extraction that references two or more field transforms.
</li></ul><p>Let's say you have logs that contain multiple field name/field value pairs. While the fields vary from event to event, the pairs always appear in one of two formats. 
</p><p>The logs often come in this format:
</p>
<div class="samplecode">
<code><font size="2">[fieldName1=fieldValue1] [fieldName2=fieldValue2]</font></code></div>
<p>However, at times they are more complicated, logging multiple name/value pairs as a list, in which case the format looks like:
</p>
<div class="samplecode">
<code><font size="2">[headerName=fieldName1] [headerValue=fieldValue1], [headerName=fieldName2] [headerValue=fieldValue2]</font></code></div>
<p>Note that the list items are separated by commas, and that each <code><font size="2">fieldName</font></code> is matched with a corresponding <code><font size="2">fieldValue</font></code>. In these secondary cases you still want to pull out the field names and values so that the search results are
</p>
<div class="samplecode">
<code><font size="2"><br>fieldName1=fieldValue1<br>fieldName2=fieldValue2<br></font></code></div>
<p>and so on. 
</p><p>To make things more clear, here's an example of an HTTP request event that combines both of the above formats.
</p>
<div class="samplecode">
<code><font size="2">[method=GET] [IP=10.1.1.1] [headerName=Host] [headerValue=www.example.com], [headerName=User-Agent] [headerValue=Mozilla], [headerName=Connection] [headerValue=close] [byteCount=255]</font></code></div>
<p>You want to develop a single field extraction that would pull the following field/value pairs from that event:
</p>
<div class="samplecode">
<code><font size="2"><br>method=GET<br>IP=10.1.1.1<br>Host=www.example.com<br>User-Agent=Mozilla<br>Connection=close<br>byteCount=255<br></font></code></div>
<h5> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_solution"><span class="mw-headline" id="Solution"> Solution </span></a></h5>
<p>To efficiently and reliably pull out both formats of field/value pairs, you'll want to design two different regexes that are optimized for each format. One regex will identify events with the first format and pull out all of the matching field/value pairs. The other regex will identify events with the other format and pull out those field/value pairs. 
</p><p>You then create two unique transforms in <code><font size="2">transforms.conf</font></code>--one for each regex--and then unite them in the corresponding field extraction stanza in <code><font size="2">props.conf</font></code>. 
</p><p>The first transform you add to <code><font size="2">transforms.conf</font></code> catches the fairly conventional <code><font size="2">[fieldName1=fieldValue1] [fieldName2=fieldValue2]</font></code> case.
</p>
<div class="samplecode">
<code><font size="2"><br>[myplaintransform]<br>REGEX=\[(?!(?:headerName|headerValue))([^\s\=]+)\=([^\]]+)\]<br>FORMAT=$1::$2<br></font></code></div>
<p>The second transform (also added to <code><font size="2">transforms.conf</font></code>) catches the slightly more complex <code><font size="2">[headerName=fieldName1] [headerValue=fieldValue1], [headerName=fieldName2] [headerValue=fieldValue2]</font></code> case:
</p>
<div class="samplecode">
<code><font size="2"><br>[mytransform]<br>REGEX= \[headerName\=(\w+)\],\s\[headerValue=([^\]]+)\]<br>FORMAT= $1::$2<br></font></code></div>
<p>Both transforms use the <code><font size="2">&lt;fieldName&gt;::&lt;fieldValue&gt;</font></code> <code><font size="2">FORMAT</font></code> to match each field name in the event with its corresponding value. This setting in FORMAT enables Splunk Enterprise to keep matching the regex against a matching event until every matching field/value combination is extracted.
</p><p>Finally, this field extraction stanza, which you create in <code><font size="2">props.conf</font></code>, references <i>both</i> of the field transforms:
</p>
<div class="samplecode">
<code><font size="2"><br>[mysourcetype]<br>KV_MODE=none<br>REPORT-a = mytransform, myplaintransform<br></font></code></div>
<p>Note that, besides using multiple field transforms, the field extraction stanza also sets <code><font size="2">KV_MODE=none</font></code>. This disables automatic field/value extraction for the identified source type (while letting your manually defined extractions continue). It ensures that these new regexes aren't overridden by automatic field extraction, and it also helps increase your search performance. (See the following subsection for more on disabling key/value extraction.)
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_configuring_delimiter-based_field_extraction"><span class="mw-headline" id="Configuring_delimiter-based_field_extraction"> Configuring delimiter-based field extraction </span></a></i></b></font></h4>
<p>You can use the <code><font size="2">DELIMS</font></code> attribute in field transforms to configure field extractions for events where field values or field/value pairs are separated by delimiters such as commas, colons, tab spaces, and more.
</p><p>For example, say you have a recurring multiline event where a different field/value pair sits on a separate line, and each pair is separated by a colon followed by a tab space. Here's a sample event:
</p>
<div class="samplecode">
<code><font size="2"><br>ComponentId: &nbsp;&nbsp;&nbsp;&nbsp;Application Server<br>ProcessId: &nbsp;&nbsp;5316<br>ThreadId: &nbsp;&nbsp;&nbsp;00000000<br>ThreadName: &nbsp;P=901265:O=0:CT<br>SourceId: &nbsp;&nbsp;&nbsp;com.ibm.ws.runtime.WsServerImpl<br>ClassName: &nbsp;&nbsp;<br>MethodName: &nbsp;<br>Manufacturer: &nbsp;&nbsp;&nbsp;IBM<br>Product: &nbsp;&nbsp;&nbsp;&nbsp;WebSphere<br>Version: &nbsp;&nbsp;&nbsp;&nbsp;Platform 7.0.0.7 [BASE 7.0.0.7 cf070942.55]<br>ServerName: &nbsp;sfeserv36Node01Cell\sfeserv36Node01\server1<br>TimeStamp: &nbsp;&nbsp;2010-04-27 09:15:57.671000000<br>UnitOfWork: &nbsp;<br>Severity: &nbsp;&nbsp;&nbsp;3<br>Category: &nbsp;&nbsp;&nbsp;AUDIT<br>PrimaryMessage: &nbsp;WSVR0001I: Server server1 open for e-business<br>ExtendedMessage: <br></font></code></div>
<p>Now you could set up a bulky, wordy search-time field extraction stanza in <code><font size="2">props.conf</font></code> that handles <i>all</i> of these fields:
</p>
<div class="samplecode">
<code><font size="2"><br>[activityLog]<br>LINE_BREAKER = [-]{8,}([\r\n]+)<br>SHOULD_LINEMERGE = false<br>EXTRACT-ComponentId = ComponentId:\t(?.*)<br>EXTRACT-ProcessId = ProcessId:\t(?.*)<br>EXTRACT-ThreadId = ThreadId:\t(?.*)<br>EXTRACT-ThreadName = ThreadName:\t(?.*)<br>EXTRACT-SourceId = SourceId:\t(?.*)<br>EXTRACT-ClassName = ClassName:\t(?.*)<br>EXTRACT-MethodName = MethodName:\t(?.*)<br>EXTRACT-Manufacturer = Manufacturer:\t(?.*)<br>EXTRACT-Product = Product:\t(?.*)<br>EXTRACT-Version = Version:\t(?.*)<br>EXTRACT-ServerName = ServerName:\t(?.*)<br>EXTRACT-TimeStamp = TimeStamp:\t(?.*)<br>EXTRACT-UnitOfWork = UnitOfWork:\t(?.*)<br>EXTRACT-Severity = Severity:\t(?.*)<br>EXTRACT-Category = Category:\t(?.*)<br>EXTRACT-PrimaryMessage = PrimaryMessage:\t(?.*)<br>EXTRACT-ExtendedMessage = ExtendedMessage:\t(?.*)<br></font></code></div>
<p>But that solution is pretty over-the-top. Is there a more elegant way to handle it that would remove the need for all these <code><font size="2">EXTRACT</font></code> lines? Yes!
</p><p>Configure the following stanza in <code><font size="2">transforms.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[activity_report]<br>DELIMS = "\n", ":\t"<br></font></code></div>
<p>This states that the field/value pairs in the event are on separate lines (<code><font size="2">"\n"</font></code>), and then specifies that the field name and field value on each line is separated by a colon and tab space (<code><font size="2">":\t"</font></code>).
</p><p>To complete this configuration, rewrite the wordy <code><font size="2">props.conf</font></code> stanza mentioned above as:
</p>
<div class="samplecode">
<code><font size="2"><br>[activitylog]<br>LINE_BREAKER = [-]{8,}([\r\n]+)<br>SHOULD_LINEMERGE = false<br>REPORT-activity = activity_report<br></font></code></div>
<p>These two brief configurations will extract the same set of fields as before, but they leave less room for error and are more flexible.
</p>
<h4><font size="3"><b><i> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_handling_events_with_multivalue_fields"><span class="mw-headline" id="Handling_events_with_multivalue_fields"> Handling events with multivalue fields </span></a></i></b></font></h4>
<p>You can use the <code><font size="2">MV_ADD</font></code> attribute to extract fields in situations where the same field is used more than once in an event, but has a different value each time. Ordinarily, Splunk Enterprise only extracts the first occurrence of a field in an event; every subsequent occurrence is discarded. But when MV_ADD is set to true in <code><font size="2">transforms.conf</font></code>, Splunk Enterprise treats the field like a multivalue field and extracts each unique field/value pair in the event.
</p><p>Say you have a set of events that look like this: 
</p>
<div class="samplecode">
<code><font size="2"><br>event1.epochtime=1282182111 type=type1 value=value1 type=type3 value=value3<br>event2.epochtime=1282182111 type=type2 value=value4 type=type3 value=value5 type=type4 value=value6<br></font></code></div>
<p>See how the <code><font size="2">type</font></code> and <code><font size="2">value</font></code> fields are repeated several times in each event? What you'd like to do is search <code><font size="2">type=type3</font></code> and have both of these events be returned. Or you'd like to run a <code><font size="2">count(type)</font></code> report on these two events that returns <code><font size="2">5</font></code>. 
</p><p>So, what you want to do is create a custom multivalue extraction of the <code><font size="2">type</font></code> field for these events. Here's how you would set up your <code><font size="2">transforms.conf</font></code> and <code><font size="2">props.conf</font></code> files to enable it:
</p><p>First, <code><font size="2">transforms.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[mv-type]<br>REGEX = type=(?&lt;type&gt;\s+)<br>MV_ADD = true<br></font></code></div>
<p>Then, in <code><font size="2">props.conf</font></code> for your sourcetype or source, set:
</p>
<div class="samplecode">
<code><font size="2">REPORT-type = mv-type</font></code></div>
<h3> <a name="createandmaintainsearch-timefieldextractionsthroughconfigurationfiles_disabling_automatic_search-time_extraction_for_specific_sources.2c_source_types.2c_or_hosts"><span class="mw-headline" id="Disabling_automatic_search-time_extraction_for_specific_sources.2C_source_types.2C_or_hosts"> Disabling automatic search-time extraction for specific sources, source types, or hosts </span></a></h3>
<p>You can disable automatic search-time field extraction for specific sources, source types, or hosts through edits in <code><font size="2">props.conf</font></code>. Add <code><font size="2">KV_MODE = none</font></code> for the appropriate <code><font size="2">[&lt;spec&gt;]</font></code> in <code><font size="2">props.conf</font></code>. 
</p><p><b>Note:</b> Custom field extractions set up manually via the configuration files or Splunk Web will still be processed for the affected source, source type, or host when <code><font size="2">KV_MODE = none</font></code>.
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;spec&gt;]<br>KV_MODE = none<br></font></code></div>
<p><code><font size="2">&lt;spec&gt;</font></code> can be:
</p>
<ul><li> <code><font size="2">&lt;sourcetype&gt;</font></code> - an event source type.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event.
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event.
</li></ul><a name="configuresplunktoparsemulti-valuefields"></a><h2> <a name="configuresplunktoparsemulti-valuefields_configure_multivalue_fields"><span class="mw-headline" id="Configure_multivalue_fields"> Configure multivalue fields</span></a></h2>
<p>Multivalue fields are fields that can appear multiple times in an event and have a different value for each appearance. One of the more common examples of multivalue fields is that of email address fields, which typically appears two to three times in a single sendmail event--once for the sender, another time for the list of recipients, and possibly a third time for the list of Cc addresses, if one exists. If all of these fields are labeled identically (as "AddressList," for example), they lose meaning that they might otherwise have if they're identified separately as "From", "To", and "Cc". 
</p><p>Splunk Enterprise parses multivalue fields at search time, and enables you to process the values in the search pipeline. Search commands that work with multivalue fields include makemv, mvcombine, mvexpand, and nomv. For more information on these and other commands see the topic on multivalue fields in the User manual, and the Search Reference manual.
</p><p>Use the <code><font size="2">TOKENIZER</font></code> key to configure multivalue fields in fields.conf. <code><font size="2">TOKENIZER</font></code> uses a regular expression to tell Splunk Enterprise how to recognize and extract multiple field values for a recurring field in an event. Edit <code><font size="2">fields.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.
</p><p>For more information on configuration files in general, see "About configuration files" in the Admin manual.
</p><p>For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test regexes by using them in searches with the rex search command. Splunk Enterprise also maintains a list of useful third-party tools for writing and testing regular expressions. 
</p>
<h3> <a name="configuresplunktoparsemulti-valuefields_configure_a_multivalue_field_via_fields.conf"><span class="mw-headline" id="Configure_a_multivalue_field_via_fields.conf"> Configure a multivalue field via fields.conf </span></a></h3>
<p>Define a multivalue field by adding a stanza for it in <code><font size="2">fields.conf</font></code>. Then add a line with the <code><font size="2">TOKENIZER</font></code> key and a corresponding regular expression that shows how the field can have multiple values. 
</p><p><b>Note:</b> If you have other attributes to set for a multivalue field, set them in the same stanza underneath the <code><font size="2">TOKENIZER</font></code> line. See the fields.conf topic in the Admin manual for more information.
</p>
<code><font size="2"><br>[&lt;field name 1&gt;]<br>TOKENIZER = &lt;regular expression&gt;<br><br>[&lt;field name 2&gt;]<br>TOKENIZER = &lt;regular expression&gt;<br></font></code>
<ul><li> <code><font size="2">&lt;regular expression&gt;</font></code> should indicate how the field in question can take on multiple values. 
</li><li> <code><font size="2">TOKENIZER</font></code> defaults to empty. When <code><font size="2">TOKENIZER</font></code> is empty, the field can only take on a single value. 
</li><li> Otherwise the first group is taken from each match to form the set of field values.
</li><li> The <code><font size="2">TOKENIZER</font></code> key is used by the where, timeline, and stats commands. It also provides the summary and XML outputs of the asynchronous search API. 
</li></ul><p><b>Note:</b> Tokenization of indexed fields (fields extracted at index time) is not supported. If you have set <code><font size="2">INDEXED=true</font></code> for a field, you cannot also use the <code><font size="2">TOKENIZER</font></code> key for that field. You can use a search-time extraction defined in <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> to break an indexed field into multiple values. 
</p>
<h3> <a name="configuresplunktoparsemulti-valuefields_example"><span class="mw-headline" id="Example"> Example </span></a></h3>
<p>Say you have a poorly formatted email log file where all of the addresses involved are grouped together under <code><font size="2">AddressList</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>From: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sender@splunkexample.com<br>To: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recipient1@splunkexample.com, recipient2@splunkexample.com, recipient3@splunkexample.com<br>CC: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cc1@splunkexample.com, cc2@splunkexample.com, cc3@splunkexample.com<br>Subject: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Multivalue fields are out there!<br>X-Mailer: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Febooti Automation Workshop (Unregistered)<br>Content-Type: &nbsp;text/plain; charset=UTF-8<br>Date: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wed, 3 Nov 2014 17:13:54 +0200<br>X-Priority: &nbsp;&nbsp;&nbsp;3 (normal)<br></font></code></div>
<p>This example from <code><font size="2">$SPLUNK_HOME/etc/system/README/fields.conf.example</font></code> breaks email fields <code><font size="2">To</font></code>, <code><font size="2">From</font></code>, and <code><font size="2">CC</font></code> into multiple values. 
</p>
<div class="samplecode">
<code><font size="2"><br>[To]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br><br>[From]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br><br>[Cc]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br></font></code></div>

<a name="definecalcfields"></a><h2> <a name="definecalcfields_define_calculated_fields"><span class="mw-headline" id="Define_calculated_fields"> Define calculated fields </span></a></h2>
<p>If you spend a fair amount of time constructing searches in Splunk Enterprise you'll become familiar with the <code><font size="2">eval</font></code> command, which enables you to devise an expression that involves automatically <b>extracted fields</b> and create a new field that takes the value that is the result of that expression's evaluation. (For more information and numerous <code><font size="2">eval</font></code> usage examples, see the documentation for the command in the Search Reference.)
</p><p>The <code><font size="2">eval</font></code> command is immensely versatile and useful. But while some <code><font size="2">eval</font></code> expressions are relatively simple, they often can be quite complex. If you find that you need to use a particularly long and complex <code><font size="2">eval</font></code> expression on a regular basis, you may find that retyping the expression accurately in search after search is tedious business. 
</p><p>This is where <b>calculated fields</b> come to the rescue. Calculated fields enable you to define fields with eval expressions in <code><font size="2">props.conf</font></code> . Then, when you're writing out a search, you can cut out the eval expression entirely and reference the field like you would any other extracted field. When you run the search, the fields will be extracted at search time and will be added to the events that include the fields in the eval expressions.
</p><p>For example, take this example search from the Search Reference discussion of the <code><font size="2">eval</font></code> command, which examines earthquake data and classifies quakes by their depth by creating a new <code><font size="2">Description</font></code> field:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=eqs7day-M1.csv | eval Description=case(Depth&lt;=70, "Shallow", Depth&gt;70 AND Depth&lt;=300, "Mid", Depth&gt;300 AND Depth&lt;=700, "Deep") | table Datetime, Region, Depth, Description</font></code><br></div>
<p>Using calculated fields, you could define the eval expression for the <code><font size="2">Description</font></code> field in <code><font size="2">props.conf</font></code> and write the search as:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=eqs7day-M1.csv | table Datetime, Region, Depth, Description</font></code><br></div>
<p>You can now search on <code><font size="2">Description</font></code> as if it is any other extracted field. Splunk Enterprise will find the calculated field key in <code><font size="2">props.conf</font></code> and evaluate it for every event that contains a <code><font size="2">Depth</font></code> field. You can also run searches like this:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=eqs7day-M1.csv Description=Deep</font></code><br></div>
<p><b>Note:</b> In the next section we show you how the <code><font size="2">Description</font></code> calculated field would be set up in <code><font size="2">props.conf</font></code>.
</p>
<h3> <a name="definecalcfields_create_a_calculated_field_by_editing_props.conf"><span class="mw-headline" id="Create_a_calculated_field_by_editing_props.conf">Create a calculated field by editing props.conf</span></a></h3>
<p>To create a calculated field, you add a calculated field key to a new or preexisting <code><font size="2">props.conf</font></code> stanza. You can find <code><font size="2">props.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>. (We recommend using the latter directory if you want to make it easy to transfer your data customizations to other search servers.)
</p><p><b>Note:</b> <i><b>Do not</b></i> edit files in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>.
</p><p>For more information on configuration files in general, see "About configuration files" in the Admin Manual.
</p><p>The format of a calculated field key in <code><font size="2">props.conf</font></code> is as follows:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;stanza&gt;]<br>EVAL-&lt;field_name&gt; = &lt;eval statement&gt;</font></code></div>
<ul><li> <code><font size="2">&lt;stanza&gt;</font></code> can be:
<ul><li> <code><font size="2">&lt;source type&gt;</font></code>, the source type of an event.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event.
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event.
</li></ul></li></ul><ul><li> Calculated field keys must start with "EVAL-" (including the hyphen), but "EVAL" is not case-sensitive (can be "eVaL" for example).
</li><li> <code><font size="2">&lt;field_name&gt;</font></code> <b>is</b> case sensitive, however. This is consistent with all other field names in Splunk Enterprise.
</li><li> <code><font size="2">&lt;eval_statement&gt;</font></code> is just as flexible as it is for the <code><font size="2">eval</font></code> search command. It can be evaluated to any value type, including multivals, boolean, or null. 
</li></ul><p>So, to set up the <code><font size="2">Description</font></code> calculated field (see the example at the top of this topic), you'd create the following stanza in <code><font size="2">props.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>&lt;Stanza&gt;<br>Eval-Description = case(Depth&lt;=70, "Shallow", Depth&gt;70 AND Depth&lt;=300, "Mid", Depth&gt;300 AND Depth&lt;=700, "Deep")</font></code></div>
<p>Once you have properly defined a calculated field key in this manner, Splunk Enterprise calculates the field at search time for events that have the extracted fields that appear in the eval statement. Calculated field evaluation takes place <i>after</i> <b>search-time field extraction</b> and <b>field aliasing</b>, but <i>before</i> derivation of <b>lookup fields</b>.
</p><p><b>Note:</b> When you search on a calculated field, performance-wise the action is equivalent to <b>filtering</b>. It won't actually use the index to search, because the field values won't be found there.   
</p>
<h4><font size="3"><b><i> <a name="definecalcfields_prevent_overrides_of_existing_fields"><span class="mw-headline" id="Prevent_overrides_of_existing_fields">Prevent overrides of existing fields</span></a></i></b></font></h4>
<p>If a calculated field has the same name as a field that has been extracted by normal means, the calculated field will override the extracted field, even if the eval statement evaluates to null. You can cancel this override effect by using the <code><font size="2">coalesce</font></code> function for eval in conjunction with the eval expression. <code><font size="2">coalesce</font></code> takes an arbitrary number of arguments and returns the first value that is not null.
</p><p>If you do not want the calculated field to override existing fields when the eval statement returns a value, use this construction: 
</p>
<div class="samplecode"><code><font size="2"><br>EVAL-field = coalesce(field, &lt;eval expression&gt;)<br></font></code></div>
<p>If you do not want the calculated field to override existing fields when the eval statement returns null, use this construction:
</p>
<div class="samplecode"><code><font size="2"><br>EVAL-field = coalesce(&lt;eval expression&gt;, field)<br></font></code></div>
<p>For more information about <code><font size="2">coalesce</font></code> and other <code><font size="2">eval</font></code> functions see "Functions for eval and where" in the Search Reference.
</p>
<h3> <a name="definecalcfields_examples_demonstrating_that_all_calculated_fields_are_determined_independently_of_others"><span class="mw-headline" id="Examples_demonstrating_that_all_calculated_fields_are_determined_independently_of_others">Examples demonstrating that all calculated fields are determined independently of others</span></a></h3>
<p>When Splunk Enterprise evaluates calculated fields, it evaluates each expression as if it were independent of all of the others. This means you can't "chain" calculated field expressions, where the evaluation of one calculated field is used in the expression for another calculated field. 
</p><p>In the following example, for any individual event, the value of <code><font size="2">x</font></code> is equivalent to the value of calculated field <code><font size="2">y</font></code> because the two calculations are carried out independently of each other -- both expressions use the original value of <code><font size="2">x</font></code> when they calculate <code><font size="2">x*2</font></code>.
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;foo&gt;]<br>EVAL-x = x * 2<br>EVAL-y = x * 2</font></code></div>
<p>So if for a specific event <code><font size="2">x=4</font></code>, these calculated fields would replace the value of <code><font size="2">x</font></code> with <code><font size="2">8</font></code>, and would add <code><font size="2">y=8</font></code> to the event.
</p><p>Here's another slightly more "real-world" example, which involves the extracted field <code><font size="2">response_time</font></code>. When it is first extracted, the value of <code><font size="2">response_time</font></code> is expressed in milliseconds. We've designed two calculated fields that make use of <code><font size="2">response_time</font></code> in different ways. 
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;access_common&gt;]<br>EVAL-response_time = &nbsp;response_time/1000<br>EVAL-bitrate = bytes*1000/response_time</font></code></div>
<p>In this example two things are happening for all data with the <code><font size="2">access_common</font></code> source type. 
</p>
<ul><li> The first EVAL changes the value of the <code><font size="2">response_time</font></code> in all <code><font size="2">sourcetype=access_common</font></code> events so that it is expressed in seconds rather than milliseconds. The new "in seconds" value overrides the old "in milliseconds" value. 
</li><li> The second EVAL calculates a new field called <code><font size="2">bitrate</font></code> for all <code><font size="2">sourcetype=access_common</font></code> events. It is expressed in terms of <code><font size="2">bytes</font></code> per second. (Note that <code><font size="2">bytes</font></code> is another extracted field.)
</li></ul><p>In both calculations, <code><font size="2">response_time</font></code> is initially expressed in terms of milliseconds, as both EVALs are calculated independently of the other. 
</p>
<h3> <a name="definecalcfields_lookup_fields_and_calculated_fields"><span class="mw-headline" id="Lookup_fields_and_calculated_fields">Lookup fields and calculated fields</span></a></h3>
<p>You <i>cannot</i> base calculated fields on lookup fields. It won't work if you try. This is because, as mentioned above, the evaluation of calculated fields takes place <i>after</i> <b>search-time field extraction</b> and <b>field aliasing</b>, but <i>before</i> derivation of <b>lookup fields</b>.
</p>
<a name="usedefaultfields"></a><h2> <a name="usedefaultfields_use_default_fields"><span class="mw-headline" id="Use_default_fields"> Use default fields</span></a></h2>
<p><b>Fields</b> are searchable name/value pairs in event data. When you <b>search</b>, you're matching search terms against segments of your <b>event data</b>; you can search more precisely by using fields. Fields are <b>extracted</b> from event data at either index time or search time. The fields that Splunk Enterprise extracts automatically at index time are known as  <b>default fields</b>. 
</p><p>Default fields serve a number of purposes.  For example, the default field <code><font size="2">index</font></code> identifies the index in which the event is located. The default field  <code><font size="2">linecount</font></code> describes the number of lines the event contains, and <code><font size="2">timestamp</font></code> specifies the time at which the event occurred. Splunk Enterprise uses the values in some of the fields, particularly <code><font size="2">sourcetype</font></code>, when indexing the data, in order to create events properly. Once the data has been indexed, you can use the default fields in your searches.
</p><p>For more information on using default fields in search commands, see "About the search language" in the Search manual. For information on configuring default fields, see "About default fields" in the Getting Data In manual.
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Type of field
</th><th bgcolor="#C0C0C0">List of fields
</th><th bgcolor="#C0C0C0">Description
</th></tr><tr><td valign="center" align="left">Internal fields
</td><td valign="center" align="left"><code><font size="2">_raw, _time, _indextime, _cd</font></code>
</td><td valign="center" align="left">These are fields that contain general information about events in Splunk Enterprise.
</td></tr><tr><td valign="center" align="left">Default fields
</td><td valign="center" align="left"><code><font size="2">host, index, linecount, punct, source, sourcetype, splunk_server, timestamp</font></code>
</td><td valign="center" align="left">These are fields that contain information about where an event originated, in which index it's located, what type it is, how many lines it contains, and when it occurred. These fields are indexed and added to the Fields menu by default.
</td></tr><tr><td valign="center" align="left">Default datetime fields
</td><td valign="center" align="left"><code><font size="2">date_hour, date_mday, date_minute, date_month, date_second, date_wday, date_year, date_zone</font></code>
</td><td valign="center" align="left">These are fields that provide additional searchable granularity to event timestamps.
<p><b>Note:</b> Only events that have timestamp information in them as generated by their respective systems will have date_* fields. If an event has a date_* field, it represents the value of time/date directly from the event itself. If you have specified any timezone conversions or changed the value of the time/date at indexing or input time (for example, by setting the timestamp to be the time at index or input time), these fields will not represent that. 
</p>
</td></tr></table><p>A field can have more than one value; for more information about how to handle such fields and their values, see the "Manipulate and evaluate fields with multiple values" topic in this chapter.
</p><p>You can extract additional (non-default) fields with Splunk Web or by using extracting search commands. For more information, see the <a href="#aboutfields" class="external text">"About fields"</a> topic in this manual.
</p><p>You might also want to change the name of a field, or group it with other similar fields. This is easily done with tags or aliases for the fields and field values. For more information, see the <a href="#tagandaliasfieldvaluesinsplunkweb_how_to_tag_and_alias_field_values" class="external text">"Tag and alias field values"</a> topic in this manual.
</p><p>This topic discusses the internal and other default fields that Splunk Enterprise automatically adds when you index data.
</p>
<h3> <a name="usedefaultfields_internal_fields"><span class="mw-headline" id="Internal_fields"> Internal fields </span></a></h3>
<p>Fields that begin with an underscore are internal fields. 
</p><p><b>Note:</b> We do not recommend that you override internal fields unless you are absolutely sure you know what you are doing. 
</p>
<h4><font size="3"><b><i> <a name="usedefaultfields_raw"><span class="mw-headline" id="raw"> _raw </span></a></i></b></font></h4>
<p>The <code><font size="2">_raw</font></code> field contains the original raw data of an event.  Splunk's <code><font size="2">search</font></code> command uses the data in <code><font size="2">_raw</font></code> when performing searches and data extraction.  
</p><p>You can't always search directly on values of <code><font size="2">_raw</font></code>, but you can filter on it with commands like <code><font size="2">regex</font></code> or <code><font size="2">sort</font></code>. 
</p><p><b>Example:</b> Return sendmail events that contain an IP address that starts with "10".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">eventtype=sendmail | regex _raw=*10.\d\d\d\.\d\d\d\.\d\d\d\*</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_time"><span class="mw-headline" id="time"> _time </span></a></i></b></font></h4>
<p>The <code><font size="2">_time</font></code> field contains an event's timestamp expressed in Unix time.  Splunk Enterprise uses this field to create the event timeline in Splunk Web.
</p><p><b>Note:</b> The <code><font size="2">_time</font></code> field is stored internally in UTC format. It is translated to human-readable Unix time format when Splunk Enterprise renders the search results (the very last step of <b>search time</b> event processing).  
</p><p><b>Example:</b> Search all sources of type "mail" for mail addressed to the user "strawsky@bigcompany.com", then sorts the search results by timestamp. 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=mail to=strawsky@bigcompany.com | sort _time </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_indextime"><span class="mw-headline" id="indextime">_indextime</span></a></i></b></font></h4>
<p>The <code><font size="2">_indextime</font></code> field contains the time that an event was indexed, expressed in Unix time. You might use this field to focus on or filter out events that were indexed within a specific range of time.
</p>
<h4><font size="3"><b><i> <a name="usedefaultfields_cd"><span class="mw-headline" id="cd">_cd</span></a></i></b></font></h4>
<p>The <code><font size="2">_cd</font></code> field essentially provides an "address" for an event within the index. It is composed of two numbers, a short number and a long number. The short number indicates the specific index bucket that the event resides in. The long number is an index bucket offset. It provides the exact location of the event within its bucket. Because <code><font size="2">_cd</font></code> is used for internal reference only, we don't recommend that you set up searches that involve it.
</p>
<h3> <a name="usedefaultfields_other_default_fields"><span class="mw-headline" id="Other_default_fields"> Other default fields </span></a></h3>
<h4><font size="3"><b><i> <a name="usedefaultfields_host"><span class="mw-headline" id="host"> host </span></a></i></b></font></h4>
<p>The <code><font size="2">host</font></code> field contains the originating hostname or IP address of the network device that generated the event. . Use the <code><font size="2">host</font></code> field to narrow searches by specifying a <code><font size="2">host</font></code> value that events must match. You can use wildcards to specify multiple hosts with a single expression (Example: <code><font size="2">host=corp*</font></code>).  
</p><p>You can use <code><font size="2">host</font></code> to filter results in data-generating commands, or as an argument in data-processing commands.
</p><p><b>Example 1:</b> Search for events on all "corp" servers for accesses by the user "strawsky".  It then reports the 20 most recent events. 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">host=corp* eventtype=access user=strawsky | head 20</font></code><br></div>
<p><b>Example 2:</b> Search for events containing the term "404", and are from any host that starts with "192".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">404 |  regex host=*192.\d\d\d\.\d\d\d\.\d\d\d\*</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_index"><span class="mw-headline" id="index"> index </span></a></i></b></font></h4>
<p>The<code><font size="2">_index</font></code> field contains the name of the index in which a given event is indexed.  Specify an index to use in your searches by using: <code><font size="2">index="name_of_index"</font></code>.  By default, all events are indexed in the <code><font size="2">main</font></code> index (<code><font size="2">index="main"</font></code>). 
</p><p><b>Example:</b> Search the <code><font size="2">myweb</font></code> index for events that have the ".php" extension.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index="myweb" *.php </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_linecount"><span class="mw-headline" id="linecount"> linecount </span></a></i></b></font></h4>
<p>The <code><font size="2">linecount</font></code> field contains the number of lines an event contains. This is the number of lines an event contains before it is indexed. Use <code><font size="2">linecount</font></code> to search for events that match a certain number of lines, or as an argument in data-processing commands.   To specify a matching range, use a greater-than and less-than expression (Example: <code><font size="2">linecount&gt;10 linecount&lt;20</font></code>).
</p><p><b>Example:</b> Search corp1 for events that contain "40" and have 40 lines, and omit events that contain 400.  
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">40 linecount=40 host=corp1 NOT 400</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_punct"><span class="mw-headline" id="punct"> punct </span></a></i></b></font></h4>
<p>The <code><font size="2">punct</font></code> field contains a punctuation pattern that is extracted from an event. The punctuation pattern is unique to types of events. Use <code><font size="2">punct</font></code> to filter events during a search or as a field argument in data-processing commands. 
</p><p>You can use wildcards in the <code><font size="2">punct</font></code> field to search for multiple punctuation patterns that share some common characters that you know you want to search for. You must use quotation marks when defining a punctuation pattern in the <code><font size="2">punct</font></code> field.
</p><p><b>Example 1:</b> Search for all punctuation patterns that start and end with <b>:</b>
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">punct=":*:"</font></code><br></div> 
<p><b>Example 2:</b> Search the php_error.log for php error events that have the punctuation pattern:"[--_::]__:___:____<i>/-..-</i>///.___".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source="/var/www/log/php_error.log" punct="[--_::]__:___:____''/-..-''///.___" </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_source"><span class="mw-headline" id="source"> source </span></a></i></b></font></h4>
<p>The <code><font size="2">source</font></code> field contains the name of the file, stream, or other input from which the event originates. Use <code><font size="2">source</font></code> to filter events during a search, or as an argument in a data-processing command.  You can use wildcards to specify multiple sources with a single expression (Example: <code><font size="2">source=*php.log*</font></code>).  
</p><p>You can use <code><font size="2">source</font></code> to filter results in data-generating commands, or as an argument in data-processing commands.
</p><p><b>Example:</b> Search for events from the source "/var/www/log/php_error.log".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source="/var/www/log/php_error.log"</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_sourcetype"><span class="mw-headline" id="sourcetype"> sourcetype </span></a></i></b></font></h4>
<p>The <code><font size="2">sourcetype</font></code> field specifies the format of the data input from which the event originates, such as <code><font size="2">access_combined</font></code> or <code><font size="2">cisco_syslog</font></code>. Use <code><font size="2">sourcetype</font></code> to filter events during a search, or as an argument in a data-processing command. You can use wildcards to specify multiple sources with a single expression (Example: <code><font size="2">sourcetype=access*</font></code>).  
</p><p><br><b>Example:</b> Search for all events that are of the source type "access log".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_log</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_splunk_server"><span class="mw-headline" id="splunk_server"> splunk_server </span></a></i></b></font></h4>
<p>The <code><font size="2">splunk_server</font></code> field contains the name of the Splunk Enterprise server containing the event. Useful in a distributed Splunk environment.
</p><p><b>Example:</b> Restrict a search to the main index on a server named "remote".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">splunk_server=remote index=main 404</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_timestamp"><span class="mw-headline" id="timestamp"> timestamp </span></a></i></b></font></h4>
<p>The <code><font size="2">timestamp</font></code> field contains an event's timestamp value. The method by which Splunk Enterprise extracts timestamps is configurable. You can use <code><font size="2">timestamp</font></code> as a <code><font size="2">search</font></code> command argument to filter your search.  
</p><p>For example, you can add <code><font size="2">timestamp=none</font></code> to your search to filter your search results to include only events that have no recognizable timestamp value. 
</p><p><b>Example:</b> Return the number of events in your data that have no recognizable timestamp.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">timestamp=none | stats count(_raw) as count </font></code><br></div>
<h3> <a name="usedefaultfields_default_datetime_fields"><span class="mw-headline" id="Default_datetime_fields"> Default datetime fields </span></a></h3>
<p>You can use datetime fields to filter events during a search or as a field argument in data-processing commands. 
</p><p><b>If you are located in a different timezone from the Splunk Enterprise server, time-based searches use the timestamp of the event as specified on the server where the event was indexed.</b> The datetime values are the literal values parsed from the event when it is indexed, regardless of its timezone. So, a string such as "05:22:21" will be parsed into indexed fields: "date_hour::5 date_minute::22 date_second::21".
</p>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_hour"><span class="mw-headline" id="date_hour"> date_hour </span></a></i></b></font></h4>
<p>The <code><font size="2">date_hour</font></code> field contains the value of the hour in which an event occurred (range: 0-23).  This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>).  
</p><p><b>Example:</b> Search for events with the term "apache" that occurred between 10pm and 12am on the current day.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache (date_hour &gt;= 22 AND date_hour &lt;= 24)</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_mday"><span class="mw-headline" id="date_mday"> date_mday </span></a></i></b></font></h4>
<p>The <code><font size="2">date_mday</font></code> field contains the value of the day of the month on which an event occurred (range: 1-31). This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>). 
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred between the 1st and 15th day of the current month.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache (date_mday &gt;= 1 AND date_mday &lt;= 15)</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_minute"><span class="mw-headline" id="date_minute"> date_minute </span></a></i></b></font></h4>
<p>The <code><font size="2">date_minute</font></code> field contains the value of the minute in which an event occurred (range: 0-59). This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>). 
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred between the 15th and 20th minute of the current hour.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache (date_minute &gt;= 15 AND date_minute &lt;= 20)</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_month"><span class="mw-headline" id="date_month"> date_month </span></a></i></b></font></h4>
<p>The <code><font size="2">date_month</font></code> field contains the value of the month in which an event occurred. This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>).  
</p><p><b>Example:</b> Search for events with the term "apache" that occurred in January.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache date_month=1</font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_second"><span class="mw-headline" id="date_second"> date_second </span></a></i></b></font></h4>
<p>The <code><font size="2">date_second</font></code> field contains the value of the seconds portion of an event's timestamp (range: 0-59).  This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>). 
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred between the 1st and 15th second of the current minute.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache (date_second &gt;= 1 AND date_second &lt;= 15) </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_wday"><span class="mw-headline" id="date_wday"> date_wday </span></a></i></b></font></h4>
<p>The <code><font size="2">date_wday</font></code> field contains the day of the week on which an event occurred (Sunday, Monday, etc.). Splunk Enterprise extracts the date from the event's timestamp (the value in <code><font size="2">_time</font></code>) and determines what day of the week that date translates to. This day of the week value is then placed in the <code><font size="2">date_wday</font></code> field. 
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred on Sunday.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache date_wday="sunday" </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_year"><span class="mw-headline" id="date_year"> date_year </span></a></i></b></font></h4>
<p>The <code><font size="2">date_year</font></code> field contains the value of the year in which an event occurred.  This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>).  
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred in 2008.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache date_year=2008 </font></code><br></div>
<h4><font size="3"><b><i> <a name="usedefaultfields_date_zone"><span class="mw-headline" id="date_zone"> date_zone </span></a></i></b></font></h4>
<p>The <code><font size="2">date_zone</font></code> field contains the value of time for the local timezone of an event, expressed as hours in Unix Time.  This value is extracted from the event's timestamp (the value in <code><font size="2">_time</font></code>).  Use <code><font size="2">date_zone</font></code> to offset an event's timezone by specifying an offset in minutes (range: -720 to 720).  
</p><p><b>Example:</b> Search for events containing the term "apache" that occurred in the current timezone (local).
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">apache date_zone=local</font></code><br></div>

<a name="aboutsplunkregularexpressions"></a><h2> <a name="aboutsplunkregularexpressions_about_splunk_enterprise_regular_expressions"><span class="mw-headline" id="About_Splunk_Enterprise_regular_expressions"> About Splunk Enterprise regular expressions</span></a></h2>
<p>This primer helps you create valid regular expressions in Splunk Enterprise. For a discussion of regular expression syntax and usage, see an online resource such as www.regular-expressions.info or a manual on the subject.
</p><p>Regular expressions match patterns of characters in text. Splunk Enterprise uses regular expressions for extracting default fields, recognizing binary file types, and automatic assignation of source types. You also use regular expressions when you define custom field extractions, filter events, route data, and correlate searches. Search commands that use regular expressions include <code><font size="2">rex</font></code> and <code><font size="2">regex</font></code> and eval functions such as <code><font size="2">match</font></code> and <code><font size="2">replace</font></code>.
</p><p>Splunk Enterprise regular expressions are PCRE (Perl Compatible Regular Expressions). They specifically use the PCRE C library.
</p>
<h3> <a name="aboutsplunkregularexpressions_regular_expressions_terminology_and_syntax"><span class="mw-headline" id="Regular_expressions_terminology_and_syntax">Regular expressions terminology and syntax</span></a></h3>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0">Term
</th><th bgcolor="#C0C0C0">Description
</th></tr><tr><td width="25%" valign="center" align="left"> literal
</td><td valign="center" align="left"> The exact text of characters to match using a regular expression.
</td></tr><tr><td valign="center" align="left"> regular expression
</td><td valign="center" align="left"> The metacharacters that define the pattern that Splunk Enterprise uses to match against the literal.
</td></tr><tr><td valign="center" align="left"> groups
</td><td valign="center" align="left"> Regular expressions allow groupings indicated by the type of bracket used to enclose the regular expression characters. Groups can define character classes, repetition matches, named capture groups, modular regular expressions, and more. You can apply quantifiers to and use alternation within enclosed groups.
</td></tr><tr><td valign="center" align="left"> character class
</td><td valign="center" align="left"> Characters enclosed in square brackets. Used to match a string. To set up a character class, define a range with a hyphen, such as <code><font size="2">[A-Z]</font></code>, to match any uppercase letter. Begin the character class with a caret (^) to define a negative match, such as <code><font size="2">[^A-Z]</font></code> to match any lowercase letter.
</td></tr><tr><td valign="center" align="left"> character type
</td><td valign="center" align="left"> Similar to a wildcard, character types represent specific literal matches. For example, a period <code><font size="2">.</font></code> matches any character, <code><font size="2">\w</font></code> matches words or alphanumeric characters including an underscore, and so on.
</td></tr><tr><td valign="center" align="left"> anchor
</td><td valign="center" align="left"> Character types that match text formatting positions, such as return (<code><font size="2">\r</font></code>) and newline (<code><font size="2">\n</font></code>).
</td></tr><tr><td valign="center" align="left"> alternation
</td><td valign="center" align="left"> Refers to supplying alternate match patterns in the regular expression. Use a vertical bar or pipe character ( | ) to separate the alternate patterns, which can include full regular expressions. For example, <code><font size="2">grey|gray</font></code> matches either <code><font size="2">grey</font></code> or <code><font size="2">gray</font></code>.
</td></tr><tr><td valign="center" align="left"> quantifiers, or repetitions
</td><td valign="center" align="left"> Use (<code><font size="2"> *, +,&nbsp;?</font></code> ) to define how to match the groups to the literal pattern. For example, <code><font size="2">*</font></code> matches 0 or more, <code><font size="2">+</font></code> matches 1 or more, and <code><font size="2">?</font></code> matches 0 or 1.
</td></tr><tr><td valign="center" align="left"> back references
</td><td valign="center" align="left"> Literal groups that you can recall for later use. In Splunk Enterprise, indicate a back reference to the value with a dollar symbol (<code><font size="2">$</font></code>) and a number (not zero).
</td></tr><tr><td valign="center" align="left"> lookarounds
</td><td valign="center" align="left"> A way to define a group to determine the position in a string. This definition matches the regular expression in the group but gives up the match to keep the result. For example, use a lookaround to match <code><font size="2">x</font></code> that is followed by <code><font size="2">y</font></code> without matching <code><font size="2">y</font></code>.
</td></tr></table><h4><font size="3"><b><i> <a name="aboutsplunkregularexpressions_character_types"><span class="mw-headline" id="Character_types"> Character types </span></a></i></b></font></h4>
<p>Character types are short for literal matches.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0">Term
</th><th bgcolor="#C0C0C0">Description
</th><th bgcolor="#C0C0C0">Example
</th><th bgcolor="#C0C0C0">Explanation
</th></tr><tr><td valign="center" align="left"> <code><font size="2">\w</font></code>
</td><td valign="center" align="left"> Match a word character (a letter, number, or underscore character).
</td><td valign="center" align="left"> <code><font size="2">\w\w\w</font></code>
</td><td valign="center" align="left"> Matches any three word characters.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">\W</font></code>
</td><td valign="center" align="left"> Match a non-word character.
</td><td valign="center" align="left"> <code><font size="2">\W\W\W</font></code>
</td><td valign="center" align="left"> Matches any three non-word characters.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">\d</font></code>
</td><td valign="center" align="left"> Match a digit character.
</td><td valign="center" align="left"> <code><font size="2">\d\d\d-\d\d-\d\d\d\d</font></code>
</td><td valign="center" align="left"> Matches a Social Security number, or a similar 3-2-4 number string.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">\D</font></code>
</td><td valign="center" align="left"> Match a non-digit character.
</td><td valign="center" align="left"> \D\D\D
</td><td valign="center" align="left"> Matches any three non-digit characters.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">\s</font></code>
</td><td valign="center" align="left"> Match a whitespace character.
</td><td valign="center" align="left"> <code><font size="2">\d\s\d</font></code>
</td><td valign="center" align="left"> Matches a sequence of a digit, a whitespace, and then another digit.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">\S</font></code>
</td><td valign="center" align="left"> Match a non-whitespace character.
</td><td valign="center" align="left"> <code><font size="2">\d\S\d</font></code>
</td><td valign="center" align="left"> Matches a sequence of a digit, a non-whitespace character, and another digit.
</td></tr><tr><td width="25%" valign="center" align="left"> <code><font size="2">.</font></code>
</td><td valign="center" align="left"> Match any character. Use sparingly.
</td><td valign="center" align="left"> <code><font size="2">\d\d.\d\d.\d\d</font></code>
</td><td valign="center" align="left"> Matches a date string such as 12/31/14 or 01.01.15, but can also match 99A99B99.
</td></tr></table><h4><font size="3"><b><i> <a name="aboutsplunkregularexpressions_groups.2c_quantifiers.2c_and_alternation"><span class="mw-headline" id="Groups.2C_quantifiers.2C_and_alternation"> Groups, quantifiers, and alternation </span></a></i></b></font></h4>
<p>Regular expressions allow groupings indicated by the type of bracket used to enclose the regular expression characters. You can apply quantifiers ( <code><font size="2">*, +,&nbsp;? </font></code>) to the enclosed group and use alternation within the group.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0">Term
</th><th bgcolor="#C0C0C0">Description
</th><th bgcolor="#C0C0C0">Example
</th><th bgcolor="#C0C0C0">Explanation
</th></tr><tr><td valign="center" align="left"> <code><font size="2"> * </font></code>
</td><td valign="center" align="left"> Match zero or more times.
</td><td valign="center" align="left"> <code><font size="2">\w*</font></code>
</td><td valign="center" align="left"> Matches zero or more word characters.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> + </font></code>
</td><td valign="center" align="left"> Match one or more times.
</td><td valign="center" align="left"> <code><font size="2">\d+</font></code>
</td><td valign="center" align="left"> Match at least one digit.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">&nbsp;? </font></code>
</td><td valign="center" align="left"> Match zero or one time.
</td><td valign="center" align="left"> <code><font size="2">\d\d\d-?\d\d-?\d\d\d\d</font></code>
</td><td valign="center" align="left"> Matches a Social Security Number with or without dashes.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> ( ) </font></code>
</td><td valign="center" align="left"> Parentheses define match or capture groups, atomic groups, and lookarounds.
</td><td valign="center" align="left"> <code><font size="2">(H..).(o..)</font></code>
</td><td valign="center" align="left"> When given the string <code><font size="2">Hello World</font></code>, this matches <code><font size="2">Hel</font></code> and <code><font size="2">o W</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> [ ] </font></code>
</td><td valign="center" align="left"> Square brackets define character classes.
</td><td valign="center" align="left"> <code><font size="2">[a-z0-9#]</font></code>
</td><td valign="center" align="left"> Matches any character that is <code><font size="2">a</font></code> through <code><font size="2">z</font></code>, <code><font size="2">0</font></code> through <code><font size="2">9</font></code>, or <code><font size="2">#</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> { } </font></code>
</td><td valign="center" align="left"> Curly brackets define repetitions.
</td><td valign="center" align="left"> <code><font size="2">\d{3,5}</font></code>
</td><td valign="center" align="left"> Matches a string of 3 to 5 digits in length.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> &lt; &gt; </font></code>
</td><td valign="center" align="left"> Angle brackets define named capture groups. Use the syntax <code><font size="2"> (?P&lt;var&gt;  ...) </font></code> to set up a named field extraction.
</td><td valign="center" align="left"> <code><font size="2">(?P&lt;ssn&gt;\d\d\d-\d\d-\d\d\d\d)</font></code>
</td><td valign="center" align="left"> Pulls out a Social Security Number and assigns it to the <code><font size="2">ssn</font></code> field.
</td></tr><tr><td valign="center" align="left"> <code><font size="2"> 	[[ ]] </font></code>
</td><td valign="center" align="left"> Double brackets define Splunk-Enterprise-specific modular regular expressions.
</td><td valign="center" align="left"> <code><font size="2">[[octet]]</font></code>
</td><td valign="center" align="left"> A validated 0-255 range integer.
</td></tr></table><h4><font size="3"><b><i> <a name="aboutsplunkregularexpressions_a_simple_example_of_groups.2c_quantifiers.2c_and_alternation"><span class="mw-headline" id="A_simple_example_of_groups.2C_quantifiers.2C_and_alternation">A simple example of groups, quantifiers, and alternation</span></a></i></b></font></h4>
<p>This example shows two ways to match either <code><font size="2">to</font></code> or <code><font size="2">too</font></code>. 
</p><p>The first regular expression uses the <code><font size="2">?</font></code> quantifier to match up to one more "o" after the first. 
</p><p>The second regular expression uses alternation to specify the pattern.
</p>
<div class="samplecode">
<code><font size="2"><br>to(o)?<br>(to|too)<br></font></code></div>
<h3> <a name="aboutsplunkregularexpressions_capture_groups_in_regular_expressions"><span class="mw-headline" id="Capture_groups_in_regular_expressions">Capture groups in regular expressions</span></a></h3>
<p>A named capture group is a regular expression grouping that extracts a field value when regular expression matches an event. Capture groups include the name of the field. They are notated with angle brackets as follows: 
</p><p><code><font size="2">matching text (?&lt;field_name&gt;capture pattern) more matching text</font></code>.
</p><p>For example, you have this event text:
</p><p><code><font size="2">131.253.24.135 fail admin_user</font></code>
</p><p>Here are two regular expressions that use different syntax in their capturing groups to pull the same set of fields from that event.
</p>
<ul><li> Expression A: <code><font size="2">(?&lt;ip&gt;\d+\.\d+\.\d+\.\d+) (?&lt;result&gt;\w+) (?&lt;user&gt;.*)</font></code>
</li><li> Expression B: <code><font size="2">(?&lt;ip&gt;\S+) (?&lt;result&gt;\S+) (?&lt;user&gt;\S+)</font></code>
</li></ul><p>In Expression A, the pattern-matching characters used for the first capture group (<code><font size="2">ip</font></code>) are specific. <code><font size="2">\d</font></code> means "digit" and <code><font size="2">+</font></code> means "one or more." So <code><font size="2">\d+</font></code> means "one or more digits." <code><font size="2">\.</font></code> refers to a period. 
</p><p>The capture group for <code><font size="2">ip</font></code> wants to match one or more digits, followed by a period, followed by one or more digits, followed by a period, followed by one or more digits, followed by a period, followed by one or more digits, followed by a period. This describes the syntax for an ip address.
</p><p>The second capture group in Expression A for the <code><font size="2">result</font></code> field has the pattern <code><font size="2">\w+</font></code>, which means "one or more alphanumeric characters." The third capture group in Expression A for the <code><font size="2">user</font></code> field has the pattern <code><font size="2">.*</font></code>, which means "match everything that's left."
</p><p>Expression B uses a common technique called negative matching. With negative matching, the regular expression does not try to define which text to match. Instead it defines what the text is not. In this Expression B, the values that should be extracted from the sample event are "not space" characters (<code><font size="2">\S</font></code>). It uses the <code><font size="2">+</font></code> to specify "one or more" of the "not space" characters. 
</p><p>So Expression B says: 
</p><p><b>1.</b> Pull out the first string of not-space characters for the <code><font size="2">ip</font></code> field value. 
</p><p><b>2.</b> Ignore the following space. 
</p><p><b>3.</b> Then pull out the second string of not-space characters for the <code><font size="2">result</font></code> field value. 
</p><p><b>4.</b> Ignore the second space. 
</p><p><b>5.</b> Pull out the third string of not-space characters for the <code><font size="2">user</font></code> field value."
</p>
<h3> <a name="aboutsplunkregularexpressions_modular_regular_expressions"><span class="mw-headline" id="Modular_regular_expressions"> Modular regular expressions </span></a></h3>
<p>Modular regular expressions refer to small chunks of regular expressions that are defined to be used in longer regular expression definitions. Splunk Enterprise defines modular regular expressions in transforms.conf.
</p><p>For example, you can define an integer and then use that regular expression definition to define a float.
</p>
<div class="samplecode">
<code><font size="2"><br>[int]<br># matches an integer or a hex number<br>REGEX = 0x[a-fA-F0-9]+|\d+<br><br>[float]<br># matches a float (or an int)<br>REGEX = \d*\.\d+|[[int]]<br></font></code></div>
<p>In the regular expression for <code><font size="2">[float]</font></code>, the modular regular expression for an integer or hex number match is invoked with double square brackets, <code><font size="2">[[int]]</font></code>.
</p><p>You can also use the modular regular expression in field extractions.
</p>
<div class="samplecode">
<code><font size="2"><br>[octet] <br># this would match only numbers from 0-255 (one octet in an ip)<br>REGEX = (?:2(?:5[0-5]|[0-4][0-9])|[0-1][0-9][0-9]|[0-9][0-9]?)<br><br>[ipv4]<br># matches a valid IPv4 optionally followed by&nbsp;:port_num the <br># octets in the ip would also be validated 0-255 range<br># Extracts: ip, port<br>REGEX = (?&lt;ip&gt;[[octet]](?:\.[[octet]]){3})(?::[[int:port]])?<br></font></code></div>

<h1>Data classification: Event types and transactions</h1><a name="abouteventtypes"></a><h2> <a name="abouteventtypes_about_event_types"><span class="mw-headline" id="About_event_types"> About event types</span></a></h2>
<p>Event types are a categorization system to help you make sense of your data. Event types let you sift through huge amounts of data, find similar patterns, and create alerts and reports. 
</p>
<h3> <a name="abouteventtypes_events_versus_event_types"><span class="mw-headline" id="Events_versus_event_types"> Events versus event types </span></a></h3>
<p>An event is a single record of activity within a log file. An event typically includes a timestamp and provides information about what occurred on the system being monitored or logged.
</p><p>An event type is a user-defined field that simplifies search by letting you categorize events. Event types let you classify events that have common characteristics.  When your search results come back, they're checked against known event types. An event type is applied to an event at search time if that event matches the event type definition in eventtypes.conf. Tag or save event types after indexing your data. 
</p>
<h3> <a name="abouteventtypes_event_type_classification"><span class="mw-headline" id="Event_type_classification"> Event type classification </span></a></h3>
<p>There are several ways to create your own event types.  Define event types via Splunk Web or through configuration files, or you can save any search as an event type.  When saving a search as an event type, you may want to use the <code><font size="2">punct</font></code> field to craft your searches.  The <code><font size="2">punct</font></code> field helps you narrow down searches based on the structure of the event.
</p>
<h4><font size="3"><b><i> <a name="abouteventtypes_use_the_punct_field_to_search_on_similar_events"><span class="mw-headline" id="Use_the_punct_field_to_search_on_similar_events"> Use the punct field to search on similar events </span></a></i></b></font></h4>
<p>Because the format of an event is often unique to an event type, Splunk Enterprise indexes the punctuation characters of events as a field called <code><font size="2">punct.</font></code> The <code><font size="2">punct</font></code> field stores the first 30 punctuation characters in the first line of the event. This field is useful for finding similar events quickly.
</p><p>When you use <code><font size="2">punct</font></code>, keep in mind:
</p>
<ul><li> Quotes and backslashes are escaped.
</li><li> Spaces are replaced with an underscore (_).
</li><li> Tabs are replaced with a "t".
</li><li> Dashes that follow alphanumeric characters are ignored. 
</li><li> Interesting punctuation characters are:
</li></ul><dl><dd> ",;-#$%&amp;+./:=?@\\'|*\n\r\"(){}&lt;&gt;[]^!"
</dd></dl><ul><li> The punct field is not available for events in the _audit index because those events are signed using PKI at the time they are generated. 
</li></ul><p>For an introduction to the punct field and other methods of event classification, see <a href="#classifyandgroupsimilarevents" class="external text">"Classify and group similar events"</a> topic in this manual.
</p>
<h5> <a name="abouteventtypes_punct_examples"><span class="mw-headline" id="Punct_examples"> Punct examples </span></a></h5>
<p>This event:
</p><p><code><font size="2">####&lt;Jun 3, 2005 5:38:22 PM MDT&gt; &lt;Notice&gt; &lt;WebLogicServer&gt; &lt;bea03&gt; &lt;asiAdminServer&gt; &lt;WrapperStartStopAppMain&gt; &lt;&gt;WLS Kernel&lt;&gt; &lt;&gt; &lt;BEA-000360&gt; &lt;Server started in RUNNING mode&gt;</font></code>
</p><p>Produces this punctuation: 
</p><p><code><font size="2"> ####&lt;_,__::__&gt;_&lt;&gt;_&lt;&gt;_&lt;&gt;_&lt;&gt;_&lt;&gt;_</font></code>
</p><p>This event:
</p><p><code><font size="2">172.26.34.223 - - [01/Jul/2005:12:05:27 -0700] "GET /trade/app?action=logout HTTP/1.1" 200 2953</font></code>
</p><p>Produces this punctuation:
</p><p><code><font size="2">..._-_-_[<i>:::_-]_\"_</i>?=_/.\"__</font></code>
</p>
<h3> <a name="abouteventtypes_event_type_discovery"><span class="mw-headline" id="Event_type_discovery"> Event type discovery </span></a></h3>
<p>Pipe any search to the <a href="#classifyandgroupsimilarevents_use_typelearner_to_discover_new_event_types" class="external text">typelearner command</a> and create event types directly from Splunk Web. The file eventdiscoverer.conf is mostly deprecated, although you can still specify terms to ignore when learning new event types in Splunk Web.
</p>
<h3> <a name="abouteventtypes_create_new_event_types"><span class="mw-headline" id="Create_new_event_types"> Create new event types </span></a></h3>
<p>The simplest way to create a new event type is through Splunk Web.  After you run a search that would make a good event type, click <b>Save As</b> and select <i>Event Type</i>.  This opens the <b>Save as Event Type</b> dialog, where you can provide the event type name and optionally apply tags to it. For more information about saving searches as event types, see  <a href="#classifyandgroupsimilarevents_save_a_search_as_a_new_event_type" class="external text">"Classify and group similar events"</a>, in this manual.
</p><p>You can also create new event types by modifying <code><font size="2">eventtypes.conf</font></code>. For more information about manually configuring event types in this manner, see  <a href="#configureeventtypes" class="external text">"Configure event types directly in eventtypes.conf"</a>, in this manual.
</p>
<h3> <a name="abouteventtypes_event_type_tags"><span class="mw-headline" id="Event_type_tags"> Event type tags </span></a></h3>
<p>Tag event types to organize your data into categories. There can be multiple tags per event. For more information about event type tagging, see the <a href="#tageventtypes" class="external text">"Tag event types"</a> topic in this manual
</p>
<h3> <a name="abouteventtypes_configuration_files_for_event_types"><span class="mw-headline" id="Configuration_files_for_event_types"> Configuration files for event types </span></a></h3>
<p>Event types are stored in eventtypes.conf.
</p><p>Terms for event type discovery are set in eventdiscoverer.conf.
</p>
<a name="classifyandgroupsimilarevents"></a><h2> <a name="classifyandgroupsimilarevents_classify_and_group_similar_events"><span class="mw-headline" id="Classify_and_group_similar_events"> Classify and group similar events </span></a></h2>
<p>An <b>event</b> is not the same thing as an <b>event type</b>. An event is a single instance of data &mdash; a single log entry, for example. An event type is a classification used to label and group events.
</p><p>The names of the matching event types for an event are set on the event, in a multivalue field called <code><font size="2">eventtype</font></code>. You can search for these groups of events (for example, SSH logins) the same way you search for any field value.
</p><p>This topic discusses how to save event types and use them in searches. For more information about events, how Splunk recognizes them, and what it does when it processes them for indexing, see the "Overview of event processing" topic in the Getting Data In manual.
</p>
<h3> <a name="classifyandgroupsimilarevents_save_a_search_as_a_new_event_type"><span class="mw-headline" id="Save_a_search_as_a_new_event_type"> Save a search as a new event type </span></a></h3>
<p>When you search your event data, you're essentially weeding out all unwanted events. Therefore, the results of your search are events that share common characteristics, and you can give them a collective name.  
</p><p>For example, if you often search for failed logins on different host machines, you can create an event type for the events that your search finds and call it <code><font size="2">failed_login</font></code>:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">"failed login" OR "FAILED LOGIN" OR "Authentication failure"  OR "Failed to authenticate user"</font></code><br></div> 
<p>To save this search as an eventtype:
</p><p><b>1.</b> Run the search. Once the search is running, click <b>Create</b> and select <i>Event type...</i> You don't have to wait for the search to complete to do this.
</p><p><b>2.</b> In <b>Save As Event Type</b>, give your search a <b>Name</b>.
For our search example, we'll name it "failed_login".
</p><p><img alt="4.3 save as event type dialog.png" src="images/f/f8/4.3_save_as_event_type_dialog.png" width="396" height="352"></p><p>If necessary, you can modify the <b>Search string</b> field, which should be populated automatically with the search you just ran. 
</p><p>You can also optionally add a list of tags that should be applied to the event type in the <b>Tag(s)</b> field. For more about this see the subsection about tagging event types, below.
</p><p><b>3.</b> Click "Save" to save your event type name.
</p><p>Now, you can quickly search for all the events that match this event type the same way you can search for any field.
</p><p>For example, you may be interested it in finding failed logins on specific host machines:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">host=target eventtype=failed_login</font></code><br></div>
<p>Or you may want to investigate a suspicious user's activities:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">user=suspicious eventtype=failed_login</font></code><br></div>
<h3> <a name="classifyandgroupsimilarevents_important_event_type_definition_restrictions"><span class="mw-headline" id="Important_event_type_definition_restrictions">Important event type definition restrictions</span></a></h3>
<p>You cannot base an event type on a search that includes a <b>pipe operator</b> or a <b>subsearch</b> . 
</p><p>In addition, you cannot base an event type on a search that references a report. For example, if you took the search in the previous example and saved it as a report with the name <code><font size="2">failed_login_search</font></code>, you can't create an event type that is defined by the search <code><font size="2">savedsearch=failed_login_search</font></code>. In a case like this you should always give the event type the same search string as the report.
</p>
<h3> <a name="classifyandgroupsimilarevents_identify_similar_events_with_punct"><span class="mw-headline" id="Identify_similar_events_with_punct"> Identify similar events with punct </span></a></h3>
<p>Because the punctuation of an event is often unique to a specific type of event, Splunk Enterprise indexes the punctuation characters of event in the <code><font size="2">punct</font></code> field. The values of this field may look cryptic, but they can be an effective way of characterizing similar events.
</p><p>To apply the <code><font size="2">punct</font></code> field to your search results, use the Fields popup discussed in the "Use fields to search" topic in the Splunk Tutorial. Select the <code><font size="2">punct</font></code> value for an SSH login event. This updates your search to include this <code><font size="2">punct</font></code> combination in the search bar. You may want to consider wildcarding the punctuation to match insignificant variations (for example, "punct=::[]*/*").
</p>
<h3> <a name="classifyandgroupsimilarevents_use_typelearner_to_discover_new_event_types"><span class="mw-headline" id="Use_typelearner_to_discover_new_event_types"> Use typelearner to discover new event types </span></a></h3>
<p>Pass any of your searches into the <code><font size="2">typelearner</font></code> command to see Splunk Enterprise's suggestions for event types. By default, <code><font size="2">typelearner</font></code> compares the punctuation of the events resulting from the search, grouping those that have similar punctuation and terms together.
</p><p>You can specify a different field for Splunk Enterprise to group the events; <code><font size="2">typelearner</font></code> works the same way with any field. The result is a set of events (from your search results) that have this field and phrases in common.
</p><p>For more information and examples, see  "typelearner" in the Search Reference.
</p>
<h3> <a name="classifyandgroupsimilarevents_use_tags_to_group_and_find_similar_events"><span class="mw-headline" id="Use_tags_to_group_and_find_similar_events"> Use tags to group and find similar events </span></a></h3>
<p>Event types can have one or more tags associated with them. You can add these tags while you save a search as an event type and from the event type manager, located in <b>Settings &gt; Event types</b>. From the list of event types in this window, select the one you want to edit. 
</p><p>After you add tags to your event types, you can search for them in the same way you search for any tag. Let's say you saved a search for firewall events as the event type <code><font size="2">firewall_allowed</font></code>, and then saved a search for login events as the event type <code><font size="2">login_successful</font></code>.  If you tagged both of these event types with <i>allow</i>, all events of either of those event types can be retrieved by using the search: 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag::eventtype="allow"</font></code><br></div>
<p>For more information about using tags, see the <a href="#tagandaliasfieldvaluesinsplunkweb" class="external text">"Tag and alias field values"</a> topic in this manual.
</p>
<a name="defineeventtypes"></a><h2> <a name="defineeventtypes_define_and_maintain_event_types_in_splunk_web"><span class="mw-headline" id="Define_and_maintain_event_types_in_Splunk_Web"> Define and maintain event types in Splunk Web </span></a></h2>
<p>You base event types on searches that return useful collections of events in their results. A single event can match multiple event types. 
</p><p>Any event types you create through Splunk Web are automatically added to eventtypes.conf in <code><font size="2">$SPLUNK_HOME/etc/users/&lt;your-username&gt;/&lt;app&gt;/local/</font></code>, where <code><font size="2">&lt;app&gt;</font></code> is the <b>app</b> you were in when you created the event type. If you change the permissions on the event type to make it available to all users (either in the app, or globally to all apps), Splunk Enterprise moves the event type to <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;App&gt;/local/</font></code>.
</p>
<h3> <a name="defineeventtypes_important_event_type_definition_restrictions"><span class="mw-headline" id="Important_event_type_definition_restrictions">Important event type definition restrictions</span></a></h3>
<p>You cannot base an event type on a search that includes a <b>pipe operator</b> or a <b>subsearch</b> . 
</p><p>In addition, you cannot base an event type on a search that references a report. For example, if you have a report with the name <code><font size="2">failed_login_search</font></code>, you can't create an event type that is defined by <code><font size="2">savedsearch=failed_login_search</font></code>. In a case like this you should always give the event type the same search string as the report. 
</p>
<h3> <a name="defineeventtypes_save_a_search_as_an_event_type"><span class="mw-headline" id="Save_a_search_as_an_event_type"> Save a search as an event type </span></a></h3>
<p>To save a search as an event type:
</p>
<ul><li> Enter the search and run it.
</li><li> Click <b>Save As</b> and select <i>Event Type.</i>
</li></ul><p>The <b>Save As Event Type</b> dialog box pops up, pre-populated with your search string.
</p>
<ul><li> Name the event type.
</li><li> Optionally add one or more comma-separated <b>tags</b> for the event type.
</li><li> Click <b>Save</b>.
</li></ul><p>You can now use your event type in searches. If you named your event type <i>foo</i>, you'd use it in a search like this:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">eventtype=foo</font></code><br></div>
<h3> <a name="defineeventtypes_automatically_find_and_build_event_types"><span class="mw-headline" id="Automatically_find_and_build_event_types"> Automatically find and build event types </span></a></h3>
<p>Unsure whether you have any potentially useful event types in your IT data? Splunk Enterprise provides utilities that dynamically and intelligently locate and create useful event types:
</p>
<ul><li> <b>Find event types:</b> The <code><font size="2">findtypes</font></code> search command analyzes a given set of events and identifies common patterns that could be turned into useful event types. 
</li><li> <b>Build event types:</b> The <b>Build Event Type utility</b> enables you to dynamically create event types based on events returned by searches. This utility also enables you to assign specific colors to event types. For example, if you say that a "sendmail error" event type is red, then the next time you run a search that returns events that fit that event type, they'll be easy to spot, because they'll show up as red in the event listing.
</li></ul><h4><font size="3"><b><i> <a name="defineeventtypes_find_event_types"><span class="mw-headline" id="Find_event_types"> Find event types </span></a></i></b></font></h4>
<p>To use the event type finder, add this to the end of your search: 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">...| findtypes</font></code><br></div>
<p>Searches that use the <code><font size="2">findtypes</font></code> command return a breakdown of the most common groups of events found in the search results. They are: 
</p>
<ul><li> ordered in terms of "coverage" (frequency). This helps you easily identify kinds of events that are subsets of larger event groupings. 
</li><li> coupled with searches that can be used as the basis for event types that will help you locate similar events.
</li></ul><p><img alt="FieldtypesSearchResults.png" src="images/7/77/FieldtypesSearchResults.png" width="650" height="347"></p><p>By default, <code><font size="2">findtypes</font></code> returns the top 10 potential event types  found in the sample, in terms of the number of events that match each kind of event discovered. You can increase this number by adding a <code><font size="2">max</font></code> argument: <code><font size="2">findtypes max=30</font></code>
</p><p>Splunk Enterprise also indicates whether or not the event groupings discovered with <code><font size="2">findtypes</font></code> have already been associated with other event types. 
</p><p><b>Note:</b> The <code><font size="2">findtypes</font></code> command analyzes 5000 events at most to return these results. You can lower this number using the <code><font size="2">head</font></code> command for a more efficient search:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">...| head 1000 | findtypes</font></code><br></div>
<h5> <a name="defineeventtypes_test_potential_searches_before_saving_them_as_event_types"><span class="mw-headline" id="Test_potential_searches_before_saving_them_as_event_types"> Test potential searches before saving them as event types </span></a></h5>
<p>When you identify a potentially useful event grouping, test the search associated with it to see if it returns the results you want. Click <b>Test</b> for the event grouping in which you are interested in to see its associated search run in a separate window. After the search runs, review the results it returns to determine whether or not it is capturing the specific information you want. 
</p>
<h5> <a name="defineeventtypes_save_a_tested_search_as_an_event_type"><span class="mw-headline" id="Save_a_tested_search_as_an_event_type"> Save a tested search as an event type </span></a></h5>
<p>When you find a search that returns the right collection of results, save it as an event type by clicking <b>Save</b> for the event grouping with which it is associated. The Save Event Type dialog appears. Enter a name for the event type, and optionally identify one or more tags that should be associated with it, separated by commas. You can also edit the search if necessary.
</p>
<h4><font size="3"><b><i> <a name="defineeventtypes_build_event_types"><span class="mw-headline" id="Build_event_types"> Build event types </span></a></i></b></font></h4>
<p>If you find an event in your search results that you'd like to base an event type on, open the dropdown event menu (find the down arrow next to the event timestamp) and click <b>Build event type</b>. 
</p><p><img alt="SelectBuildEventType.png" src="images/e/ec/SelectBuildEventType.png" width="282" height="181"></p><p>Splunk Enterprise takes you to the <b>Build Event Type utility</b> (often referred to as the "Event Type Builder"). You can use this utility to design a search that returns a select set of events, and then create an event type based on that search.
</p><p>The Build Event Type utility finds a set of sample events that are similar to the one you selected from your search results. In the <b>Event type features</b> sidebar, you'll find possible field/value pairings that you can use to narrow down the event type search further. 
</p><p>The Build Event Type utility also displays a search string under <b>Generated event type</b> at the top of the page. This is the search that the event type you're building will be based upon. As you select other field/value pairs in the Event type features sidebar, the <b>Generated event type</b> updates to include those selections. The list of sample events updates as well, to reflect the kinds of events that the newly modified event type search would return.
</p><p>If you want to edit the event type search directly, click <b>Edit</b>. This brings up the Edit Event Type dialog, which you can use to edit the search string. 
</p>
<h5> <a name="defineeventtypes_test_potential_searches_before_saving_them_as_event_types_2"><span class="mw-headline" id="Test_potential_searches_before_saving_them_as_event_types_2"> Test potential searches before saving them as event types </span></a></h5>
<p>When you build a search that you think might be a useful event type, test it. Click <b>Test</b> to see the search run in a separate window.  
</p>
<h5> <a name="defineeventtypes_save_a_tested_search_as_an_event_type_2"><span class="mw-headline" id="Save_a_tested_search_as_an_event_type_2"> Save a tested search as an event type </span></a></h5>
<p>If you test a search and it looks like it's returning the correct set of events, you can click <b>Save</b> to save it as an event type. The Save Event Type dialog appears. 
</p><p><img alt="SaveEventTypeDialog.png" src="images/c/c5/SaveEventTypeDialog.png" width="404" height="244"></p><p>Enter a name for the event type. Then, you can optionally use the <b>Style</b> list to associate a color for the event type. After you save, any event that matches the event type will appear in search results in that color. For example, say you create an event type called <code><font size="2">sendmail_bounce</font></code> and save it with a <b>Style</b> of <i>red</i>. Then, when you run a search that returns events that match this event type, those events will be easy to spot, because they'll be colored red. 
</p><p>You can use the <b>Priority</b> list to help Splunk Enterprise handle situations where events match more than one event type with a <b>Style</b> setting. For example, say you have two event types: one with a <i>High</i> priority and a red style, and one with an <i>Average</i> priority and a teal style. If an event in your results matches both of these event types, the <i>High</i> priority event type trumps the <i>Average</i> priority event type, and the event appears red in your search results.
</p>
<h3> <a name="defineeventtypes_add_and_maintain_event_types_in_splunk_web"><span class="mw-headline" id="Add_and_maintain_event_types_in_Splunk_Web"> Add and maintain event types in Splunk Web </span></a></h3>
<p>The Event Types page in Splunk Web lets you view and maintain details of the event types that you have created or which you have permission to edit. You can also add new event types through the Event Types page. Event types displayed on the Event Types page may be available globally (system-wide) or they may apply to specific Apps. 
</p>
<h4><font size="3"><b><i> <a name="defineeventtypes_adding_an_event_type_in_splunk_web"><span class="mw-headline" id="Adding_an_event_type_in_Splunk_Web"> Adding an event type in Splunk Web </span></a></i></b></font></h4>
<p>To add an event type in Splunk Web, navigate to the Event Types page and click <b>New</b>. Splunk Enterprise takes you to the Add New event types page. 
</p><p><img alt="Add-new-eventtype.png" src="images/c/c4/Add-new-eventtype.png" width="384" height="356"></p><p>From this page you enter the new event type's <b>Destination App</b>, <b>Name</b>, and the <b>Search string</b> that ultimately defines the event type (see "Save a search as an event", above).
</p><p><b>Note:</b> All event types are initially created for a specific App. To make a particular event type available to all users on a global basis, you have to locate the event type on the Event Types page, click its <b>Permissions</b> link, and change the <b>This app only</b> selection to <b>All apps</b>.  For more information about setting permissions for event types (and other knowledge object types), see <a href="#manageknowledgeobjectpermissions" class="external text">"Manage knowledge object permissions,"</a> in this manual.
</p><p>You can optionally include <b>Tags</b> for the event type. For more information about tagging event types and other kinds of Splunk Enterprise knowledge, see <a href="#abouttagsandaliases" class="external text">"About tags and aliases"</a> in this manual.
</p><p>You can also optionally select a <b>Priority</b> for the event type, where <i>1</i> is the highest priority and <i>10</i> is the lowest. The <b>Priority</b> setting is important for common situations where you have events that fit two or more event types. When the event turns up in search results, Splunk Enterprise displays the event types associated with the event in a specific order. You use the <b>Priority</b> setting to ensure that certain event types take precedence over others in this display order. 
</p><p>If you have a number of overlapping event types, or event types that are subsets of larger ones, you may want to give the precisely focused event types a higher priority. For example, you could easily have a set of events that are part of a wide-ranging <code><font size="2">system_error</font></code> event type. Within that large set of events, you could have events that also belong to more precisely focused event types like <code><font size="2">critical_disc_error</font></code> and <code><font size="2">bad_external_resource_error</font></code>. 
</p><p>In a situation like this, you could give the <code><font size="2">system_error</font></code> event type a <b>Priority</b> of 10, while giving the other two error codes <b>Priority</b> values in the  <i>1</i> to <i>5</i> range. This way, when events that match both <code><font size="2">system_error</font></code> and <code><font size="2">critical_disc_error</font></code> appear in search results, the <code><font size="2">critical_disc_error</font></code> event type is always listed ahead of the <code><font size="2">system_error</font></code> event type.
</p>
<h4><font size="3"><b><i> <a name="defineeventtypes_maintaining_event_types_in_splunk_web"><span class="mw-headline" id="Maintaining_event_types_in_Splunk_Web"> Maintaining event types in Splunk Web </span></a></i></b></font></h4>
<p>To update the details of an event type, locate it in the list on the Event Types page (<b>Settings &gt; Event Types</b>), and click its name. Splunk Enterprise takes you to the details page for the event type, where you can edit the <b>Search string</b>, <b>Tags</b>, and <b>Priority</b> for the event type, if you have the permissions to do so. You can also update permissions for event types and delete event types through the Event Types page, if you have edit permissions for them.
</p>
<a name="configureeventtypes"></a><h2> <a name="configureeventtypes_configure_event_types_directly_in_eventtypes.conf"><span class="mw-headline" id="Configure_event_types_directly_in_eventtypes.conf"> Configure event types directly in eventtypes.conf</span></a></h2>
<p>You can add new event types and update existing event types by configuring eventtypes.conf. There are a few default event types defined in <code><font size="2">$SPLUNK_HOME/etc/system/default/eventtypes.conf</font></code>.  Any <a href="#defineeventtypes" class="external text">event types you create through Splunk Web</a> are automatically added to <code><font size="2">$SPLUNK_HOME/etc/system/local/eventtypes.conf</font></code>.
</p>
<h3> <a name="configureeventtypes_important_event_type_definition_restrictions"><span class="mw-headline" id="Important_event_type_definition_restrictions">Important event type definition restrictions</span></a></h3>
<p>You cannot base an event type on a search that includes a <b>pipe operator</b> or a <b>subsearch</b> . 
</p><p>In addition, you cannot base an event type on a search that references a saved search. For example, if you have a saved search with the name <code><font size="2">failed_login_search</font></code>, you can't create an event type that is defined by the search <code><font size="2">savedsearch=failed_login_search</font></code>. In a case like this you should always give the event type the same search string as the saved search.
</p>
<h3> <a name="configureeventtypes_configuration"><span class="mw-headline" id="Configuration"> Configuration </span></a></h3>
<p>Make changes to event types in <code><font size="2">eventtypes.conf</font></code>. Use <code><font size="2">$SPLUNK_HOME/etc/system/README/eventtypes.conf.example</font></code> as an example, or create your own <code><font size="2">eventtypes.conf</font></code>.  
</p><p>Edit <code><font size="2">eventtypes.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.  For more information on configuration files in general, see "About configuration files" in the Admin manual.
</p><p><code><font size="2">[$EVENTTYPE]</font></code>
</p>
<ul><li> Header for the event type
</li><li> <code><font size="2">$EVENTTYPE</font></code> is the name of your event type.
<ul><li> You can have any number of event types, each represented by a stanza and any number of the following attribute/value pairs.  
</li></ul></li></ul><p><b>Note:</b> If the name of the event type includes field names surrounded by the percent character (for example, <code><font size="2">%$FIELD%</font></code>) then the value of <code><font size="2">$FIELD</font></code> is substituted at search time into the event type name for that event. For example, an event type with the header <code><font size="2">[cisco-%code%]</font></code> that has <code><font size="2">code=432</font></code> becomes labeled <code><font size="2">[cisco-432]</font></code>.
</p><p><code><font size="2">disabled = &lt;1 or 0&gt;</font></code>
</p>
<ul><li> Toggle event type on or off.
</li><li> Set to 1 to disable.
</li></ul><p><code><font size="2">search = &lt;string&gt;</font></code>
</p>
<ul><li> Search terms for this event type. 
</li><li> For example: error OR warn.
</li></ul><p><code><font size="2">tags = &lt;string&gt;</font></code>
</p>
<ul><li> Space separated words that are used to tag an event type.
</li></ul><p><code><font size="2">description = &lt;string&gt;</font></code>
</p>
<ul><li> Optional human-readable description of the event type.
</li></ul><p><code><font size="2">priority = &lt;integer&gt;</font></code>
</p>
<ul><li> Splunk Enterprise uses this value to determine the order in which it displays matching event types for an event. 1 is the highest, and 10 is the lowest.
</li></ul><p><b>Note:</b> You can tag <code><font size="2">eventtype</font></code> field values the same way you tag any other field/value combination. See the <code><font size="2">tags.conf</font></code> spec file for more information.
</p>
<h3> <a name="configureeventtypes_example"><span class="mw-headline" id="Example"> Example </span></a></h3>
<p>Here are two event types; one is called <code><font size="2">web</font></code>, and the other is called <code><font size="2">fatal</font></code>.
</p>
<code><font size="2"><br>[web]<br>search = html OR http OR https OR css OR htm OR html OR shtml OR xls OR cgi<br><br>[fatal]<br>search = FATAL<br></font></code>
<h3> <a name="configureeventtypes_disable_event_types"><span class="mw-headline" id="Disable_event_types"> Disable event types </span></a></h3>
<p>Disable an event type by adding <code><font size="2">disabled = 1</font></code> to the event type stanza <code><font size="2">eventtypes.conf</font></code>:
</p>
<code><font size="2"><br>[$EVENTTYPE]<br>disabled = 1<br></font></code>
<p><code><font size="2">$EVENTTYPE</font></code> is the name of the event type you wish to disable.
</p><p>So if you want to disable the <code><font size="2">web</font></code> event type, add the following entry to its stanza:
</p>
<code><font size="2"><br>[web]<br>disabled = 1<br></font></code>

<a name="configureeventtypetemplates"></a><h2> <a name="configureeventtypetemplates_configure_event_type_templates"><span class="mw-headline" id="Configure_event_type_templates"> Configure event type templates</span></a></h2>
<p>Event type templates create event types at search time. Define event type templates in eventtypes.conf. Edit <code><font size="2">eventtypes.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>. 
</p><p>For more information on configuration files in general, see "About configuration files" in the Admin manual.
</p>
<h3> <a name="configureeventtypetemplates_event_type_template_configuration"><span class="mw-headline" id="Event_type_template_configuration"> Event type template configuration </span></a></h3>
<p>Event type templates use a field name surrounded by percent characters to create event types at search time where the <code><font size="2">%$FIELD%</font></code> value is substituted into the name of the event type.
</p>
<code><font size="2"><br>[$NAME-%$FIELD%]<br>$SEARCH_QUERY<br></font></code>
<p>So if the search query in the template returns an event where <code><font size="2">%$FIELD%=bar</font></code>, Splunk Enterprise creates an event type titled <code><font size="2">$NAME-bar</font></code> for that event. 
</p>
<h3> <a name="configureeventtypetemplates_example"><span class="mw-headline" id="Example"> Example </span></a></h3>
<code><font size="2"><br>[cisco-%code%]<br>search = cisco<br></font></code>
<p>If a search on "cisco" returns an event that has <code><font size="2">code=432</font></code>, Splunk Enterprise creates an event type titled "cisco-432".
</p>
<a name="abouttransactions"></a><h2> <a name="abouttransactions_about_transactions"><span class="mw-headline" id="About_transactions"> About transactions</span></a></h2>
<p>A <b>transaction</b> is a group of conceptually-related events that spans time. A <b>transaction type</b> is a transaction that has been configured in <code><font size="2">transactiontypes.conf</font></code> and saved as a <b>field</b> in Splunk. 
</p><p>Transactions can include: 
</p>
<ul><li> Different events from the same source and the same host.
</li><li> Different events from different sources from the same host.
</li><li> Similar events from different hosts and different sources.
</li></ul><p>For example, a customer purchase in an online store could generate a transaction that ties together events from several sources:
</p>
<ul><li> <b>A set of web access events</b> share a session ID with.... 
</li><li> ....<b>a corresponding event in the application server log</b>, which also contains related account, product, and transaction IDs. The transaction ID in that application server event also appears in...
</li><li> ...<b>a message queue event</b>, which contains a message ID. This message ID is in turn shared by...
</li><li> ...<b>a purchase fulfillment event</b> logged by the fulfillment application, which also includes the shipping status of the item that the customer purchased.
</li></ul><p>All of the highlighted here, when grouped together, represent a single user transaction. If you were to define it as a transaction type you might call it an "item purchase" transaction. Other kinds of transactions include web access, application server downloads, emails, security violations, and system failures.
</p>
<h3> <a name="abouttransactions_transaction_search"><span class="mw-headline" id="Transaction_search"> Transaction search </span></a></h3>
<p>A transaction search enables you to identify transaction events that each stretch over multiple logged events. Use the <code><font size="2">transaction</font></code> command and its options to define a search that returns transactions (groups of events). See the documentation of the command in the Search Reference for a variety of examples that show you how you can: 
</p>
<ul><li> Find groups of events where the first and last events are separated by a span of time that does not exceed a certain amount (set with the <code><font size="2">maxspan</font></code> option) 
</li><li> Find groups of events where the span of time between included events does not exceed a specific value (set with the <code><font size="2">maxpause</font></code> option).
</li><li> Find groups of related events where the total number of events does not exceed a specific number (set with the <code><font size="2">maxevents</font></code> option)
</li><li> Design a transaction that finds event groups where the final event contains a specific text string (set with the <code><font size="2">endswith</font></code> option). 
</li></ul><p>Study the <code><font size="2">transaction</font></code> command topic to get the full list of available options for the command.
</p><p>You can also use the <code><font size="2">transaction</font></code> command to override transaction options that you have configured in <code><font size="2">transactiontypes.conf</font></code>.
</p><p>To learn more about searching with <code><font size="2">transaction</font></code>, read "Identify and group events into transactions" in the Search Manual.
</p>
<h3> <a name="abouttransactions_configure_transaction_types"><span class="mw-headline" id="Configure_transaction_types"> Configure transaction types </span></a></h3>
<p>After you create a transaction search that you find worthy of repeated reuse, you can make it persistable by adding it to <code><font size="2">transactiontypes.conf</font></code> as a transaction type.
</p><p>To learn more about configuring transaction types, read <a href="#definetransactions" class="external text">"Configure transaction types,"</a> in this manual.
</p>
<h3> <a name="abouttransactions_when_to_use_stats_instead_of_transactions"><span class="mw-headline" id="When_to_use_stats_instead_of_transactions"> When to use stats instead of transactions </span></a></h3>
<p>Transactions aren't the most efficient method to compute aggregate statistics on transactional data. If you want to compute aggregate statistics over transactions that are defined by data in a single field, use the <code><font size="2">stats</font></code> command.
</p><p>For example, if you wanted to compute the statistics of the duration of a transaction defined by the field <code><font size="2">session_id</font></code>:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">* | stats min(_time) AS earliest max(_time) AS latest by session_id | eval duration=latest-earliest | stats min(duration) max(duration) avg(duration) median(duration) perc95(duration)</font></code><br></div>
<p>Similary, if you wanted to compute the number of hits per <code><font size="2">clientip</font></code> in an access log:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_combined | stats count by clientip | sort -count</font></code><br></div>
<p>Also, if you wanted to compute the number of distinct session (parameterized by <code><font size="2">cookie</font></code>) per <code><font size="2">clientip</font></code> in an access log:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_combined | stats dc(cookie) as sessions by clientip | sort -sessions</font></code><br></div>
<p>Read the stats command reference for more information about using the search command.
</p>
<a name="searchfortransactions"></a><h2> <a name="searchfortransactions_search_for_transactions"><span class="mw-headline" id="Search_for_transactions"> Search for transactions</span></a></h2>
<p>Search for transactions using the transaction search command either in Splunk Web or at the CLI. The <code><font size="2">transaction</font></code> command yields groupings of events which can be used in reports. To use <code><font size="2">transaction</font></code>, either call a transaction type (that you <a href="#definetransactions" class="external text">configured via transactiontypes.conf</a>), or define transaction constraints in your search by setting the search options of the <code><font size="2">transaction</font></code> command.
</p>
<h3> <a name="searchfortransactions_search_options"><span class="mw-headline" id="Search_options"> Search options </span></a></h3>
<p>Transactions returned at search time consist of the raw text of each event, the shared event types, and the field values. Transactions also have additional data that is stored in the fields: <code><font size="2">duration</font></code> and <code><font size="2">transactiontype</font></code>. 
</p>
<ul><li> <b><code><font size="2">duration</font></code></b> contains the duration of the transaction (the difference between the timestamps of the first and last events of the transaction). 
</li><li> <b><code><font size="2">transactiontype</font></code></b> is the name of the transaction (as defined in <code><font size="2">transactiontypes.conf</font></code> by the transaction's stanza name).
</li></ul><p>You can add <code><font size="2">transaction</font></code> to any search. For best search performance, craft your search and then pipe it to the transaction command. For more information see the topic on the <code><font size="2">transaction</font></code> command in the Search Reference manual.
</p><p>Follow the <code><font size="2">transaction</font></code> command with the following options.  <b>Note:</b> Some <code><font size="2">transaction</font></code> options do not work in conjunction with others.
</p><p><i>[field-list]</i>
</p>
<ul><li> This is a comma-separated list of fields, such as <code><font size="2">...|transaction host,cookie</font></code>	
</li><li> If set, each event must have the same field(s) to be considered part of the same transaction.
</li><li> Events with common field names and different values will not be grouped. 
<ul><li> For example, if you add <code><font size="2">...|transaction host</font></code>, then a search result that has <code><font size="2">host=mylaptop</font></code> can never be in the same transaction as a search result with <code><font size="2">host=myserver</font></code>. 
</li><li> A search result that has no <code><font size="2">host</font></code> value can be in a transaction with a result that has <code><font size="2">host=mylaptop</font></code>. 
</li></ul></li></ul><p><code><font size="2">match=closest</font></code>
</p>
<ul><li> Specify the matching type to use with a transaction definition. 
</li><li> The only value supported currently is closest.
</li></ul><p><code><font size="2">maxspan=[&lt;integer&gt; s|m|h|d]</font></code>
</p>
<ul><li> Set the maximum span across events in a transaction.
</li><li> Can be in seconds, minutes, hours or days. 
<ul><li> For example:  5s, 6m, 12h or 30d.
</li></ul></li><li> Defaults to <code><font size="2">maxspan=-1</font></code>, for an "all time" timerange.
</li></ul><p><code><font size="2">maxpause=[&lt;integer&gt; s|m|h|d]</font></code>
</p>
<ul><li> Specifies the maximum pause between transactions. 
</li><li> Requires there be no pause between the events within the transaction greater than maxpause. 
</li><li> If the value is negative, the maxspause constraint is disabled. 
</li><li> Defaults to <code><font size="2">maxpause=-1</font></code>. 
</li></ul><p><code><font size="2">startswith=&lt;string&gt;</font></code>
</p>
<ul><li> A search or eval-filtering expression which, if satisfied by an event, marks the beginning of a new transaction. 
</li><li> For example:
<ul><li> <code><font size="2">startswith="login"</font></code>
</li><li> <code><font size="2">startswith=(username=foobar)</font></code>
</li><li> <code><font size="2">startswith=eval(speed_field &lt; max_speed_field)</font></code>
</li><li> <code><font size="2">startswith=eval(speed_field &lt; max_speed_field/12)</font></code>
</li></ul></li><li> Defaults to <code><font size="2">""</font></code>.
</li></ul><p><code><font size="2">endswith=&lt;transam-filter-string&gt;</font></code>
</p>
<ul><li> A search or eval-filtering expression which, if satisfied by an event, marks the end of a transaction. 
</li><li> For example:
<ul><li> <code><font size="2">endswith="logout"</font></code>
</li><li> <code><font size="2">endswith=(username=foobar)</font></code>
</li><li> <code><font size="2">endswith=eval(speed_field &lt; max_speed_field)</font></code>
</li><li> <code><font size="2">endswith=eval(speed_field &lt; max_speed_field/12)</font></code>
</li></ul></li><li> Defaults to <code><font size="2">""</font></code>.
</li></ul><p>For <code><font size="2">startswith</font></code> and <code><font size="2">endswith</font></code>, <code><font size="2">&lt;transam-filter-string&gt;</font></code> is defined with the following syntax:
<code><font size="2">"&lt;search-expression&gt;" | (&lt;quoted-search-expression&gt;) | eval(&lt;eval-expression&gt;</font></code>
</p>
<ul><li> <code><font size="2">&lt;search-expression&gt;</font></code> is a valid search expression that does not contain quotes.
</li><li> <code><font size="2">&lt;quoted-search-expression&gt;</font></code> is a valid search expression that contains quotes.
</li><li> <code><font size="2">&lt;eval-expression&gt;</font></code> is a valid eval expression that evaluates to a boolean.
</li></ul><p>Examples:
</p>
<ul><li> search expression: <code><font size="2">(name="foo bar")</font></code>
</li><li> search expression: <code><font size="2">"user=mildred"</font></code>
</li><li> search expression: <code><font size="2">("search literal")</font></code>
</li><li> eval bool expression: <code><font size="2">eval(distance/time &lt; max_speed)</font></code>
</li></ul><h3> <a name="searchfortransactions_transactions_and_macro_search"><span class="mw-headline" id="Transactions_and_macro_search"> Transactions and macro search </span></a></h3>
<p>Transactions and macro searches are a powerful combination that allow substitution into your transaction searches.  Make a transaction search and then save it with <code><font size="2">$field$</font></code> to allow substitution. 
</p><p>For an example of how to use macro searches and transactions, see "Create and use search macros" in the Search Manual.
</p>
<h3> <a name="searchfortransactions_example_transaction_search"><span class="mw-headline" id="Example_transaction_search"> Example transaction search </span></a></h3>
<p><b>Run a search that groups together all of the web pages a single user (or client IP address) looked at over a time range.</b> 
</p><p>This search takes events from the access logs, and creates a transaction from events that share the same <code><font size="2">clientip</font></code> value that occurred within 5 minutes of each other (within a 3 hour time span).   
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_combined  | transaction clientip maxpause=5m maxspan=3h</font></code><br></div>

<a name="definetransactions"></a><h2> <a name="definetransactions_configure_transaction_types"><span class="mw-headline" id="Configure_transaction_types"> Configure transaction types</span></a></h2>
<p>Any series of events can be turned into a transaction type.  Read more about use cases in <a href="#abouttransactions" class="external text">"About transactions"</a>, in this manual.
</p><p>You can create transaction types via transactiontypes.conf. See below for configuration details. 
</p><p>For more information on configuration files in general, see "About configuration files" in the Admin manual.
</p>
<h3> <a name="definetransactions_configure_transaction_types_in_transactiontypes.conf"><span class="mw-headline" id="Configure_transaction_types_in_transactiontypes.conf"> Configure transaction types in transactiontypes.conf </span></a></h3>
<p><b>1.</b> Create a <code><font size="2">transactiontypes.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.
</p><p><b>2.</b> Define transactions by creating a stanza and listing specifications for each transaction within its stanza.  Use the following attributes:
</p>
<code><font size="2"><br>[&lt;transactiontype&gt;]<br>maxspan = &nbsp;[&lt;integer&gt; s|m|h|d|-1]<br>maxpause = [&lt;integer&gt; s|m|h|d|-1]<br>fields = &lt;comma-separated list of fields&gt;<br>startswith = &lt;transam-filter-string&gt;<br>endswith=&lt;transam-filter-string&gt;<br></font></code>
<p><code><font size="2">[&lt;TRANSACTIONTYPE&gt;]</font></code>
</p>
<ul><li> Create any number of transaction types, each represented by a stanza name and any number of the following attribute/value pairs.
</li><li> Use the stanza name, <code><font size="2">[&lt;TRANSACTIONTYPE&gt;]</font></code>, to search for the transaction in Splunk Web.
</li><li> If you do not specify an entry for each of the following attributes, Splunk Enterprise uses the default value.
</li></ul><p><code><font size="2">maxspan = [&lt;integer&gt; s|m|h|d|-1]</font></code>
</p>
<ul><li> Set the maximum time span for the transaction.
</li><li> Can be in seconds, minutes, hours or days, or set to -1 for unlimited.
<ul><li> For example:  5s, 6m, 12h or 30d.
</li></ul></li><li> Defaults to -1.
</li></ul><p><code><font size="2">maxpause = [&lt;integer&gt; s|m|h|d|-1]</font></code>
</p>
<ul><li> Set the maximum pause between the events in a transaction.
</li><li> Can be in seconds, minutes, hours or days, or set to -1 for unlimited.
<ul><li> For example:  5s, 6m, 12h or 30d.
</li></ul></li><li> Defaults to -1.
</li></ul><p><code><font size="2">maxevents = &lt;integer&gt;</font></code>
</p>
<ul><li> The maximum number of events in a transaction. This constraint is disabled if the value is a negative integer.
</li><li> Defaults to 1000.
</li></ul><p><code><font size="2">fields = &lt;comma-separated list of fields&gt;</font></code>
</p>
<ul><li> If set, each event must have the same field(s) to be considered part of the same transaction.
<ul><li> For example: <code><font size="2">fields = host,cookie</font></code>
</li></ul></li><li> Defaults to " ".
</li></ul><p><code><font size="2">connected= [true|false]</font></code>
</p>
<ul><li> Relevant only if <code><font size="2">fields</font></code> is not empty. Controls whether an event that is not inconsistent and not consistent with the fields of a transaction opens a new transaction (connected=true) or is added to the transaction. 
</li><li> An event can be not inconsistent and not consistent if it contains fields required by the transaction but none of these fields has been instantiated in the transaction (by a previous event addition).
</li><li> Defaults to: <code><font size="2">connected = true</font></code>
</li></ul><p><code><font size="2">startswith = &lt;transam-filter-string&gt;</font></code>
</p>
<ul><li> A search or eval filtering expression which, if satisfied by an event, marks the beginning of a new transaction
</li><li> For example:
<ul><li> <code><font size="2">startswith="login"</font></code>
</li><li> <code><font size="2">startswith=(username=foobar)</font></code>
</li><li> <code><font size="2">startswith=eval(speed_field &lt; max_speed_field)</font></code>
</li><li> <code><font size="2">startswith=eval(speed_field &lt; max_speed_field/12)</font></code>
</li></ul></li><li> Defaults to: " ".
</li></ul><p><code><font size="2">endswith=&lt;transam-filter-string&gt;</font></code>
</p>
<ul><li> A search or eval filtering expression which if satisfied by an event marks the end of a transaction
</li><li> For example:
<ul><li> <code><font size="2">endswith="logout"</font></code>
</li><li> <code><font size="2">endswith=(username=foobar)</font></code>
</li><li> <code><font size="2">endswith=eval(speed_field &gt; max_speed_field)</font></code>
</li><li> <code><font size="2">endswith=eval(speed_field &gt; max_speed_field/12)</font></code>
</li></ul></li><li> Defaults to: " "
</li></ul><p>For both <code><font size="2">startswith</font></code> and <code><font size="2">endswith</font></code>, <code><font size="2">&lt;transam-filter-string&gt;</font></code> has the following syntax: 
</p><p><code><font size="2">"&lt;search-expression&gt;" | (&lt;quoted-search-expression&gt; | eval(&lt;eval-expression&gt;)</font></code>
</p><p>Where:
</p>
<ul><li> <code><font size="2">&lt;search-expression&gt;</font></code> is a valid search expression that does not contain quotes.
</li><li> <code><font size="2">&lt;quoted-search-expression&gt;</font></code> is a valid search expression that contains quotes.
</li><li> <code><font size="2">&lt;eval-expression&gt;</font></code> is a valid eval expression that evaluates to a boolean. For example, <code><font size="2">startswith=eval(foo&lt;bar*2)</font></code> will match events where <code><font size="2">foo</font></code> is less than 2 x <code><font size="2">bar</font></code>.
</li></ul><p>Examples:
</p>
<ul><li> <code><font size="2">"&lt;search-expression&gt;"</font></code>: <code><font size="2">startswith="foo bar"</font></code>
</li><li> <code><font size="2">&lt;quoted-search-expression&gt;</font></code>: <code><font size="2">startswith=(name="foo bar")</font></code>
</li><li> <code><font size="2">&lt;quoted-search-expression&gt;</font></code>: <code><font size="2">startswith=("search literal")</font></code>
</li><li> <code><font size="2">eval(&lt;eval-expression&gt;)</font></code>: <code><font size="2">eval(distance/time &lt; max_speed)</font></code>
</li></ul><p><b>3.</b> Use the transaction command in Splunk Web to call your defined transaction (by its transaction type name). You can override configuration specifics during search. 
</p><p>For more information about searching for transactions, see <a href="#searchfortransactions" class="external text">"Search for transactions"</a> in this manual.
</p>
<h3> <a name="definetransactions_additional_transaction_configuration_attributes"><span class="mw-headline" id="Additional_transaction_configuration_attributes">Additional transaction configuration attributes</span></a></h3>
<p><code><font size="2">transactions.conf</font></code> includes a few more sets of attributes that are designed to handle situations such as multivalue fields and memory constraint issues. 
</p>
<h4><font size="3"><b><i> <a name="definetransactions_transaction_options_for_memory_constraint_issues"><span class="mw-headline" id="Transaction_options_for_memory_constraint_issues">Transaction options for memory constraint issues</span></a></i></b></font></h4>
<p><code><font size="2">maxopentxn=&lt;int&gt;</font></code>
</p>
<ul><li> Specifies the maximum number of not yet closed transactions to keep in the open pool before starting to evict transactions, using LRU (least-recently-used memory cache algorithm) policy.
</li><li> The default value of this attribute is read from the transactions stanza in <code><font size="2">limits.conf</font></code>.
</li></ul><p><code><font size="2">maxopenevents=&lt;int&gt;</font></code>
</p>
<ul><li> Specifies the maximum number of events (which are) part of open transactions before transaction eviction starts happening, using LRU (least-recently-used memory cache algorithm) policy.
</li><li> The default value of this attribute is read from the transactions stanza in <code><font size="2">limits.conf</font></code>.
</li></ul><p><code><font size="2">keepevicted=[true|false]</font></code>
</p>
<ul><li> Whether to output evicted transactions. Evicted transactions can be distinguished from non-evicted transactions by checking the value of the <code><font size="2">evicted</font></code> field, which is set to <code><font size="2">1</font></code> for evicted transactions.
</li><li> Defaults to <code><font size="2">keepevicted=false</font></code>.
</li></ul><h4><font size="3"><b><i> <a name="definetransactions_transaction_options_for_rendering_multivalue_fields"><span class="mw-headline" id="Transaction_options_for_rendering_multivalue_fields">Transaction options for rendering multivalue fields</span></a></i></b></font></h4>
<p><code><font size="2">mvlist=[true|false]|&lt;field-list&gt;</font></code>
</p>
<ul><li> The <code><font size="2">mvlist</font></code> attribute controls whether the multivalue fields of the transaction are (1) a list of the original events ordered in arrival order or (2) a set of unique field values ordered lexigraphically. If a comma- or space-delimited list of fields is provided, only those fields are rendered as lists.
</li><li> Defaults to: <code><font size="2">mvlist=false</font></code>.
</li></ul><p><code><font size="2">delim=&lt;string&gt;</font></code>
</p>
<ul><li> A string used to delimit the original event values in the transaction event fields.
</li><li> Defaults to: <code><font size="2">delim=" "</font></code>
</li></ul><p><code><font size="2">nullstr=&lt;string&gt;</font></code>
</p>
<ul><li> The string value to use when rendering missing field values as part of multivalue fields in a transaction.
</li><li> This option applies <i>only</i> to fields that are rendered as lists.
</li><li> Defaults to: <code><font size="2">nullstr=NULL</font></code>
</li></ul><h1>Data enrichment: Lookups and workflow actions</h1><a name="aboutlookupsandfieldactions"></a><h2> <a name="aboutlookupsandfieldactions_about_lookups_and_workflow_actions"><span class="mw-headline" id="About_lookups_and_workflow_actions"> About lookups and workflow actions</span></a></h2>
<p>Lookups and workflow actions enable you to enrich and extend the usefulness of your event data through interactions with external resources. 
</p>
<h3> <a name="aboutlookupsandfieldactions_lookup_tables"><span class="mw-headline" id="Lookup_tables"> Lookup tables </span></a></h3>
<p><b>Lookup tables</b> use information in your events to determine how to add other fields from external data sources such as static tables (CSV files), Python- and binary-based scripts, and App Key Value Store (KV Store) collections. Each of these lookup types can optionally add fields based on time information. 
</p><p>An example of this functionality would be a CSV lookup that takes the <code><font size="2">http_status</font></code> value in an event, matches that value with its definition in a CSV file, and then adds that definition to the event as the value of a new <code><font size="2">status_description</font></code> field. So if you have an event where <code><font size="2">http_status = 503</font></code> the lookup would add <code><font size="2">status_description = Service Unavailable, Server Error</font></code> to that event. 
</p><p>Of course, there are more advanced ways to work with lookups. For example, you can:
</p>
<ul><li> Arrange to have a static lookup table be populated by the results of a report. 
</li><li> Define a field lookup that is based on an external Python script rather than a lookup table. For example, you could create a lookup that uses a Python script that returns an IP address when given a host name, and returns a host name when given an IP address.  
</li><li> Define a lookup that matches fields in your events with fields in a KV Store lookup, and then returns fields to your events. You can also design searches that write search results to KV Store collections.
</li><li> Create a time-based lookup, if you are working with a lookup table that includes a field value that represents time. For example, this could come in handy if you need to use DHCP logs to identify users on your network based on their IP address and the event timestamp. 
</li></ul><p>For more information, see <a href="#addfieldsfromexternaldatasources" class="external text">"Configure CSV and external lookups"</a> and "<a href="#configurekvstorelookups" class="external text">Configure KV Store lookups</a>"in this manual.
</p>
<h3> <a name="aboutlookupsandfieldactions_workflow_actions"><span class="mw-headline" id="Workflow_actions"> Workflow actions </span></a></h3>
<p><b>Workflow actions</b> enable you to set up interactions between specific fields in your data and other applications or web resources. A really simple workflow action would be one that is associated with an <code><font size="2">IP_address</font></code> field, which, when launched, opens an external WHOIS search in a separate browser window based on the <code><font size="2">IP_address</font></code> value. 
</p><p>You can also set up workflow actions that:
</p>
<ul><li> Apply only to particular fields (as opposed to all fields in an event).
</li><li> Apply only to events belonging to a specific event type or group of event types.
</li><li> Are accessed either via event dropdown menus, field dropdown menus, or both.
</li><li> Perform HTTP GET requests, enabling you to pass information to an external web resource, such as a search engine or IP lookup service.
</li><li> Perform HTTP POST requests that can send field values to an external resource. For example, you could design one that sends a status value to an external issue-tracking application.
</li><li> Take certain field values from a chosen event and insert them into a secondary search that is populated with those field values and which launches in a secondary browser window. 
</li></ul><p>For information about setting workflow actions up in Splunk Web, see <a href="#createworkflowactionsinsplunkweb" class="external text">"Create and maintain workflow actions in Splunk Web"</a>, in this chapter.
</p>
<a name="usefieldlookupstoaddinformationtoyourevents"></a><h2> <a name="usefieldlookupstoaddinformationtoyourevents_use_field_lookups_to_add_information_to_your_events"><span class="mw-headline" id="Use_field_lookups_to_add_information_to_your_events"> Use field lookups to add information to your events </span></a></h2>
<p>The lookup feature lets you reference fields in an external CSV file that match fields in your event data. Using this match, you can enrich your event data by adding more searchable fields to them. You can base your field lookups on any field, including a temporal field, or on the output of a Python script. 
</p><p>This topic discusses how to use the Lookups page, located in Splunk Web, at <b>Settings &gt; Lookups</b>, to:
</p>
<ul><li> List existing lookup tables or upload a new file.
</li><li> Edit existing lookup definitions or define a new file-based or external lookup.
</li><li> Edit existing automatic lookups or configure a new lookup to run automatically.
</li></ul><p>For more details about lookups, see <a href="#addfieldsfromexternaldatasources" class="external text">"Configure CSV and external lookups"</a> and "<a href="#configurekvstorelookups" class="external text">Configure KV store lookups</a>" in this manual.
</p>
<h3> <a name="usefieldlookupstoaddinformationtoyourevents_list_existing_lookup_tables_or_upload_a_new_file"><span class="mw-headline" id="List_existing_lookup_tables_or_upload_a_new_file"> List existing lookup tables or upload a new file </span></a></h3>
<p>View existing lookup table files in <b>Settings &gt; Lookups &gt; Lookup table files</b>, or click "Add new" to upload more CSV files to use in your definitions for file-based lookups.
</p><p><b>Note:</b> CSV files with Pre-OSX OS9 and earlier "Classic Macintosh"-style line endings (aka just a carriage return, "\r") are not supported. 
</p><p>To upload new files:
</p><p><b>1.</b> Select a <b>Destination app</b>.
</p><p>This tells Splunk Enterprise to save your lookup table file in the app's directory: <code><font size="2">$SPLUNK_HOME/etc/users/&lt;username&gt;/&lt;app_name&gt;/lookups/</font></code>.
</p><p><b>2.</b> Give your lookup table file a <b>Name</b>.
</p><p>This will be the name you use to refer to the file in a lookup definition.
</p><p><b>3.</b> <b>Browse</b> for the CSV file to upload.
</p><p><b>4.</b> Click <b>Save</b>.
</p>
<h3> <a name="usefieldlookupstoaddinformationtoyourevents_edit_existing_lookup_definitions_or_define_a_new_file-based_or_external_lookup"><span class="mw-headline" id="Edit_existing_lookup_definitions_or_define_a_new_file-based_or_external_lookup"> Edit existing lookup definitions or define a new file-based or external lookup </span></a></h3>
<p>Use the <b>Settings &gt; Lookups &gt; Lookup definitions</b> page to define the lookup table or edit existing lookup definitions. You can specify the type of lookup (file-based or external) and whether or not it is time-based. Once you've defined the lookup table, you can invoke the lookup in a search (using the lookup command) or you can configure the lookup to occur automatically.
</p><p><b>Note:</b> This is equivalent to defining your lookup in transforms.conf.
</p>
<h4><font size="3"><b><i> <a name="usefieldlookupstoaddinformationtoyourevents_configure_a_time-based_lookup"><span class="mw-headline" id="Configure_a_time-based_lookup"> Configure a time-based lookup </span></a></i></b></font></h4>
<p>File-based and external lookups can also be time-based (or temporal), if the field matching depends on time information (a field in the lookup table that represents the timestamp).
</p><p>To <b>Configure a time-based lookup</b>, select <b>Configure time-based lookup</b>, then specify the <i>Name of the time field</i>. You can also specify a strptime format for this time information and offsets for the time matching.
</p>
<h4><font size="3"><b><i> <a name="usefieldlookupstoaddinformationtoyourevents_include_advanced_options"><span class="mw-headline" id="Include_advanced_options"> Include advanced options </span></a></i></b></font></h4>
<p>Under <b>Advanced options</b>, you can specify:
</p>
<ul><li> The minimum number of matches for each input lookup value. 
</li><li> The maximum number of matches for each input lookup value. 
</li><li> A default value to output if fewer than the minimum number of matches are present for a given input.
</li></ul><h3> <a name="usefieldlookupstoaddinformationtoyourevents_edit_existing_automatic_lookups_or_configure_a_new_lookup_to_run_automatically"><span class="mw-headline" id="Edit_existing_automatic_lookups_or_configure_a_new_lookup_to_run_automatically"> Edit existing automatic lookups or configure a new lookup to run automatically </span></a></h3>
<p>Instead of invoking the lookup command when you want to apply a fields lookup to your events, you can set the lookup to run automatically. Use the <b>Settings&gt; Lookups &gt; Automatic lookups</b> page to edit or configure automatic lookups:
</p><p>To edit an existing automatic lookup, select the lookup and modify the values in the fields displayed for that lookup.
</p><p>To add a new new lookup to run automatically:
</p><p><b>1.</b> Select <b>New</b> in the <b>Automatic lookups</b> page:
</p><p><b>2.</b> Select the <b>Destination app</b>. 
</p><p><b>3.</b> Select the <b>Lookup table</b> that you want use in your fields lookup.
</p>
<dl><dd> This is the name of the lookup definition that you define on the Lookup Definition page.
</dd></dl><p><b>4.</b> In the <b>Apply to</b> menu, select a host, source, or source type value to apply the lookup and give it a name.
</p><p><b>5.</b> Under <b>Lookup input fields</b> provide one or more pairs of input fields.
</p>
<dl><dd> The first field is the field in the lookup table that you want to match. The second field is a field from your events that should match the lookup table field. For example, you could have an <code><font size="2">ip_address</font></code> field in your events that matches an <code><font size="2">ip</font></code> field in the lookup table. So you would enter <code><font size="2">ip = ip_address</font></code> in the automatic lookup definition.
</dd></dl><p><b>6.</b> Under <b>Lookup output fields</b> provide one or more pairs of output fields.
</p>
<dl><dd> The first field is the corresponding field that you want to output to events. The second field is the name that the output field should have in your events. For example the lookup table may have a field named <code><font size="2">country</font></code> that you may want to output to your events as <code><font size="2">ip_city</font></code>. 
</dd></dl><p><b>7.</b> You can also select <b>Overwrite field values</b> to overwrite the field values each time the lookup runs.
</p><p><b>Note:</b> This is equivalent to configuring your fields lookup in <code><font size="2">props.conf</font></code>.
</p>
<h3> <a name="usefieldlookupstoaddinformationtoyourevents_example_of_http_status_lookup"><span class="mw-headline" id="Example_of_HTTP_status_lookup"> Example of HTTP status lookup </span></a></h3>
<p>This examples walks through defining a static lookup that adds two informational fields, <code><font size="2">status_description</font></code> and <code><font size="2">status_type</font></code>, into your Web access events. This lets you search for the events you want when you might not know the specific error code. For example, instead of searching for all the server error codes, you can use <code><font size="2">status="Server Error"</font></code>.
</p>
<h4><font size="3"><b><i> <a name="usefieldlookupstoaddinformationtoyourevents_upload_the_lookup_table_to_splunk_enterprise"><span class="mw-headline" id="Upload_the_lookup_table_to_Splunk_Enterprise"> Upload the lookup table to Splunk Enterprise </span></a></i></b></font></h4>
<p><b>1.</b> Download the <code><font size="2">http_status.csv</font></code> file:
</p><p>http_status.csv
</p><p>Here's a sampling of the file:
</p>
<div class="samplecode">
<code><font size="2"><br>status,status_description,status_type<br>100,Continue,Informational<br>101,Switching Protocols,Informational<br>200,OK,Successful<br>201,Created,Successful<br>202,Accepted,Successful<br>203,Non-Authoritative Information,Successful<br>...<br></font></code></div>
<p><b>2.</b> Go back to the Search app, then select <b>Settings &gt; Lookups</b>.
</p><p><img alt="Manager from search b.png" src="images/e/ea/Manager_from_search_b.png" width="426" height="190"></p><p><b>3.</b> In the <i>Lookups</i> page, select <b>Add new</b> for <b>Lookup table files</b>.
</p><p><img alt="Add new lookup file b.png" src="images/4/4c/Add_new_lookup_file_b.png" width="650" height="241"></p><p><b>4.</b> In the <b>Add new</b> page,
</p>
<ul><li> Select <i>search</i> for the destination app.
</li><li> Browse for the CSV file that you downloaded earlier.
</li><li> Name the lookup table <i>http_status</i>.
</li><li> Click <i>Save</i>.
</li></ul><p><img alt="Upload http status 4.2 b.png" src="images/f/f4/Upload_http_status_4.2_b.png" width="650" height="266"></p><p>After Splunk Enterprise saves the file, it takes you to the following view:
</p><p><img alt="Upload lookup table file b.png" src="images/3/3c/Upload_lookup_table_file_b.png" width="650" height="285"></p><p>Now, let's go back to the <b>Settings &gt; Lookups</b> view. To do this, click on the <b>Lookups</b> link in the page's breadcrumb. You can always use this to navigate back to a previous view.
</p><p><img alt="Back to manager lookups b.png" src="images/d/d1/Back_to_manager_lookups_b.png" width="350" height="107"></p>
<h4><font size="3"><b><i> <a name="usefieldlookupstoaddinformationtoyourevents_define_the_lookup"><span class="mw-headline" id="Define_the_lookup"> Define the lookup </span></a></i></b></font></h4>
<p><b>1.</b> From <i>Settings &gt; Lookups</i>, select <b>Add new</b> for <b>Lookup definitions</b>. 
</p><p>In the <i>Add new</i> page:
</p><p><img alt="Manager add new lookup definition b.png" src="images/2/2d/Manager_add_new_lookup_definition_b.png" width="650" height="463"></p><p><b>2.</b>  Select <b>search</b> for the <b>Destination app</b>.
</p><p><b>3.</b>  <b>Name</b> your lookup definition <i>http_status</i>.
</p><p><b>4.</b>  Select <i>File-based</i> under <b>Type</b>.
</p><p><b>5.</b>  Click <i>Save</i>.
</p><p>After Splunk Enterprise saves your lookup definition, it takes you to the following page:
</p><p><img alt="Lookup def saved-b.png" src="images/3/3f/Lookup_def_saved-b.png" width="650" height="422"></p><p>Notice there are some actions you can take on your lookup definition. <b>Permissions</b> lets you change the accessibility of the lookup table. You can <b>Disable</b>, <b>Clone</b>, and <b>Move</b> the lookup definition to a different app. Or, you can <b>Delete</b> the definition.
</p><p>Once you define the lookup, you can use the <code><font size="2">lookup</font></code> command to invoke it in a search or you can configure the lookup to run automatically.
</p>
<h4><font size="3"><b><i> <a name="usefieldlookupstoaddinformationtoyourevents_set_the_lookup_to_run_automatically"><span class="mw-headline" id="Set_the_lookup_to_run_automatically"> Set the lookup to run automatically </span></a></i></b></font></h4>
<p><b>1.</b> Return to the <i>Settings &gt; Lookups</i> view and select <i>Add new</i> for <b>Automatic lookups</b>.
</p><p>In the <b>Add new</b> page: 
</p><p><img alt="Add new automatic lookup b.png" src="images/3/3c/Add_new_automatic_lookup_b.png" width="541" height="634"></p><p><b>2.</b> Select <b>search</b> for the <b>Destination app</b>.
</p><p><b>3.</b> Name the lookup <b>http_status</b>.
</p><p><b>4.</b> Select <i>http_status</i> from the <b>Lookup table</b> drop down.
</p><p><b>5.</b> Apply the lookup to the <code><font size="2">sourcetype</font></code> named <code><font size="2">access_combined</font></code>.
</p><p><img alt="Apply lookup to field b.png" src="images/4/4e/Apply_lookup_to_field_b.png" width="231" height="57"></p><p><b>6.</b> <i>Lookup input fields</i> are the fields in our events that you want to match with the lookup table. Here, both are named <code><font size="2">status</font></code> (the CSV column name goes on the left and the field that you want to match goes on the right):
</p><p><img alt="Lookup input fields b.png" src="images/8/87/Lookup_input_fields_b.png" width="400" height="61"></p><p><b>7.</b> <i>Lookup output fields</i> are the fields from the lookup table that you want to add to your events: status_description and status_type. The CSV column name goes on the left and the field that you want to match goes on the right.
</p><p><img alt="Lookup output fields b.png" src="images/2/23/Lookup_output_fields_b.png" width="406" height="101"></p><p><b>8.</b> Click <i>Save</i>.
</p><p><img alt="Auto lookup saved 4.2 b.png" src="images/7/7f/Auto_lookup_saved_4.2_b.png" width="650" height="208"></p>
<a name="addfieldsfromexternaldatasources"></a><h2> <a name="addfieldsfromexternaldatasources_configure_csv_and_external_lookups"><span class="mw-headline" id="Configure_CSV_and_external_lookups"> Configure CSV and external lookups</span></a></h2>
<p>Lookups add fields from an external source to your events based on the values of fields currently present in those events. This topic discusses two kinds of lookups: CSV and external. 
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0"> Lookup type
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr><td valign="center" align="left"> <a href="#addfieldsfromexternaldatasources_create_a_csv_lookup_stanza" class="external text">CSV lookup</a>
</td><td valign="center" align="left">  Populates your events with fields pulled from CSV files. Also referred to as a "static lookup" because CSV files represent static tables of data. Each column in a CSV table is interpreted as the potential values of a field.
</td></tr><tr><td valign="center" align="left"> <a href="#addfieldsfromexternaldatasources_create_an_external_lookup_stanza" class="external text">External lookup</a>
</td><td valign="center" align="left"> Uses Python scripts or binary executables to populate your events with field values from an external source.
</td></tr></table><p>KV store lookups are a third kind of lookup that you can define using KV store collections. See <a href="#configurekvstorelookups" class="external text">"Configure KV store lookups"</a> in this manual.
</p><p>This topic shows you how to set up and manage CSV and external lookups by configuring lookup stanzas in  <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. Configuration files give you a greater degree of control over lookup design and behavior than you get when you set up lookup files using Splunk Web.
</p><p>If you do not have access to the <code><font size="2">.conf</font></code> files, or if you prefer to maintain lookups through Splunk Web whenever possible, you can configure all three lookup types using the pages at <b>Settings &gt; Lookups</b>. See <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">"Use lookups to add information to your events"</a> in this manual.
</p>
<h3> <a name="addfieldsfromexternaldatasources_step_1_-_add_a_lookup_stanza_to_transforms.conf"><span class="mw-headline" id="Step_1_-_Add_a_lookup_stanza_to_transforms.conf"> Step 1 - Add a lookup stanza to transforms.conf </span></a></h3>
<p>The lookup stanza identifies the lookup type (CSV or external) and the location of the lookup table. It can optionally include field matching rules and rules for time-bounded lookups. The details of how you configure the lookup in <code><font size="2">transforms.conf</font></code> vary according to the lookup type.
</p><p>If you want a lookup to be available globally, add its lookup stanza to the version of <code><font size="2">transforms.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>. If you want the lookup to be specific to a particular app, add its stanza to the version of <code><font size="2">transforms.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/</font></code>. 
</p><p><b>Caution:</b> Do not edit configuration files in <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>. 
</p>
<h4><font size="3"><b><i> <a name="addfieldsfromexternaldatasources_create_a_csv_lookup_stanza"><span class="mw-headline" id="Create_a_CSV_lookup_stanza">Create a CSV lookup stanza</span></a></i></b></font></h4>
<p>CSV lookups match field values from your events to field values in the static table represented by a CSV file. Then they output corresponding field values from that table to your events.
</p><p><b>1.</b> Add the CSV file for the lookup to your Splunk Enterprise implementation.
</p>
<dl><dd> The CSV file must be located in one of two places:
</dd></dl><dl><dd><dl><dd> <code><font size="2">$SPLUNK_HOME/etc/system/lookups</font></code>
</dd><dd> <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/lookups</font></code>
</dd></dl></dd></dl><dl><dd> Create the lookups directory if it does not exist.
</dd></dl><dl><dd> The table represented by the CSV file should have at least two columns. One of those columns should represent a field with a set of values that includes those belonging to a field in your events. The column does not have to have the same name as the event field. Any column can have multiple instances of the same value, as this represents a multivalue field. 
</dd></dl><dl><dd> The CSV file cannot contain non-utf-8 characters. Plain ascii text is ok, as is any character set that is also valid utf-8.
</dd></dl><dl><dd> CSV files with Pre-OS X (OS9 or earlier) Macintosh-style line endings (aka carriage return only, "\r") are not supported. 
</dd></dl><p><b>2.</b> Add a CSV lookup stanza to <code><font size="2">transforms.conf</font></code>. 
</p>
<dl><dd> The CSV lookup stanza names the lookup table and provides the name of the CSV file that the lookup uses. It uses these required attributes.
</dd></dl><ul><li> <code><font size="2">[&lt;lookup_name&gt;]</font></code>: The name of the lookup.
</li><li> <code><font size="2">filename = &lt;string&gt;</font></code>: The name of the CSV file that the lookup references.
</li></ul><dl><dd> The CSV lookup stanza can an optionally include attributes that: 
</dd></dl><ul><li> Define field/value matching rules. See <a href="#addfieldsfromexternaldatasources_optional_field_matching_rules_for_lookup_configurations" class="external text">"Optional field matching rules for lookup configurations."</a>
</li><li> Define a time-bounded CSV lookup, also referred to as a temporal lookup. See <a href="#addfieldsfromexternaldatasources_set_up_a_time-bounded_lookup" class="external text">"Set up a time-bounded lookup."</a>
</li></ul><p><b>3.</b> Save your changes to the <code><font size="2">.conf</font></code> file.
</p><p><b>4.</b> If you want to make this an automatic lookup, <a href="#addfieldsfromexternaldatasources_step_2_-_make_the_lookup_automatic" class="external text">move on to Step 2</a>. 
</p>
<dl><dd> Otherwise, <a href="#addfieldsfromexternaldatasources_step_3_-_restart_splunk_enterprise" class="external text">go to Step 3</a> and restart Splunk Enterprise.
</dd></dl><h4><font size="3"><b><i> <a name="addfieldsfromexternaldatasources_create_an_external_lookup_stanza"><span class="mw-headline" id="Create_an_external_lookup_stanza">Create an external lookup stanza</span></a></i></b></font></h4>
<p>External lookups invoke a script that matches fields in your events with fields in an external source and outputs corresponding fields from that external source and adds them to your events. This kind of lookup is also referred to as a scripted lookup.
</p><p><b>1.</b> Add the script for the lookup to your Splunk Enterprise implementation.
</p>
<dl><dd> The script must be located in one of two places:
</dd></dl><ul><li> <code><font size="2">$SPLUNK_HOME/etc/searchscripts</font></code>
</li><li> <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/bin</font></code>
</li></ul><dl><dd> See "More about the external lookup script" for more information about how such scripts work.
</dd></dl><p><b>2.</b> Add an external lookup stanza to <code><font size="2">transforms.conf</font></code>. 
</p>
<dl><dd> The external lookup stanza names the lookup table, provides the script and argument to perform lookups, identifies the script type, and supplies a list of fields that are supported by the script. It uses these required attributes.
</dd></dl><ul><li> <code><font size="2">[&lt;lookup_name&gt;]</font></code>: The name of the lookup.
</li><li> <code><font size="2">external_cmd = &lt;string&gt;</font></code>: The command and arguments that Splunk Enterprise should invoke to perform the lookup. The command is expected to be the name of the script, such as <code><font size="2">external_lookup.py</font></code>. 
</li><li> <code><font size="2">external_type = [python|executable|kvstore]</font></code>: The type of script being used for the lookup. Can be <code><font size="2">python</font></code>, for a Python script, or <code><font size="2">executable</font></code>, for a binary executable. The <code><font size="2">kvstore</font></code> value is reserved for KV store lookups.
</li><li> <code><font size="2">fields_list = &lt;string&gt;</font></code>: is a list of all fields that are supported by the external lookup. The fields must be delimited by a comma followed by a space.
</li></ul><p>The external lookup stanza can optionally include attributes that: 
</p>
<ul><li> Define field/value matching rules. See <a href="#addfieldsfromexternaldatasources_optional_field_matching_rules_for_lookup_configurations" class="external text">"Optional field matching rules for lookup configurations."</a>
</li><li> Define a time-bounded external lookup, also referred to as a temporal lookup. See <a href="#addfieldsfromexternaldatasources_set_up_a_time-bounded_lookup" class="external text">"Set up a time-bounded lookup."</a>
</li></ul><p><b>3.</b> Save your changes to the <code><font size="2">.conf</font></code> file.
</p><p><b>4.</b> If you want to make this an automatic lookup, <a href="#addfieldsfromexternaldatasources_step_2_-_make_the_lookup_automatic" class="external text">move on to Step 2</a>. 
</p>
<dl><dd> Otherwise, <a href="#addfieldsfromexternaldatasources_step_3_-_restart_splunk_enterprise" class="external text">go to Step 3</a> and restart Splunk Enterprise.
</dd></dl><h3> <a name="addfieldsfromexternaldatasources_step_2_-_make_the_lookup_automatic"><span class="mw-headline" id="Step_2_-_Make_the_lookup_automatic">Step 2 - Make the lookup automatic</span></a></h3>
<p>In this step you create a lookup configuration in <code><font size="2">props.conf</font></code> that: 
</p>
<ul><li> References the lookup table you created in Step 1
</li><li> Specifies the fields in your events that the lookup should match in the lookup table 
</li><li> Specifies the corresponding fields that the lookup should output from the lookup table to your events. 
</li></ul><p>The result is an automatic lookup: a lookup that automatically adds fields to your events from the lookup table. 
</p><p>This step is the same for CSV and external lookup types. It does not apply to KV store lookups.
</p><p><b>Note:</b> This step is optional. Skip it if you do not want to create an automatic lookup: a lookup that automatically adds fields to your events from a lookup table whenever you run a search with a related host, source, or source type. If you do not create an automatic lookup you can still invoke the <code><font size="2">transforms.conf</font></code> configuration with the <code><font size="2">lookup</font></code>, <code><font size="2">inputlookup</font></code>, and <code><font size="2">outputlookup</font></code> commands.
</p><p>Automatic lookups can access any data that belongs to you or which is shared. When you access lookups with search commands, only shared data can be accessed.
</p><p><b>1.</b> In <code><font size="2">props.conf</font></code>, locate the stanza for the host, source, or source type that you want to associate this lookup with. 
</p>
<dl><dd> If the stanza does not exist, create it. 
</dd></dl><dl><dd> The format of the stanza header is <code><font size="2">[&lt;spec&gt;]</font></code>. <code><font size="2">&lt;spec&gt;</font></code> can be:
</dd></dl><ul><li> <code><font size="2">&lt;sourcetype&gt;</font></code>, the source type of an event.
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where &lt;host&gt;&lt;/code&gt; is the host, or host-matching pattern, for an event.
</li><li> <code><font size="2">source::&lt;source&gt;, where &lt;source&gt;</font></code> is the source, or source-matching pattern, for an event.
</li></ul><dl><dd> <code><font size="2">&lt;spec&gt;</font></code> cannot use regular expression syntax.
</dd></dl><p><b>2.</b> Add a <code><font size="2">LOOKUP-&lt;class&gt;</font></code> configuration to the stanza that you have identified or created.
</p>
<dl><dd> The <code><font size="2">LOOKUP-&lt;class&gt;</font></code> configuration outputs fields to events with the host, source, or source type of the stanza, if those events have fields that match fields in the lookup table. Follow this syntax:
</dd></dl><div class="samplecode">
<code><font size="2"><br>[&lt;spec&gt;]<br>LOOKUP-&lt;class&gt; = $TRANSFORM &lt;match_field_in_table&gt; OUTPUT|OUTPUTNEW &lt;output_field_in_table&gt;<br></font></code></div>
<ul><li> <code><font size="2">$TRANSFORM</font></code>: References the <code><font size="2">transforms.conf</font></code> stanza that defines the lookup table.
</li><li> <code><font size="2">match_field_in_table</font></code>: Each column in a CSV table represents a field and its possible values. This variable is the CSV table column that matches to a field in events with the same host, source, or source as the <code><font size="2">props.conf</font></code> stanza.
</li><li> <code><font size="2">output_field_in_table</font></code>: The column in the lookup table that you want Splunk Enterprise to add to your events. Use <code><font size="2">OUTPUTNEW</font></code> if you do not want to overwrite existing values in your output field.
</li><li> <code><font size="2">match_field_in_table</font></code>: Each column in a CSV table represents a field and its possible values. This variable is the CSV table column that matches to a field in events with the same host, source, or source as the <code><font size="2">props.conf</font></code> stanza.
</li><li> <code><font size="2">output_field_in_table</font></code>: The column in the lookup table that you want Splunk Enterprise to add to your events. 
</li></ul><dl><dd> You can have multiple fields on either side of the lookup. For example, you can have <code><font size="2">$TRANSFORM &lt;match_field1&gt;, &lt;match_field2&gt; OUTPUT|OUTPUTNEW &lt;match_field3&gt;, &lt;match_field4&gt;</font></code>. You can also have one matching field return two output fields, three matching fields return one output field, and so on. 
</dd></dl><dl><dd> If you do not include an <code><font size="2">OUTPUT|OUTPUTNEW</font></code> clause, Splunk Enterprise adds all the field names and values from the lookup table to your events.
</dd></dl><dl><dd> Use the <code><font size="2">AS</font></code> clause if the field names in the lookup table and your events do not match or if you want to <i>rename</i> the field in your event:
</dd></dl><div class="samplecode">
<code><font size="2"><br>[&lt;stanza name&gt;]<br>LOOKUP-&lt;class&gt; = $TRANSFORM &lt;match_field_in_table&gt; AS &lt;match_field_in_event&gt; <br>OUTPUT|OUTPUTNEW &lt;output_field_in_table&gt; AS &lt;output_field_in_event&gt;<br></font></code></div>
<p><b>Note:</b> You can have multiple <code><font size="2">LOOKUP-&lt;class&gt;</font></code> configurations in a single <code><font size="2">props.conf</font></code> stanza. Each lookup should have its own unique lookup name. For example, if you have multiple lookups, you can name them <code><font size="2">LOOKUP-table1</font></code>, <code><font size="2">LOOKUP-table2</font></code>, and so on.
</p>
<h3> <a name="addfieldsfromexternaldatasources_step_3_-_restart_splunk_enterprise"><span class="mw-headline" id="Step_3_-_Restart_Splunk_Enterprise">Step 3 - Restart Splunk Enterprise</span></a></h3>
<p>The changes you have made to the configuration files are not implemented until you restart Splunk Enterprise. 
</p><p>If you have set up an automatic lookup, after restart you should see the <code><font size="2">output</font></code> fields from your lookup table listed in the fields sidebar. From there, you can select the fields to display in each of the matching search results.
</p>
<h3> <a name="addfieldsfromexternaldatasources_csv_lookup_example"><span class="mw-headline" id="CSV_lookup_example"> CSV lookup example </span></a></h3>
<p>This example explains how you can set up a lookup for HTTP status codes in an <code><font size="2">access_combined</font></code> log. In this example, you design a lookup that matches the <code><font size="2">status</font></code> field in your events with the <code><font size="2">status</font></code> column in a lookup table named <code><font size="2">http_status.csv</font></code>. Then you have the lookup output the corresponding <code><font size="2">status_description</font></code> and <code><font size="2">status_type</font></code> fields to your events. 
</p><p>The following is the text for the <code><font size="2">http_status.csv</font></code> file. 
</p>
<div class="samplecode">
<code><font size="2"><br>status,status_description,status_type<br>100,Continue,Informational<br>101,Switching Protocols,Informational<br>200,OK,Successful<br>201,Created,Successful<br>202,Accepted,Successful<br>203,Non-Authoritative Information,Successful<br>204,No Content,Successful<br>205,Reset Content,Successful<br>206,Partial Content,Successful<br>300,Multiple Choices,Redirection<br>301,Moved Permanently,Redirection<br>302,Found,Redirection<br>303,See Other,Redirection<br>304,Not Modified,Redirection<br>305,Use Proxy,Redirection<br>307,Temporary Redirect,Redirection<br>400,Bad Request,Client Error<br>401,Unauthorized,Client Error<br>402,Payment Required,Client Error<br>403,Forbidden,Client Error<br>404,Not Found,Client Error<br>405,Method Not Allowed,Client Error<br>406,Not Acceptable,Client Error<br>407,Proxy Authentication Required,Client Error<br>408,Request Timeout,Client Error<br>409,Conflict,Client Error<br>410,Gone,Client Error<br>411,Length Required,Client Error<br>412,Precondition Failed,Client Error<br>413,Request Entity Too Large,Client Error<br>414,Request-URI Too Long,Client Error<br>415,Unsupported Media Type,Client Error<br>416,Requested Range Not Satisfiable,Client Error<br>417,Expectation Failed,Client Error<br>500,Internal Server Error,Server Error<br>501,Not Implemented,Server Error<br>502,Bad Gateway,Server Error<br>503,Service Unavailable,Server Error<br>504,Gateway Timeout,Server Error<br>505,HTTP Version Not Supported,Server Error<br></font></code></div>
<p><b>1.</b> Put the <code><font size="2">http_status.csv</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/apps/search/lookups/</font></code>. This indicates that the lookup is specific to the Search App.
</p><p><b>2.</b> In the <code><font size="2">transforms.conf</font></code> file located in  <code><font size="2">$SPLUNK_HOME/etc/apps/search/local</font></code>, put:
</p>
<div class="samplecode">
<code><font size="2"><br>[http_status]<br>filename = http_status.csv<br></font></code></div>
<p><b>3.</b> In the <code><font size="2">props.conf</font></code> file located in <code><font size="2">$SPLUNK_HOME/etc/apps/search/local/</font></code>, put:
</p>
<div class="samplecode">
<code><font size="2"><br>[access_combined]<br>LOOKUP-http = http_status status OUTPUT status_description, status_type<br></font></code></div>
<p><b>4.</b> Restart Splunk Enterprise.
</p><p>Now, when you run a search that returns Web access information for the <code><font size="2">access_combined</font></code> source type, you will see the fields <code><font size="2">status_description</font></code> and <code><font size="2">status_type</font></code> listed in your fields sidebar menu. All events with an <code><font size="2">http_status</font></code> value get corresponding <code><font size="2">status_description</font></code> and <code><font size="2">status_type</font></code> field/value pairs from the lookup.
</p>
<h3> <a name="addfieldsfromexternaldatasources_external_lookup_example"><span class="mw-headline" id="External_lookup_example"> External lookup example </span></a></h3>
<p>Here's an example of an external lookup that is delivered with Splunk Enterprise. It matches with information from a DNS server. It does not have a <code><font size="2">props.conf</font></code> component, so it is not an automatic lookup. You access it by running a search with the lookup command.
</p><p>Splunk Enterprise ships with a script located in <code><font size="2">$SPLUNK_HOME/etc/system/bin/</font></code> called <code><font size="2">external_lookup.py</font></code>, which is a DNS lookup script that:
</p>
<ul><li> if given a host, returns the IP address.
</li><li> if given an IP address, returns the host name.
</li></ul><p>Splunk also ships with a configuration for this script in <code><font size="2">$SPLUNK_HOME/etc/system/default/transforms.conf</font></code>.
</p>
<div class="samplecode">
<code><font size="2"><br>[dnslookup]<br>external_cmd = external_lookup.py clienthost clientip <br>fields_list = clienthost,clientip<br></font></code></div>
<p>You can run a search with the lookup command that uses the the <code><font size="2">[dnslookup]</font></code> stanza from the default <code><font size="2">transforms.conf</font></code>. 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_combined | lookup dnslookup clienthost AS host | stats count by clientip</font></code><br></div>
<p>This search:
</p>
<ul><li> Matches the <code><font size="2">clienthost</font></code> field in the external lookup table with the <code><font size="2">host</font></code> field in your events&lt;/code&gt;
</li><li> Returns a table that provides a count for each of the <code><font size="2">clientip</font></code> values that corresponds with the <code><font size="2">clienthost</font></code> matches. 
</li></ul><p>This search does not add fields to your events.
</p><p>You can design a search that performs a reverse lookup, which returns a host value for each IP address it receives.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_combined | lookup dnslookup clientip | stats count by clienthost</font></code><br></div>
<p>Note that this reverse lookup search does not include an AS clause. This is because Splunk automatically extracts IP addresses as <code><font size="2">clientip</font></code>.
</p>
<h4><font size="3"><b><i> <a name="addfieldsfromexternaldatasources_more_about_the_external_lookup_script"><span class="mw-headline" id="More_about_the_external_lookup_script"> More about the external lookup script </span></a></i></b></font></h4>
<p>Your external lookup script must take in a partially empty CSV file and output a filled-in CSV file. The arguments that you pass to the script are the headers for these input and output files.
</p><p>In the DNS lookup example above, the CSV file contains two fields: <code><font size="2">clienthost</font></code> and <code><font size="2">clientip</font></code>. The fields that you pass to this script are the ones you specify in <code><font size="2">transforms.conf</font></code> using the <code><font size="2">external_cmd</font></code> attribute. If you do not pass these arguments, the script returns an error.
</p>
<div class="samplecode">
<code><font size="2"><br>external_cmd = external_lookup.py clienthost clientip<br></font></code></div>
<p>When you run this search string:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">... | lookup dnsLookup clienthost</font></code><br></div>
<p>You are telling Splunk Enterprise to:
</p>
<ol><li> Use the lookup table that you defined in <code><font size="2">transforms.conf</font></code> as <code><font size="2">[dnsLookup]</font></code> 
</li><li> Pass the values for the <code><font size="2">clienthost</font></code> field into the external command script as a CSV file. The CSV file looks like this.
</li></ol><div class="samplecode">
<code><font size="2"><br>clienthost,clientip<br>work.com<br>home.net<br></font></code></div>
<p>This is a CSV file with <code><font size="2">clienthost</font></code> and <code><font size="2">clientip</font></code> as column headers, but without values for <code><font size="2">clientip</font></code>. The script includes the two headers because they are the fields you specified in the <code><font size="2">fields_list</font></code> attribute of the <code><font size="2">[dnslookup]</font></code> stanza in the default <code><font size="2">transforms.conf</font></code>.
</p><p>The script then outputs the following CSV file and returns it to Splunk Enterprise, which populates the <code><font size="2">clientip</font></code> field in your results:
</p>
<div class="samplecode">
<code><font size="2"><br>host,ip<br>work.com,127.0.0.1<br>home.net,127.0.0.2<br></font></code></div>
<p><b>Note:</b> When writing your script, if you refer to any external resources (such as a file), the reference must be relative to the directory where the script is located.
</p>
<h3> <a name="addfieldsfromexternaldatasources_optional_field_matching_rules_for_lookup_configurations"><span class="mw-headline" id="Optional_field_matching_rules_for_lookup_configurations">Optional field matching rules for lookup configurations </span></a></h3>
<p>These attributes provide field matching rules for lookups. They can be applied to all three lookup types. Add them to the <code><font size="2">transforms.conf</font></code> stanza for your lookup. 
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0"> Attribute
</th><th bgcolor="#C0C0C0"> Type
</th><th bgcolor="#C0C0C0"> Description
</th><th bgcolor="#C0C0C0"> Default
</th></tr><tr><td valign="center" align="left"> <code><font size="2">max_matches</font></code>
</td><td valign="center" align="left"> Integer
</td><td valign="center" align="left"> The maximum number of possible matches for each value input to the lookup table from your events. Range is 1-1000. If the <code><font size="2">time_field</font></code> attribute is is not specified, Splunk Enterprise uses the first <code><font size="2">&lt;integer&gt;</font></code> entries, in file order. If the <code><font size="2">time_field</font></code> attribute is specified (because it is a time-bounded lookup), Splunk Enterprise uses the first <code><font size="2">&lt;integer&gt;</font></code> entries, in descending time order. In other words, up to <code><font size="2">&lt;max_matches&gt;</font></code> are allowed to match. When this number is surpassed, Splunk Enterprise uses the matches closest to the lookup value.
</td><td valign="center" align="left"> 1000 if the <code><font size="2">time_field</font></code> attribute is not specified. 1 if the <code><font size="2">time_field</font></code> attribute is specified.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">min_matches</font></code>
</td><td valign="center" align="left"> Integer
</td><td valign="center" align="left"> The minimum number of possible matches for each value input to the lookup table from your events. You can use <code><font size="2">default_match</font></code> to help with situations where Splunk Enterprise finds fewer than <code><font size="2">min_matches</font></code> for any given input.
</td><td valign="center" align="left"> 0 for both non-time-bounded lookups and time-bounded lookups, which means Splunk Enterprise outputs nothing to your event if it cannot find a match.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">default_match</font></code>
</td><td valign="center" align="left"> String
</td><td valign="center" align="left"> When <code><font size="2">min_matches</font></code> is greater than 0 and and Splunk Enterprise finds less than <code><font size="2">min_matches</font></code> for any given input, it provides this <code><font size="2">default_match</font></code> value one or more times until the <code><font size="2">min_matches</font></code> threshold is reached.
</td><td valign="center" align="left"> Empty string
</td></tr><tr><td valign="center" align="left"> <code><font size="2">case_sensitive_match</font></code>
</td><td valign="center" align="left"> Boolean
</td><td valign="center" align="left"> When set to true, Splunk Enterprise performs case sensitive matching for all fields in the lookup table. When set to false, Splunk Enterprise disregards case when matching lookup table fields.
</td><td valign="center" align="left"> True
</td></tr><tr><td valign="center" align="left"> <code><font size="2">match_type</font></code>
</td><td valign="center" align="left"> String
</td><td valign="center" align="left"> Allows non-exact matching of one or more fields arranged in a list delimited by a comma followed by a space. Format is <code><font size="2">match_type = &lt;match_type&gt;(&lt;field_name1&gt;, &lt;field_name2&gt;,...&lt;field_nameN&gt;)</font></code>. Set <code><font size="2">match_type</font></code> to <code><font size="2">WILDCARD</font></code> to apply wildcard matching, or set it to <code><font size="2">CIDR</font></code> to apply CIDR matching (specifically for IP address values).
</td><td valign="center" align="left"> <code><font size="2">EXACT</font></code> (does not need to be specified)
</td></tr></table><h3> <a name="addfieldsfromexternaldatasources_set_up_a_time-bounded_lookup"><span class="mw-headline" id="Set_up_a_time-bounded_lookup"> Set up a time-bounded lookup </span></a></h3>
<p>If your lookup table has a field that represents time, you can use it to create a time-bounded lookup (also referred to as a temporal lookup). You can configure all three lookup types as time-bounded lookups.
</p><p>To create a time-bounded lookups, add the following lines to your lookup stanza in <code><font size="2">transforms.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>time_field = &lt;field_name&gt;<br>time_format = &lt;string&gt;<br></font></code></div>
<p>If the <code><font size="2">time_field</font></code> attribute is present, <code><font size="2">max_matches = 1</font></code> by default and Splunk Enterprise applies the first matching entry in descending order. 
</p><p>The <code><font size="2">time_format</font></code> attribute specifies the strptime() format of the <code><font size="2">time_field</font></code> attribute. The default value for the <code><font size="2">time_format</font></code>  attribute is  <code><font size="2">%s.%Q</font></code>, where you enter a Unix epoch time value in seconds (%s) and can optionally include milliseconds (%Q). 
</p><p><b>Note:</b> Splunk Enterprise lets you use some nonstandard date-time <code><font size="2">strptime()</font></code> formats. For example, when you define ISO 8601 timestamps (a Unix epoch time value in seconds), you may use <code><font size="2">time_format = '%s.%Q'</font></code>, where <code><font size="2">%s</font></code> represents seconds and <code><font size="2">%Q</font></code> represents milliseconds. See the subtopic "Enhanced strptime() support" in "Configure timestamp recognition," in the <i>Getting Data In Manual</i>. 
</p><p>For a match to occur with time-bounded lookups, you can also specify offsets for the minimum and maximum amounts of time that an event may be later than a lookup entry. To do this, add the following lines to your stanza:
</p>
<div class="samplecode">
<code><font size="2"><br>max_offset_secs = &lt;integer&gt;<br>min_offset_secs = &lt;integer&gt;<br></font></code></div>
<p>By default there is no maximum offset. The default minimum offset is 0.
</p>
<h4><font size="3"><b><i> <a name="addfieldsfromexternaldatasources_time-bounded_lookup_example"><span class="mw-headline" id="Time-bounded_lookup_example"> Time-bounded lookup example </span></a></i></b></font></h4>
<p>Here's an example of how you might use DHCP logs to identify users on your network based on their IP address and the timestamp. Let's say the DHCP logs are in a file, <code><font size="2">dhcp.csv</font></code>, which contains the timestamp, IP address, and the user's name and MAC address.
</p><p><b>1.</b> In a <code><font size="2">transforms.conf</font></code> file, put:
</p>
<div class="samplecode">
<code><font size="2"><br>[dhcpLookup]<br>filename = dhcp.csv<br>time_field = timestamp<br>time_format =&nbsp;%d/%m/%y&nbsp;%H:%M:%S<br></font></code></div>
<p><b>2.</b> In a <code><font size="2">props.conf</font></code> file, put:
</p>
<div class="samplecode">
<code><font size="2"><br>[dhcp]<br>LOOKUP-table = dhcpLookup ip mac OUTPUT user<br></font></code></div>
<p><b>3.</b> Restart Splunk Enterprise.
</p>
<h3> <a name="addfieldsfromexternaldatasources_use_search_results_to_populate_a_lookup_table"><span class="mw-headline" id="Use_search_results_to_populate_a_lookup_table"> Use search results to populate a lookup table </span></a></h3>
<p>You can edit a local or app-specific copy of <code><font size="2">savedsearches.conf</font></code> to use the results of a report to populate a lookup table.
</p><p>In a report stanza, where the search returns a results table:
</p><p><b>1.</b> Add the following line to enable the lookup population action.
</p>
<div class="samplecode">
<code><font size="2"><br>action.populate_lookup = 1<br></font></code></div>
<p>This tells Splunk Enterprise to save your results table into a CSV file.
</p><p><b>2.</b> Add the following line to tell Splunk Enterprise where to copy your lookup table.
</p>
<div class="samplecode">
<code><font size="2"><br>action.populate_lookup.dest = &lt;string&gt;<br></font></code></div>
<p>The <code><font size="2">action.populate_lookup.dest</font></code> value is a lookup name from <code><font size="2">transforms.conf</font></code> or a path to a CSV file where Splunk Enterprise should copy the search results. If it is a path to a CSV file, the path should be relative to $SPLUNK_HOME. 
</p><p>For example, if you want to save the results to a global lookup table, you might include: 
</p>
<div class="samplecode">
<code><font size="2"><br>action.populate_lookup.dest = etc/system/lookups/myTable.csv<br></font></code></div>
<p>The destination directory, <code><font size="2">$SPLUNK_HOME/etc/system/lookups</font></code> or <code><font size="2">$SPLUNK_HOME/etc/&lt;app_name&gt;/lookups</font></code>, should already exist.
</p><p><b>3.</b> Add the following line if you want this search to run when Splunk Enterprise starts up.
</p>
<div class="samplecode">
<code><font size="2"><br>run_on_startup = true<br></font></code></div>
<p>If it does not run on startup, it will run at the next scheduled time. We recommend that you set <code><font size="2">run_on_startup = true</font></code> for scheduled searches that populate lookup tables. 
</p><p>Because Splunk Enterprise copies the results of the report to a CSV file, you can set up this lookup the same way you set up a CSV lookup.
</p>
<h3> <a name="addfieldsfromexternaldatasources_troubleshooting_lookups_-_using_identical_names_in_lookup_stanzas"><span class="mw-headline" id="Troubleshooting_lookups_-_Using_identical_names_in_lookup_stanzas">Troubleshooting lookups - Using identical names in lookup stanzas</span></a></h3>
<p>In <code><font size="2">props.conf</font></code>, table definitions are identified with the <code><font size="2">LOOKUP-&lt;class&gt;</font></code> attribute. It is best if all of your <code><font size="2">props.conf</font></code> lookup definitions have different <code><font size="2">&lt;class&gt;</font></code> names. This practice reduces the chance of things going wrong. 
</p><p>When you do give two or more lookups the same <code><font size="2">&lt;class&gt;</font></code> name you can run into trouble unless you know what you're trying to do:
</p>
<ul><li> If two or more lookups with the same name share the same <code><font size="2">props.conf</font></code> stanza (the same host, source, or source type), the first lookup with that stanza in <code><font size="2">fields.conf</font></code> overrides the others. All lookups with the same host, source, or source type should have different names.
</li><li> If you have lookups with different hosts, sources, or source types that share the same name, you can end up with a situation where only one of them seems to work at any given point in time. You may set this up on purpose, but in most cases it is inconvenient.
</li></ul><p>For example, say you have the following two lookups named <code><font size="2">LOOKUP-table</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[host::machine_name]<br>LOOKUP-table = logs_per_day host OUTPUTNEW average_logs AS logs_per_day<br></font></code></div>
<div class="samplecode">
<code><font size="2"><br>[sendmail]<br>LOOKUP-table = location host OUTPUTNEW building AS location<br></font></code></div>
<p>Any events that overlap between these two lookups are only be affected by one of them. In other words:
</p>
<ul><li> Events that match the host get the host lookup.
</li><li> Events that match the source type get the source type lookup.
</li><li> Events that match both get the host lookup. 
</li></ul><p>When you name your lookup <code><font size="2">LOOKUP-table</font></code>, you are saying this is the lookup that achieves some purpose or action described by "table." In this example, these lookups achieve different goals. One lookup determines something about logs per day, and the other lookup has something to do with location. Rename them.
</p>
<div class="samplecode">
<code><font size="2"><br>[host::machine_name]<br>LOOKUP-table = logs_per_day host OUTPUTNEW average_logs AS logs_per_day<br></font></code></div>
<div class="samplecode">
<code><font size="2"><br>[sendmail]<br>LOOKUP-location = location host OUTPUTNEW building AS location<br></font></code></div>
<p>Now you have two different configurations that do not collide.
</p>
<a name="configurekvstorelookups"></a><h2> <a name="configurekvstorelookups_configure_kv_store_lookups"><span class="mw-headline" id="Configure_KV_Store_lookups"> Configure KV Store lookups</span></a></h2>
<p>Lookups add fields from an external source to your events based on the values of fields currently present in those events. This topic discusses KV Store lookups, which populate your events with fields pulled from your App Key Value Store (KV Store) collections. KV Store lookups can only be invoked through REST endpoints or by using the following search commands: <code><font size="2">lookup</font></code>, <code><font size="2">inputlookup</font></code>, and <code><font size="2">outputlookup</font></code>. You cannot set up KV Store lookups as automated lookups.
</p><p>This topic shows you how to set up and manage KV Store lookups by configuring lookup stanzas in  <code><font size="2">props.conf</font></code>. Configuration files give you a greater degree of control over lookup design and behavior than you get when you set up lookup files using Splunk Web.
</p><p>If you do not have access to the <code><font size="2">.conf</font></code> files, or if you prefer to maintain lookups through Splunk Web whenever possible, you can configure KV Store lookups using the pages at <b>Settings &gt; Lookups</b>. See <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">"Use lookups to add information to your events"</a> in this manual.
</p><p>You can also define lookups that:
</p>
<ul><li> Populate your events with fields pulled from CSV files.
</li><li> Use Python scripts or binary executables to populate your events with field values from an external source.
</li></ul><p>See <a href="#addfieldsfromexternaldatasources" class="external text">"Configure CSV and external lookups"</a> in this manual.
</p><p>For developer-focused KV Store lookup configuration instructions, see "Use lookups with KV Store data" in the Splunk Developer Portal.
</p>
<h3> <a name="configurekvstorelookups_about_kv_store_collections"><span class="mw-headline" id="About_KV_Store_collections"> About KV Store collections </span></a></h3>
<p>Before you create a KV Store lookup, your Splunk Enterprise implementation must have at least one KV Store collection defined in <code><font size="2">collections.conf</font></code>. See "Use configuration files to create a KV Store collection store" on the Splunk Developer Portal.
</p><p>KV Store collections are containers of data similar to a database. They store your data as key/value pairs. When you create a KV Store lookup, the collection should have at least two fields. One of those fields should have a set of values that that match with the values of a field in your event data, so that lookup matching can take place. 
</p><p>When you invoke the lookup in a search with the <code><font size="2">lookup</font></code> command, you designate a field in your search data to match with the field in your KV Store collection. When a value of this field in an event matches a value of the designated field in your KV Store collection, the corresponding value(s) for the other field(s) in your KV Store collection can be added to that event. 
</p><p>The KV Store field does not have to have the same name as the field in your events. Each KV Store field can be <b>multivalued</b>. 
</p><p><b>Note:</b> KV Store collections live on the search head, while CSV files are replicated to indexers. If your lookup data changes frequently you may find that KV Store lookups offer better performance than an equivalent CSV lookup.
</p>
<h3> <a name="configurekvstorelookups_define_a_kv_store_lookup_stanza_in_transforms.conf"><span class="mw-headline" id="Define_a_KV_Store_lookup_stanza_in_transforms.conf"> Define a KV Store lookup stanza in transforms.conf </span></a></h3>
<p>A KV Store lookup matches fields in your events with fields that Splunk Enterprise has stored in KV store collections. KV store lookups can only be invoked using search commands: <code><font size="2">lookup</font></code>, <code><font size="2">inputlookup</font></code>, and <code><font size="2">outputlookup</font></code>. You cannot set up KV Store lookups as automated lookups.
</p><p>A KV Store lookup stanza provides the location of the KV Store collection that is to be used as a lookup table. It can optionally include field matching rules and rules for time-bounded lookups. 
</p><p>If you want a KV Store lookup to be available globally, add its lookup stanza to the version of <code><font size="2">transforms.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>. If you want the lookup to be specific to a particular app, add its stanza to the version of <code><font size="2">transforms.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/</font></code>. 
</p><p><b>Caution:</b> Do not edit configuration files in <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>. 
</p>
<h4><font size="3"><b><i> <a name="configurekvstorelookups_the_kv_store_lookup_stanza_format"><span class="mw-headline" id="The_KV_Store_lookup_stanza_format"> The KV Store lookup stanza format </span></a></i></b></font></h4>
<p>When you add a KV Store lookup stanza to <code><font size="2">transforms.conf</font></code> it should follow this format. 
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;lookup_name&gt;]<br>external_type = kvstore<br>collection = &lt;string&gt;<br>fields_list = &lt;string&gt;<br></font></code></div>
<ul><li> <code><font size="2">[&lt;lookup_name&gt;]</font></code> is the name of the lookup.
</li><li> <code><font size="2">external_type</font></code> should be set to <code><font size="2">kvstore</font></code> if you are defining a KV store lookup. 
</li><li> <code><font size="2">collection</font></code> is the name of the KV Store collection associated with the lookup. 
</li><li> <code><font size="2">fields_list</font></code> is a list of all fields that are supported by the KV Store lookup. The fields must be delimited by a comma followed by a space. A field can be any combination of key and value that you have in your KV store collection.
</li></ul><dl><dd> By default, each KV Store record has a unique key ID, which is stored in the internal "_key" field. Add <code><font size="2">_key</font></code> to the list of fields in <code><font size="2">fields_list</font></code> if you want to be able to modify specific records through your KV Store lookup. You can then specify the key ID value in your lookup operations. 
</dd></dl><dl><dd> When you use the <code><font size="2">outputlookup</font></code> command to write to the KV Store without specifying a key ID, Splunk Enterprise generates a key ID for you.
</dd></dl><p>The <code><font size="2">transforms.conf</font></code> KV store lookup stanza can optionally include attributes that: 
</p>
<ul><li> Define field (key/value) matching rules. See <a href="#addfieldsfromexternaldatasources_optional_field_matching_rules_for_lookup_configurations" class="external text">"Optional field matching rules for lookup configurations."</a>
</li><li> Define a time-bounded KV Store lookup, also referred to as a temporal lookup. See <a href="#addfieldsfromexternaldatasources_set_up_a_time-bounded_lookup" class="external text">"Set up a time-bounded lookup"</a>.
</li></ul><h4><font size="3"><b><i> <a name="configurekvstorelookups_configure_a_kv_store_lookup"><span class="mw-headline" id="Configure_a_KV_Store_lookup"> Configure a KV Store lookup </span></a></i></b></font></h4>
<p><b>1.</b> Define a KV Store collection in <code><font size="2">collections.conf</font></code>. See "Use configuration files to create a KV Store collection store" on the Splunk Developer Portal. 
</p><p><b>2.</b> Create a KV Store lookup stanza in <code><font size="2">transforms.conf</font></code>, following the stanza format <a href="#configurekvstorelookups_define_a_kv_store_lookup_stanza_in_transforms.conf" class="external text">described above</a>. 
</p><p><b>3.</b> Save your <code><font size="2">.conf</font></code> file changes.
</p><p><b>4.</b> Restart Splunk Enterprise to add the lookup to your system.
</p>
<h3> <a name="configurekvstorelookups_kv_store_lookup_example"><span class="mw-headline" id="KV_store_lookup_example"> KV store lookup example </span></a></h3>
<p>Here is a KV Store lookup called <code><font size="2">employee_info</font></code>. It is located in <code><font size="2">$SPLUNK_HOME/etc/system/bin/</font></code>. 
</p>
<div class="samplecode">
<code><font size="2"><br>[employee_info]<br>external_type = kvstore<br>collection = kvstorecoll<br>fields_list = _key, CustID, CustName, CustStreet, CustCity, CustZip<br></font></code></div>
<p>The <code><font size="2">employee_info</font></code> lookup takes an employee ID in an event and outputs corresponding employee information to that event such as the employee name, street address, city, and zip code. The lookup works with a KV Store collection called <code><font size="2">kvstorecoll</font></code>.
</p>
<h3> <a name="configurekvstorelookups_search_commands_and_kv_store_lookups"><span class="mw-headline" id="Search_commands_and_KV_Store_lookups">Search commands and KV Store lookups</span></a></h3>
<p>After you save a KV Store lookup stanza and restart Splunk, you can interact with the new KV store lookup through search commands.
</p><p>Use <code><font size="2">lookup</font></code> to match values in a KV Store collection with field values in the search results and then output corresponding field values to those results. This search uses the <code><font size="2">employee_info</font></code> lookup defined in the preceding use case example.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">... | lookup employee_info CustID AS ID OUTPUT CustName AS Name | ...</font></code><br></div>
<p>It matches employee id values in <code><font size="2">kvstorecoll</font></code> with employee id values in your events and outputs the corresponding employee name values to your events. 
</p><p>You can use the <code><font size="2">inputlookup</font></code> search command to search on the contents of a KV Store collection. See the Search Reference topic on <code><font size="2">inputlookup</font></code> for examples.
</p><p>You can use the <code><font size="2">outputlookup</font></code> search command to write search results from the search pipeline into a KV store collection. See the Search Reference topic on <code><font size="2">outputlookup</font></code> for examples.
</p><p>You can also find several examples of KV Store lookup searches in "Use lookups with KV Store data" in the Splunk Developer Portal.
</p>
<a name="createworkflowactionsinsplunkweb"></a><h2> <a name="createworkflowactionsinsplunkweb_create_and_maintain_workflow_actions_in_splunk_web"><span class="mw-headline" id="Create_and_maintain_workflow_actions_in_Splunk_Web"> Create and maintain workflow actions in Splunk Web</span></a></h2>
<p>Enable a wide variety of interactions between indexed or extracted fields and other web resources with workflow actions. Workflow actions have a wide variety of applications. For example, you can define workflow actions that enable you to:
</p>
<ul><li> Perform an external WHOIS lookup based on an IP address found in an event.
</li><li> Use the field values in an HTTP error event to create a new entry in an external issue management system.
</li><li> Perform an external search (using Google or a similar web search application) on the value of a specific field found in an event.
</li><li> Launch secondary Splunk Enterprise searches that use one or more field values from selected events.
</li></ul><p>In addition, you can define workflow actions that:
</p>
<ul><li> Are targeted to events that contain a specific field or set of fields, or which belong to a particular event type.
</li><li> Appear either in field menus or event menus in search results. You can also set them up to only appear in the menus of specific fields, or in all field menus in a qualifying event.
</li><li> When selected, open either in the current window or in a new one.
</li></ul><h3> <a name="createworkflowactionsinsplunkweb_define_workflow_actions_using_splunk_web"><span class="mw-headline" id="Define_workflow_actions_using_Splunk_Web"> Define workflow actions using Splunk Web </span></a></h3>
<p>You can set up all of the workflow actions described in the bulleted list at the top of this chapter and many more using Splunk Web. To begin, navigate to <b>Settings &gt; Fields &gt; Workflow actions</b>. On the Workflow actions page you can review and update existing workflow actions by clicking on their names. Or you can click <b>Add new</b> to create a new workflow action. Both methods take you to the workflow action detail page, where you define individual workflow actions.
</p><p>If you're creating a new workflow action, you need to give it a <b>Name</b> and identify its <b>Destination app</b>.
</p><p>There are three kinds of workflow actions that you can set up:
</p>
<ul><li> <b><a href="#createworkflowactionsinsplunkweb_set_up_a_get_workflow_action" class="external text">GET workflow actions</a></b>, which create typical HTML links to do things like perform Google searches on specific values or run domain name queries against external WHOIS databases.
</li><li> <b><a href="#createworkflowactionsinsplunkweb_set_up_a_post_workflow_action" class="external text">POST workflow actions</a></b>, which generate an HTTP POST request to a specified URI. This action type enables you to do things like create entries in external issue management systems using a set of relevant field values.
</li><li> <b><a href="#createworkflowactionsinsplunkweb_set_up_a_secondary_search_that_is_dynamically_populated_with_field_values_from_an_event" class="external text">Search workflow actions</a></b>, which launch secondary searches that use specific field values from an event, such as a search that looks for the occurrence of specific combinations of <code><font size="2">ipaddress</font></code> and <code><font size="2">http_status</font></code>' field values in your index over a specific time range.
</li></ul><h3> <a name="createworkflowactionsinsplunkweb_target_workflow_actions_to_a_narrow_grouping_of_events"><span class="mw-headline" id="Target_workflow_actions_to_a_narrow_grouping_of_events"> Target workflow actions to a narrow grouping of events </span></a></h3>
<p>When you create workflow actions in Splunk Web, you can optionally target workflow actions to a narrow grouping of events. You can restrict workflow action scope by field, by event type, or a combination of the two.
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_narrow_workflow_action_scope_by_field"><span class="mw-headline" id="Narrow_workflow_action_scope_by_field"> Narrow workflow action scope by field </span></a></i></b></font></h4>
<p>You can set up workflow actions that only apply to events that have a specified field or set of fields. For example, if you have a field called <code><font size="2">http_status</font></code>, and you would like a workflow action to apply only to events containing that field, you would declare  <i>http_status</i> in the <b>Apply only to the following fields</b> setting. 
</p><p>If you want to have a workflow action apply only to events that have a <i>set</i> of fields, you can declare a comma-delimited list of fields in <b>Apply only to the following fields</b>. When more than one field is listed the workflow action is displayed only if <i>the entire list</i> of fields are present in the event.
</p><p>For example, say you want a workflow action to only apply to events with <code><font size="2">ip_client</font></code> and <code><font size="2">ip_server</font></code> fields. To do this, you would enter <i>ip_client, ip_server</i> in <b>Apply only to the following fields</b>. 
</p><p>Workflow action field scoping also supports use of the wildcard asterisk. For example, if you declare a simple field listing of <i>ip_*</i> Splunk Enterprise applies the resulting workflow action to events with either <code><font size="2">ip_client</font></code> or <code><font size="2">ip_server</font></code> as well as a combination of both (as well as any other event with a field that matches <i>ip_*</i>). 
</p><p>By default the field list is set to *, which means that it matches all fields.
</p><p>If you need more complex selecting logic, we suggest you use event type scoping instead of field scoping, or combine event type scoping with field scoping.
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_narrow_workflow_action_scope_by_event_type"><span class="mw-headline" id="Narrow_workflow_action_scope_by_event_type"> Narrow workflow action scope by event type </span></a></i></b></font></h4>
<p>Event type scoping works exactly the same way as field scoping. You can enter a single event type or a comma-delimited list of event type into the <b>Apply only to the following event types</b> setting to create a workflow action that Splunk Enterprise only applies to events belonging to that event type or set of event types. You can also use wildcard matching to identify events belonging to a range of event types.
</p><p>You can also narrow the scope of workflow actions through a combination of fields and event types. For example, perhaps you have a field called <code><font size="2">http_status</font></code>, but you only want the resulting workflow action to appear in events containing that field if the <code><font size="2">http_status</font></code> is greater than or equal to 500. To accomplish this you would first set up an event type called <code><font size="2">errors_in_500_range</font></code> that is applied to events matching a search like
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">http_status &gt;= 500</font></code><br></div>
<p>You would then define a workflow action that has <b>Apply only to the following fields</b> set to <i>http_status</i> and <b>Apply only to the following event types</b> set to <i>errors_in_500_range</i>.
</p><p>For more information about event types, see <a href="#abouteventtypes" class="external text">"About event types"</a> in this manual.
</p>
<h3> <a name="createworkflowactionsinsplunkweb_control_workflow_action_appearance_in_field_and_event_menus"><span class="mw-headline" id="Control_workflow_action_appearance_in_field_and_event_menus"> Control workflow action appearance in field and event menus </span></a></h3>
<p>When workflow actions are set up correctly, they appear in menus associated with fields and events in your search results. You can arrange for workflow actions to be event-level (meaning they apply to an entire event), field-level (meaning they apply to specific fields within events), or both.
</p><p>To select event-level workflow actions: 
</p>
<ul><li> Run a search.
</li><li> Go to the <b>Events</b> tab.
</li><li> Expand an event in your search results and click <b>Event Actions.</b> 
</li></ul><p>Here's an example of "Show Source," an event-level workflow action that, when clicked, displays the source for the event in your raw search data. 
</p><p><img alt="6.0 wkflw actions event1.png" src="images/5/58/6.0_wkflw_actions_event1.png" width="700" height="262"></p><p>Alternatively, you can have the workflow action appear in the <b>Actions</b> menus for fields within an event. Here's an example of a workflow action that opens a Google search in a separate window for the selected field and value. 
</p><p><img alt="6.0 wkflow actions field1.png" src="images/3/36/6.0_wkflow_actions_field1.png" width="700" height="297"></p><p>Both of these examples are of workflow actions that use the "GET" link method. This is one of three kinds of workflow actions that you can implement in Splunk Enterprise (the other two are "POST" link workflow actions and search workflow actions). Read on for instructions on setting up all three.
</p><p>You can also define workflow actions that appear both at the event level and the field level. For example, you might do this for workflow actions that do something with the value of a specific field in an event, such as <code><font size="2">User_ID</font></code>.
</p>
<h3> <a name="createworkflowactionsinsplunkweb_set_up_a_get_workflow_action"><span class="mw-headline" id="Set_up_a_GET_workflow_action"> Set up a GET workflow action </span></a></h3>
<p>GET link workflow actions drop one or more values into an HTML link. Clicking that link performs an HTTP GET request in a browser, allowing you to pass information to an external web resource, such as a search engine or IP lookup service. 
</p><p>To define a GET workflow action:
</p><p><b>1.</b> Navigate to <b>Settings &gt; Fields &gt; Workflow Actions</b>. 
</p><p><b>2.</b> Click <b>New</b> to open up a new workflow action form. 
</p><p><b>3.</b> Define a <b>Label</b> for the action. 
</p>
<dl><dd> The <b>Label</b> field enables you to define the text that is displayed in either the field or event workflow menu. Labels can be static or include the value of relevant fields. 
</dd></dl><p><b>4.</b> Determine whether the workflow action applies to specific fields or event types in your data.
</p>
<dl><dd> Use <b>Apply only to the following fields</b> to identify one or more fields. When you identify fields, the workflow action only appears events that have those fields, either in their event menu or field menus. If you leave it blank or enter an asterisk the action appears in menus for all fields. 
</dd></dl><dl><dd> Use <b>Apply only to the following event types</b> to identify one or more event types. If you identify an event type, the workflow action only appears in the event menus for events that belong to the event type.
</dd></dl><p><b>5.</b> For <b>Show action in</b> determine whether you want the action to appear in the <b>Event menu</b>, the <b>Fields menus</b>, or <b>Both</b>.
</p><p><b>6.</b> Set <b>Action type</b> to <b>link</b>.
</p><p><b>7.</b> In <b>URI</b> provide a URI for the location of the external resource that you want to send your field values to.
</p>
<dl><dd> Similar to the <b>Label</b> setting, when you declare the value of a field, you use the name of the field enclosed by dollar signs.
</dd></dl><dl><dd> Variables passed in GET actions via URIs are automatically URL encoded during transmission. This means you can include values that have spaces between words or punctuation characters. 
</dd></dl><p><b>8.</b> Under <b>Open link in</b>, determine whether the workflow action displays in the current window or if it opens the link in a new window. 
</p><p><b>9.</b> Set the <b>Link method</b> to <b>get</b>. 
</p><p><b>10.</b> Click <b>Save</b> to save your workflow action definition. 
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_google_search_from_field_values"><span class="mw-headline" id="Example_-_Google_search_from_field_values">Example - Google search from field values</span></a></i></b></font></h4>
<p>Here's an example of the setup for a GET link workflow action that sets off a Google search on values of the <code><font size="2">topic</font></code> field in search results:
</p><p><img alt="GET workflow action ex1 b.png" src="images/4/47/GET_workflow_action_ex1_b.png" width="670" height="699"></p><p>In this example, we set the <b>Label</b> value to <code><font size="2">Google $topic$</font></code> because we have a field called <code><font size="2">topic</font></code> in our events and we want the value of <code><font size="2">topic</font></code> to be included in the label for this workflow action. For example, if the value for <code><font size="2">topic</font></code> in an event is <code><font size="2">CreatefieldactionsinSplunkWeb</font></code> the field action displays as <i>Google CreatefieldactionsinSplunkWeb</i> in the <code><font size="2">topic</font></code> field menu.
</p><p>The <code><font size="2">Google $topic$</font></code> action applies to all events.
</p><p>The <code><font size="2">Google $topic$</font></code> action <b>URI</b> uses the GET method to submit the <code><font size="2">topic</font></code> value to Google for a search.
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_provide_an_external_ip_lookup"><span class="mw-headline" id="Example_-_Provide_an_external_IP_lookup"> Example - Provide an external IP lookup </span></a></i></b></font></h4>
<p>You have configured your Splunk Enterprise app to extract domain names in web services logs and specify them as a field named <code><font size="2">domain</font></code>. You want to be able to search an external WHOIS database for more information about the domains that appear. 
</p><p>Here's how you would set up the GET workflow action that helps you with this.
</p><p>In the Workflow actions details page, set <b>Action type</b> to <i>link</i> and set <b>Link method</b> to <i>get</i>. 
</p><p>You then use the <b>Label</b> and <b>URI</b> fields to identify the field involved. Set a <b>Label</b> value of <i>WHOIS: $domain$</i>. Set a <b>URI</b> value of <i>http://whois.net/whois/$domain$</i>. 
</p><p>After that, you can determine:
</p>
<ul><li> whether the link shows up in the field menu, the event menu, or both.
</li><li> whether the link opens the WHOIS search in the same window or a new one.
</li><li> restrictions for the events that display the workflow action link. You can target the workflow action to events that have specific fields, that belong to specific event types, or some combination of the two.
</li></ul><h3> <a name="createworkflowactionsinsplunkweb_set_up_a_post_workflow_action"><span class="mw-headline" id="Set_up_a_POST_workflow_action"> Set up a POST workflow action </span></a></h3>
<p>You set up POST workflow actions in a manner similar to that of GET link actions. However, POST requests are typically defined by a form element in HTML along with some inputs that are converted into POST arguments. This means that you have to identify POST arguments to send to the identified URI.
</p><p><b>1.</b> Navigate to <b>Settings &gt; Fields &gt; Workflow Actions</b>. 
</p><p><b>2.</b> Click <b>New</b> to open up a new workflow action form. 
</p><p><b>3.</b> Define a <b>Label</b> for the action. 
</p>
<dl><dd> The <b>Label</b> field enables you to define the text that is displayed in either the field or event workflow menu. Labels can be static or include the value of relevant fields. 
</dd></dl><p><b>4.</b> Determine whether the workflow action applies to specific fields or event types in your data.
</p>
<dl><dd> Use <b>Apply only to the following fields</b> to identify one or more fields. When you identify fields, the workflow action only appears events that have those fields, either in their event menu or field menus. If you leave it blank or enter an asterisk the action appears in menus for all fields. 
</dd></dl><dl><dd> Use <b>Apply only to the following event types</b> to identify one or more event types. If you identify an event type, the workflow action only appears in the event menus for events that belong to the event type.
</dd></dl><p><b>5.</b> For <b>Show action in</b> determine whether you want the action to appear in the <b>Event menu</b>, the <b>Fields menus</b>, or <b>Both</b>.
</p><p><b>6.</b> Set <b>Action type</b> to <b>Link</b>.
</p><p><b>7.</b> Under <b>URI</b> provide the URI for a web resource that responds to POST requests.
</p><p><b>8.</b> Under <b>Open link in</b>, determine whether the workflow action displays in the current window or if it opens the link in a new window. 
</p><p><b>9.</b> Set <b>Link method</b> to <b>Post</b>.
</p><p><b>10.</b> Under <b>Post arguments</b> define arguments that should be sent to web resource at the identified URI.
</p>
<dl><dd> These arguments are key and value combinations. On both the key and value sides of the argument, you can use field names enclosed in dollar signs to identify the field value from your events that should be sent over to the resource. You can define multiple key/value arguments in one POST workflow action.
</dd></dl><dl><dd> Enter the key in the first field, and the value in the second field. Click <b>Add another field</b> to create an additional POST argument.
</dd></dl><p><b>11.</b> Click <b>Save</b> to save your workflow action definition. 
</p><p>Splunk Enterprise automatically HTTP-form encodes variables that it passes in POST link actions via URIs. This means you can include values that have spaces between words or punctuation characters. 
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_allow_an_http_error_to_create_an_entry_in_an_issue_tracking_application"><span class="mw-headline" id="Example_-_Allow_an_http_error_to_create_an_entry_in_an_issue_tracking_application"> Example - Allow an http error to create an entry in an issue tracking application </span></a></i></b></font></h4>
<p>You have configured your Splunk Enterprise app to extract HTTP status codes from a web service log as a field called <code><font size="2">http_status</font></code>. Along with the <code><font size="2">http_status</font></code> field the events typically contain either a normal single-line description request, or a multiline python stacktrace originating from the python process that produced an error. 
</p><p>You want to design a workflow action that only appears for error events where <code><font size="2">http_status</font></code> is in the 500 range. You want the workflow action to send the associated python stacktrace and the HTTP status code to an external issue management system to generate a new bug report. However, the issue management system only accepts POST requests to a specific endpoint. 
</p><p>Here's how you might set up the POST workflow action that fits your requirements:
</p><p><img alt="POST wf action example b.png" src="images/1/1c/POST_wf_action_example_b.png" width="650" height="787"></p><p>Note that the first POST argument sends <code><font size="2">server error $http_status$</font></code> to a <code><font size="2">title</font></code> field in the external issue tracking system. If you select this workflow action for an event with an <code><font size="2">http_staus</font></code> of <code><font size="2">500</font></code>, then it opens an issue with the title <code><font size="2">server error 500</font></code> in the issue tracking system.
</p><p>The second POST argument uses the <code><font size="2">_raw</font></code> field to include the multiline python stacktrace in the <code><font size="2">description</font></code> field of the new issue.
</p><p>Finally, note that the workflow action has been set up so that it only applies to events belonging to the <code><font size="2">errors_in_500_range</font></code> event type. This is an event type that is  only applied to events carrying <code><font size="2">http_error</font></code> values in the typical HTTP error range of 500 or greater. Events with HTTP error codes below 500 do not display the <i>submit error report</i> workflow action in their event or field menus.
</p>
<h3> <a name="createworkflowactionsinsplunkweb_set_up_a_secondary_search_that_is_dynamically_populated_with_field_values_from_an_event"><span class="mw-headline" id="Set_up_a_secondary_search_that_is_dynamically_populated_with_field_values_from_an_event"> Set up a secondary search that is dynamically populated with field values from an event </span></a></h3>
<p>To set up workflow actions that launch dynamically populated secondary searches, you start by setting <b>Action type</b> to <i>search</i> on the Workflow actions detail page. This reveals a set of <b>Search configuration</b> fields that you use to define the specifics of the secondary search.
</p><p>In <b>Search string</b> enter a search string that includes one or more placeholders for field values, bounded by dollar signs. For example, if you're setting up a workflow action that searches on client IP values that turn up in events, you might simply enter <code><font size="2">clientip=$clientip$</font></code> in that field. 
</p><p>Identify the app that the search runs in. If you want it to run in a view other than the current one, select that view. And as with all workflow actions, you can determine whether it opens in the current window or a new one.
</p><p>Be sure to set a time range for the search (or identify whether it should use the same time range as the search that created the field listing) by entering relative time modifiers in the in the <i>Earliest time</i> and <i>Latest time</i> fields. If these fields are left blank the search runs over all time by default. 
</p><p>Finally, as with other workflow action types, you can restrict the search workflow action to events containing specific sets of fields and/or which belong to particular event types.
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_launch_a_secondary_search_that_finds_errors_originating_from_a_specific_ruby_on_rails_controller"><span class="mw-headline" id="Example_-_Launch_a_secondary_search_that_finds_errors_originating_from_a_specific_Ruby_On_Rails_controller"> Example - Launch a secondary search that finds errors originating from a specific Ruby On Rails controller </span></a></i></b></font></h4>
<p>Say your company uses a web infrastructure that is built on Ruby on Rails. You've set up an event type to sort out errors related to Ruby controllers (titled <code><font size="2">controller_error</font></code>), but sometimes you just want to see all the errors related to a particular controller. Here's how you might set up a workflow action that does this:
</p><p><b>1.</b> On the Workflow actions detail page, set up an action with the following <b>Label</b>: <code><font size="2">See other errors for controller $controller$ over past 24h</font></code>.
</p><p><b>2.</b> Set <b>Action type</b> to <i>Search</i>.
</p><p><b>3.</b> Enter the following <b>Search string</b>: <code><font size="2">sourcetype=rails controller=$controller$ error=* </font></code>
</p><p><b>4.</b> Set an <b>Earliest time</b> of <i>-24h</i>. Leave <b>Latest time</b> blank.
</p><p><b>5.</b> Using the <b>Apply only to the following...</b> settings, arrange for the workflow action to only appear in events that belong to the <code><font size="2">controller_error</font></code> event type, and which contain the <code><font size="2">error</font></code> and <code><font size="2">controller</font></code> fields.
</p><p><img alt="5.0-Mgr-Workflow Actions SearchEx b.png" src="images/b/b7/5.0-Mgr-Workflow_Actions_SearchEx_b.png" width="700" height="694"></p><p>Those are the basics. You can also determine which app or view the workflow action should run in (for example, you might have a dedicated view for this information titled <code><font size="2">ruby_errors</font></code>) and identify whether the action works in the current window or opens a new one.
</p>
<h3> <a name="createworkflowactionsinsplunkweb_use_special_parameters_in_workflow_actions"><span class="mw-headline" id="Use_special_parameters_in_workflow_actions"> Use special parameters in workflow actions </span></a></h3>
<p>Splunk Enterprise provides special parameters for workflow actions that begin with an "@" sign.
Two of these special parameters are for field menus only. They enable you to set up workflow actions that apply to all fields in the events to which they apply. 
</p>
<ul><li> <b>@field_name</b> - Refers to the name of the field being clicked on. 
</li><li> <b>@field_value</b> - Refers to the value of the field being clicked on. 
</li></ul><p>The other special parameters are:
</p>
<ul><li> <b>@sid</b> - Refers to the sid of the job that returned the event
</li><li> <b>@offset</b> - Refers to the offset of the event in the job
</li><li> <b>@namespace</b> - Refers to the namespace from which the job was dispatched
</li><li> <b>@latest_time</b> - Refers to the latest time the event occurred. It is used to distinguish similar events from one another. It is not always available for all fields.  
</li></ul><h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_create_a_workflow_action_that_applies_to_all_fields_in_an_event"><span class="mw-headline" id="Example_-_Create_a_workflow_action_that_applies_to_all_fields_in_an_event"> Example - Create a workflow action that applies to all fields in an event </span></a></i></b></font></h4>
<p>You can update the Google search example discussed above (in the GET link workflow action section) so that it enables a search of the field name and field value for every field in an event to which it applies. All you need to do is change the title to <code><font size="2">Google this field and value</font></code> and replace the URI of that action with <code><font size="2">http://www.google.com/search?q=$@field_name$+$@field_value$</font></code>. 
</p><p>This results in a workflow action that searches on whichever field/value combination you're viewing a field menu for. If you're looking at the field menu for <code><font size="2">sourcetype=access_combined</font></code> and select the <b>Google this field and value</b> field action, the resulting Google search is <i>sourcetype accesscombined</i>.
</p><p><b>Remember:</b> Workflow actions using the <b>@field_name</b> and/or <b>@field_value</b> parameters are not compatible with event-level menus.
</p>
<h4><font size="3"><b><i> <a name="createworkflowactionsinsplunkweb_example_-_show_the_source_of_an_event"><span class="mw-headline" id="Example_-_Show_the_source_of_an_event"> Example - Show the source of an event </span></a></i></b></font></h4>
<p>This workflow action uses the other special parameters to show the source of an event in your raw search data. 
</p><p>The <b>Action type</b> is <i>link</i> and its <b>Link method</b> is <i>get</i>. Its <b>Title</b> is <i>Show source</i>. The <b>URI</b> is <code><font size="2">/app/$@namespace$/show_source?sid=$@sid$&amp;offset=$@offset$&amp;latest_time=$@latest_time$</font></code>. It's only applied to events that have the <code><font size="2">_cd</font></code> field.
</p><p>Try setting this workflow action up in your app (if it isn't installed already) and see how it works.
</p>
<a name="configureworkflowactionsthroughworkflow%20actions.conf"></a><h2> <a name="configureworkflowactionsthroughworkflow%20actions.conf_configure_workflow_actions_through_workflow_actions.conf"><span class="mw-headline" id="Configure_workflow_actions_through_workflow_actions.conf"> Configure workflow actions through workflow_actions.conf</span></a></h2>
<p>This topic coming soon. In the meantime, <a href="#createworkflowactionsinsplunkweb" class="external text">learn how to set up and administrate workflow actions via Splunk Web</a>.
</p>
<h1>Data normalization: Tags and aliases</h1><a name="abouttagsandaliases"></a><h2> <a name="abouttagsandaliases_about_tags_and_aliases"><span class="mw-headline" id="About_tags_and_aliases"> About tags and aliases</span></a></h2>
<p>In your data, you might have groups of events with related field values. To help you search more efficiently for these particular groups of event data, you can assign tags to their field values. You can assign one or more tags to any field/value combination (including event type, host, source, or source type).
</p><p>You can use tags to:
</p>
<ul><li> <b>Help you track abstract field values</b>, like IP addresses or ID numbers. For example, you could have an IP address related to your main office with the value 192.168.1.2. Tag that <code><font size="2">IPaddress</font></code> value as <i>mainoffice</i>, and then search on that tag to find events with that IP address.
</li><li> <b>Use one tag to group a set of field values together</b>, so you can search on them with one simple command. For example, you might find that you have two host names that relate to the same computer. You could give both of those values the same tag. When you search on that tag, Splunk Enterprise returns events involving both host name values.
</li><li> <b>Give specific extracted fields multiple tags that reflect different aspects of their identity</b>, which enable you to perform tag-based searches that help you quickly narrow down the results you want. To understand how this could work, see the following example.
</li></ul><p><b>Example:</b> 
</p><p>Let's say you have an extracted field called <code><font size="2">IPaddress</font></code>, which refers to the IP addresses of the data sources within your company intranet. You can make <code><font size="2">IPaddress</font></code> useful by tagging each IP address based on its functionality or location.  You can tag all of your routers' IP addresses as <i>router</i>.  You can also tag each IP address based on its location, for example: <i>SF</i> or <i>Building1</i>.  An IP address of a router located in San Francisco inside Building 1 could have the tags <i>router</i>, <i>SF</i>, and <i>Building1</i>.  
</p><p>To search for all routers in San Francisco that are not in <i>Building1</i>, you'd search for the following: 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag=router tag=SF NOT (tag=Building1)</font></code><br></div>

<a name="tagandaliasfieldvaluesinsplunkweb"></a><h2> <a name="tagandaliasfieldvaluesinsplunkweb_tag_and_alias_field_values_in_splunk_web"><span class="mw-headline" id="Tag_and_alias_field_values_in_Splunk_Web"> Tag and alias field values in Splunk Web</span></a></h2>
<p>In your data, you might have groups of events with related field values. To help you search more efficiently for these groups of fields, you can assign tags to their field values. You can assign one or more tags to any extracted field (including event type, host, source, or source type).
</p><p>For more information, read <a href="#abouttagsandaliases" class="external text">"About tags and aliases"</a> in the Knowledge Manager manual.
</p>
<h3> <a name="tagandaliasfieldvaluesinsplunkweb_how_to_tag_and_alias_field_values"><span class="mw-headline" id="How_to_tag_and_alias_field_values"> How to tag and alias field values </span></a></h3>
<p>You can tag field/value pairs. You can also alias field names.
</p>
<h4><font size="3"><b><i> <a name="tagandaliasfieldvaluesinsplunkweb_tag_field_value_pairs"><span class="mw-headline" id="Tag_field_value_pairs"> Tag field value pairs </span></a></i></b></font></h4>
<p>You can use Splunk Web to tag any field value pair directly from the search results. 
</p><p><b>1.</b> Locate an event with a field value pair that you would like to tag. 
</p><p><b>2.</b> Open the event by clicking on the arrow in the <i><b>i</b></i> column to see the full list of fields that Splunk Enterprise has extracted from the event.
</p><p><b>3.</b> Click the <b>Actions</b> arrow for the field value pair that you would like to create a tag for and select <b>Edit Actions</b>. 
</p>
<dl><dd> This opens the Create Actions dialog.
</dd></dl><p><img alt="6.1.x open event to create tag.png" src="images/5/5f/6.1.x_open_event_to_create_tag.png" width="700" height="254"></p><p><b>4.</b> In the Create Actions dialog, define one or more <b>Tag(s)</b> for the field value pair. 
</p>
<dl><dd> Values for the <b>Tags</b> field must not be enclosed within double quotes. 
</dd></dl><p><img alt="6.1.x create tags modal.png" src="images/5/54/6.1.x_create_tags_modal.png" width="459" height="252"></p><p><b>5.</b> Click <b>Save</b> to save the tag.
</p>
<h4><font size="3"><b><i> <a name="tagandaliasfieldvaluesinsplunkweb_removing_url-encoded_values_from_tag_definitions"><span class="mw-headline" id="Removing_URL-encoded_values_from_tag_definitions">Removing URL-encoded values from tag definitions</span></a></i></b></font></h4>
<p>When you tag a field value pair, "value" part of the pair cannot be URL-encoded. If your tag has any <code><font size="2">%##</font></code> format URL-encoding, decode it and then save the tag with the decoded URL. 
</p><p>For example, say you want to give this field value pair the tag "Useful": 
</p><p><code><font size="2">url=http%3A%2F%2Fdocs.splunk.com%2FDocumentation</font></code>.
</p><p><b>1.</b> Create and save the tag in Splunk Web. 
</p><p><b>2.</b> Navigate to <b>Settings &gt; Tags &gt; List by tag name</b> and click on the <b>Useful</b> tag name to open the detail page for that tag.
</p><p><b>3.</b> Under <b>Field value pair</b> replace <code><font size="2">url=http%3A%2F%2Fdocs.splunk.com%2FDocumentation</font></code> with the decoded version: <code><font size="2">url=http://docs.splunk.com/Documentation</font></code>.
</p><p><b>4.</b> Click <b>Save</b> to save your changes.
</p><p>See <a href="#defineandusetags" class="external text">"Define and manage tags"</a> for more information about using the Settings pages for tags.
</p>
<h4><font size="3"><b><i> <a name="tagandaliasfieldvaluesinsplunkweb_alias_field_names"><span class="mw-headline" id="Alias_field_names"> Alias field names </span></a></i></b></font></h4>
<p>You can add multiple aliases to a field name or use these field aliases to normalize different field names. This does not rename or remove the original field name. After you alias a field, you can search for it using any of its name aliases. To alias a field name, you need to have access to props.conf. For information on how to do this, see <a href="#addaliasestofields" class="external text">"Create aliases for fields"</a> in the Knowledge Manager manual.
</p>
<h3> <a name="tagandaliasfieldvaluesinsplunkweb_search_for_tagged_field_values"><span class="mw-headline" id="Search_for_tagged_field_values"> Search for tagged field values </span></a></h3>
<p>There are two ways to search for tags. If you are searching for a tag associated with a value on any field, you can use the following syntax:
</p>
<code><font size="2"><br>tag=&lt;tagname&gt;<br></font></code>
<p>Or, if you are looking for a tag associated with a value on a specific field, you can use the following syntax:
</p>
<code><font size="2">tag::&lt;field&gt;=&lt;tagname&gt;<br></font></code>
<h4><font size="3"><b><i> <a name="tagandaliasfieldvaluesinsplunkweb_use_wildcards_to_search_for_tags"><span class="mw-headline" id="Use_wildcards_to_search_for_tags"> Use wildcards to search for tags </span></a></i></b></font></h4>
<p>You can use the asterisk (<code><font size="2">*</font></code>) wildcard when searching keywords and field values, including for eventtypes and tags. 
</p><p>For example, if you have multiple event-type tags for various types of IP addresses, such as <code><font size="2">IP-src</font></code> and <code><font size="2">IP-dst</font></code>, you can search for all of them with:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag::eventtype=IP-*</font></code><br></div>
<p>If you wanted to find all hosts whose tags contain "local", you can search for the tag:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag::host=*local*</font></code><br></div>
<p>Also, if you wanted to search for the events with eventtypes that have no tags, you can search for the Boolean expression:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">NOT tag::eventtype=*</font></code><br></div>
<h3> <a name="tagandaliasfieldvaluesinsplunkweb_disabling_and_deleting_tags"><span class="mw-headline" id="Disabling_and_deleting_tags"> Disabling and deleting tags </span></a></h3>
<p>If you have a tag that you no longer want to use, or want to have associated with a particular field, you have the option of either disabling it or removing it. You can:
</p>
<ul><li> Remove a tag association for a specific field value through the Search app.
</li><li> Disable or delete tags, even if they are associated with multiple field values, via Splunk Web.
</li></ul><p>For more information about using Splunk Web to manage tags, see <a href="#defineandusetags" class="external text">"Define and manage tags"</a> in the Knowledge Manager manual.
</p>
<h4><font size="3"><b><i> <a name="tagandaliasfieldvaluesinsplunkweb_remove_a_tag_association_for_a_specific_field_value_in_search_results"><span class="mw-headline" id="Remove_a_tag_association_for_a_specific_field_value_in_search_results"> Remove a tag association for a specific field value in search results </span></a></i></b></font></h4>
<p>If you no longer want to have a tag associated with a specific field value in your search results, click the arrow next to that event, then under Actions click on the arrow next to that field value, then select Edit Tags to bring up the <b>Create Tags</b> popup window. 
</p><p>Erase the tag or tags that you want to disable from the <b>Tags</b> field and click <b>Save</b>. This removes this particular tag and field value association from the system. If this is the only field value with which a tag is associated, then the tag is removed from the system.
</p>
<h3> <a name="tagandaliasfieldvaluesinsplunkweb_rename_source_types"><span class="mw-headline" id="Rename_source_types"> Rename source types </span></a></h3>
<p>When you configure a source type in props.conf, you can rename the source type. Multiple source types can share the same name; this can be useful if you want to group a set of source types together for searching purposes. For example, you can normalize source type names that include "-too_small" to remove the classifier. For information on how to do this, see "Rename source types" in the Getting Data In Manual.
</p>
<a name="defineandusetags"></a><h2> <a name="defineandusetags_define_and_manage_tags"><span class="mw-headline" id="Define_and_manage_tags"> Define and manage tags</span></a></h2>
<p>Splunk Enterprise provides a set of methods for tag creation and management. Most users will go with the simplest method--tagging field/value pairs directly in search results, a method discussed in detail in <a href="#tagandaliasfieldvaluesinsplunkweb" class="external text">"Tag and alias field values,"</a> in this manual. 
</p><p>However, as a knowledge manager, you will use the Tags page in Splunk Web to manage the tags created by users of your Splunk Enterprise implementation. This topic explains how to:
</p>
<ul><li> Use the Tags page in Settings to manage tags for your Splunk Enterprise implementation.
</li><li> Create new tags using Splunk Web. 
</li><li> Disable or delete tags using Splunk Web.
</li></ul><p>To navigate to the Tags page, select <b>Settings &gt; Tags</b>.
</p>
<h3> <a name="defineandusetags_using_the_tag_page_in_settings"><span class="mw-headline" id="Using_the_Tag_page_in_Settings">Using the Tag page in Settings </span></a></h3>
<p>The Tags page in Settings provides three views of the tags in your Splunk Enterprise implementation: 
</p>
<ul><li> <b>Tags by field value pair(s)</b>, which you access by clicking <i>List by field value pair</i> on the Tags page.
</li><li> <b>List by tag name</b>.
</li><li> <b>Tags by unique ID</b>, which you access by clicking <i>All unique tag objects</i> on the Tags page. 
</li></ul><p>Using these pages you can manage your tag collection in different ways and get quick access to associations that have been made between tags and field/value pairs over time. You can also create and remove associations between tags. Select the page that meets your tag management needs. 
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_managing_tag_sets_associated_with_specific_field_value_pairs"><span class="mw-headline" id="Managing_tag_sets_associated_with_specific_field_value_pairs"> Managing tag sets associated with specific field value pairs </span></a></i></b></font></h4>
<p>What if you want to see a list of all of the field/value pairs in your system that have tags associated with them? Furthermore, what if you want to review and even update the set of tags that are associated with a specific field/value pairing? Or define a set of tags for a particular field/value pair?
</p><p>The <b>List by field value pair</b> page enables you to review and edit the tag sets that have been associated with particular field/value pairs. 
</p><p>You can also use this page to manage the permissions for a particular field/value combination with tags.
</p><p>To see the list of tags for a specific field/value pair, locate that pairing and click on the field/value pair. This takes you to the associated detail page for that pair.
</p><p>Here's an example of a set of tags that have been defined for the <code><font size="2">eventtype=auditd_create</font></code> field/value pair:
</p><p><img alt="Defining tags for field-value pairs b.png" src="images/4/4b/Defining_tags_for_field-value_pairs_b.png" width="720" height="272"></p><p>You can add and delete tags from this detailed view (if you have the permissions to do so).
</p><p>Click <b>New</b> on the <b>List by field value pair</b> page, to define a set of tags for a new field/value pair. 
</p><p>When you create or update a tag list for a field/value pairing, keep in mind that you may be creating new tags, or associating existing tags with a different kind of field/value pair than they were originally designed to work with. As a knowledge manager you should consider sticking to a carefully designed and maintained set of tags. This practice aids with data normalization, and can reduce confusion on the part of your users. (For more information see the <a href="#curatesplunkknowledgewithmanager" class="external text">"Manage knowledge objects through Settings pages"</a> chapter of this manual.)
</p><p><b>Note:</b> You may want to verify the existence of a field/value pair that you add to the Tags by field/value pair(s) page. The system <i>will not</i> prevent you from defining a list of tags for a nonexistent field/value pair.
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_reviewing_and_updating_sets_of_field_value_pairs_associated_with_specific_tags"><span class="mw-headline" id="Reviewing_and_updating_sets_of_field_value_pairs_associated_with_specific_tags"> Reviewing and updating sets of field value pairs associated with specific tags </span></a></i></b></font></h4>
<p>What if you want to see a list of all of the tags in your system that have one or more tags associated with them? Furthermore, what if you want to review and even update the set of field/value pairings that are associated with a specific tag? Or define a set of field/value pairings for a new tag?
</p><p>These questions are answered by the <b>List by tag name</b> page in Splunk Web. You can review and edit the sets of field/value pairs that have been associated with specific tags. 
</p><p>You can not manage permissions for the set of field/value pairs associated with a tag, however, on this page.
</p><p>You can see the list of field/value pairings for a particular tag. Find the tag in the <b>List by tag name</b> page, and click on the tag name in the <b>Tag name</b> column. This takes you to the detail page for the tag.
</p><p>Here's an example displaying the various field/value pairings that the <code><font size="2">modify</font></code> tag has been associated with.
</p><p><img alt="Defining field-values for tags in manager b.png" src="images/4/47/Defining_field-values_for_tags_in_manager_b.png" width="510" height="295"></p><p>You can add and delete field/value associations (if you have the permissions to do so). 
</p><p>To define a set of field/value pairings for a new tag, click <b>New</b> on the List by tag name page.
</p><p>When you create or update a set of field/value pairings for a tag, note that you may be creating new field/value pairings. You may want to verify the existence of field/value pairs that you associate with a tag. The system <i>will not</i> prevent you from adding nonexistent field/value associations.
</p><p>Be careful when you create new tags. Tags may already exist that serve the purpose you're trying to address. As a knowledge manager you should consider sticking to a carefully designed and maintained set of tags. This practice aids with data normalization, and can reduce confusion on the part of your users. (For more information see the <a href="#curatesplunkknowledgewithmanager" class="external text">"Manage knowledge objects through Settings pages"</a> chapter of this manual.)
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_reviewing_all_unique_field.2fvalue_pair_and_tag_combinations"><span class="mw-headline" id="Reviewing_all_unique_field.2Fvalue_pair_and_tag_combinations"> Reviewing all unique field/value pair and tag combinations </span></a></i></b></font></h4>
<p>The <b>All unique tag objects</b> page breaks out all of the unique tag name and field/value pairings in your system. Unlike the previous two pages, this page only lets you edit one-to-one relationships between tags and field/value pairs. 
</p><p>You can search on a particular tag to quickly see all of the field/value pairs with which it's associated, or vice versa. Use this page if you want to disable or clone a particular tag and field/value association, or if you want to maintain permissions at that level of granularity.
</p>
<h3> <a name="defineandusetags_disabling_and_deleting_tags"><span class="mw-headline" id="Disabling_and_deleting_tags">Disabling and deleting tags</span></a></h3>
<p>If you have a tag that you no longer want to use, or want to have associated with a particular field/value pairing, you have the option of either disabling it or removing it. If you have the permissions to do so, you can:
</p>
<ul><li> Remove a tag association for a specific field/value pair in the search results.
</li><li> Bulk disable or delete a tag, even if it is associated to multiple field values, via the List by tag name page.
</li><li> Bulk disable or delete the associations between a field/value pair and a set of tags via the List by field value pair page.
</li></ul><p>For information about deleting tag associations with specific field/value pairs in your search results, see <a href="#tagandaliasfieldvaluesinsplunkweb" class="external text">"Tag and alias field values"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_delete_a_tag_with_multiple_field.2fvalue_pair_associations"><span class="mw-headline" id="Delete_a_tag_with_multiple_field.2Fvalue_pair_associations"> Delete a tag with multiple field/value pair associations </span></a></i></b></font></h4>
<p>You can use Splunk Web to completely remove a tag from your system, even if it is associated with dozens of field/value pairs. This method enables you to get rid of all of these associations in one step. 
</p><p>Navigate to <b>Settings &gt; Tags &gt; List by tag name</b>. Delete the tag. If you don't see a delete link for the tag, you don't have permission to delete it. When you delete tags, try to be aware of downstream dependencies by their removal. For more information, see <a href="#curatesplunkknowledgewithmanager" class="external text">"Manage knowledge objects through Settings pages"</a> in this manual. 
</p><p><b>Note:</b> You can also go into the edit view for a particular tag and delete a field/value pair association directly.
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_disable_or_delete_the_associations_between_a_field.2fvalue_pairing_and_a_set_of_tags"><span class="mw-headline" id="Disable_or_delete_the_associations_between_a_field.2Fvalue_pairing_and_a_set_of_tags"> Disable or delete the associations between a field/value pairing and a set of tags </span></a></i></b></font></h4>
<p>Use this method to bulk-remove the set of tags that is associated to a field/value pair. This method enables you to get rid of these associations in a single step. It <i>does not</i> remove the field/value pairing from your data, however.
</p><p>Navigate to <b>Settings &gt; Tags &gt; List by field value pair</b>. Delete the field/value pair. If you don't see a delete link for the field/value pair, you don't have permission to delete it. When you delete these associations, try to be aware of downstream dependencies that may be adversely affected by their removal. For more information, see <a href="#curatesplunkknowledgewithmanager" class="external text">"Manage knowledge objects through Settings pages"</a> in this manual. 
</p><p><b>Note:</b> You can also delete a tag association directly in the edit view for a particular field value.
</p>
<h4><font size="3"><b><i> <a name="defineandusetags_disable_tags"><span class="mw-headline" id="Disable_tags"> Disable tags </span></a></i></b></font></h4>
<p>Depending on your permissions to do so, you can also disable tag and field/value associations using the three Tags pages in Splunk Web. When an association between a tag and a field/value pair is disabled, it stays in the system but is inactive until it is enabled again.
</p>
<a name="addaliasestofields"></a><h2> <a name="addaliasestofields_create_aliases_for_fields"><span class="mw-headline" id="Create_aliases_for_fields"> Create aliases for fields</span></a></h2>
<p>You can create multiple aliases for a field. The original field is not removed. This process enables you to search for the original field using any of its aliases.
</p><p><b>Important:</b> Field aliasing is performed <i>after</i> key/value extraction but <i>before</i> field lookups. Therefore, you can specify a lookup table based on a field alias. This can be helpful if there are one or more fields in the lookup table that are identical to fields in your data, but have been named differently. For more information read "<a href="#addfieldsfromexternaldatasources" class="external text">Configure CSV and external lookups</a>" and "<a href="#configurekvstorelookups" class="external text">Configure KV store lookups</a>" in this manual.
</p><p>You can define aliases for fields that are extracted at index time as well as those that are extracted at search time.
</p><p>You add your field aliases to props.conf, which you edit in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>, or your own custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>. (We recommend using the latter directory if you want to make it easy to transfer your data customizations to other index servers.)
</p><p><b>Note:</b> Splunk Enterprise's field aliasing functionality does not currently support multivalue fields.
</p><p>To alias fields:
</p><p><b>1.</b> Add the following line to a stanza in <code><font size="2">props.conf</font></code>:
</p>
<code><font size="2"><br>FIELDALIAS-&lt;class&gt; = &lt;orig_field_name&gt; AS &lt;new_field_name&gt;<br></font></code>
<ul><li> &lt;orig_field_name&gt; is the original name of the field.
</li><li> &lt;new_field_name&gt; is the alias to assign to the field.
</li><li> You can include multiple field alias renames in one stanza.
</li></ul><p><b>2.</b> Restart Splunk Enterprise for your changes to take effect. 
</p>
<h3> <a name="addaliasestofields_example_of_field_alias_additions_for_a_lookup"><span class="mw-headline" id="Example_of_field_alias_additions_for_a_lookup"> Example of field alias additions for a lookup </span></a></h3>
<p>Say you're creating a lookup for an external static table CSV file where the field you've extracted at search time as "ip" is referred to as "ipaddress." In the <code><font size="2">props.conf</font></code> file where you've defined the extraction, you would add a line that defines "ipaddress" as an alias for "ip," as follows:
</p>
<code><font size="2"><br>[accesslog]<br>EXTRACT-extract_ip = (?&lt;ip&gt;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})<br>FIELDALIAS-extract_ip = ip AS ipaddress<br></font></code>
<p>When you set up the lookup in <code><font size="2">props.conf</font></code>, you can just use <code><font size="2">ipaddress</font></code> where you'd otherwise have used <code><font size="2">ip</font></code>:
</p>
<code><font size="2"><br>[dns]<br>lookup_ip = dnsLookup ipaddress OUTPUT host<br></font></code>
<p>For more information about search-time field extraction configuration, see "<a href="#createandmaintainsearch-timefieldextractionsthroughconfigurationfiles" class="external text">Create and maintain search-time field extractions through configuration files</a>" in this manual.
</p><p>For more information about lookup configuration, see <a href="#addfieldsfromexternaldatasources" class="external text">"Configure CSV and external lookups"</a> and "<a href="#configurekvstorelookups" class="external text">Configure KV store lookups</a>" in this manual.
</p>
<a name="tagthehostfield"></a><h2> <a name="tagthehostfield_tag_the_host_field"><span class="mw-headline" id="Tag_the_host_field"> Tag the host field </span></a></h2>
<p>Tagging the host field is useful for knowledge capture and sharing, and for crafting more precise searches.  You can tag the host field with one or more words.  Use this to group hosts by function or type, to enable users to easily search for all activity on a group of similar servers. If you've changed the value of the host field for a given input, you can also tag events that are already in the index with the new host name to make it easier to search across your data set. 
</p>
<h3> <a name="tagthehostfield_add_a_tag_to_the_host_field_with_splunk_web"><span class="mw-headline" id="Add_a_tag_to_the_host_field_with_Splunk_Web"> Add a tag to the host field with Splunk Web </span></a></h3>
<p>To add a tag to a host field/value combination in Splunk Web:
</p><p><b>1.</b>Perform a search for data from the host you'd like to tag. 
</p><p><b>2.</b>In the search results, click on the arrow associated with the event containing the field you want to tag. In the expanded list, click on the arrow under <b>Actions</b> associated the field, then select <b>Edit Tags</b>. 
</p><p><img alt="Tag dropdown example b.png" src="images/4/43/Tag_dropdown_example_b.png" width="573" height="406"></p><p><b>3.</b> In the Create Tags dialog  enter the host field value that you'd like to tag, for example in Field Value enter <b>Tag host= &lt;current host value&gt;</b>. Enter your tag or tags, separated by commas or spaces, and click <b>Save</b>.
</p>
<h3> <a name="tagthehostfield_host_names_vs._tagging_the_host_field"><span class="mw-headline" id="Host_names_vs._tagging_the_host_field"> Host names vs. tagging the host field </span></a></h3>
<p>The value of the host field is set when an event is indexed. It can be set by default based on the Splunk Enterprise server hostname, set for a given input, or extracted from each event's data.  Tagging the host field with an alternate hostname doesn't change the actual value of the host field, but it lets you search for the tag you specified instead of having to use the host field value.  Each event can have only one host name, but multiple host tags.
</p><p>For example, if your Splunk Enterprise server is receiving compliance data from a specific host, tagging that host with <b>compliance</b> will help your compliance searches.  With host tags, you can create a loose grouping of data without masking or changing the underlying host name.
</p><p>You might also want to tag the host field with another host name if you indexed some data from a particular input source and then decided to change the value of the host field for that input--all the new data coming in from that input will have the new host field value, but the data that already exists in your index will have the old value. Tagging the host field for the existing data lets you search for the new host value without excluding all the existing data.
</p>
<a name="tageventtypes"></a><h2> <a name="tageventtypes_tag_event_types"><span class="mw-headline" id="Tag_event_types"> Tag event types</span></a></h2>
<p>Tag event types to add information to your data. Any event type can have multiple tags.  For example, you can tag all firewall event types as <b>firewall</b>, tag a subset of firewall event types as <b>deny</b> and tag another subset as <b>allow</b>.  Once an event type is tagged, any event type matching the tagged pattern will also be tagged.
</p><p><b>Note:</b> You can tag an event type when you <a href="#defineeventtypes" class="external text">create it in Splunk Web</a> or configure it in eventtypes.conf.
</p>
<h3> <a name="tageventtypes_add_tags_to_event_types_using_splunk_web"><span class="mw-headline" id="Add_tags_to_event_types_using_Splunk_Web"> Add tags to event types using Splunk Web </span></a></h3>
<p>Splunk Web enables you to view and edit lists of event types.
</p>
<ul><li> Navigate to <b>Settings &gt; Event types</b>.
</li><li> Locate the event type you want to tag and click on its name to go to its detail page. 
<ul><li> <b>Note:</b> Keep in mind that event types are often associated with specific Splunk Enterprise apps. They also have role-based permissions that can prevent you from seeing and/or editing them.
</li></ul></li><li> On the detail page for the event type, add or edit tags in the <b>Tags</b> field.
</li><li> Click <b>Save</b> to confirm your changes.
</li></ul><p>Once you have tagged an event type, you can search for it in the search bar with the syntax <code><font size="2">tag::&lt;field&gt;=&lt;tagname&gt;</font></code> or <code><font size="2">tag=&lt;tagname&gt;</font></code>:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag=foo</font></code><br></div>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">tag::host=*local*</font></code><br></div>

<h1>Build a data model</h1><a name="aboutdatamodels"></a><h2> <a name="aboutdatamodels_about_data_models"><span class="mw-headline" id="About_data_models"> About data models</span></a></h2>
<p>Data models drive the Pivot tool. They enable users of Pivot to create compelling reports and dashboards without designing the searches that generate them. Data models can have other uses, especially for Splunk Enterprise app developers. 
</p><p>Splunk Enterprise knowledge managers design and maintain data models. These knowledge managers understand the format and semantics of their indexed data and are familiar with the Splunk Enterprise search language. In building a typical data model, knowledge managers use knowledge object types such as <b>lookups</b>, <b>transactions</b>, search-time <b>field extractions</b>, and <b>calculated fields</b>.
</p>
<h3> <a name="aboutdatamodels_what_is_a_data_model.3f"><span class="mw-headline" id="What_is_a_data_model.3F">What is a data model?</span></a></h3>
<p>A data model is a hierarchically structured search-time mapping of semantic knowledge about one or more datasets. It encodes the domain knowledge necessary to build a variety of specialized searches of those datasets. These specialized searches are used by Splunk Enterprise to generate reports for Pivot users. 
</p><p>When a Pivot user designs a pivot report, she selects the data model that represents the category of event data that she wants to work with, such as Web Intelligence or Email Logs. Then she selects an <b>object</b> within that data model that represents the specific dataset on which she wants to report. Data models are composed of objects, which can be arranged in hierarchical structures of parent and child objects. Each child object represents a subset of the dataset covered by its parent object.
</p><p>If you are familiar with relational database design, think of data models as analogs to database schemas. When you plug them into the Pivot Editor, they let you generate statistical tables, charts, and visualizations based on column and row configurations that you select. 
</p><p>To create an effective data model, you must understand your data sources and your data semantics. This information can affect your data model architecture--the manner in which the objects that make up the data model are organized. 
</p><p>For example, if your dataset is based on the contents of a table-based data format, such as a .csv file, the resulting data model is flat, with a single top-level root object that encapsulates the fields represented by the columns of the table. The root object may have child objects beneath it. But these child objects do not contain additional fields beyond the set of attributes that the child objects inherit from the root object. 
</p><p>Meanwhile, a data model derived from a heterogeneous system log might have several root objects (events, searches, and transactions). You can associate each of these root with a hierarchy of objects in nested parent and child relationships. Each of those child objects can have new fields in addition to the fields they inherit from ancestor objects. 
</p><p>Data models can get their fields from <b>extractions</b> that you set up in the Field Extractions section of Manager or by configured directly in <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. When you define your data model, you can arrange to have it get additional fields at search time through regular-expression-based field extractions, <b>lookups</b>, and <code><font size="2">eval</font></code> expressions. 
</p><p>In data model terminology, the fields that data models use are called <b>attributes</b>. They break down into the categories described above (auto-extracted, eval expression, regular expression) and more (lookup, geo IP). See <a href="#aboutdatamodels_object_attributes" class="external text">"Object attributes"</a>, below.
</p><p><b>Note:</b> Data models are a category of <b>knowledge object</b> and are fully permissionable. A data model's permissions cover all of its data model objects. For information about setting data model permissions, see <a href="#managedatamodels" class="external text">"Manage data models,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_data_models_generate_searches"><span class="mw-headline" id="Data_models_generate_searches">Data models generate searches</span></a></i></b></font></h4>
<p>When you consider what data models are and how they work it can also be helpful to think of them as a collection of structured information that generates different kinds of searches. Each object within a data model can be used to generate a search that returns a particular dataset. When we say that a data model object "represents a dataset" we're really talking about the dataset returned by the object you select. 
</p><p>We go into more detail about this relationship between data models, data model objects, and searches in the following subsections. 
</p>
<ul><li> <b>Object constraints</b> determine the first part of the search through: 
<ul><li> Simple search filters (Root event objects and all child objects).
</li><li> <code><font size="2">transaction</font></code> definitions (Root transaction objects).
</li><li> More complex search strings that may use transforming commands, among others (Root search objects).
</li></ul></li><li> <b>Object attributes</b> are essentially fields. When you select an object for Pivot, the unhidden attributes you define for that object comprise the list of fields that you'll choose from in Pivot when you decide what you want to report on. The fields you select are added to the search that the object generates, and they can include <b>calculated fields</b>, user-defined field extractions, and fields added to your data by lookups.
</li></ul><p>The last parts of the object-generated-search are determined by your Pivot Editor selections. They can determine the nature of the transforming search commands that Splunk Enterprise will use to format the results as a statistical table that it can use in turn as the basis for a chart visualization.
</p><p>For more information about how you use the Pivot Editor to create pivot tables, charts, and visualizations that are based on data model objects, see "Introduction to Pivot" in the Pivot Manual.
</p>
<h3> <a name="aboutdatamodels_objects"><span class="mw-headline" id="Objects">Objects</span></a></h3>
<p>Data models are composed of one or more <b>objects.</b> Here are some basic facts about data model objects:
</p>
<ul><li> <b>An object is a specification for a dataset.</b> Each data model object corresponds in some manner to a set of data in an index. You can apply data models to different indexes and get different datasets.
</li><li> <b>Objects break down into four types.</b> These types are: <i>Event</i> objects, <i>search</i> objects, <i>transaction</i> objects, and <i>child</i> objects.
</li><li> <b>Objects are hierarchical.</b> Objects in data models can be arranged hierarchically in parent/child relationships. The top-level event, search, and transaction objects in data models are collectively referred to as "root objects."
</li><li> <b>Child objects have inheritance.</b> Data model objects are defined by characteristics that mostly break down into <i>constraints</i> and <i>attributes</i>. Child objects inherit constraints and attributes from their parent objects and have additional constraints and attributes of their own.
</li></ul><p>We'll dive into more detail about these and other aspects of data model objects in the following subsections.
</p>
<ul><li> <b>Child objects provide a way of filtering events from parent objects</b> - Because a child object always provides an additional constraint on top of the constraints it has inherited from its parent object, the dataset it represents is always a <i>subset</i> of the dataset that its parent represents.
</li></ul><h4><font size="3"><b><i> <a name="aboutdatamodels_root_objects_and_object_types"><span class="mw-headline" id="Root_objects_and_object_types">Root objects and object types</span></a></i></b></font></h4>
<p>The top-level objects in data models are referred to as "root objects." Data models can contain multiple root objects of various types, and each of these root objects can be a parent to more child objects. This association of base and child objects is an "object tree." The overall set of data represented by an object tree is selected first by its root object and then refined and extended by its child objects.
</p><p>Root objects can be defined by a search constraint, a search, or a transaction:
</p>
<ul><li> <b>Root event objects</b> are the most commonly-used type of root data model object. Each root event object broadly represents a type of event. For example, an <i>HTTP Access</i> root event object could correspond to access log events, while an <i>Error</i> event corresponds to events with error messages. 
</li></ul><dl><dd> Root event objects are typically defined by a simple constraint (see <a href="#aboutdatamodels_object_constraints" class="external text">"Object Constraints,"</a> below)--it's what an experienced Splunk Enterprise user might think of as the first portion of a search, before the pipe character, commands, and arguments are applied. For example, <code><font size="2">status &gt; 600</font></code> and <code><font size="2">sourcetype=access_* OR sourcetype=iis*</font></code> are possible event object definitions. <br><br></dd><dd> <b>Note:</b> Child objects of all three types--event, transaction, and search--are defined with simple constraints that narrow down the set of data that they inherit from their ancestor objects. 
</dd></dl><ul><li> <b>Root transaction objects</b> enable you to create data models that represent <b>transactions</b>: groups of related events that span time. Transaction object definitions utilize fields that have already been added to the model via event or search object, which means that you can't create data models that are composed <i>only</i> of transaction objects and their child objects. Before you create a transaction object you must already have some event or search object trees in your model.
</li><li> <b>Root search objects</b> use an arbitrary Splunk search that includes transforming commands to define the dataset that they represent. If you want to define a base dataset that includes one or more fields that aggregate over the entire dataset, you might need to use a root search object. For example: a system security dataset that has various system intrusion events broken out by category over time. 
</li></ul><p><b>Object types and data model acceleration:</b>
</p><p>You can optionally use <b>data model acceleration</b> to speed up generation of pivot tables and charts. However, there are a few restrictions to this functionality related to data model objects that may have some bearing on how you construct your data model, if you think your users would benefit from data model acceleration.
</p><p>In a data model, only root event objects and their children can be accelerated. Base search objects, base transaction objects, and the children of those objects cannot be accelerated. You may want to avoid using a base search object if you want to accelerate it and you can set up base event object that covers the same dataset.
</p><p>For more information on enabling acceleration for your data models see <a href="#managedatamodels" class="external text">"Manage data models,"</a> in this manual.
</p><p>The following example shows the first several objects in a "Call Detail Records" data model. Four top-level root objects are displayed: <b>All Calls</b>, <b>All Switch Records</b>, <b>Conversations</b>, and <b>Outgoing Calls</b>. 
</p><p><img alt="6.0-dm-root objects example.png" src="images/b/bd/6.0-dm-root_objects_example.png" width="375" height="556"></p><p><b>All Calls</b> and <b>All Switch Records</b> are root event objects that represent all of the calling records and all of the carrier switch records, respectively. Both of these root event objects have child objects that deal with subsets of the data owned by their parents. The <b>All Calls</b> root event object has child objects that break down into different call classifications: Voice, SMS, Data, and Roaming. If you were a Pivot user who only wanted to report on aspects of cellphone data usage, you'd select the Data object. But if you wanted to create reports that compare the four call types, you'd choose the <b>All Calls</b> root event object instead.
</p><p><b>Conversations</b> and <b>Outgoing Calls</b> are root transaction objects. They both represent transactions--groupings of related events that span a range of time. The "Conversations" object only contains call records  of conversations between two or more people where the maximum pause between conversation call record events is less than two hours and the total length of the conversation is less than one day.
</p><p>For details about defining the different types of objects, see the topic <a href="#designdatamodelobjects" class="external text">"Design data model objects,"</a> in this manual.
</p>
<h3> <a name="aboutdatamodels_object_constraints"><span class="mw-headline" id="Object_constraints">Object constraints</span></a></h3>
<p>All data model objects are defined by sets of <b>constraints</b>. Object constraints filter out events that aren't relevant to the object; they help to define the dataset that the object represents. 
</p>
<ul><li> <b>For a root event object or a child object of any type</b>, the constraint looks like a simple search, without additional pipes and search commands. For example, the constraint for <code><font size="2">HTTP Request</font></code>, one of the root event objects of the Web Intelligence data model, is <code><font size="2">sourcetype=access_* OR sourcetype=iis*</font></code>. 
</li><li> <b>For a root search object</b>, the constraint is the object's base search string.
</li><li> <b>For a root transaction object</b>, the constraint is the transaction definition. Transaction object definitions must identify <i>Group Objects</i> (either one or more event objects, a search object, or a transaction object) and one or more <i>Group By</i> fields. They can also optionally include <b>Max Pause</b> and <b>Max Span</b> values.
</li></ul><p>Constraints are inherited by child objects. Constraint inheritance ensures that each child object represents a subset of the data represented by its parent objects. Your Pivot users can then use these child objects to design reports with datasets that already have extraneous data prefiltered out. 
</p><p>For example, the Web Intelligence data model's <code><font size="2">HTTP Success</font></code> object is a child of the root event object <code><font size="2">HTTP Request</font></code>. It inherits <code><font size="2">sourcetype=access_* OR sourcetype=iis*</font></code> from <code><font size="2">HTTP Request</font></code> and then adds the additional constraint of <code><font size="2">status = 2*</font></code>, which narrows the set of events represented by the object down to HTTP request events that result in success. A Pivot user might use this object for reporting if they already know that they only want to report on successful HTTP request events.
</p><p><img alt="6.0-dm-constraints example.png" src="images/c/c7/6.0-dm-constraints_example.png" width="650" height="268"></p><p>The above example shows the constraints for the <code><font size="2">DocAccess</font></code> object, which is two more levels down the Web Intelligence data model hierarchy from the <code><font size="2">HTTP Success</font></code> object discussed in the previous paragraph. It includes constraints that were inherited from its parent, grandparent and great grandparent objects (<code><font size="2">AssetAccess</font></code>, <code><font size="2">HTTP Success</font></code>, and <code><font size="2">HTTP Request</font></code>, respectively) and adds a new set of constraints. The end result is a base search that is continually narrowed down by each set of constraints: 
</p><p><b>1.</b>  <code><font size="2">HTTP Request</font></code> starts by setting up a search that only finds webserver access events <br></p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* OR sourcetype=iis*</font></code><br></div>
<p><b>2.</b>  <code><font size="2">HTTP Success</font></code> further narrows the focus down to <i>successful</i> webserver access events. <br></p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">status=2*</font></code><br></div>
<p><b>3.</b>  <code><font size="2">Asset Access</font></code> includes a constraint that cuts out all events that involve website pageviews (which leaves only asset access events). <br></p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">uri_path!=*.php OR uri_path!=*.html OR uri_path!=*.shtml OR uri_path!=*.rhtml OR uri_path!=*.asp</font></code><br></div>
<p><b>4.</b>  And finally, <code><font size="2">Doc Access</font></code> adds a constraint that reduces the set of asset access events returned by the search down to events that only involve access of documents (<code><font size="2">.doc</font></code> or <code><font size="2">.pdf</font></code> files). <br></p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">uri_path=*.doc  OR uri_path=*.pdf</font></code><br></div>
<p>When all the constraints are added together, the base search for the object <code><font size="2">Doc Access</font></code> looks something like this:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* OR sourcetype=iis* status=2* uri_path!=*.php OR uri_path!=*.html OR uri_path!=*.shtml OR uri_path!=*.rhtml OR uri_path!=*.asp uri_path=*.doc OR search uri_path=*.pdf</font></code><br></div>
<p>For details about objects and object constraints, see the topic <a href="#designdatamodelobjects" class="external text">"Design data model objects,"</a> in this manual.
</p>
<h3> <a name="aboutdatamodels_object_attributes"><span class="mw-headline" id="Object_attributes">Object attributes</span></a></h3>
<p>An object's attributes are essentially a set of fields associated with the dataset that the object represents. There are five types of object attributes: 
</p>
<ul><li> <b>Auto-extracted</b>: A field that Splunk Enterprise derives at <b>search time</b>. You can only add auto-extracted attributes to root objects. Child objects can only inherit them, and they cannot add new auto-extracted attributes of their own. Auto-extracted attributes can be:
<ul><li> Fields that Splunk Enterprise recognizes and extracts automatically, like <code><font size="2">uri</font></code> or <code><font size="2">version</font></code>. This includes fields indexed through structured data inputs, such as fields extracted from the headers of indexed CSV files. 
</li><li> <b>Field extractions</b>, <b>lookups</b>, or <b>calculated fields</b> that you have defined in Settings or configured in <code><font size="2">props.conf</font></code>. 
</li><li> Fields that you have manually added to the attribute because they aren't currently in the object dataset, but should be in the future. Can include fields that are added to the object dataset by generating commands such as <code><font size="2">inputcsv</font></code> or <code><font size="2">dbinspect</font></code>.
</li></ul></li><li> <b>Eval Expression</b>: A field derived from an <code><font size="2">eval</font></code> expression that you enter in the attribute definition. Eval expressions often involve one or more extracted fields.
</li><li> <b>Lookup</b>: A field that is added to the events in the object dataset with the help of a <b>lookup</b> that you configure in the attribute definition. Lookups add fields from external data sources such as CSV files and scripts. When you define a lookup attribute you can use any lookup that you have <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">defined in Settings</a> and associate it with any other attribute that has already been associated with that same object. 
</li><li> <b>Regular Expression</b>: This attribute type represents a field that is extracted from the object event data using a regular expression that you provide in the attribute definition. A regular expression attribute definition can use a regular expression that extracts multiple fields; each field will appear in the object attribute list as a separate regular expression attribute. 
</li><li> <b>Geo IP</b>: A specific type of <b>lookup</b> that adds geographical attributes, such as latitude, longitude, country, and city to events in the object dataset that have valid IP address fields. Useful for map-related visualizations. 
</li></ul><p>For more information about defining each of the five attribute types, see the topic <a href="#designdatamodelobjects" class="external text">"Design data model objects,"</a> in this manual. 
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_attribute_categories"><span class="mw-headline" id="Attribute_categories">Attribute categories</span></a></i></b></font></h4>
<p>The Data Model Editor groups attributes into three categories:
</p>
<ul><li> <b>Inherited</b> - All objects have at least a few inherited attributes. Child attributes inherit attributes from their parent object, and these inherited attributes always appear in the Inherited category. Root event, search, and transaction objects also have default attributes that Splunk Enterprise categorizes as inherited. 
</li><li> <b>Extracted</b> - Any auto-extracted attribute that you add to an object will be listed in the "Extracted" attribute category.
</li><li> <b>Calculated</b> - Calculated attributes are attributes that are derived through a calculation or lookup of some sort. When you add Eval Expression, Regular Expression, Lookup, and Geo IP attribute types to an object, they all appear in this attribute category.
</li></ul><p><b>Note:</b> The Data Model Editor lets you rearrange the order of calculated attributes. This is useful when you have a set of attributes that must be processed in a specific order, because Splunk Enterprise always processes attributes in order from the top of the list to the bottom. See "Attributes serve several purposes," below, for more information.
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_attributes_are_inherited"><span class="mw-headline" id="Attributes_are_inherited">Attributes are inherited</span></a></i></b></font></h4>
<p>All objects have inherited attributes.
</p><p>A child object will automatically have all of the attributes that belong to its parent. All of these inherited attributes will appear in the child object's "Inherited" category, even if the attributes were categorized otherwise in the parent object.
</p><p>You can add additional attributes to a child object. The Data Model Editor will categorize these objects either as extracted attributes or calculated attributes depending on their attribute type.
</p><p>You can design a relatively simple data model where all of the necessary attributes for an object tree are defined in its root object, meaning that all of the child objects in the tree have the exact same set of attributes as that root object. In such a data model, the child objects would be differentiated from the root object and from each other only by their constraints. 
</p><p>Root event, search, and transaction objects also have inherited attributes. These inherited attributes are default fields that are extracted from from every event, such as <code><font size="2">_time</font></code>, <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, and <code><font size="2">sourcetype</font></code>. 
</p><p>You cannot delete inherited attributes, and you cannot edit their definitions. The only way to edit or remove an inherited attribute belonging to a child object is to delete or edit the attribute from the parent object it originates from as an extracted or calculated attribute. If the attribute originates in a root object as an inherited attribute, you won't be able to delete it or edit it.
</p><p>You can hide attributes from Pivot users as an alternative to attribute deletion. See "Attributes can be visible or hidden to Pivot users," below. 
</p><p>You can also determine whether inherited attributes are optional for an object dataset or required. See "Attributes can be required or optional for an object dataset," below.
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_attributes_serve_several_purposes"><span class="mw-headline" id="Attributes_serve_several_purposes">Attributes serve several purposes</span></a></i></b></font></h4>
<p>Their most obvious function is to provide the set of fields that Pivot users use to define and generate a pivot report. The set of fields that a Pivot user has access to is determined by the object the user chooses when she enters the Pivot Editor. You might add attributes to a child object to provide fields to Pivot users that are specific to the dataset covered by that object.
</p><p>On the other hand, you can also design calculated attributes whose only function is to set up the definition of other attributes or constraints. This is why <b>attribute listing order matters:</b> Splunk Enterprise processes each attribute in the order that it is listed in the Data Model Editor. This is why The Data Model Editor allows you to rearrange the listing order of calculated attributes.
</p><p>For example, you could design a <i>chained set</i> of three Eval Expression attributes. The first two Eval Expression attributes would create what are essentially  <b>calculated fields</b>. The third Eval Expression attribute would use those two calculated fields in its eval expression. 
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_attributes_can_be_visible_or_hidden_to_pivot_users"><span class="mw-headline" id="Attributes_can_be_visible_or_hidden_to_Pivot_users">Attributes can be visible or hidden to Pivot users</span></a></i></b></font></h4>
<p>When you define an attribute you can determine whether it is <i>visible</i> or <i>hidden</i> for Pivot users. This can come in handy if each object in your data model has lots of attributes but only a few attributes per object are actually useful for Pivot users.
</p><p><b>Note:</b> An attribute can be visible in some objects and hidden in others. Hiding an attribute in a parent object does <i>not</i> cause it to be hidden in the child objects that descend from it. 
</p><p>Attributes are visible by default. Attributes that have been hidden for an object are marked as such in the object's attribute list.
</p><p>The determination of what attributes to include in your model and which attributes to expose for a particular object is something you do to make your objects easier to use in Pivot. It's often helpful to your Pivot users if each object exposes only the data that is relevant to that object, to make it easier to build meaningful reports. This means, for example, that you can add attributes to a root object that are hidden throughout the model except for a specific object elsewhere in the hierarchy, where their visibility makes sense in the context of that object and its particular dataset. 
</p><p>Consider the example mentioned in the previous subsection, where you have a set of three "chained" Eval Expression attributes. You may want to hide the first two Eval Expression attributes because they're just there as "inputs" to the third attribute. You'd leave the third attribute visible because it's the final "output"--the attribute that matters for Pivot purposes.
</p>
<h4><font size="3"><b><i> <a name="aboutdatamodels_attributes_can_be_required_or_optional_for_an_object_dataset"><span class="mw-headline" id="Attributes_can_be_required_or_optional_for_an_object_dataset">Attributes can be required or optional for an object dataset</span></a></i></b></font></h4>
<p>During the attribute design process you can also determine whether an attribute is <i>required</i> or <i>optional.</i> This can act as a filter for the event set represented by the object. If you say an attribute is <i>required,</i> you're saying that every event represented by the object <i>must</i> have that attribute. If you define an attribute as <i>optional,</i> the object may have events that do not have that attribute at all. 
</p><p><b>Note:</b> As with attribute visibility (see above) an attribute can be required in some objects and optional in others. Marking an attribute as <i>required</i> in a parent object <i>will not</i> automatically make that attribute <i>required</i> in the child objects that descend from that parent object. 
</p><p>Attributes are optional by default. Attributes that have had their status changed to <i>required</i> for an object are marked as such in the object's attribute list.
</p>
<a name="managedatamodels"></a><h2> <a name="managedatamodels_manage_data_models"><span class="mw-headline" id="Manage_data_models"> Manage data models</span></a></h2>
<p>The Data Models management page is where you go to create data models and maintain some of their "higher order" aspects such as permissions and acceleration. On this page you can:
</p>
<ul><li> <b>Create a new data model</b> - It's as easy as clicking a button.
</li><li> <b>Set permissions</b> - Data models are knowledge objects and as such are permissionable. You use permissions to determine who can see and update the data model. 
</li><li> <b>Enable data model acceleration</b> - This can speed up Pivot performance for data models that cover large datasets. 
</li><li> <b>Clone data models</b> - Useful for quick creation of new data models that are based on existing data models, or to copy data models into other apps. 
</li><li> <b>Upload and download data models</b> - Download a data model (export it outside of Splunk). Upload an exported data model into a different Splunk implementation.
</li><li> <b>Delete data models</b> - Remove data models that are no longer useful.  
</li></ul><p>In this topic we'll discuss these aspects of data model management. 
When you need to define the object hierarchies that make up a data model, you go to the Data Model Editor. For more information, <a href="#designdatamodelobjects" class="external text">"Design data models and objects,"</a> in this manual. 
</p>
<h3> <a name="managedatamodels_navigating_to_the_data_models_management_page"><span class="mw-headline" id="Navigating_to_the_Data_Models_management_page">Navigating to the Data Models management page</span></a></h3>
<p>The Data Models management page is essentially a listing page, similar to the Alerts, Reports, and Dashboards listing pages. It enables management of permissions and acceleration and also enables data model cloning and removal. It's different from the Select a Data Model page that you may see when you first enter Pivot (you'll only see it if you have more than one data model), as that page exists only to enable Pivot users to choose the data model they wish to use for pivot creation.
</p><p>The Data Models management page lists all of the data models in your system in a paginated table. This table can be filtered by app, owner, and name. It can also display all data models that are visible to users of a selected app or just show those data models that were actually created within the app. 
</p><p>Splunk Enterprise enables you to navigate to the Data Models management page via a few different avenues:  
</p>
<ul><li> You can access the page from anywhere in Splunk Web through the Settings list. Just navigate to <b>Settings &gt; Data Models</b>. 
</li><li> From the Data Models listing page in Pivot, click the <b>Manage Data Models</b> button.
</li><li> From the Data Model Editor, click <b>Back To Models</b>.
</li></ul><h3> <a name="managedatamodels_create_a_new_data_model"><span class="mw-headline" id="Create_a_new_data_model">Create a new data model</span></a></h3>
<p>You create data models by navigating to the Data Models management page (see above for instructions) and clicking <b>New Data Model</b>.
</p><p><b>Note:</b> You can only create data models if your role's permissions enable you to do so (your role must have the ability to write to at least one app). If your role has insufficient permissions the <b>New Data Model</b> button will not appear. For more information see the subtopic <a href="#managedatamodels_enable_roles_to_create_data_models" class="external text">"Enable roles to create data models,"</a> below.
</p><p>When you click <b>New Data Model</b>, Splunk Enterprise displays the <b>Create New Data Model</b> dialog. Enter the data model <b>Title</b> and optional <b>Description</b>.
</p><p>The <b>Title</b> field can accept any character except asterisks. It can also accept blank spaces between characters. It's what you'll see on the Select a Data Model page and the Data Models management page, and elsewhere in Splunk Enterprise where the data model name is displayed. 
</p><p>The data model <b>ID</b> field will fill in as you enter the title; we advise that you do not update it. The data model <b>ID</b> must be a unique identifier for the data model. It can only contain letters, numbers, and underscores. Spaces between characters are also not allowed. Once you click <b>Create</b> you can't change the <b>ID</b> value. 
</p><p><b>App</b> will display the app context that you are in currently. If you want the data model to belong to a different app, change the <b>App</b> value.
</p><p>Click <b>Create</b> to open the new data model in the Data Model Editor, where you can begin adding and defining the objects that make up the data model. 
</p><p><img alt="Bubbles dm createnew mod.png" src="images/4/42/Bubbles_dm_createnew_mod.png" width="455" height="340"></p><p>When you first enter the Data Model Editor for a new data model it will not have any objects. To define the data model's first object, click <b>Add Object</b> and select an object type. For more information about object definition, see the following sections on adding field, search, transaction, and child objects.
</p><p>For all the details on the Data Model Editor and the work of creating data model objects, see <a href="#designdatamodelobjects" class="external text">"Design data models and objects,"</a> in this manual. 
</p>
<h4><font size="3"><b><i> <a name="managedatamodels_enable_roles_to_create_data_models"><span class="mw-headline" id="Enable_roles_to_create_data_models">Enable roles to create data models</span></a></i></b></font></h4>
<p>By default only users with the <i>admin</i> or <i>power</i> role can create data models. For other users, the ability to create a data model is tied to whether their roles have "write" access to an app. To grant another role write access to an app, follow these steps: 
</p><p><b>1.</b> Click the <b>App</b> dropdown at the top of the page and select <i>Manage Apps</i> to go to the Apps page. 
</p><p><b>2.</b> On the Apps page, find the app that you want to grant data model creation permissions for and click <b>Permissions</b>.
</p><p><b>3.</b> On the Permissions page for the app, select <b>Write</b> for the roles that should be able to create data models for the app.
</p><p><b>4.</b> Click <b>Save</b> to save your changes.
</p><p><b>Note:</b> Giving roles the ability to create data models can have other implications. See <a href="#disableordeleteknowledgeobjects" class="external text">"Disable or delete knowledge objects"</a> in this manual for more information.
</p>
<h3> <a name="managedatamodels_about_data_model_permissions"><span class="mw-headline" id="About_data_model_permissions">About data model permissions</span></a></h3>
<p>Data models are knowledge objects, and as such the ability to view and edit them is governed by role-based permissions. When you first create a data model it is private to you, which means that no other user can view it on the Select a Data Model page or Data Models management page or update it in any way. 
</p><p>To edit the permissions for a data model, go to the Data Models management page, locate the data model and either:
</p>
<ul><li> Click <b>Edit</b> and select <i>Edit Permissions</i>.
</li><li> Expand the row for the data model in question and click <b>Edit</b> for permissions.
</li></ul><p>This brings up the <b>Edit Permissions</b> dialog, which you can use to share private data models with others, and to determine the access levels that various roles have to the data models. 
</p><p>For more information about setting permissions for data models see <a href="#manageknowledgeobjectpermissions" class="external text">"Manage knowledge object permissions"</a> in this manual. By default any role can create a data model, but any data models those roles create will be private until a user with an <i>admin</i> or <i>power</i> role shares them. Only users with an <i>admin</i> or <i>power</i> role can create <i>and</i> share a data model.
</p><p><b>Important:</b> When you share a data model the knowledge objects associated with that data model (such as lookups or field extractions) must have the same permissions. Otherwise you run the risk of running into errors when other people try to use the data model. 
</p><p>For example, if your data model is shared to all users of the Search app but uses a lookup table that is only shared with users that have the Admin role, everything will work fine for Admin role users, but all other users will get errors that say things like "the lookup table does not exist" when they try to use the data model in Pivot. The solution is either to restrict the data model to Admin users or to share the lookup to all users of the Search app.
</p><p>You'll also run into problems if your data model is private and the related lookup tables and lookup definitions are private, and then you decide to accelerate the data model. To accelerate a data model you must share it. If you do not share the related lookup tables and lookup definition in exactly the same way, your users will see "the lookup table does not exist" messages.
</p>
<h3> <a name="managedatamodels_enable_data_model_acceleration"><span class="mw-headline" id="Enable_data_model_acceleration">Enable data model acceleration</span></a></h3>
<p>Data model acceleration enables you to speed up the dataset represented by a data model for reporting purposes. After a data model is accelerated, pivots, reports, and dashboard panels that use that data model should return results faster than they did before. 
</p><p>Data model acceleration is powered by the <b>high performance analytics store</b>. With the power of the high performance analytics store, data model acceleration builds a data summary for a data model at the index level (this summary can in fact be made up of several smaller summaries, distributed across your indexers). After the summary is completely built, pivots that use accelerated data model objects will run against the summary rather than the full array of <code><font size="2">_raw</font></code> data when possible. This can speed up pivot result return time by a significant amount. 
</p><p>While data model acceleration is useful for speeding up extremely large datasets, it comes with a few important caveats:
</p>
<ul><li> <b>By default, only users with <i>admin</i> permissions can accelerate data models.</b> Data model acceleration can be resource-intensive, so it should be used conservatively by a limited number of Splunk Enterprise users. The ability to accelerate a data model is tied to the <code><font size="2">accelerate_datamodel</font></code> <b>capability</b>. 
</li><li> <b>Data models that are private cannot be accelerated.</b> You must share a data model with the users of an app to make it eligible for acceleration. When you do this, you need to share related knowledge objects (such as lookup tables and lookup definitions that your lookup attributes are dependent upon) as well, in exactly the same way. See "About data model permissions," above, for more information.
</li><li> <b>Once a data model is accelerated, it can no longer be edited.</b> You can't change an accelerated data model in any way until its acceleration is disabled. Reaccelerating the data model can also be resource-intensive so it's best to avoid disabling acceleration if you can.
</li><li> <b>Data model acceleration can only be applied to root event objects and their child objects.</b> Splunk Enterprise cannot accelerate object hierarchies based on root search and root transaction objects. Pivots that use those unaccelerated objects fall back to <code><font size="2">_raw</font></code> data. 
</li><li> <b>Data model acceleration is most efficient if the root event object being accelerated includes in its initial constraint search the index(es) that Splunk Enterprise should search over</b>. Otherwise Splunk Enterprise will search over all available indexes for the data model, which could lead it to waste time accelerating unnecessary data. 
</li></ul><p>For details about data model acceleration, including an explanation of what's happening behind the scenes and a discussion of ad hoc data model acceleration, see <a href="#acceleratedatamodels" class="external text">"Accelerate data models,"</a> in this manual. 
</p>
<h4><font size="3"><b><i> <a name="managedatamodels_to_enable_data_model_acceleration"><span class="mw-headline" id="To_enable_data_model_acceleration">To enable data model acceleration</span></a></i></b></font></h4>
<p>If your permissions are sufficient to accelerate a data model, follow these steps:
</p><p><b>1.</b> Navigate to the Data Models management page.
</p><p><b>2.</b> Find the data model you want to accelerate and either click <b>Edit</b>  and select <i>Edit Acceleration</i> OR expand the data model's row and click <b>Add</b> for <b>ACCELERATION</b>.
</p><p><b>3.</b> The <b>Edit Acceleration</b> dialog appears. Select the <b>Accelerate</b> checkbox to enable acceleration for the data model.
</p><p><img alt="6.0 dm edit acceleration dialog.png" src="images/a/a7/6.0_dm_edit_acceleration_dialog.png" width="456" height="292"></p><p><b>4.</b> The <b>Summary Range</b> field appears. Select from <i>1 Day</i>, <i>7 Days</i>, <i>1 Month</i>, <i>3 Months</i>, <i>1 Year</i>, or <i>All Time</i> depending on the range of time over which you plan to run pivots that use the accelerated objects within the data model. For example, if you only plan to run pivots over periods of time <i>within</i> the last seven days, choose <i>7 Days.</i> 
</p><p><b>Note:</b> If you require a different summary range than the ones supplied by the <b>Summary Range</b> field, you can configure it for your data model in <code><font size="2">datamodels.conf</font></code>.
</p><p><b>5.</b> Click <b>Save</b> to save your acceleration settings. 
Once your data model is accelerated, the "lightning bolt" symbol for the model on the Data Models management page will be lit up with a yellow color. 
</p><p><img alt="6.0 dm acceleration lightning bolt.png" src="images/2/23/6.0_dm_acceleration_lightning_bolt.png" width="90" height="100"></p>
<h4><font size="3"><b><i> <a name="managedatamodels_inspect_data_model_acceleration_metrics"><span class="mw-headline" id="Inspect_data_model_acceleration_metrics">Inspect data model acceleration metrics</span></a></i></b></font></h4>
<p>After a data model is accelerated, you can find detail information about the model's acceleration on the Data Models management page. Just expand the row for the accelerated data model and review the information that appears under <b>ACCELERATION</b>. 
</p><p><img alt="6.0 dm acceleration metrics.png" src="images/b/bf/6.0_dm_acceleration_metrics.png" width="311" height="188"></p>
<ul><li> <b>Status</b> tells you whether the acceleration summary for the data model is complete. If it is in <i>Building</i> status it will tell you what percentage of the summary is complete. Keep in mind that many data model summaries are constantly updating with new data; just because a summary is "complete" now doesn't mean it won't be "building" later. 
</li><li> <b>Access Count</b> tells you how many times the data model summary has been accessed since it was created, and when the last access time was. This can be useful if you're trying to determine which data models are not being used frequently. Because data model acceleration uses system resources you may not want to accelerate data models that aren't accessed on a regular basis.
</li><li> <b>Size on Disk</b> hows you how much space the data model's acceleration summary takes up in terms of storage. You can use this metric along with the <b>Access Count</b> to determine which summaries are an unnecessary load on your system and ought to be deleted. If the acceleration summary for your data model is taking up a large amount of space on disk, you might also consider reducing its summary range. 
</li><li> <b>Summary Range</b> presents the range of the data model, in seconds, always relative to the present moment. You set this range up when you define acceleration for the data model.
</li><li> <b>Buckets</b> displays the number of index <b>buckets</b> spanned by the data model acceleration summary. 
</li></ul><p>Click <b>Rebuild</b> to make Splunk Enterprise rebuild the summary from scratch. You may want to do this in situations where you suspect there has been data loss due to a system crash or similar mishap. Splunk Enterprise automatically rebuilds summaries when you disable and then reenable acceleration for a summary (to edit the data model, for example). 
</p><p>Click <b>Update</b> to refresh the acceleration summary detail information.
</p><p>Click <b>Edit</b> to open the <b>Edit Acceleration</b> dialog and change the <b>Summary Range</b> or disable acceleration for the data model altogether.
</p>
<h3> <a name="managedatamodels_clone_a_data_model"><span class="mw-headline" id="Clone_a_data_model">Clone a data model</span></a></h3>
<p>Data model cloning is a way to quickly create a data model that is based on an existing data model. You can then edit it so it focuses on a different overall dataset or has a different object structure that divides up the dataset in a different way than the original. To clone a data model go to the Data Model management page,  click <b>Edit</b> for the data model that you want to clone, and select <i>Clone</i>. Splunk Enterprise will create a new data model that is identical to the original. You will have to give the cloned data model a unique name.
</p><p><b>Note:</b> You can also clone a data model from the Data Model Editor. Simply click <b>Edit</b> and select <i>Clone.</i>
</p><p>You can edit the cloned data model with the Data Model management page (as described in this topic) and the Data Model Editor (as described in <a href="#designdatamodelobjects" class="external text">"Design data models and objects,"</a> in this manual).
</p>
<h3> <a name="managedatamodels_upload_and_download_data_models"><span class="mw-headline" id="Upload_and_download_data_models">Upload and download data models</span></a></h3>
<p>Splunk's data model download/upload functionality lets you use the Splunk Web interface to export a data model out of Splunk and then upload that data model into another Splunk implementation. You can use this feature to back up important data models, or to collaborate on data models with other Splunk users by emailing them to those users. You might also use it to move data models between staging and production instances of Splunk.
</p><p><b>Note:</b> You can manually move data model JSON files between Splunk implementations, but this is an unsupported procedure with many opportunities for error. For more information, see "Manual data model management" at the bottom of this topic.
</p>
<h4><font size="3"><b><i> <a name="managedatamodels_download_a_data_model"><span class="mw-headline" id="Download_a_data_model">Download a data model</span></a></i></b></font></h4>
<p>Download a data model from the Data Model Editor. You can only download one data model at a time.
</p><p>To download a data model, open the data model in the Data Model Editor and click the <b>Download</b> button at the top right. Splunk will download the JSON file for the data model to your designated download directory. If you haven't designated this directory, you may see a dialog that asks you to identify the directory you want to save the file to. 
</p><p><img alt="Cupk dm download button.png" src="images/5/52/Cupk_dm_download_button.png" width="363" height="73"></p><p>The name of the downloaded JSON file will be the same as the data model's <b>ID</b>. You provide the <b>ID</b> only once, when you first create the data model. Unlike the data model <b>Title</b>, once the <b>ID</b> is saved with the creation of the model, you can't change it. 
</p><p>You can see the <b>ID</b> for an existing data model when you view the model in the Data Model Editor. The <b>ID</b> appears near the top left corner of the Editor, under the model's title. 
</p><p>When you upload the data model you will have an opportunity to give it a new <b>ID</b> that is different from the <b>ID</b> of the original data model.
</p>
<h4><font size="3"><b><i> <a name="managedatamodels_upload_a_data_model"><span class="mw-headline" id="Upload_a_data_model">Upload a data model</span></a></i></b></font></h4>
<p>Upload a data model from the Data Models management page. You can only upload one data model at a time. 
</p><p><b>Note:</b> Splunk validates any file that you try to upload to your Splunk implementation. It cannot upload files that contain anything other than valid JSON data model code. 
</p><p>To upload a data model to your Splunk implementation, click <b>Upload Data Model</b> (near the top right corner of the page) to open the <b>Upload New Data Model</b> dialog box. Start by identifying the JSON <b>File</b> that you want to upload. 
</p><p>The <b>ID</b> field will populate with the original ID of the data model. You can change this ID if you wish. Keep in mind that once you save the data model file to your system you will not be able to change this ID (but you can edit the data model title). 
</p><p>Provide the name of the <b>App</b> that the data model belongs to and identify whether the data model is <i>Private</i> or <i>Shared in App</i> (meaning that it is shared with all other users of the app). 
</p><p>For more information about data model permissions, see <a href="#managedatamodels_about_data_model_permissions" class="external text">"About data model permissions,"</a> above.
</p><p>If you select <i>Shared in App</i> you can also enable acceleration for the data model by selecting <b>Accelerate</b> and choosing a <b>Summary Range.</b>
</p><p>For more information about enabling data model acceleration, see <a href="#managedatamodels_enable_data_model_acceleration" class="external text">"Enable data model acceleration,"</a> above.
</p>
<h3> <a name="managedatamodels_delete_a_data_model"><span class="mw-headline" id="Delete_a_data_model">Delete a data model</span></a></h3>
<p>You can delete a data model from the Data Model management page or the Data Model Editor. Just click <b>Edit</b> and select <i>Delete</i>. 
</p><p><b>Note:</b> If your role grants you the ability to create data models, it should grant you the ability to delete them as well. For more information about this see <a href="#managedatamodels_enable_roles_to_create_data_models" class="external text">"Enable roles to create data models,"</a> above. 
</p>
<h3> <a name="managedatamodels_manual_data_model_management"><span class="mw-headline" id="Manual_data_model_management">Manual data model management</span></a></h3>
<p>Splunk does not recommend that you manage data models manually by hand-moving their files or hand-coding data model files. You should create and edit data models in Splunk Web whenever possible. When you edit models in Splunk Web the Data Model Editor validates your changes; this won't happen for models created or edited by hand. 
</p><p>Data models are stored on disk as JSON files, and they have associated configs in <code><font size="2">datamodels.conf</font></code> and metadata in <code><font size="2">local.meta</font></code> (for models that you create) and <code><font size="2">default.meta</font></code> (for models delivered with the product).
</p><p>Models that you create are stored in <code><font size="2">&lt;yourapp&gt;/local/data/models</font></code> while models delivered with the product can be found in <code><font size="2">&lt;yourapp&gt;/default/data/models</font></code>.
</p><p>You can move model files between Splunk implementations manually but it's far easier to use the Data Model Download/Upload feature in Splunk Web (described above). If you absolutely must move model files manually, take care to move their <code><font size="2">datamodels.conf</font></code> stanzas and <code><font size="2">local.meta</font></code> metadata when you do so. 
</p><p>The same goes for deleting data models; in general it's best to do it via Splunk Web so all the appropriate cleanup is carried out.
</p>
<a name="designdatamodelobjects"></a><h2> <a name="designdatamodelobjects_design_data_models_and_objects"><span class="mw-headline" id="Design_data_models_and_objects"> Design data models and objects</span></a></h2>
<p>In Splunk Web, you use the Data Model Editor to design new <b>data models</b> and edit existing models. This topic shows you how to use the Data Model Editor to create and edit:
</p>
<ul><li> Build out <b>data model object</b> hierarchies by adding root objects and child objects to data models.
</li><li> Define object datasets (by providing <b>constraints</b>, search strings, or transaction definitions).
</li><li> Rename objects.
</li><li> Delete objects.
</li></ul><p>You can also use the Data Model Editor to create and edit object <b>attributes</b>. For more information, see <a href="#definedatamodelattributes" class="external text">"Define object attributes."</a>
</p><p><b>Note:</b> This topic will not spend much time explaining basic data model concepts. If you have not worked with data models in Splunk Enterprise before you may find it helpful to review the topic <a href="#aboutdatamodels" class="external text">"About data models,"</a> in this manual. It provides a lot of background detail around what data models and data model objects actually are and how they work. 
</p><p><b>For information about creating and managing new data models</b>, see <a href="#managedatamodels" class="external text">"Manage data models,"</a> in this manual. Aside from creating new data models via the Data Models management page, this topic will also show you how to manage data model permissions and acceleration.
</p>
<h3> <a name="designdatamodelobjects_the_data_model_editor"><span class="mw-headline" id="The_Data_Model_Editor">The Data Model Editor</span></a></h3>
<p>Data models are collections of data model objects arranged in hierarchical structures. To design a new data model or redesign an existing data model, you go to the Data Model Editor. In the Data Model Editor, you can create objects for your data model, define their constraints and <b>attributes</b>, arrange them in logical object hierarchies, and maintain them. 
</p><p><img alt="Cupk data model builder.png" src="images/6/69/Cupk_data_model_builder.png" width="720" height="505"></p>
<h4><font size="3"><b><i> <a name="designdatamodelobjects_manage_objects_in_an_existing_data_model"><span class="mw-headline" id="Manage_objects_in_an_existing_data_model">Manage objects in an existing data model</span></a></i></b></font></h4>
<p>There are five ways to access the Data Model Editor for an existing data model:
</p>
<ul><li> In Pivot, on the Select a Data Model page, expand the row for a data model to reveal detail information for that model. Click <b>Edit Objects</b> to access the Data Model Editor for that data model.
</li><li> In Pivot, select a data model. On the Select an Object page for that data model, click <b>Edit Objects.</b>
</li><li> In Pivot, select a data model and then select an object from that data model. On the New Pivot page, click <b>Data Source</b> and select <b>Edit Object.</b>
</li><li> Navigate to <b>System &gt; Data Models,</b> click the <b>Edit</b> dropdown for the data model you want to edit, and select <b>Edit Objects.</b>
<ul><li> Alternatively, navigate to <b>System &gt; Data Models</b>, expand the row for the data model you want to edit, and select <b>Edit</b> for the listed objects. 
</li></ul></li></ul><p><b>Note:</b> You can only edit a specific data model if your permissions enable you to do so.
</p>
<h3> <a name="designdatamodelobjects_add_a_root_event_object_to_a_data_model"><span class="mw-headline" id="Add_a_root_event_object_to_a_data_model">Add a root event object to a data model</span></a></h3>
<p>Data models are composed chiefly of object hierarchies built on root event objects. Each root event object represents a set of data that is defined by a <b>constraint</b>: a simple search that filters out events that aren't relevant to the object. Constraints look like the first part of a search, before pipe characters and additional search commands are added. 
</p><p>Constraints for root event objects are usually designed to return a fairly wide range of data. A large dataset gives you something to work with when you associate child event objects with the root event object, as each child event object adds an additional constraint to the ones it inherits from its ancestor objects, narrowing down the dataset that it represents. 
</p><p><b>Note:</b> For more information on how constraints work to narrow down datasets in an object hierarchy, see <a href="#aboutdatamodels_object_constraints" class="external text">the section on object constraints</a> in the "About data models" topic, in this manual.
</p><p>To add an root event object to your data model, click <b>Add Object</b> in the Data Model Editor and select <i>Root Event</i>. This takes you to the Add Event Object page.
</p><p><img alt="Bubb dm addeventobject.png" src="images/8/88/Bubb_dm_addeventobject.png" width="720" height="413"></p><p>Give the root event object an <b>Object Name</b>, <b>Object ID,</b> and one or more <b>Constraints.</b> 
</p><p>The <b>Object Name</b> field can accept any character except asterisks. It can also accept blank spaces between characters. It's what you'll see on the Choose an Object page and other places where data model objects are listed.
</p><p>The <b>Object ID</b> must be a unique identifier for the object. It cannot contain spaces or any characters that aren't alphanumeric, underscores, or hyphens (<i>a-z</i>, <i>A-Z</i>, <i>0-9</i>, <i>_</i>, or <i>-</i>). Spaces between characters are also not allowed. Once you save the <b>Object ID</b> value you can't edit it.
</p><p>After you provide <b>Constraints</b> for the root event object you can click <i>Preview</i> to test whether the constraints you've supplied return the kinds of events you want.
</p>
<h3> <a name="designdatamodelobjects_add_a_root_transaction_object_to_a_data_model"><span class="mw-headline" id="Add_a_root_transaction_object_to_a_data_model">Add a root transaction object to a data model</span></a></h3>
<p>Root transaction objects enable you to create object hierarchies that are based on a dataset made up of <b>transaction</b> events. A transaction event is actually a collection of conceptually-related events that spans time, such as all website access events related to a single customer hotel reservation session, or all events related to a firewall intrusion incident. When you define a root transaction object, you define the transaction that pulls out a set of transaction events. 
</p><p>Read up on transactions and the <code><font size="2">transaction</font></code> command if you're unfamiliar with how they work. Get started at <b>About transactions,</b> in the Search Manual. Get detail information on the <code><font size="2">transaction</font></code> command at its entry in the Search Reference. 
</p><p><b>Note:</b> Root transaction objects and their children do not benefit from data model acceleration.
</p><p>To add a root transaction object to your data model, click <b>Add Object</b> in the Data Model Editor and select <i>Root Transaction</i>. This takes you to the Add Transaction Object page.
</p><p><img alt="Bubbles dm addtransobj.png" src="images/5/51/Bubbles_dm_addtransobj.png" width="720" height="603"></p><p>Root transaction object definitions require an <b>Object Name</b> and <b>Object ID</b> and at least one <b>Group Object</b>. The <b>Group by</b>, <b>Max Pause</b>, and <b>Max Span</b> fields are optional, but the transaction definition is incomplete until at least one of those three fields is defined.
</p><p>The <b>Object Name</b> field can accept any character except asterisks. It can also accept blank spaces between characters. It's what you'll see on the Choose an Object page and other places where data model objects are listed.
</p><p>The <b>Object ID</b> must be a unique identifier for the object. It cannot contain spaces or any characters that aren't alphanumeric, underscores, or hyphens (<i>a-z</i>, <i>A-Z</i>, <i>0-9</i>, <i>_</i>, or <i>-</i>). Spaces between characters are also not allowed. Once you save the <b>Object ID</b> value you can't edit it. 
</p><p>All root transaction object definitions require one or more <b>Group Object</b> names to define the pool of data from which the transaction object will derive its transactions. There are restrictions on what you can add under <b>Group Object</b>, however. <b>Group Object</b> can contain one of the following three options:
</p>
<ul><li> One or more event objects (either root event objects or child event objects)
</li><li> One transaction object (either root or child)
</li><li> One search object (either root or child)
</li></ul><p>In addition, you are restricted to objects that exist within the currently selected data model. 
</p><p>If you're familiar with how the <code><font size="2">transaction</font></code> command works, you can think of the <b>Group Objects</b> as the way we provide the portion of the search string that appears <i>before</i> the transaction command. Take the example presented in the preceding screenshot, where we've added the <i>Apache Access Search</i> object to the definition of the root transaction object Web Session. <i>Apache Access Search</i> represents a set of successful webserver access events--its two constraints are <code><font size="2">status &lt; 600</font></code> and <code><font size="2">sourcetype = access_* OR source = *.log</font></code>. So the start of the transaction search that this root transaction object represents would be:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">status&lt;600 sourcetype=access_* OR source=*.log | transaction... </font></code><br></div>
<p>Now we only have to define the rest of the <code><font size="2">transaction</font></code> argument.
</p><p><b>Group by</b> is where you can list fields for the root transaction object. If you choose two or more fields you're saying that the transaction will find events that share the same values for the chosen fields. The <b>Group by</b> dropdown does not appear until you identify at least one <b>Group Object,</b> and it only allows you to choose attributes that belong to the group object or objects that you have listed. When you have multiple objects selected, <b>Group by</b> only allows you to select from the set of attributes that are shared by all of the selected objects. 
</p><p><b>Note:</b> While the <b>Group by</b> fields are attributes that belong to the <b>Group Object(s)</b> selected for the root transaction object, selecting them does <i>not</i> add them to the root transaction object as attributes. They are used to define the transaction.
</p><p>You can change the list order of <b>Group by</b> fields. In edge cases where you have events within a transaction with the same timestamp, field reordering can affect how Splunk Enterprise sorts those events. 
</p><p>The duration-based options, <b>Max Pause</b> and <b>Max Span,</b> have the same function as the <code><font size="2">maxpause</font></code> and <code><font size="2">maxspan</font></code> arguments in the <code><font size="2">transaction</font></code> search command. Both of these settings help you to ensure that the transactions found by your transaction object do not contain erroneous events. 
</p><p>The <b>Max Pause</b> field enables you to set a limit to the amount of time that can separate the events that make up a single transaction. For example, you may know that a certain transaction shouldn't contain any events with timestamps that are more than 20 seconds apart--if a matching event does come in  more than 20 seconds after the last one, it might signal the start of a new transaction event, or it could just be an outlying event. To ensure that your transaction object returns transactions that follow this rule, set the value of <b>Max Pause</b> to <i>20 seconds</i>.
</p><p>The <b>Max Span</b> field enables you to control the overall span of time that a particular transaction encompasses. For example, if you know that the transactions you're looking for never span more than 5 minutes, you could set <i>5 minutes</i> as the <b>Max Span</b> value for the corresponding transaction object. 
</p><p>So to return to our example in the screenshot, if we were to represent transaction object as a search string, it would look like this:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">status&lt;600 sourcetype=access_* OR source=*.log | transaction clientip useragent maxspan=1m</font></code><br></div>
<p>This looks through the data returned by the <code><font size="2">status&lt;600 sourcetype=access_* OR source=*.log</font></code> and seeks out "web session" transactions: transactional groupings of events with identical values for <code><font size="2">clientip</font></code> and <code><font size="2">useragent</font></code> where the duration of the transaction as a whole does not exceed one minute.
</p><p>After you define your root transaction object's <b>Group by</b> you can click <i>Preview</i> to test whether the constraints you've supplied return the kinds of events you want.
</p>
<h3> <a name="designdatamodelobjects_add_a_root_search_object_to_a_data_model"><span class="mw-headline" id="Add_a_root_search_object_to_a_data_model">Add a root search object to a data model</span></a></h3>
<p>Root search objects enable you to create object hierarchies where the base dataset is the result of an arbitrary <b>transforming search</b>--a search that uses statistical commands to define a base dataset where one or more fields aggregate over the entire dataset. 
</p><p>The results returned by root search objects and their children are essentially table rows (unless the search is <i>not</i> a transforming search, in which case events are returned, just as they are for root event objects and their children). 
</p><p><b>Note:</b> We suggest that whenever possible, you avoid using root search objects to create an object hierarchy, because root search objects and their children <i><b>do not benefit from data model acceleration.</b></i> There are many kinds of searches that can be set up as root event objects. For more information, see "When not to create root search objects," below.
</p><p>To add a root search object to your data model, click <b>Add Object</b> in the Data Model Editor and select <i>Root Search</i>. This takes you to the Add Search Object page.
</p><p><img alt="Cupk base search obj.png" src="images/d/d4/Cupk_base_search_obj.png" width="720" height="360"></p><p>Give the root search object an <b>Object Name</b>, <b>Object ID,</b> and search string. To preview the results of the search in the section at the bottom of the page, click the magnifying glass icon to run the search, or just hit return on your keyboard while your cursor is in the search bar.
</p><p>The <b>Object Name</b> field can accept any character except asterisks. It can also accept blank spaces between characters. It's what you'll see on the Choose an Object page and other places where data model objects are listed.
</p><p>The <b>Object ID</b> must be a unique identifier for the object. It cannot contain spaces or any characters that aren't alphanumeric, underscores, or hyphens (<i>a-z</i>, <i>A-Z</i>, <i>0-9</i>, <i>_</i>, or <i>-</i>). Spaces between characters are also not allowed. Once you save the <b>Object ID,</b> value you can't edit it. 
</p><p>For more information about designing search strings, see the Search Manual. 
</p>
<h4><font size="3"><b><i> <a name="designdatamodelobjects_when_not_to_create_root_search_objects"><span class="mw-headline" id="When_not_to_create_root_search_objects">When not to create root search objects</span></a></i></b></font></h4>
<p>As we stated above, you should avoid creating root search objects if you can design an root event object that will do the job. Root event objects have a major advantage over root search objects: Event object hierarchies can be accelerated, while search object hierarchies cannot. If you accelerate your data model and then select an object from that model to run a Pivot operation with (such as the generation of a chart visualization), the operation will complete much faster if you can use an object from an event object hierarchy than it would if you used an object from a search object hierarchy.
</p><p><b>Don't create a root search object for your search if:</b>
</p>
<ul><li> The search only involves the <code><font size="2">eval</font></code>, <code><font size="2">rex</font></code>, or <code><font size="2">lookup</font></code> commands (or some combination of those three commands). You can set this sort of search up as an root event search with the help of the <i>Eval Expression,</i> <i>Regular Expression,</i> and <i>Lookup</i> attributes. 
</li></ul><dl><dd> For example, say you have this search:
</dd><dd> <div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* | eval userid=clientip+useragent</font></code><br></div>
</dd><dd> You can utilize this search in a root event object by first defining its constraint as <code><font size="2">sourcetype=access_*</font></code>. Save the object, and then, in the Data Model Editor for that object, add an <i>Eval Expression</i> attribute to the object using <code><font size="2">userid=clientip+useragent</font></code> as the attribute definition. 
</dd></dl><ul><li> You would like to get the benefits of data model acceleration but you want to use commands that can be configured in <code><font size="2">.conf</font></code> files, so they run automatically as events are indexed. Many commands that extract fields can be configured in this way, meaning that the fields they extract will be available as auto-extracted attributes when you define the data model.
<ul><li> Many commands can be set up in <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> to aid in field extraction. 
</li><li> Additionally, if you typically use the <code><font size="2">multikv</font></code> command to extract fields from <code><font size="2">.csv</font></code> files and the like, you can configure <code><font size="2">multikv.conf</font></code> to extract fields as the <code><font size="2">.csv</font></code> file is indexed. 
</li><li> If you <i>do not need to accelerate</i> the underlying root-level object, you can create a root search object that uses these commands in its defining search.
</li></ul></li><li> The search is a simple <code><font size="2">transaction</font></code> search. Set it up as a root transaction object.
</li></ul><p><b>You should create root search objects for any searches that do not map directly to Splunk events.</b> In other words, searches that involve input or output that is not in the format of an event. This includes searches that:
</p>
<ul><li> Make use of <b>transforming commands</b> such as <code><font size="2">stats</font></code>, <code><font size="2">chart</font></code>, and <code><font size="2">timechart</font></code>. Transforming commands organize the data they return into tables rather than event lists.
</li><li> Use other commands that do not return events.
</li><li> Pull in data from external non-Splunk sources using a command other than <code><font size="2">lookup</font></code>. This data cannot be guaranteed to have default fields like <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, <code><font size="2">sourcetype</font></code>, or <code><font size="2">_time</font></code> and therefore might not be event-mappable. An example would be using the <code><font size="2">inputcsv</font></code> command to get information from an external .csv file.
</li></ul><h3> <a name="designdatamodelobjects_add_a_child_object_to_a_data_model"><span class="mw-headline" id="Add_a_child_object_to_a_data_model">Add a child object to a data model</span></a></h3>
<p>You can add child objects to root objects and other child objects. A child object inherits all of the constraints and attributes that belong to its parent object. A single object can be associated with multiple child objects
</p><p>When you define a new child object, you give it one or more additional constraints, to further focus the dataset that the object represents. For example, if your Web Intelligence data model has a root event object called HTTP Request that captures all webserver access events, you could give it three child event objects: HTTP Success, HTTP Error, and HTTP Redirect. Each child event object focuses on a specific subset of the HTTP Request dataset:
</p>
<ul><li> The child event object <i>HTTP Success</i> uses the additional constraint <code><font size="2">status = 2*</font></code> to focus on successful webserver access events.
</li><li>  <i>HTTP Error</i> uses the additional constraint <code><font size="2">status = 4*</font></code> to focus on failed webserver access events.  
</li><li> <i>HTTP Redirect</i> uses the additional constraint <code><font size="2">status = 3*</font></code> to focus on redirected webserver access events.
</li></ul><p>The addition of attributes beyond those that are inherited from the parent object is optional. For more information about attribute definition, see <a href="#designdatamodelobjects_manage_object_attributes_with_the_edit_objects_page" class="external text">"Manage Object attributes with the Data Model Editor,"</a> below.
</p><p>To add a child object to your data model, select the parent object in the left-hand object hierarchy, click <b>Add Object</b> in the Data Model Editor, and select <i>Child</i>. This takes you to the Add Child Object page.
</p><p><img alt="Cupk add child obj.png" src="images/8/84/Cupk_add_child_obj.png" width="720" height="485"></p><p>Give the child object an <b>Object Name</b> and <b>Object ID.</b> 
</p><p>The <b>Object Name</b> field can accept any character except asterisks. It can also accept blank spaces between characters. It's what you'll see on the Choose an Object page and other places where data model objects are listed.
</p><p>The <b>Object ID</b> must be a unique identifier for the object. It cannot contain spaces or any characters that aren't alphanumeric, underscores, or hyphens (<i>a-z</i>, <i>A-Z</i>, <i>0-9</i>, <i>_</i>, or <i>-</i>). Spaces between characters are also not allowed. Once you save the <b>Object ID</b> value you can't edit it. 
</p><p>After you define a <b>Constraint</b> for the child object you can click <i>Preview</i> to test whether the constraints you've supplied return the kinds of events you want.
</p>
<h3> <a name="designdatamodelobjects_some_best_practices_for_data_model_design"><span class="mw-headline" id="Some_best_practices_for_data_model_design">Some best practices for data model design</span></a></h3>
<p>It can take some trial and error to determine the data model designs that work for you. Here are some tips that can get you off to a running start.
</p>
<ul><li> <b>Use root event objects whenever possible</b> to take advantage of the benefits of data model acceleration (and to benefit from their ease of optimization). 
</li><li> <b>Minimize object hierarchy depth whenever possible.</b> Constraint-based filtering is less efficient deeper down the tree.
</li><li> <b>Put the root event object with the deepest tree (and most matching results) first.</b> Data model acceleration applies only to the first root event object and its children.
</li><li> <b>When defining constraints for a root event object that will be accelerated, when possible include the index or indexes it is selecting from.</b> Data model acceleration efficiency is improved when the data model isn't searching across all of your indexes (this is what it does by default when indexes are not specified). 
</li><li> <b>Use attribute flags to selectively expose small groups of attributes for each object.</b> You can expose and hide different attributes for different objects. A child attribute can expose an entirely different set of attributes than those exposed by its parent. Your Pivot users will benefit from this selection by not having to deal with a bewildering array of attributes whenever they set out to make a pivot chart or table. Instead they'll see only those attributes that make sense in the context of the object they've chosen. 
</li><li> <b>Reverse-engineer your existing dashboards and searches into data models.</b> This can be a way to quickly get started with data models. Dashboards built with pivot-derived panels are easier to maintain. 
</li><li> <b>When designing a new data model, first try to understand what your Pivot users hope to be able to do with it.</b> Work backwards from there. The structure of your model should be determined by your users' needs and expectations.
</li></ul><a name="definedatamodelattributes"></a><h2> <a name="definedatamodelattributes_define_object_attributes"><span class="mw-headline" id="Define_object_attributes"> Define object attributes</span></a></h2>
<p>In this topic we talk about adding and editing <b>attributes</b> of <b>data model objects</b>. Object attributes are fields that are associated with the dataset that an object represents. They provide the set of fields that your Pivot users work with when they define and generate pivot reports. 
</p><p>Attributes can be present within the object dataset, or they can be derived and added to the dataset through the use of lookups and eval expressions. 
</p><p>You use the Data Model Editor to create and manage object attributes. It enables you to: 
</p>
<ul><li> Create new attributes.
</li><li> Update or delete existing attributes that aren't inherited.
</li><li> Override certain settings for inherited attributes.
</li></ul><p><b>Note:</b> This topic will not cover the concepts behind object attributes in detail. If you have not worked with data model attributes up to this point, you should review the topic <a href="#aboutdatamodels" class="external text">"About data models,"</a> in this manual.
</p><p>You can also use the Data Model Editor to build out data model object hierarchies, define object datasets (by providing constraints, search strings, or transaction definitions), rename objects, and delete objects. For more information about using the Data Model Editor to perform these tasks, see <a href="#designdatamodelobjects" class="external text">"Design data models and objects."</a>
</p><p><b>For information about creating and managing new data models</b>, see <a href="#managedatamodels" class="external text">"Manage data models,"</a> in this manual. Aside from creating new data models via the Data Models management page, this topic also shows you how to manage data model permissions and acceleration.
</p>
<h3> <a name="definedatamodelattributes_overview_of_object_attributes"><span class="mw-headline" id="Overview_of_object_attributes">Overview of object attributes</span></a></h3>
<p>Object attributes provide the set of fields that Pivot users work with to define and generate a pivot report. 
</p><p><img alt="6.1 add attribute list opts.png" src="images/6/63/6.1_add_attribute_list_opts.png" width="189" height="196"></p><p>You can define five different types of attributes for the objects in your data model: 
</p>
<ul><li> <b>Auto-extracted:</b> These represent fields that Splunk Enterprise extracts at index time and search time. You can only add auto-extracted attributes to root objects. Child objects can only inherit them, and they cannot add new auto-extracted attributes of their own. Auto-extracted attributes can include:
<ul><li> Fields that Splunk Enterprise recognizes and extracts automatically, like <code><font size="2">uri</font></code> or <code><font size="2">version</font></code>. This includes fields indexed through structured data inputs for CSV, IIS, and JSON files. 
</li><li> <b>Field extractions</b>, <b>lookups</b>, or <b>calculated fields</b> that you have defined in Settings or configured in <code><font size="2">props.conf</font></code>. 
</li><li> Fields that you have manually added to the attribute because they aren't currently in the object dataset, but should be in the future. Can include fields that are added to the object dataset by generating commands such as <code><font size="2">inputcsv</font></code> or <code><font size="2">dbinspect</font></code>.
</li></ul></li><li> <b>Eval Expression:</b> A field derived from a <code><font size="2">eval</font></code> expression that you enter in the attribute definition. Eval expressions often involve one or more extracted fields.
</li><li> <b>Lookup:</b> A field that is added to the events in the object dataset with the help of a <b>lookup</b> that you configure in the attribute definition. When you define a lookup attribute you can use any lookup that you have <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">defined in Settings</a> and associate it with any other attribute that has already been associated with that same object. 
</li><li> <b>Regular Expression:</b> A field that is extracted from the object event data using a regular expression that you provide in the attribute definition. 
</li><li> <b>GeoIP:</b> A specific type of lookup that adds geographical fields, such as latitude, longitude, country, and city to events in the object dataset that have valid ip address fields. Useful for map-related visualizations.
</li></ul><p>For a broader overview of object attributes--what they are, how they work, and why you need them--read the subsection on them in <a href="#aboutdatamodels_object_attributes" class="external text">"About data models,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="definedatamodelattributes_attribute_categories"><span class="mw-headline" id="Attribute_categories">Attribute categories</span></a></i></b></font></h4>
<p>The Data Model Editor groups attributes into three categories:
</p>
<ul><li> <b>Inherited</b> - All objects have at least a few inherited attributes. Child objects inherit all of the attributes that belong to their parent object. Root event, search, and transaction objects have a default set of inherited attributes. 
</li><li> <b>Extracted</b> - Any auto-extracted attribute that has been added to an object appears in this category. 
</li><li> <b>Calculated</b> - Any attribute that is derived through a calculation or lookup appears in this category. When you add Eval Expression, Regular Expression, Lookup, and Geo IP attribute types to an object, they appear in this attribute category. 
</li></ul><p><img alt="6.1 dm attributes.png" src="images/6/64/6.1_dm_attributes.png" width="731" height="624"></p>
<h4><font size="3"><b><i> <a name="definedatamodelattributes_attribute_order_and_attribute_chaining"><span class="mw-headline" id="Attribute_order_and_attribute_chaining">Attribute order and attribute chaining</span></a></i></b></font></h4>
<p>The Data Model Editor lets you rearrange the order of calculated attributes. This is useful when you have a set of attributes that must be processed in a specific order, because Splunk Enterprise always processes attributes in descending order from the top of the list to the bottom. 
</p><p>For example, you can design an Eval Expression attribute that uses the values of two auto-extracted attributes. Splunk always places extracted attributes above calculated attributes, so in this case the attributes would be processed in the correct order without any work on your part. But you might also use the eval expression attribute as input for a lookup attribute. Because Eval Expression attributes and Lookup attributes are both categorized as calculated attributes by the Data Model Editor, you would want to make sure that you order the calculated attribute list so that the Eval Expression attribute appears <i>above</i> the Lookup attribute. 
</p><p>So the order of these attributes would be:
</p>
<ul><li> Auto Extracted Attribute 1
</li><li> Auto Extracted Attribute 2
</li><li> Eval Expression Attribute (calculates a field with the values of the two Auto-Extracted attributes)
</li><li> Lookup Attribute (uses the Eval Expression attribute as an input field)
</li></ul><h3> <a name="definedatamodelattributes_marking_attributes_as_hidden_or_required"><span class="mw-headline" id="Marking_attributes_as_hidden_or_required">Marking attributes as hidden or required</span></a></h3>
<p>All object attributes are <i>shown</i> and <i>optional</i> by default. 
</p>
<ul><li> A <b>shown</b> attribute is visible and available to Pivot users when they are in the context of the object to which the attribute belongs. For example, say the <code><font size="2">url</font></code> attribute is marked as shown for the HTTP Requests object. When a user enters Pivot and selects the HTTP Requests object, they can use the <code><font size="2">url</font></code> attribute when they define a pivot report.
</li><li> An <b>optional</b> attribute is <i>not</i> required to be present in every event in the dataset represented by its object. This means that there potentially can be many events in the object dataset that do not contain the attribute.
</li></ul><p>You can change these settings to <i>hidden</i> and <i>required</i>, respectively. When you do this the attribute will be marked as <i>hidden</i> and/or <i>required</i> in the object attribute list.
</p>
<ul><li> A <b>hidden</b> attribute is <i>not</i> displayed to Pivot users when they select the object in a Pivot context. They will be unable to use it for the purpose of Pivot report definition.
<ul><li> This setting lets you expose different subsets of attributes for each object in your data model, even if all of the objects inherit the same set of attributes from a single parent object. This helps to ensure that your Pivot users only engage with attributes that make sense given the context of the dataset represented by the object.
</li><li> You can hide field attributes that are only being added to the object because they're used to define another attribute (see "Attribute order and attribute chaining," above). There may be no need for your Pivot users to engage with the first attributes in an attribute chain.
</li></ul></li><li> A <b>required</b> attribute <i>must</i> appear in every event represented by the object. This filters out any event that does not have the attribute. In effect this is another type of <b>constraint</b> on top of any formal constraints you've associated with the object.
</li></ul><p>These attribute settings are specific to each object in your data model. This means you can have the <code><font size="2">ip_address</font></code> attribute set to <i>Required</i> in a parent object but still set as optional in the child objects that descend from that parent object. Even if all of the objects in a data model have the same attributes (meaning the attributes are set in the topmost root object and then simply inherited to all the other objects in the hierarchy), the attributes that are marked hidden or required can be different from object to object in that data model.
</p><p><b>Note:</b> There is one exception to your ability to provide different "shown/hidden" and "optional/required" settings for the same attribute across different objects in a data model. <b>You cannot update these settings for inherited attributes that are categorized as "Calculated" attributes in the parent object in which they first appear.</b> For this kind of attribute you can only change the setting by updating the attributes in that parent object. Your changes will be replicated through the child objects that descend from that parent object.  
</p><p>You can set these values for extracted and calculated attributes when you first define them. You can also edit attribute names or types after they've been defined.
</p><p><b>1.</b> Click <b>Override</b> for an attribute in the Inherited category or <b>Edit</b> for an attribute in the Extracted and Calculated categories.
</p><p><b>2.</b> Change the value of the <b>Flag</b> field to the appropriate value. 
</p><p><b>3.</b> Click <b>Save</b> to save your changes.
</p><p>With the <b>Bulk Edit</b> list you can change the "shown/hidden" and "optional/required" values for multiple attributes at once.
</p><p><b>1.</b> Select the attributes you want to edit.
</p><p><b>2.</b> Click <b>Bulk Edit</b> and select either <i>Optional</i>, <i>Required</i>, <i>Hidden</i>, or <i>Shown</i>. 
</p>
<dl><dd> If you select either <i>Required</i> or <i>Hidden</i> the appropriate attributes update to display the selected status for the selected attributes. You cannot update these values for inherited attributes that are categorized as calculated attributes in the parent object in which they first appear. See the <b>Note</b> above for more information.
</dd></dl><h3> <a name="definedatamodelattributes_enter_or_update_attribute_names_and_types"><span class="mw-headline" id="Enter_or_update_attribute_names_and_types">Enter or update attribute names and types</span></a></h3>
<p>The Data Model Editor lets you give attributes in the <b>Extracted</b> and <b>Calculated</b> categories a display <b>Name</b> of your choice. It also lets you determine the <b>Type</b> for such attributes, even in cases where Splunk has auto-assigned a <b>Type</b> value to the attribute. 
</p><p>Splunk Enterprise will assign <b>Type</b> values for auto-extracted attributes. If Splunk misidentifies an auto-extracted attribute's <b>Type</b> value, you can provide the correct one. For example, based on available values for an auto-extracted field attribute, Splunk Enterprise may decide it is a <i>Number</i> type attribute when you know that it is in fact a <i>String</i> type. You can change the <b>Type</b> value to <i>String</i> if this is the case.
</p><p>Changing the display <b>Name</b> of an auto-extracted attribute won't change how the associated field is named in the Splunk Enterprise index--it just renames it in the context of this data model. 
</p><p><b>1.</b> Click <b>Edit</b> for the attribute whose <b>Name</b> or <b>Type</b> you would like to update.
</p><p><b>2.</b> Update the <b>Name</b> or change the <b>Type</b>. <b>Name</b> values cannot contain asterisk characters.
</p><p><b>3.</b> Click <b>Save</b> to save your changes.
</p><p>Use the <b>Bulk Edit</b> list to give multiple attributes the same <b>Type</b> value.
</p><p><b>1.</b> Select the attributes you want to edit.
</p><p><b>2.</b> Click <b>Bulk Edit</b> and select either <i>Boolean,</i> <i>IPv4</i>, <i>Number</i>, or <i>String</i>. 
</p>
<dl><dd> All of the selected attributes should have their <b>Type</b> value updated to the value you choose.
</dd></dl><p><b>Note:</b> You cannot change the <b>Type</b> value for inherited attributes. If you select any inherited attributes the <b>Type</b> values in the <b>Bulk Edit</b> list will be unavailable.
</p>
<a name="addanauto-extractedattribute"></a><h2> <a name="addanauto-extractedattribute_add_an_auto-extracted_attribute"><span class="mw-headline" id="Add_an_auto-extracted_attribute">Add an auto-extracted attribute</span></a></h2>
<p>You can add an auto-extracted attribute to any root object in your data model. 
</p><p><img alt="6.1 dm add auto-extracted field.png" src="images/7/7f/6.1_dm_add_auto-extracted_field.png" width="720" height="472"></p><p><b>1.</b> In the Data Model Editor, open the root object you'd like to add an auto-extracted attribute to. 
</p><p><b>2.</b> Click <b>Add Attribute</b> and select <i>Auto-extracted</i> to define an auto-extracted attribute. 
</p>
<dl><dd> The Add Auto-Extracted Field dialog appears. It includes a list of fields that can be added to your data model object as auto-extracted attributes.
</dd></dl><p><b>3.</b> Select the attributes you would like to add to your data model by marking their checkboxes. 
</p>
<dl><dd> You can select the checkbox in the header to select all fields in the list. 
</dd></dl><dl><dd> If you look at the list and don't find the fields you are expecting, try changing the event sample size, which is set to the <i>First 1000 events</i> by default. A larger event sample may contain rare fields that didn't turn up in the first thousand events. For example, you could choose a sample size like the <i>First 10,000 events</i> or the <i>Last 7 days</i>.
</dd></dl><p><b>4.</b> (Optional) <b>Rename</b> the auto-extracted field. 
</p>
<dl><dd> If you use <b>Rename</b>, do not include asterisk characters in the new field name.
</dd></dl><p><b>5.</b> (Optional) Correct the auto-extracted field <b>Type</b>.
</p><p><b>6.</b> (Optional) Update the auto-extracted field's status (<i>Optional</i>, <i>Required</i>, <i>Hidden</i>, or <i>Hidden and Required</i>) as necessary. 
</p><p><b>7.</b> Click <b>Save</b> to add the selected attributes to your root object.
</p><p><b>Note:</b> You cannot add auto-extracted attributes to child objects. Child objects inherit auto-extracted attributes from the root object at the top of their object hierarchy.
</p><p>The list of fields displayed by the Add Auto-Extracted Field dialog includes:
</p>
<ul><li> Fields that Splunk Enterprise recognizes and extracts automatically, like <code><font size="2">uri</font></code> or <code><font size="2">version</font></code>. This includes fields indexed through structured data inputs, such as fields extracted from the headers of indexed CSV files. 
</li><li> <b>Field extractions</b>, <b>lookups</b>, or <b>calculated fields</b> that you have defined in Settings or configured in <code><font size="2">props.conf</font></code>. 
</li></ul><p>Expand a field row for a field to see its top ten sample values. 
</p>
<h3> <a name="addanauto-extractedattribute_manually_add_a_field_to_the_set_of_auto-extracted_fields"><span class="mw-headline" id="Manually_add_a_field_to_the_set_of_auto-extracted_fields">Manually add a field to the set of auto-extracted fields</span></a></h3>
<p>While building a data model you may find that you are missing certain auto-extracted fields. They could be missing for a variety of reasons. For example: 
</p>
<ul><li> You may be building your data model prior to indexing the data that will make up its dataset. 
</li><li> You are indexing data, but certain rare fields that you expect to see eventually haven't been indexed yet. 
</li><li> You are utilizing a generating search command like <code><font size="2">inputcsv</font></code> that adds fields that don't display in this list. 
</li></ul><p>You can manually add auto-extracted attributes to a root object. 
</p><p><b>Note:</b> Before adding fields manually, try increasing the event sample size as described in the procedure above to pull in rare fields that aren't found in the first thousand events.
</p><p><b>1.</b> Click <b>Add by name</b> in the top right-hand corner of the Add Auto-Extracted Field dialog. 
</p>
<dl><dd> This adds a row to the field table. Note that in the example at the top of this topic a row has been added for a manually added ISBN field.
</dd></dl><p><b>2.</b> In that row, manually identify the <b>Field</b> name, <b>Type</b>, and status for an auto-extracted attribute. 
</p><p><b>3.</b> Click <b>Add by name</b> again to add additional attribute rows. 
</p><p><b>4.</b> Click the <b>X</b> in the top right-hand corner of an added row to remove it. 
</p><p><b>5.</b> Click <b>Save</b> to save your changes. 
</p>
<dl><dd> Splunk Enterprise adds any fields that you've added to the table to your root object as Extracted  in the Extracted category, along with any selected auto-extracted fields.
</dd></dl><a name="addanevalexpressionattribute"></a><h2> <a name="addanevalexpressionattribute_add_an_eval_expression_attribute"><span class="mw-headline" id="Add_an_eval_expression_attribute">Add an eval expression attribute</span></a></h2>
<p>You can add an eval expression attribute to any object in your data model. This attribute type uses eval expressions to create fields that can be added to events in your object dataset, in a manner similar to that of <b>calculated fields</b>.
</p><p><img alt="6.1 dm add eval att.png" src="images/e/ea/6.1_dm_add_eval_att.png" width="720" height="480"></p><p><b>1.</b> In the Data Model Editor, open the object that you would like to add an attribute to.
</p><p><b>2.</b> Click <b>Add Attribute</b>. Select <i>Eval Expression</i> to define an eval expression attribute. 
</p>
<dl><dd> The Add Attributes with an Eval Expression dialog appears.
</dd></dl><p><b>3.</b> Enter the <b>Eval Expression</b> that defines the attribute value. 
</p>
<dl><dd> The <b>Eval Expression</b> text area should just contain the <code><font size="2">&lt;eval-expression&gt;</font></code> portion of the <code><font size="2">eval</font></code> syntax. There's no need to type the full syntax used in Search (<code><font size="2">eval &lt;eval-field&gt;=&lt;eval-expression&gt;</font></code>). 
</dd></dl><p><b>4.</b> Under <b>Attribute</b> enter the attribute <b>Field Name</b> and <b>Display Name</b>.  
</p>
<dl><dd> The <b>Field Name</b> is the name of the attribute in your object data. The <b>Display Name</b> is the attribute name that your Pivot users see when they create pivots. <b>Note:</b> The <b>Field Name</b> cannot include whitespace, single quotes, double quotes, curly braces, or asterisks. The attribute <b>Display Name</b> cannot contain asterisks.
</dd></dl><p><b>5.</b> Define the attribute <b>Type</b> and set its <b>Flag</b>. 
</p>
<dl><dd> For more information about the <b>Flag</b> values, see the subsection on marking attributes as hidden or required in <a href="#definedatamodelattributes" class="external text">"Define object attributes,"</a> in this manual.
</dd></dl><p><b>6.</b> (Optional) Click <b>Preview</b> to verify that the eval expression is working as expected. 
</p>
<dl><dd> You should see events in table format with the new eval attribute(s) included as columns. For example, if you're working with an event-based object and you've added an eval attribute named <b>gb</b>, the preview event table should show a column labeled <b>gb</b> to the right of the first column (<b>_time</b>). 
</dd></dl><dl><dd> The preview pane has two tabs. <b>Events</b> is the default tab. It presents the events in table format. The new eval attribute should appear to the right of the first column (the <code><font size="2">_time</font></code> column). 
</dd></dl><dl><dd> If you do not see values in this column, or you see the same value repeated in the events at the top of the list, it could mean that more values appear later in the sample. Select the <b>Values</b> tab to review the distribution of eval attribute values among the selected event sample. You can also change the <b>Sample</b> value to increase the number of events in the preview sample--this can sometimes uncover especially rare values of the field created by the eval expression.
</dd></dl><dl><dd> In the example below, the three real-time searches only appeared in the value distribution when <b>Sample</b> was expanded from <i>First 1,000 events</i> to <i>First 10,000 events</i>.
</dd></dl><p><img alt="6.1 dm add eval att values preview.png" src="images/a/a4/6.1_dm_add_eval_att_values_preview.png" width="720" height="200"></p><p><b>7.</b> Click <b>Save</b> to save your changes and return to the Data Model Editor.
</p><p>For more information about the <code><font size="2">eval</font></code> command and the formatting of eval expressions, see the <code><font size="2">eval</font></code> page as well as the topic "Functions for eval and where" in the Search Reference. 
</p><p>Eval expressions can utilize attributes that have already been defined or calculated, which means you can chain attributes together. Splunk Enterprise processes attributes in the order that they are listed from top to bottom. This means that you must place prerequisite attributes above the eval expression attribute that uses those attributes in its eval expression. In other words, if you have a calculation B that depends on another calculation A, make sure that calculation A comes before calculation B in the attribute order. For more information see the subsection on attribute order and chaining in <a href="#definedatamodelattributes_attribute_order_and_attribute_chaining" class="external text">"Define object attributes"</a>, in this manual.
</p><p>You can use attributes of any type in an eval expression attribute definition. For example, you could create an eval expression attribute that uses an auto-extracted attribute and another eval expression attribute in its eval expression. It will work as long as those attributes are listed above the one you're creating. 
</p><p>When you create an eval expression attribute that uses the values of other attributes in its definition, you can optionally "hide" those other attributes by setting their <b>Flag</b> to <i>Hidden</i>. This ensures that only the final eval expression value is available to your Pivot users.
</p>
<a name="addalookupattribute"></a><h2> <a name="addalookupattribute_add_a_lookup_attribute"><span class="mw-headline" id="Add_a_lookup_attribute"> Add a lookup attribute</span></a></h2>
<p>You can add a lookup attribute to any object in your data model.
</p><p>In order to create a lookup attribute, you must have at least one <b>lookup definition</b> defined in <b>Settings &gt; Lookups &gt; Lookup definitions</b>. The lookup definition tells Splunk Enterprise where the lookup table is and how to connect to it--it can either connect directly to a table-based CSV file that you upload via Splunk Web, or it can use a Python script to connect to an external lookup table. Once the lookup definition is in place, Splunk Enterprise can match the values of the attribute you choose to values of a field in the lookup table and then return corresponding field/value combinations and apply them to your object as lookup attributes. 
</p><p><b>Note:</b> Any lookup table files and lookup definitions that you use in your lookup attribute must have the same permissions as the data model. If the data model's permissions are global (i.e., shared to "all apps"), but the lookup table file or definition is private, the attribute will be broken. (In general data models and their associated lookup table files and definitions should all be shared globally to all apps.)
</p><p>For more information about creating lookup definitions (as well as uploading CSV files), see <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">"Use field lookups to add information to your events"</a>. 
</p><p><img alt="6.1 dm add lookup att.png" src="images/e/e4/6.1_dm_add_lookup_att.png" width="720" height="449"></p><p><b>1.</b> In the Data Model Editor, open the object you'd like to add a lookup attribute to. 
</p><p><b>2.</b> Click <b>Add Attribute</b> and select <i>Lookup</i>. 
</p>
<dl><dd> This takes you to the Add Attributes with a Lookup page.
</dd></dl><p><b>3.</b> Under <b>Lookup Table</b>, select the lookup table that you intend to match an input attribute to. 
</p>
<dl><dd> All of the values in the <b>Lookup Table</b> list are <b>lookup definitions</b> that were previously defined in Settings. 
</dd></dl><dl><dd> When you select a valid lookup table, the <b>Input</b> and <b>Output</b> sections of the page are revealed and populated. The <b>Output</b> section should display a list of all of the columns in the selected <b>Lookup Table</b>. 
</dd></dl><p><b>4.</b> Under <b>Input,</b> define your lookup input fields. Choose a <b>Field in Lookup</b> (a field from the <b>Lookup Table</b> that you've chosen) and a corresponding <b>Attribute</b> from the object you're editing. 
</p>
<dl><dd> The <b>Input</b> lookup table field/attribute combination is the key that selects rows in the lookup table. For each row that this input key selects, you can bring in output field values from that row and add them to matching events. 
</dd></dl><dl><dd> For example, your object may have a <code><font size="2">productId</font></code> field in your lookup table that matches an auto-extracted <code><font size="2">Product ID</font></code> attribute in your object event data. The lookup table field and the object attribute should have the same (or very similar) value sets. In other words, if you have a row in your lookup table where <code><font size="2">productId</font></code> has a value of <code><font size="2">PD3Z002</font></code>, there should be events in your object dataset where the<code><font size="2">Product ID = PD3Z002</font></code>. Those matching events will be updated with output field/value combinations from the row where <code><font size="2">productId</font></code> has a value of <code><font size="2">PD3Z002</font></code>. See "Example of a lookup attribute setup," below, for a detailed step-by-step explanation of this process.
</dd></dl><dl><dd> In cases where multiple lookup table rows are matched by a particular input key, Splunk only returns field values from the first row match. To narrow down the set of rows that are matched, you can optionally define multiple pairs of input fields. All of these input keys must match a row for Splunk Enterprise to select it and return output field values from it. You cannot reuse <b>Field in Lookup</b> values when you have multiple inputs. 
</dd></dl><p><b>5.</b> Under <b>Output</b>, determine which fields from the lookup will be added to eligible events in your object dataset as new lookup attributes. 
</p>
<dl><dd> You should find a list of fields here, pulled from the columns in the lookup table that you've chosen. Start by selecting the fields that you would like to add to your events. Any lookup fields that you've designated as inputs will be unavailable. You must define at least one output attribute in order for the lookup attribute definition to be valid.
</dd></dl><dl><dd> If you do not find any fields here there may be a problem with the designated <b>Lookup Table</b>.
</dd></dl><p><b>6.</b> Under <b>Field Name</b>, provide the field name that the lookup attribute should have in your data. 
</p>
<dl><dd> <b>Field Name</b> values cannot include whitespace, single quotes, double quotes, curly braces, or asterisks.
</dd></dl><p><b>7.</b> Under <b>Display Name</b> provide the display name for the lookup attribute in the Data Model Editor and in Pivot. 
</p>
<dl><dd> <b>Display Name</b> values cannot include asterisk characters.
</dd></dl><p><b>8.</b> Set appropriate <b>Type</b> and <b>Flags</b> values for each lookup attribute that you define. 
</p>
<dl><dd> For more information about the <b>Type</b> field, see the subsection "Marking attributes as hidden or required" in the <a href="#definedatamodelattributes_marking_attributes_as_hidden_or_required" class="external text">"Define object attributes"</a> topic, in this manual.
</dd></dl><p><b>9.</b> (Optional) Click <b>Preview</b> to verify that Splunk Enterprise is adding the output attributes to qualifying events. 
</p>
<dl><dd> Qualifying events are events whose input attribute values match up with input field values in the lookup table). See "Preview lookup attributes," below, for more information.
</dd></dl><p><b>10.</b> If you're satisfied that the lookup is working as expected, click <b>Save</b> to save your attributes and return to the Data Model Builder. 
</p>
<dl><dd> The new lookup attributes will be added to the bottom of the object attribute list. 
</dd></dl><h3> <a name="addalookupattribute_preview_lookup_attributes"><span class="mw-headline" id="Preview_lookup_attributes">Preview lookup attributes</span></a></h3>
<p>After you set up your lookup attribute, you can click <b>Preview</b> to see whether the lookup attributes are being added to qualifying events (events where the designated input attribute values match up with corresponding input field values in the lookup table). Splunk Enterprise returns the results in two or more tabbed pages. 
</p><p>The first tab shows a sample of the events returned by the underlying search. New lookup attributes should appear to the right of the first column (the <code><font size="2">_time</font></code> column). If you do not see any values in the lookup attribute columns in the first few pages it could indicate that these values are very rare. You can check on this by looking at the remaining preview tab(s). 
</p><p><img alt="6.1 dm add lookup att values preview2.png" src="images/d/d3/6.1_dm_add_lookup_att_values_preview2.png" width="720" height="338"></p><p>Splunk Enterprise provides a tab for each lookup attribute you select in the <b>Output</b> section. Each attribute tab provides a quick summary of the value distribution in the chosen sample of events. It's set up as a top values list, organized by <b>Count</b> and percentage. 
</p><p><img alt="6.1 dm add lookup att values preview.png" src="images/3/36/6.1_dm_add_lookup_att_values_preview.png" width="720" height="336"></p>
<h3> <a name="addalookupattribute_example_of_a_lookup_attribute_setup"><span class="mw-headline" id="Example_of_a_lookup_attribute_setup">Example of a lookup attribute setup</span></a></h3>
<p>Let's say the following things are true:
</p>
<ul><li> <b>You have a data model object with an auto-extracted attribute called</b> <i><b>Product ID</b></i> <b>and another auto-extracted attribute named</b> <i><b>Product Name.</b></i> You would like to use a lookup table to add a new attribute to your dataset that provides the product price.
</li><li> <b>You have a <code><font size="2">.csv</font></code> file called <code><font size="2">product_lookup</font></code>.</b> This table includes several fields related to products, including <code><font size="2">productId</font></code> and <code><font size="2">product_name</font></code> (which have very similar value sets to the similarly-named attributes in your object), as well as <code><font size="2">price</font></code>, which is the field in the lookup table that you want to add to your object as a lookup attribute.
</li><li> <b>You know that there are a few products that have the same</b> <i><b>Product Name</b></i> <b>but different</b> <i><b>Product ID</b></i> <b>values and prices.</b> This means you can't set up a lookup definition that depends solely on <i>Product Name</i> as the input field, because it will try to apply the same <code><font size="2">price</font></code> value from the lookup table to two or more products. You'll have to design a lookup attribute definition that uses both <i>Product Name</i>and <i>Product ID</i> as input fields, matching each combination of values in your matching events to rows in the lookup table that have the same name/ID combinations. 
</li></ul><p>If this is the case, here's what you do to get <code><font size="2">price</font></code> properly added to your object data as an attribute.
</p><p><b>1.</b> In Settings, <a href="#usefieldlookupstoaddinformationtoyourevents" class="external text">create a lookup definition</a> that points at the <code><font size="2">product_lookup.csv</font></code> lookup file. Call this lookup definition <i>product_lookup</i>.
</p><p><b>2.</b> Go to Pivot and open the Data Model Editor for the object you want to add the lookup attributes to.
</p><p><b>3.</b> Click <b>Add Attribute</b> and select <i>Lookup</i>. 
</p>
<dl><dd> The Edit Attributes with a Lookup page opens.
</dd></dl><p><b>4.</b> Under <b>Lookup Table</b> select <i>product_lookup</i>. 
</p>
<dl><dd> All of the fields tracked in the lookup table will appear under <b>Output</b>.
</dd></dl><p><b>5.</b> Under <b>Input</b>, define two <b>Field in Lookup</b>/<b>Attribute</b> pairs. The first pair should have a <b>Field in Lookup</b> value of <i>ProductId</i> and an <b>Attribute</b> value of <i>Product ID</i>. The second pair should have a <b>Field in Lookup</b> value of <i>product_name</i> and an <b>Attribute</b> value of <i>Product Name</i>.
</p>
<dl><dd> The first pair matches the lookup table's <code><font size="2">productId</font></code> field with your object's <i>Product ID</i> attribute. The second pair matches the lookup table's <code><font size="2">product_name</font></code> field with your object's <i>Product Name</i> attribute. Notice that when you do this, under <b>Output</b> the rows for the <code><font size="2">productID</font></code> and <code><font size="2">product_name</font></code> fields become unavailable.
</dd></dl><p><b>6.</b> Under <b>Output</b>, select the checkbox for the <code><font size="2">price</font></code> field. 
</p>
<dl><dd> This tells Splunk Enterprise that you want to add it to the events in your object dataset that have matching input attributes.
</dd></dl><p><b>7.</b> Give the <code><font size="2">price</font></code> attribute a Display Name of <i>Price</i>. 
</p>
<dl><dd> The <code><font size="2">price</font></code> attribute should already have a <b>Type</b> value of <i>Number</i>. 
</dd></dl><p><b>8.</b> Click <b>Preview</b> to test whether <code><font size="2">price</font></code> is being added to your events. 
</p>
<dl><dd> The preview events appear in table format, and the <code><font size="2">price</font></code> field is the second column after the timestamp.
</dd></dl><p><b>9.</b> If the <code><font size="2">price</font></code> field shows up as expected in the preview results, click <b>Save</b> to save the lookup attribute.
</p><p>Now your Pivot users will be able to use <i>Price</i> as an attribute option when building Pivot reports and dashboards.
</p>
<a name="addaregularexpressionattribute"></a><h2> <a name="addaregularexpressionattribute_add_a_regular_expression_attribute"><span class="mw-headline" id="Add_a_regular_expression_attribute"> Add a regular expression attribute</span></a></h2>
<p>You can add a regular expression attribute to any object in your data model. Regular expression attributes turn the named groups in regular expression strings into separate data model attributes. You can arrange for the regular expression to extract attributes from the <code><font size="2">_raw</font></code> event text as well as specific field values. 
</p><p><img alt="6.1 dm add regex att.png" src="images/9/9a/6.1_dm_add_regex_att.png" width="720" height="373"></p><p><b>1.</b> In the Data Model Editor, open the object you'd like to add a regular expression attribute to. 
</p>
<dl><dd> For an overview of the Data Model Editor, see <a href="#designdatamodelobjects" class="external text">"Design data models and objects,"</a> in this manual.
</dd></dl><p><b>2.</b> Click <b>Add Attribute</b> and select <i>Regular Expression</i>. 
</p>
<dl><dd> This takes you to the Add Attributes with a Regular Expression page.
</dd></dl><p><b>3.</b> Under <b>Extract From</b> select the attribute that you want to extract the fields from. 
</p>
<dl><dd> The <b>Extract From</b> list should include all of the attributes currently found in your object, with the addition of <code><font size="2">_raw</font></code>. If your regular expression is designed to extract one or more attributes from values of a specific attribute, choose that attribute from the <b>Extract From</b> list. On the other hand, if your regular expression is designed to parse the entire event string, choose <i>_raw</i> from the <b>Extract From</b> list.
</dd></dl><p><b>4.</b> Provide a <b>Regular Expression</b>. 
</p>
<dl><dd> The regular expression must have at least one named group. Splunk Enterprise extracts each named expression in the regular expression as a separate attribute. Attribute names cannot include whitespace, single quotes, double quotes, curly braces, or asterisks. 
</dd></dl><dl><dd> After you provide a regular expression, the named group(s) appear under <b>Attribute(s)</b>. 
</dd></dl><dl><dd> <b>Note:</b> Regular expression attributes currently do not support sed mode or sed expressions. 
</dd></dl><p><b>5.</b> (Optional) Provide different <b>Display Name</b> values for the attribute(s).
</p>
<dl><dd> Attribute <b>Display Name</b> values cannot include asterisk characters.
</dd></dl><p><b>6.</b> (Optional) Correct attribute <b>Type</b> values.
</p>
<dl><dd> They will be given <i>String</i> by default.
</dd></dl><p><b>7.</b> (Optional) Change attribute <i>Flag</i> values to whatever is appropriate for your needs.
</p><p><b>8.</b> (Optional) Click <b>Preview</b> to get a look at how well the attributes are represented in the object dataset. 
</p>
<dl><dd> For more information about previewing attributes, see "Preview regular expression attribute representation," below.
</dd></dl><p><b>9.</b> Click <b>Save</b> to save your changes. 
</p>
<dl><dd> You will be returned to the Data Model Editor. The regular expression attributes will be added to the list of calculated object attributes.
</dd></dl><p>For a primer on regular expression syntax and usage, see Regular-Expressions.info. You can test your regex by using it in a search with the rex search command.  Splunk Enterprise also maintains a list of useful third-party tools for writing and testing regular expressions.
</p>
<h3> <a name="addaregularexpressionattribute_preview_regular_expression_attribute_representation"><span class="mw-headline" id="Preview_regular_expression_attribute_representation">Preview regular expression attribute representation</span></a></h3>
<p>When you click <b>Preview</b> after defining one or more field extraction attributes, Splunk Enterprise runs the regular expression against the objects in your dataset that have the <b>Extract From</b> attribute you've selected (or against raw data if you're extracting from <code><font size="2">_raw</font></code>) and shows you the results. The preview results appear underneath the setup fields, in a set of four or more tabbed pages. Each of these tabs shows you information taken from a sample of events in the object dataset. You can determine how this sample is determined by selecting an option from the <b>Sample</b> list, such as <i>First 1000 events</i> or <i>Last 24 hours</i>. You can also determine how many events appear per page (default is 20). 
</p><p>If the preview doesn't return any events it could indicate that you need to adjust the regular expression, or that you have selected the wrong <b>Extract From</b> attribute.
</p>
<h4><font size="3"><b><i> <a name="addaregularexpressionattribute_the_all_tab"><span class="mw-headline" id="The_All_tab">The All tab</span></a></i></b></font></h4>
<p>The <b>All</b> tab gives you a quick sense of how prevalent events that match the regular expression are in the event data. You can see an example of the <b>All</b> tab in action in the screen capture near the top of this topic.
</p><p>It shows you an unfiltered sample of the events that have the <b>Extract From</b> attribute in their data. For example, if the <b>Extract From</b> attribute you've selected is <code><font size="2">uri_path</font></code> this tab displays only events that have a <code><font size="2">uri_path</font></code> value. 
</p><p>The first column indicates whether the event matched the regular expression or not. Events that match have green checkmarks. Non-matching events have red "x" marks.
</p><p>The second column displays the value of the <b>Extract From</b> field in the event. If the <b>Extract From</b> field is <code><font size="2">_raw</font></code>, the entire event string is displayed. The remaining columns display the attribute values extracted by the regular expression, if any.
</p>
<h4><font size="3"><b><i> <a name="addaregularexpressionattribute_the_match_and_non-match_tabs"><span class="mw-headline" id="The_Match_and_Non-Match_tabs">The Match and Non-Match tabs</span></a></i></b></font></h4>
<p>The Match and Non-Match tabs are similar to the All tab except that they are filtered to display either just events that <i>match</i> the regular expression or just events that <i>do not match</i> the regular expression. These tabs help you get a better sense of the field distribution in the sample, especially if the majority of events in the sample fall in either the matching or non-matching event set.
</p><p><img alt="6.1 dm add regex att prevmatch.png" src="images/a/ad/6.1_dm_add_regex_att_prevmatch.png" width="720" height="261"></p>
<h4><font size="3"><b><i> <a name="addaregularexpressionattribute_the_attribute_tab.28s.29"><span class="mw-headline" id="The_attribute_tab.28s.29">The attribute tab(s)</span></a></i></b></font></h4>
<p>Each attribute named in the regular expression gets its own tab. An attribute tab provides a quick summary of the value distribution in the chosen sample of events. It's set up as a top values list, organized by <b>Count</b> and percentage. If you don't see the values you're expecting, or if the value distribution you are seeing seems off to you, this can be an indication that you need to fine-tune your regular expression.
</p><p>You can also increase the sample size to find rare attribute values or values that appear further back in the past. In the example below, setting <b>Sample</b> to <i>First 10,000 events</i> uncovered a number of values for the <code><font size="2">path</font></code> attribute that do not appear when only the first 1,000 events are sampled.
</p><p><img alt="6.1 dm add regex att prevatt.png" src="images/d/d3/6.1_dm_add_regex_att_prevatt.png" width="720" height="216"></p><p>The top value tables in attribute tabs are drilldown-enabled. You can click on a row to see all of the events represented by that row. For example, if you are looking at the <code><font size="2">path</font></code> attribute and you see that there are 6 events with the path <code><font size="2">/numa/</font></code>, you can click on the <code><font size="2">/numa/</font></code> row to go to a list that displays the 6 events where <code><font size="2">path="/numa/"</font></code>.
</p>
<a name="addageoipattribute"></a><h2> <a name="addageoipattribute_add_a_geo_ip_attribute"><span class="mw-headline" id="Add_a_geo_IP_attribute"> Add a geo IP attribute</span></a></h2>
<p>You can add a Geo IP attribute to any object in your data model that already has an attribute with a <b>Type</b> of <i>ipv4</i> in its attribute list. The <i>ipv4</i> attribute must appear <i>above</i> the location for the Geo IP attribute, and it cannot already be in use for a different Geo IP attribute calculation.
</p><p><img alt="6.1 dm add geoip att.png" src="images/1/1d/6.1_dm_add_geoip_att.png" width="720" height="418"></p><p>The Geo IP attribute is a type of lookup. It reads the IP address values in your object's events and can add the related longitude, latitude, city, region, and country values to those events. 
</p><p><b>1.</b> In the Data Model Editor, open the object you'd like to add an attribute to.
</p><p><b>2.</b> Click <b>Add Attribute</b> and select <i>Geo IP</i> to define a Geo IP attribute. 
</p>
<dl><dd> The "Add Geo Attributes with an IP Lookup" page opens.
</dd></dl><p><b>3.</b> Choose the <b>IP</b> attribute that you want to match, if more than one exists for the selected object. 
</p><p><b>4.</b> Select the attributes that you want to add to your object. 
</p><p><b>5.</b> (Optional) Rename selected attributes by changing their <b>Display Name</b>. 
</p>
<dl><dd> Display names cannot include asterisk characters.
</dd></dl><p><b>6.</b> (Optional) Click <b>Preview</b> to verify that the GeoIP attribute is correctly updating your events with the GeoIP attributes that you have selected.
</p>
<dl><dd> You should see events in table format with the new GeoIP attribute(s) included as columns. For example, if you're working with an event-based object and you've selected the <b>City</b>, <b>Region</b>, and <b>Country</b> GeoIP attributes, the preview event table should display <i>City<b>, </b>Region<b>, and </b>Country<b> columns to the right of the first column ('</b></i><b>_time</b>). 
</dd></dl><dl><dd> The preview pane has two tabs. <b>Events</b> is the default tab. It presents the events in table format. Select the <b>Values</b> tab to review the distribution of GeoIP attribute values among your events. 
</dd></dl><dl><dd> If you're not seeing the range of values you're expecting, try increasing the preview event sample. By default this sample is set to the first thousand events. You might increase it by setting the <b>Sample</b> value to <i>First 10,000 events</i> or <i>Last 7 days</i>.
</dd></dl><p><img alt="6.1 dm add geoip att prev.png" src="images/7/7f/6.1_dm_add_geoip_att_prev.png" width="720" height="438"></p><p><b>7.</b> Click <b>Save</b> to save your changes.
</p>
<dl><dd> You will be returned to the Data Model Editor. The Geo IP attributes that you have defined will be added to the object's set of Calculated attributes. 
</dd></dl><p><b>Note:</b> Geo IP attributes are added to your object as <i>required</i> attributes, and their <b>Type</b> values are predetermined. You cannot change these values.
</p>
<h1>Save and share search and pivot jobs</h1><a name="aboutjobsandjobmanagement"></a><h2> <a name="aboutjobsandjobmanagement_about_jobs_and_job_management"><span class="mw-headline" id="About_jobs_and_job_management"> About jobs and job management</span></a></h2>
<p>Each time you run a search, create a pivot, open a report, or load a dashboard panel, Splunk Enterprise creates a <b>job</b> in the system. This job contains the event data returned by that search, pivot, report, or panel. The Jobs page enables you to review and oversee your recently dispatched jobs, as well as those you may have saved earlier. In addition, if you have the Admin role or a role with equivalent capabilities, you can use the Jobs page to manage the jobs of all users in the system. 
</p><p>Access the Jobs page by clicking the <b>Jobs</b> link in the Activity drop down.
</p><p><img alt="5.0-Job Mgr Link b.png" src="images/1/17/5.0-Job_Mgr_Link_b.png" width="224" height="132"></p><p>For more information about using the Jobs page, see <a href="#supervisejobswiththejobspage" class="external text">"Supervise your search jobs with the Jobs page."</a>, in this manual.
</p><p>You can also manage jobs through the command line of your operating system. For more information, see <a href="#managejobsfromtheos" class="external text">"Manage search jobs from the operating system"</a>, in this manual.
</p><p><b>Note:</b> Just to be clear, <i>search jobs</i> are not the same as <i>saved searches</i> and <i>saved reports</i>. Reports contain data used to run those reports, such as search strings, time ranges, and the formatting for chart or table visualizations. Jobs are artifacts of previously run searches and reports. They contain the results of a particular run of a search or report. Jobs are dispatched by scheduled searches as well as manual runs of searches and reports.
</p><p>For more information about saving searches as reports see "Create and edit reports" in the Reporting Manual. 
</p>
<h3> <a name="aboutjobsandjobmanagement_restrict_the_jobs_users_can_run"><span class="mw-headline" id="Restrict_the_jobs_users_can_run">Restrict the jobs users can run</span></a></h3>
<p>The way to restrict how many jobs a given user can run, and how much space their job artifacts can take up is to define a role with these restrictions and assign them to it. You can apply a high level of granularity by giving unique roles to each user in your system. 
</p><p>Create a capability in a copy of <code><font size="2">authorize.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code> and give it appropriate values of:
</p>
<ul><li> <code><font size="2">srchDiskQuota:</font></code> Maximum amount of disk space (MB) that can be taken by search jobs of a user that belongs to this role.
</li><li> <code><font size="2">srchJobsQuota</font></code>: Maximum number of concurrently running searches a member of this role can have.
</li></ul><p>For more information, refer to "Add and edit roles" in the Securing Splunk Enterprise manual.
</p>
<h3> <a name="aboutjobsandjobmanagement_autopause_long-running_jobs"><span class="mw-headline" id="Autopause_long-running_jobs">Autopause long-running jobs</span></a></h3>
<p>To handle inadvertently long-running search jobs, Splunk Enterprise provides an autopause feature. The feature is enabled by default only for summary dashboard clicks, to deal with the situation where users mistakenly initiate "all time" searches. 
</p><p>When autopause is enabled for a particular search view, the search view includes an autopause countdown field during a search. If the search time limit has been reached, an information window will appear to inform the user that the search has been paused. It offers the user the option of resuming or finalizing the search. By default, the limit before autopause is 30 seconds.
</p><p><img alt="Autopause popup.png" src="images/1/12/Autopause_popup.png" width="437" height="181"></p><p>Auto-pause is configurable only by view developers.  It is not a system-wide
setting nor is it configurable by role. The autopause feature can be enabled or disabled by editing the appropriate view. See "How to turn off autopause" in the Developer manual. Also, see the <code><font size="2">host, source,</font></code> and <code><font size="2">sourcetypes</font></code> links on the summary dashboard for examples of autopause implementation.
</p>
<a name="savingandsharingjobsinsplunkweb"></a><h2> <a name="savingandsharingjobsinsplunkweb_saving_and_sharing_jobs_in_splunk_web"><span class="mw-headline" id="Saving_and_sharing_jobs_in_Splunk_Web"> Saving and sharing jobs in Splunk Web</span></a></h2>
<p>In Search, after you launch a search you can access and manage information about the search's  <b>job</b> without leaving the Search page. Once your search is running, paused, or finalized, click <b>Job</b> and choose from the available options there. 
</p><p><img alt="6.0 search job dropdown.png" src="images/6/67/6.0_search_job_dropdown.png" width="283" height="186"></p><p>You can:
</p>
<ul><li> <b>Edit the job settings.</b> Select this to open the Job Settings dialog, where you can change the job's read permissions, extend the job's lifespan, and get a URL for the job that you can use to share the job with others or put a link to the job in your browser's bookmark bar. 
</li><li> <b>Send the job to the background.</b> Select this if the search job is slow to complete and you would like to run the job in the background while you work on other Splunk Enterprise activities (including running a new search job). 
</li><li> <b>Inspect the job.</b> Opens a separate window and display information and metrics for the search job via the <b>Search Job Inspector</b>.
</li><li> <b>Delete the job.</b> Use this to delete a job that is currently running, is paused, or which has finalized. After you have deleted the job you can still save the search as a report. 
</li></ul><h3> <a name="savingandsharingjobsinsplunkweb_edit_search_job_settings"><span class="mw-headline" id="Edit_search_job_settings"> Edit search job settings </span></a></h3>
<p>You can open the Job Settings dialog after once a search job is running, paused, or finalized. Just click <b>Job</b> and select <i>Edit Job Settings</i>. 
</p><p><img alt="6.0 search job settings dialog.png" src="images/a/a9/6.0_search_job_settings_dialog.png" width="457" height="365"></p>
<h4><font size="3"><b><i> <a name="savingandsharingjobsinsplunkweb_change_permissions_for_a_search_job"><span class="mw-headline" id="Change_permissions_for_a_search_job">Change permissions for a search job</span></a></i></b></font></h4>
<p>The read permissions for a search job determine whether a given job can be seen and accessed by others on the Jobs page. All jobs have read permissions set to <i>Private</i> by default, which means that only the people that run them can see a record for them on the Jobs page.
</p><p>The Job Settings dialog has a <b>Read Permissions</b> switch that enables you to toggle the read permissions for a job to <i>Everyone</i>. The permissions are scoped to the app context that the job was run in, so an <i>Everyone</i> setting actually means that the job is visible on the Jobs page for everyone who has read permissions for the app to which the job belongs.
</p><p>You can change the read permissions for a job back to <i>Private</i> if you are the owner of the job.
</p><p>For more information about the Jobs page, see <a href="#supervisejobswiththejobspage" class="external text">"Supervise your jobs with the Jobs page,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="savingandsharingjobsinsplunkweb_save_a_job_by_extending_its_lifespan"><span class="mw-headline" id="Save_a_job_by_extending_its_lifespan">Save a job by extending its lifespan</span></a></i></b></font></h4>
<p>When you run a new search job, its default lifespan is 10 minutes. This means that the job will be retained by the system and be available for review via the Jobs page for 10 minutes from the moment that you first ran the job. After that time limit is up, the job expires--Splunk Enterprise deletes it from the system.
</p><p>Whenever you access an existing job (by running it from the Jobs page for example), the job's lifespan is resets to a set period of time beginning from the moment it was accessed. This period of time will be 10 minutes until you change the job's <b>Lifetime</b> setting (see below). 
</p><p>To effectively "save" the job you open the Job Settings dialog and select a <b>Lifetime</b> setting of <i>7 days</i>. This ensures that the job will be retained for 7 days before the system deletes it. 
</p><p>Whenever a job is accessed, its lifespan is reset to the moment of access. This happens whether the job lifespan is 10 minutes or 7 days. So if you set the lifespan for a job to 7 days and then access it (view the data it returns) 4 days later, its lifespan is reset so it won't expire for another 7 days.
</p>
<h4><font size="3"><b><i> <a name="savingandsharingjobsinsplunkweb_inspect_a_job"><span class="mw-headline" id="Inspect_a_job">Inspect a job</span></a></i></b></font></h4>
<p>You can review a broad array of metrics and properties related to a job that you've launched in Search or Pivot by viewing it through the <b>Search Job Inspector</b>. Click <b>Job</b> and select <i>Inspect Job</i> to open the Search Job Inspector in a separate window. 
</p><p>You can access the Search Job Inspector for a job as long as the search or pivot artifact still exists (which means that the associated job has not expired).
</p><p>For a detailed introduction to the Search Job Inspector and all of the information it provides, see <a href="#viewsearchjobpropertieswiththejobinspector" class="external text">"View search job properties with the Search Job Inspector"</a> in this manual.
</p><p><img alt="6.0 Search job inspector.png" src="images/8/8a/6.0_Search_job_inspector.png" width="671" height="484"></p>
<h3> <a name="savingandsharingjobsinsplunkweb_share_a_job_with_others"><span class="mw-headline" id="Share_a_job_with_others">Share a job with others</span></a></h3>
<p>You can quickly share a job with other Splunk Enterprise users by sending them a link to the job. This can be handy if you want another user to see the results returned by the job. Keep in mind that this is not the same as sharing a report: when you share a job you are sharing the results of a specific run of a search. 
</p><p>There are two ways to obtain this link. You can use the Job Settings dialog as described above or you can use the Share icon that appears above your search or pivot results. If you use the Share icon, Splunk Enterprise will automatically set the job's read permissions to <i>Everyone</i> and extend its lifespan to 7 days.
</p><p><img alt="6.0 share results icon.png" src="images/c/c9/6.0_share_results_icon.png" width="53" height="65"></p><p>Either way, you'll get a <b>Link To Job</b> that you can paste in an email or text to the Splunk Enterprise user. However, if you use the Job Settings dialog to get the link you'll want to make sure that the job's <b>Read Permissions</b> are set to <i>Everyone</i> and set its <b>Lifetime</b> to <i>7 days</i>. If the job's permissions are set to <i>Private</i> other users will be unable to access it via the link. The users you send the link to should also have permissions to use the app to which the job belongs. 
</p><p><img alt="6.0 share job dialog.png" src="images/8/80/6.0_share_job_dialog.png" width="453" height="241"></p><p>You can also save the link for your own reuse by clicking the bookmark icon to the right of the <b>Link To Job</b> field and dragging it into your browser's bookmarks bar.
</p>
<h3> <a name="savingandsharingjobsinsplunkweb_export_job_data_to_a_file"><span class="mw-headline" id="Export_job_data_to_a_file">Export job data to a file</span></a></h3>
<p>You can export the event data from a search or pivot job to a csv, xml, json, or raw data file, and then archive it or use it with a third-party charting application. 
</p><p><b>1.</b> Run a search.
</p><p><b>2.</b> Select the <b>Export</b> icon that appears above your search or pivot results.
</p><p><img alt="6.0 export search job results.png" src="images/1/1a/6.0_export_search_job_results.png" width="105" height="73"></p><p><b>3.</b> Select a data <b>Format</b> and provide a <b>File Name</b> for the file to be created.
</p><p><img alt="6.0 export results dialog.png" src="images/b/b9/6.0_export_results_dialog.png" width="460" height="291"></p><p><b>4.</b> Indicate whether the number of results that you are exporting to this file should be <b>Unlimited</b> or <b>Limited.</b>
</p>
<dl><dd> You can set a limit for the number of events you want to export, or you can go ahead and export all the results returned by your search or pivot. Some searches return enormous numbers of events. Take precautions as necessary for your situation. 
</dd></dl><p><b>5.</b> Click <b>Export</b> to export the file.
</p>
<dl><dd> Splunk Enterprise exports the file to the default download directory for your browser or operating system. For example, most Mac OSX users will see the export file appear in the default <b>Downloads</b> directory if they have not specified a different directory.
</dd></dl><p>Splunk Enterprise provides a number of options for search result export. Some are optimized for speed, while others are good for extremely large event sets. For a summary of these export methods, see "Export search results" in the <i>Search Manual</i>.
</p>
<a name="supervisejobswiththejobspage"></a><h2> <a name="supervisejobswiththejobspage_supervise_jobs_with_the_jobs_page"><span class="mw-headline" id="Supervise_jobs_with_the_Jobs_page"> Supervise jobs with the Jobs page</span></a></h2>
<p>You can use the Jobs page to review and manage any search that you own. If you have the Admin role or a role with an equivalent set of capabilities, you can manage the search jobs run by all users of your implementation. You can access these jobs in the Jobs page in that is available in Splunk Web. In Splunk Web, select <b>Activity</b> &gt; <b>Jobs</b> to do this. 
</p><p><img alt="5.0-Job Mgr Link b.png" src="images/1/17/5.0-Job_Mgr_Link_b.png" width="224" height="132"></p><p>When you click <b>Jobs,</b> Splunk Enterprise opens the Jobs page. The Jobs page displays a list of search jobs. Search jobs displayed in the Jobs page list include:  
</p>
<ul><li> Jobs resulting from searches or pivots that you have recently run manually. 
</li><li> Jobs that are artifacts of searches that are run when dashboards are loaded.
</li><li> Jobs that are artifacts of scheduled searches (searches that are designed to run on a regular interval).
</li><li> Jobs that have been saved. 
<ul><li> You can save a search job manually via the Jobs page. 
</li><li> Search jobs are also saved automatically when you manually <b>send a search to the background</b> before it completes or you finalize it. 
</li></ul></li></ul><p><b>Note:</b> If a job is canceled while you have the Jobs page open it can still appear in the Jobs page list, but you won't be able to view its results. If you close and reopen the Jobs page, the canceled job should disappear.
</p>
<h3> <a name="supervisejobswiththejobspage_search_job_lifespans"><span class="mw-headline" id="Search_job_lifespans">Search job lifespans</span></a></h3>
<p>Search jobs will remain in the Jobs page until they are automatically deleted by the Splunk Enterprise system. The default lifespan for a search job differs depending on whether it is an artifact of a search that was launched manually, or is an artifact of a scheduled search. 
</p><p><b>Search jobs from manually-launched searches and dashboard loads</b>
</p><p>When you manually launch a search and the search is finalized or completes on its own, the resulting search job has a default lifespan of 10 minutes. Search jobs from searches that are artifacts of dashboard panel loads also have 10 minute lifespans.
</p><p>You can extend a search job's expiration time to 7 days by saving it. You can save a search job two ways: you can open the Jobs page and save the search job manually, or you can save the search job by sending it to the background while the search is still running. 
</p><p><b>Note:</b> If you want to increase or decrease the retention time for saved jobs, go to <code><font size="2">limits.conf</font></code> and change the <code><font size="2">default_save_ttl</font></code> value for the <code><font size="2">[search]</font></code> stanza to a number that is more appropriate for your needs. (The acronym "ttl" stans for "time to lose.")
</p><p>Whenever you view a  search job's results (in other words, whenever you click its link in the Jobs page to bring up its results in another window) Splunk Enterprise resets the job's expiration time so that it is retained for 7 days from the moment when you accessed it. 
</p><p><b>Search jobs from scheduled searches</b>
</p><p>Scheduled searches launch search jobs on a regular interval. By default, such jobs will be retained for the interval of the scheduled search multiplied by two. So if the search runs every 6 hours, the resulting jobs will expire in 12 hours. 
</p><p><b>Note:</b> You can change the default lifespan for jobs resulting from a specific scheduled search. To do this, go to <code><font size="2">savedsearches.conf</font></code>, locate the scheduled search in question, and change its <code><font size="2">dispatch.ttl</font></code> setting to a different interval multiple.
</p>
<h3> <a name="supervisejobswiththejobspage_jobs_page_controls"><span class="mw-headline" id="Jobs_page_controls">Jobs page controls</span></a></h3>
<p><img alt="Jobspage.png" src="images/c/c5/Jobspage.png" width="700" height="283"></p><p>Use the controls on the Jobs page to:
</p>
<ul><li> See a list of the jobs you've recently dispatched or saved for later review and use it to compare job statistics (run time, total count of events matched, size, and so on). If you have the Admin role or a role with equivalent or greater capabilities you will see all jobs that have been recently dispatched for your Splunk Enterprise implementation.
</li><li> Check on the progress of ongoing backgrounded jobs (this includes both real-time searches and long-running historical searches) or jobs dispatched by scheduled searches.
</li><li> Save, pause, resume, finalize, and delete search or pivot jobs, either individually or in bulk. Select the checkbox to the left of the job(s) you want to act on and click the relevant button at the bottom of the page.
</li><li> Click on the search name or search string to view the results associated with a specific job. The results will open in a separate browser window.
<ul><li> If the job is related to a search that has not yet been saved as a report, you'll see the results in the Search view. 
</li><li> If the job is related to a report, Splunk will open the report and display the results there.
</li></ul></li><li> The <b>Expires</b> column tells you how much time each list job has before it is deleted from the system. If you want to be able to review a search job after that expiration point, or share it with others, save it. Keep in mind, however, that jobs will still expire 7 days after they are saved (unless you view the job directly during that 7 day period, in which case the expiration clock is reset). See "Search job lifespans," above, for more information. 
</li></ul><p>In Search, you can save the last search or report job you ran without accessing the Jobs page, as long as the job hasn't already expired:
</p>
<ul><li> If you want to <a href="#aboutjobsandjobmanagement" class="external text">save a search job</a> after running a search in the Search view, click <b>Job</b> and select <i>Edit Job Settings</i> to bring up the <b>Job Settings</b> dialog. Here you can set the job's <b>Read Permissions</b> (set them to <i>Everyone</i> if you want to share it with others), set the job's <b>Lifetime</b> to <i>7 days</i> if you want to keep it for inspection, and get a <i>Link To Job</i> that you can use to share the job with others (if you've set its <b>Permissions</b> to <i>Everyone</i>). 
</li></ul><p><b>Note:</b> When you set a job's <b>Lifetime</b> to <i>7 days</i>, Splunk Enterprise deletes the job after that span of time elapses if it is not viewed. If it is viewed, its lifetime resets to 7 days from the moment that it is accessed.
</p>
<ul><li> You can also save a search job that you've run manually by clicking the <b>Send to Background</b> icon while the search is still running. This action automatically extends the job's lifetime to 7 days and sets its permissions to <i>Everyone</i>. Splunk Enterprise also provides a link that you can use to share the job with others.
</li></ul><p>For more information, see <a href="#aboutjobsandjobmanagement" class="external text">"About jobs and job management"</a> in this manual.
</p>
<a name="viewsearchjobpropertieswiththejobinspector"></a><h2> <a name="viewsearchjobpropertieswiththejobinspector_view_search_job_properties_with_the_search_job_inspector"><span class="mw-headline" id="View_search_job_properties_with_the_Search_Job_Inspector"> View search job properties with the Search Job Inspector</span></a></h2>
<p>The <b>Search Job Inspector</b> is a tool that lets you take a closer look at what your search is doing and see where Splunk is spending most of its time. 
</p><p>This topic discusses how to use the search job inspector to both troubleshoot the performance of a search job and understand the behavior of knowledge objects such as event types, tags, lookups and so on within the search. For more information, read <a href="#supervisejobswiththejobspage" class="external text">"Supervise your jobs with the Jobs page"</a> in this manual.
</p>
<h3> <a name="viewsearchjobpropertieswiththejobinspector_using_the_search_job_inspector"><span class="mw-headline" id="Using_the_Search_Job_Inspector"> Using the Search Job Inspector </span></a></h3>
<p>You can access the Search Job Inspector for a search <b>job</b> as long as the search artifact still exists (which means that the search has not expired). The search does not still have to be running.
</p><p><b>To inspect a search:</b>
</p><p><b>1.</b> Run the search. 
</p><p><b>2.</b> In the Job menu, select "Inspect Job".
</p><p>This opens the Search Job Inspector in a new browser window. 
</p><p><b>To view the properties of a search artifact:</b> 
</p><p>You can use the URL to inspect a search job artifact if you have its search ID (SID). You can find the SID of a search in the Job Manager (click the <b>Jobs</b> link in the upper right hand corner) or listed in Splunk's dispatch directory, <code><font size="2">$SPLUNK_HOME/var/run/splunk/dispatch</font></code> . For more information about the Job Manager, see <a href="#supervisejobswiththejobspage" class="external text">"Supervise your jobs with the Jobs page"</a> in this manual.
</p><p>If you look at the URI path for the Search Job Inspector window, you will see something like this at the end of the string:
</p>
<code><font size="2"><br>.../inspector?sid=1299600721.22&amp;namespace=search<br></font></code>
<p>The <code><font size="2">sid</font></code> and <code><font size="2">namespace</font></code> are the SID number and the name of the app that it belongs to. Here, the SID is 1299600721.22. 
</p><p>Type the search artifact's SID into the URI path, after <code><font size="2">sid=</font></code> and hit return. As long as you have the necessary ownership permissions to view the search, you will be able to inspect it.
</p><p>Now, what exactly are you looking at?
</p>
<h3> <a name="viewsearchjobpropertieswiththejobinspector_what_the_search_job_inspector_shows_you"><span class="mw-headline" id="What_the_Search_Job_Inspector_shows_you"> What the Search Job Inspector shows you </span></a></h3>
<p>While the search is running, the Search Job Inspector shows you two different panels. <b>Execution costs</b> lists information about the components of the search and how much impact each component has on the overall performance of the search. <b>Search job properties</b> lists other characteristics of the job. When the search finishes, the Search Job Inspector tells you how many results it found and the time it took to complete the search. After the search completes, the Search Job Inspector also displays error messages at the top of the screen. Most of the information is self-explanatory, but this section will discuss the panels in more detail.
</p>
<h4><font size="3"><b><i> <a name="viewsearchjobpropertieswiththejobinspector_execution_costs"><span class="mw-headline" id="Execution_costs"> Execution costs </span></a></i></b></font></h4>
<p>The <b>Execution costs</b> panel lets you troubleshoot the efficiency of your search by narrowing it down to the performance impact of specific components relating to a search-time event processing action. It displays a table of the components that displays:
</p>
<ul><li> the component durations in seconds.
</li><li> how many times each component was invoked while the search ran.
</li><li> the input and output event counts for each component. 
</li></ul><p>The Search Job Inspector lists the components alphabetically. You will see more or fewer components depending on the search you run. The following table describes the significance of each individual search command and distributed search component. 
</p><p><b>Note:</b> These are the components you will see if you just run a keyword search.
</p>
<h5> <a name="viewsearchjobpropertieswiththejobinspector_execution_costs_of_search_commands"><span class="mw-headline" id="Execution_costs_of_search_commands"> Execution costs of search commands </span></a></h5>
<p>In general, for each command that is part of the search job, there is a parameter <code><font size="2">command.&lt;command_name&gt;</font></code>. The values for these parameters represent the time spent in processing each <code><font size="2">&lt;command_name&gt;</font></code>. For example, if the table command is used, you will see <code><font size="2">command.table</font></code>.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><td width="30%" valign="center" align="left"> <b>Search command component name</b>
</td><td valign="center" align="left"> <b>Description</b>
</td></tr><tr><td valign="center" align="left"> <code><font size="2">command.search</font></code>
</td><td valign="center" align="left"> Once Splunk identifies the events containing the indexed fields matching your search, it looks into the events themselves to identify the ones that match other criteria. These are concurrent operations, not consecutive.
<ul><li> <code><font size="2">command.search.index</font></code> - tells how long it took to look into the TSIDX files for the location to read in the rawdata.
</li><li> <code><font size="2">command.search.rawdata</font></code> - tells how long it took to read the actual events from the rawdata files.
</li><li> <code><font size="2">command.search.typer</font></code> - tells how long it took identify event types.
</li><li> <code><font size="2">command.search.kv</font></code> - tells how long it took to apply field extractions to the events.
</li><li> <code><font size="2">command.search.fieldalias</font></code> - tells how long it took to rename fields based according to <code><font size="2">props.conf</font></code>.
</li><li> <code><font size="2">command.search.lookups</font></code> - tells how long it took to create new fields based on existing fields (perform field lookups).
</li><li> <code><font size="2">command.search.filter</font></code> - tells how long it took to filter out events that don't match (for example, fields and phrases).
</li><li> <code><font size="2">command.search.typer</font></code> - tells how long it took to assign event types to events.
</li><li> <code><font size="2">command.search.tags</font></code> - tells how long it took to assign tags to events.
</li></ul></td></tr></table><p>There is a relationship between the type of commands used and the numbers you can expect to see for Invocations, Input count, and Output count. For searches that generate events, you expect the input count to be 0 and the output count to be some number of events X. If the search is both a generating search and a filtering search, the filtering search would have an input (equal to the output of the generating search, X) and an output=X. The total counts would then be input=X, output=2*X, and the invocation count is doubled.
</p>
<h5> <a name="viewsearchjobpropertieswiththejobinspector_execution_costs_of_dispatched_searches"><span class="mw-headline" id="Execution_costs_of_dispatched_searches"> Execution costs of dispatched searches </span></a></h5>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><td width="30%" valign="center" align="left"> <b>Distributed search component name</b>
</td><td valign="center" align="left"> <b>Description</b>
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.check_disk_usage</font></code>
</td><td valign="center" align="left"> The time spent checking the disk usage of this job.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.createdSearchResultInfrastructure</font></code>
</td><td valign="center" align="left"> The time to create and set up the collectors for each peer and execute the HTTP post to each peer.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.emit_prereport_files</font></code>
</td><td valign="center" align="left"> When running a <b>transforming search</b>, Splunk Enterprise cannot compute the statistical results of the report until the search completes. After it fetches events from the search peers (<code><font size="2">dispatch.fetch</font></code>), it, writes out the results to local files. <code><font size="2">dispatch.emit_prereport_files</font></code> provides the time that it takes for Splunk Enterprise to write the transforming search results to those local files.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.evaluate</font></code>
</td><td valign="center" align="left"> The time spent parsing the search and setting up the data structures needed to run the search.  This component also includes the time it takes to evaluate and run subsearches. This is broken down further for each search command that is used. In general, <code><font size="2">dispatch.evaluate.&lt;command_name&gt;</font></code> tells you the time spent parsing and evaluating the <code><font size="2">&lt;command_name&gt;</font></code> argument. For example, <code><font size="2">dispatch.evaluate.search</font></code> indicates the time spent evaluating and parsing the <code><font size="2">search</font></code>command argument.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.fetch</font></code>
</td><td valign="center" align="left"> The time spent waiting for or fetching events from search peers.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.preview</font></code>
</td><td valign="center" align="left"> The time spent generating preview results.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.process_remote_timeline</font></code>
</td><td valign="center" align="left"> The time spent decoding timeline information generated by search peers.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.reduce</font></code>
</td><td valign="center" align="left"> The time spend reducing the intermediate report output.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.stream.local</font></code>
</td><td valign="center" align="left"> The time spent by search head on the streaming part of the search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.stream.remote</font></code>
</td><td valign="center" align="left"> The time spent executing the remote search in a distributed search environment, aggregated across all peers. Additionally, the time spent executing the remote search on each remote search peer is indicated with: <code><font size="2">dispatch.stream.remote.&lt;search_peer_name&gt;</font></code>. <code><font size="2">output_count</font></code> represents bytes sent rather than events in this case.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.timeline</font></code>
</td><td valign="center" align="left"> The time spent generating the timeline and fields sidebar information.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatch.writeStatus</font></code>
</td><td valign="center" align="left"> The time spent periodically updating <code><font size="2">status.csv</font></code> and <code><font size="2">info.csv</font></code> in the job's dispatch directory.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">startup.handoff</font></code>
</td><td valign="center" align="left"> The time elapsed between the forking of a separate search process and the beginning of useful work of the forked search processes. In other words it is the approximate time it takes to build the search apparatus. This is cumulative across all involved peers. If this takes a long time, it could be indicative of I/O issues with .conf files or the dispatch directory.
</td></tr></table><h4><font size="3"><b><i> <a name="viewsearchjobpropertieswiththejobinspector_search_job_properties"><span class="mw-headline" id="Search_job_properties"> Search job properties </span></a></i></b></font></h4>
<p>The <b>Search job properties</b> fields are listed in alphabetical order.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><td width="30%" valign="center" align="left"> <b>Parameter name</b>
</td><td valign="center" align="left"> <b>Description</b>
</td></tr><tr><td valign="center" align="left"> <code><font size="2">cursorTime</font></code>
</td><td valign="center" align="left"> The earliest time from which no events are later scanned. Can be used to indicate progress. See description for <code><font size="2">doneProgress</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">delegate</font></code>
</td><td valign="center" align="left"> For saved searches, specifies jobs that were started by the user. Defaults to scheduler.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">diskUsage</font></code>
</td><td valign="center" align="left"> The total amount of disk space used, in bytes.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dispatchState</font></code>
</td><td valign="center" align="left"> The state of the search. Can be any of QUEUED, PARSING, RUNNING, PAUSED, FINALIZING, FAILED, or DONE.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">doneProgress</font></code>
</td><td valign="center" align="left"> A number between 0 and 1.0 that indicates the approximate progress of the search.
<p><code><font size="2">doneProgress = (latestTime &acirc;&#128;&#147; cursorTime) / (latestTime &acirc;&#128;&#147; earliestTime)</font></code>
</p>
</td></tr><tr><td valign="center" align="left"> <code><font size="2">dropCount</font></code>
</td><td valign="center" align="left"> For real-time searches only, the number of possible events that were dropped due to the <code><font size="2">rt_queue_size</font></code> (defaults to 100000).
</td></tr><tr><td valign="center" align="left"> <code><font size="2">earliestTime</font></code>
</td><td valign="center" align="left"> The earliest time a search job is configured to start. Can be used to indicate progress. See description for <code><font size="2">doneProgress</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eai:acl</font></code>
</td><td valign="center" align="left"> Describes the app and user-level permissions. For example, is the app shared globally, and what users can run or view the search?
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventAvailableCount</font></code>
</td><td valign="center" align="left"> The number of events that are available for export.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventCount</font></code>
</td><td valign="center" align="left"> The number of events returned by the search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventFieldCount</font></code>
</td><td valign="center" align="left"> The number of fields found in the search results.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventIsStreaming</font></code>
</td><td valign="center" align="left"> Indicates if the events of this search are being streamed.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventIsTruncated</font></code>
</td><td valign="center" align="left"> Indicates if events of the search have not been stored, and thus not available from the events endpoint for the search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventSearch</font></code>
</td><td valign="center" align="left"> Subset of the entire search that is before any transforming commands. The timeline and events endpoint represents the result of this part of the search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">eventSorting</font></code>
</td><td valign="center" align="left"> Indicates if the events of this search are sorted, and in which order. <code><font size="2">asc</font></code> = ascending; <code><font size="2">desc</font></code> = descending; <code><font size="2">none</font></code> = not sorted
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isBatchMode</font></code>
</td><td valign="center" align="left"> Indicates whether or not the search in running in batch mode. This applies only to searches that include transforming commands.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isDone</font></code>
</td><td valign="center" align="left"> Indicates if the search has completed.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isFailed</font></code>
</td><td valign="center" align="left"> Indicates if there was a fatal error executing the search.  For example, if the search string had invalid syntax.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isFinalized</font></code>
</td><td valign="center" align="left"> Indicates if the search was finalized (stopped before completion).
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isPaused</font></code>
</td><td valign="center" align="left"> Indicates if the search has been paused.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isPreviewEnabled</font></code>
</td><td valign="center" align="left"> Indicates if previews are enabled.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isRealTimeSearch</font></code>
</td><td valign="center" align="left"> Indicates if the search is a real time search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isRemoteTimeline</font></code>
</td><td valign="center" align="left"> Indicates if the remote timeline feature is enabled.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isSaved</font></code>
</td><td valign="center" align="left"> Indicates that the search job is saved, storing search artifacts on disk for 7 days from the last time that the job has been viewed or touched. Add or edit the <code><font size="2">default_save_ttl</font></code> value in <code><font size="2">limits.conf</font></code> to override the default value of 7 days.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isSavedSearch</font></code>
</td><td valign="center" align="left"> Indicates if this is a saved search run using the scheduler.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">isZombie</font></code>
</td><td valign="center" align="left"> Indicates if the process running the search is dead, but with the search not finished.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">keywords</font></code>
</td><td valign="center" align="left"> All positive keywords used by this search. A positive keyword is a keyword that is not in a NOT clause.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">label</font></code>
</td><td valign="center" align="left"> Custom name created for this search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">latestTime</font></code>
</td><td valign="center" align="left"> The latest time a search job is configured to start. Can be used to indicate progress. See description for <code><font size="2">doneProgress</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">numPreviews</font></code>
</td><td valign="center" align="left"> Number of previews that have been generated so far for this search job.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">messages</font></code>
</td><td valign="center" align="left"> Errors and debug messages.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">performance</font></code>
</td><td valign="center" align="left"> This is another representation of the <b>Execution costs</b>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">remoteSearch</font></code>
</td><td valign="center" align="left"> The search string that is sent to every search peer.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">reportSearch</font></code>
</td><td valign="center" align="left"> If reporting commands are used, the reporting search.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">request</font></code>
</td><td valign="center" align="left"> GET arguments that the search sends to <code><font size="2">splunkd</font></code>.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">resultCount</font></code>
</td><td valign="center" align="left"> The total number of results returned by the search. In other words, this is the subset of scanned events (represented by the <code><font size="2">scanCount</font></code>) that actually matches the search terms.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">resultIsStreaming</font></code>
</td><td valign="center" align="left"> Indicates if the final results of the search are available using streaming (for example, no transforming operations).
</td></tr><tr><td valign="center" align="left"> <code><font size="2">resultPreviewCount</font></code>
</td><td valign="center" align="left"> The number of result rows in the latest preview results.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">runDuration</font></code>
</td><td valign="center" align="left"> Time in seconds that the search took to complete.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">scanCount</font></code>
</td><td valign="center" align="left"> The number of events that are scanned or read off disk.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">search</font></code>
</td><td valign="center" align="left"> The search string.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">searchProviders</font></code>
</td><td valign="center" align="left"> A list of all the search peers that were contacted.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">sid</font></code>
</td><td valign="center" align="left"> The search ID number.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">statusBuckets</font></code>
</td><td valign="center" align="left"> Maximum number of timeline buckets.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">ttl</font></code>
</td><td valign="center" align="left"> The time to live, or time before the search job expires after it completes.
</td></tr><tr><td valign="center" align="left"> Additional info
</td><td valign="center" align="left"> Links to further information about your search. These links may not always be available.
<ul><li> timeline
</li><li> field summary
</li><li> search.log
</li></ul></td></tr></table><p><b>Note:</b> When troubleshooting search performance, it's important to understand the difference between the <code><font size="2">scanCount</font></code> and <code><font size="2">resultCount</font></code> costs. For dense searches, the <code><font size="2">scanCount</font></code> and <code><font size="2">resultCount</font></code> are similar (<code><font size="2">scanCount = resultCount</font></code>); and for sparse searches, the <code><font size="2">scanCount</font></code> is much greater than the result count (<code><font size="2">scanCount</font></code> &gt;&gt; <code><font size="2">resultCount</font></code>). Search performance should not so much be measured using the <code><font size="2">resultCount</font></code>/time rate but <code><font size="2">scanCount</font></code>/time instead. Typically, the <code><font size="2">scanCount</font></code>/second event rate should hover between 10k and 20k events per second for performance to be deemed good.
</p>
<h4><font size="3"><b><i> <a name="viewsearchjobpropertieswiththejobinspector_debug_messages"><span class="mw-headline" id="Debug_messages"> Debug messages </span></a></i></b></font></h4>
<p>If there are errors in your search, these messages (which in previous versions were displayed as banners across the dashboard) are presented as DEBUG messages at the top of the Search Job Inspector window. For example, if there are fields missing from your results, the debug messages will say so.
</p><p><b>Note:</b> You won't see these messages until the search has completed.
</p>
<h3> <a name="viewsearchjobpropertieswiththejobinspector_examples_of_search_job_inspector_output"><span class="mw-headline" id="Examples_of_Search_Job_Inspector_output"> Examples of Search Job Inspector output </span></a></h3>
<p>Here's an example of the execution costs for a dedup search, run over All time:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">* | dedup punct</font></code><br></div>
<p>The search commands component of the <b>Execution costs</b> panel might look something like this:
</p><p><img alt="SearchInspectorExecCostsDedupEx.png" src="images/1/19/SearchInspectorExecCostsDedupEx.png" width="637" height="327"></p><p>The command.search component and everything under it, gives you the performance impact of the <code><font size="2">search</font></code> command portion of your search, which is everything before the pipe character.
</p><p>Then, <code><font size="2">command.prededup</font></code> gives you the performance impact of processing the results of the <code><font size="2">search</font></code> command before passing it into the <code><font size="2">dedup</font></code> command. The Input count of <code><font size="2">command.prededup</font></code> matches the Output count of <code><font size="2">command.search</font></code>, and the Input count of <code><font size="2">command.prededup</font></code> matches the Output count of <code><font size="2">command.prededup</font></code>. In this case, the Output count of <code><font size="2">command.prededup</font></code> should match the number of events returned at the completion of the search (which is the value of resultCount, under <b>Search job properties</b>).
</p>
<h3> <a name="viewsearchjobpropertieswiththejobinspector_answers"><span class="mw-headline" id="Answers"> Answers </span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has about using the Search Job Inspector.
</p>
<a name="managejobsfromtheos"></a><h2> <a name="managejobsfromtheos_manage_jobs_from_the_operating_system"><span class="mw-headline" id="Manage_jobs_from_the_operating_system"> Manage jobs from the operating system</span></a></h2>
<p>This topic explains how to manage search jobs from the operating system. It has instructions for doing so on two platforms:
</p>
<ul><li> *nix
</li><li> Windows
</li></ul><p>For information on how to manage search jobs in Splunk Web, see <a href="#supervisejobswiththejobspage" class="external text">"Supervise your search jobs with the Jobs page"</a> in this manual.
</p>
<h3> <a name="managejobsfromtheos_manage_jobs_in_.2anix"><span class="mw-headline" id="Manage_jobs_in_.2Anix"> Manage jobs in *nix </span></a></h3>
<p>When Splunk Enterprise is running a job, it will manifest itself as a process in the OS called <code><font size="2">splunkd search</font></code>. You can manage the job's underlying processes at the OS command line. 
</p><p>To see the job's processes and its arguments, type:
</p>
<div class="samplecode">
<code><font size="2"><br>&gt; top<br>&gt; c<br></font></code></div>
<p>This will show you all the processes running and all their arguments. 
</p><p>Typing <code><font size="2">ps -ef | grep "search"</font></code> will isolate all the Splunk Enterprise search processes within this list. It looks like this:
</p>
<div class="samplecode">
<code><font size="2"><br>[pie@fflanda ~]$ ps -ef | grep 'search'<br>530369338 71126 59262 &nbsp;&nbsp;0 11:19AM&nbsp;?? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0:01.65 [splunkd pid=59261] search --id=rt_1344449989.64 --maxbuckets=300 --ttl=600 --maxout=10000 --maxtime=0 --lookups=1 --reduce_freq=10 --rf=* --user=admin --pro --roles=admin:power:user &nbsp;AhjH8o/Render TERM_PROGRAM_VERSION=303.2<br>530369338 71127 71126 &nbsp;&nbsp;0 11:19AM&nbsp;?? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0:00.00 [splunkd pid=59261] search --id=rt_1344449989.64 --maxbuckets=300 --ttl=600 --maxout=10000 --maxtime=0 --lookups=1 --reduce_freq=10 --rf=* --user=admin --pro --roles=<br></font></code></div>
<p>There will be two processes for each search job; the second one is a "helper" process used by the <code><font size="2">splunkd</font></code> process to do further work as needed. The main job is the one using system resources. The helper process will die on its own if you kill the main process. 
</p><p>The process info includes:
</p>
<ul><li> the search string (search=)
</li><li> the job ID for that job (id=)
</li><li> the ttl, or length of time that job's artifacts (the output it produces) will remain on disk and available (ttl=)
</li><li> the user who is running the job (user=)
</li><li> what role(s) that user belongs to (roles=)
</li></ul><p>When a job is running, its data is being written to <code><font size="2">$SPLUNK_HOME/var/run/splunk/dispatch/&lt;job_id&gt;/</font></code>
Scheduled jobs (scheduled saved searches) include the saved search name as part of the directory name. 
</p><p>The value of <code><font size="2">ttl</font></code> for a process will determine how long the data remains in this spot, even after you kill a job. 
When you kill a job from the OS, you might want to look at its job ID before killing it if you want to also remove its artifacts.
</p>
<h3> <a name="managejobsfromtheos_manage_jobs_in_windows"><span class="mw-headline" id="Manage_jobs_in_Windows"> Manage jobs in Windows </span></a></h3>
<p>Splunk Enterprise also spawns a separate process for each search job it runs on Windows. Windows does not have a command-line equivalent for the *nix <code><font size="2">top</font></code> command, but there are several ways in which you can view the command line arguments of executing search jobs:
</p>
<ul><li> By using the Process Explorer (http://technet.microsoft.com/en-us/scriptcenter/dd742419.aspx) utility to find the command line of the process performing the search.
</li><li> By using the <code><font size="2">TASKLIST</font></code> and <code><font size="2">Get-WMIObj</font></code> commands in the Powershell (http://technet.microsoft.com/en-us/scriptcenter/dd742419.aspx) command line environment to get the <code><font size="2">ProcessID</font></code> and <code><font size="2">CommandLine</font></code> arguments for a search job.
</li></ul><p>When Splunk Enterprise runs a search, it writes data for that search into <code><font size="2">%SPLUNK_HOME\var\run\splunk\dispatch\&lt;epoch_time_at_start_of_search&gt;.&lt;number_separator&gt;</font></code>. Saved searches are written to similar directories that have a naming convention of "<code><font size="2">admin__admin__search_</font></code>" and a randomly-generated hash of numbers in addition to the epoch time.
</p>
<h3> <a name="managejobsfromtheos_use_the_filesystem_to_manage_jobs"><span class="mw-headline" id="Use_the_filesystem_to_manage_jobs">Use the filesystem to manage jobs</span></a></h3>
<p>Splunk Enterprise allows you to manage jobs via creation and deletion of items in that job's artifact directory:
</p>
<ul><li> To cancel a job, go into that job's artifact directory create a file called 'cancel'.
</li><li> To preserve that job's artifacts (and ignore its ttl setting), create a file called 'save'.
</li><li> To pause a job, create a file called 'pause', and to unpause it, delete the 'pause' file.
</li></ul><h1>Use data summaries to accelerate searches</h1><a name="aboutsummaryindexing"></a><h2> <a name="aboutsummaryindexing_overview_of_summary-based_search_and_pivot_acceleration"><span class="mw-headline" id="Overview_of_summary-based_search_and_pivot_acceleration"> Overview of summary-based search and pivot acceleration</span></a></h2>
<p>Splunk Enterprise is capable of generating reports on massive amounts of data. However, the amount of time it takes to compute such reports is directly proportional to the number of events they summarize. Plainly put, it can take a lot of time to report on very large data sets. If you only have to do this on an occasional basis, the length of time may not be an issue. But running such reports on a regular schedule (or using them as the basis for panels in popular dashboards) is impractical--and this impracticality only increases exponentially as more and more users in your organization use Splunk Enterprise to run similar reports. 
</p><p>To efficiently report on large volumes of data, you need to create data summaries that are populated by the results of background runs of the search upon which the report is based. When you next run the report against data that has been summarized in this manner, it should complete significantly faster, because the summaries are much smaller than the original events from which they were generated. 
</p><p>Splunk Enterprise provides three data summary creation methods: 
</p>
<ul><li> <b>Report acceleration</b> - Uses automatically-created summaries to speed up completion times for certain kinds of reports.
</li><li> <b>Data model acceleration</b> - Uses automatically-created summaries to speed up completion times for pivots.
</li><li> <b>Summary indexing</b> - Enables acceleration of searches and reports through the manual creation of separate summary indexes that exist separately from your main indexes.
</li></ul><h3> <a name="aboutsummaryindexing_report_acceleration"><span class="mw-headline" id="Report_acceleration">Report acceleration</span></a></h3>
<p><b>Report acceleration</b> is used to accelerate individual reports. It's easy to set up for any <b>transforming search</b> or report that runs over a large dataset. 
</p><p>In early versions of Splunk Enterprise, summary indexing was used to accelerate reports. Report acceleration is preferable over summary indexing for the following reasons:
</p>
<ul><li> <b>Kicking off report acceleration is as easy as clicking a checkbox and selecting a time range.</b> Everything after that happens behind the scenes. Subsequent runs of accelerated reports should complete faster as long as they're run (at least partially) within their selected time ranges. For summary indexing you need to design a search to populate the index that includes special search commands; you may need to create the summary index as well. 
</li><li> <b>Splunk Enterprise automatically shares report acceleration summaries with similar searches.</b> Say an employee named Mary sets up report acceleration for a report, which leads to Splunk Enterprise building a summary for it. Then, a few days later, Joe designs a report that is nearly identical to Mary's report, with a few variations. When Joe turns on report acceleration for the report and saves it, Splunk Enterprise automatically assigns it to the summary that was already built for Mary's report, which means that Joe won't need to wait for the summary to be built. 
</li><li> <b>Report acceleration features automatic backfill.</b> If for some reason you have a data interruption, Splunk Enterprise can detect this and automatically update or rebuild your summaries as appropriate.
</li><li> <b>Report acceleration summaries are stored alongside the buckets in your indexes.</b> Summary indexes, on the other hand, reside on the search head. Storing summaries in indexes at the bucket level enables Splunk Enterprise to easily handle the dilemma of late-arriving events--something that can force full rebuilds of summary indexes. Because Splunk Enterprise summaries can simultaneously span both hot and warm buckets, they can effortlessly summarize late-arriving data, because such data can only be added to hot buckets. 
</li></ul><p>It's important to note that <i>not all searches qualify for report acceleration.</i> Only searches that utilize <b>transforming commands</b>--searches that transform their results into statistical tables and charts--are eligible. In addition, any commands used in the search before the transforming command must be <b>streaming commands</b>. This limitation is related to the fact that the summaries are built at the index level rather than the search head.
</p><p>In Splunk Web, you can enable report acceleration for an eligible search when you save it as a report. You can enable report acceleration for an eligible existing report by:
</p>
<ul><li> On the Reports page, expanding a row for a report and clicking <b>Edit</b> to open the <b>Edit Acceleration dialog</b>.  If your report qualifies for acceleration and your permissions allow for report acceleration, the Edit Acceleration dialog will display a checkbox labeled Accelerate Report. Select it. The Summary Range field should appear. Select the range of time over which you plan to run the report, then click <b>Save</b>.
</li><li> in <b>Settings</b> &gt; <b>Searches and reports</b> opening the detail page for a report, clicking <b>Accelerate this search</b> and setting a <b>Summary range</b>.
</li></ul><p>For more information, see "Accelerate reports" in the Reporting Manual. 
</p><p>You use the Report acceleration summaries page in System to review and manage the summaries created through report acceleration. For more information about this, see <a href="#manageacceleratedsearchsummaries" class="external text">"Manage report acceleration"</a> in this manual. This topic also explains how summaries work and includes examples of qualifying and non-qualifying searches.
</p>
<h4><font size="3"><b><i> <a name="aboutsummaryindexing_when_should_i_use_report_acceleration.3f"><span class="mw-headline" id="When_should_I_use_report_acceleration.3F">When should I use report acceleration?</span></a></i></b></font></h4>
<p>Report acceleration is good for just about any slow-completing report that has 100k or more <b>hot bucket</b> events and which meets the qualifying conditions outlined above. 
</p><p>For more information and examples of qualifying and nonqualifying searches see <a href="#manageacceleratedsearchsummaries_how_searches_qualify_for_acceleration" class="external text">"Manage report acceleration"</a> in this manual.
</p>
<h3> <a name="aboutsummaryindexing_data_model_acceleration"><span class="mw-headline" id="Data_model_acceleration">Data model acceleration</span></a></h3>
<p>You use data model acceleration to accelerate all of the fields defined in a data model. When a data model is accelerated, any pivot or report generated by that data model should complete much quicker than it would without the acceleration, even if the data model represents a significantly large dataset. 
</p><p>There are two types of data model acceleration, ad hoc and persistent.  Ad hoc acceleration applies to a single object, is run over all time, and exists for the duration of a user's pivot session, while persistent acceleration is turned on by an admin, happens in the background, and can be scoped to shorter time ranges such as a week or a month.  Persistent acceleration is used any time a search is run against an object in an acceleration-enabled data model.
</p><p>Data model acceleration makes use of Splunk Enterprise's high performance analytics store (HPAS) technology, which, in a manner similar to that of report acceleration, builds summaries alongside the <b>buckets</b> in your indexes. Also like report acceleration, persistent data model acceleration is easy to enable; you just click a checkbox for the data model you want to accelerate and select a summary range. Once you do this, Splunk Enterprise starts building a summary that spans the indicated range. When the summary is complete, any pivot, report, or dashboard panel that uses an accelerated data model object will run against the summary rather than the full array of <code><font size="2">_raw</font></code> whenever possible, and result return time should be improved by a significant amount.
</p><p>There are restrictions for persistent data model acceleration. 
</p>
<ul><li> <b>Persistent data model acceleration only applies to event object hierarchies.</b> Object hierarchies based on root search objects and root transaction objects cannot be accelerated. 
<ul><li> All data model objects can benefit from "ad hoc" data model acceleration. See the subsection on this below.
</li></ul></li><li> <b>Once a data model is persistently accelerated it cannot be edited.</b> After you enable acceleration for a data model, the only way to edit it is to disable its acceleration. 
</li><li> <b>By default only users with</b> <i><b>admin</b></i> <b>permissions can persistently accelerate data models.</b>
</li><li> <b>Data models that are private cannot be persistently accelerated.</b> You must share a data model with users of at least one app to make it eligible for acceleration. 
</li></ul><p>In Splunk Web, you can enable data model acceleration for an eligible data model on the Data Models management page, which you can access in a variety of ways (including navigating to <b>Settings &gt; Data Models</b>). 
</p><p>For more information about enabling persistent data model acceleration, see "Manage data models," in this manual. 
</p><p>For technical background information on data model acceleration and how the high performance analytics store works behind the scenes, see "Accelerate data models" in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutsummaryindexing_ad_hoc_data_model_acceleration"><span class="mw-headline" id="Ad_hoc_data_model_acceleration">Ad hoc data model acceleration</span></a></i></b></font></h4>
<p>Ad hoc data model acceleration is a process that runs behind the scenes for all data model objects that are not "persistently" accelerated beforehand. Unlike persistent data model acceleration, ad hoc data model acceleration applies to all object types, including root search objects, root transaction objects, and their children. 
</p><p>Whenever you build a pivot based on an object that isn't already accelerated, Splunk Enterprise will use ad hoc data model acceleration to build a temporary acceleration summary in a dispatch directory that exists only while you define the pivot in the Pivot Editor. The result is that as you fine-tune a particular pivot in the Pivot Editor you'll find that the pivot performance improves, returning results faster than it did when you first entered the editor. 
</p><p>This isn't as good as persistent data model acceleration, where Splunk Enterprise maintains summaries for the data model objects on an ongoing basis, ensuring speedy performance from the moment you enter the Pivot Editor--but it's still helpful. 
</p><p>For more information about ad hoc data model acceleration see <a href="#acceleratedatamodels" class="external text">"Accelerate data models,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutsummaryindexing_when_should_i_use_persistent_data_model_acceleration.3f"><span class="mw-headline" id="When_should_I_use_persistent_data_model_acceleration.3F">When should I use persistent data model acceleration?</span></a></i></b></font></h4>
<p>If you are struggling with slow-completing pivots in the Pivot Editor and the source objects for those pivots belongs to a topmost root event object hierarchy, you should consider enabling acceleration for that data model. It will ensure that the pivots based on those objects return results faster than they would otherwise.
</p><p>Furthermore, any report or dashboard panel that references a persistently accelerated data model object will also get this acceleration benefit (this will not happen with ad hoc data model acceleration).
</p>
<h4><font size="3"><b><i> <a name="aboutsummaryindexing_report_acceleration_versus_data_model_acceleration"><span class="mw-headline" id="Report_acceleration_versus_data_model_acceleration">Report acceleration versus data model acceleration</span></a></i></b></font></h4>
<p>In general, data model acceleration is faster than report acceleration. However, there are specific kinds of searches that allow report acceleration to come out ahead of data model acceleration.  
</p><p>The more aggregating your transforming search is, the faster it can be. Report acceleration is especially fast when it runs with a search that aggregates down to one item per index bucket. For example, if you get a couple of billion events per day and you just want a monthly count and average, your return report acceleration will be better at this than data model acceleration. In this case you would be maxing out the aggregation capabilities of report acceleration.
</p><p>Report acceleration and data model acceleration go about accelerating searches in similar ways. They both automatically preprocess events on the indexers, and they both create bucket-level acceleration summaries. But the general advantage for data model acceleration lies in how its summaries differ from those created by report acceleration. 
</p><p>Report acceleration is designed to create summaries that include precalculated statistics. Data model acceleration, on the other hand, builds its summaries in a format that is much more efficient to read, enabling Splunk Enterprise to calculate the statistics on demand without giving up performance. So if the search is a relatively complicated one you'll be better off going with data model acceleration.
</p>
<h3> <a name="aboutsummaryindexing_summary_indexing"><span class="mw-headline" id="Summary_indexing">Summary indexing</span></a></h3>
<p><b>Summary indexing</b> is a method you can use to speed up long-running reports that <i>don't</i> qualify for report acceleration, such as reports that use search commands that are not <b>streamable</b> before the transforming command. It's similar to report acceleration in that it involves populating a data summary with the results of a search, but in this case the data summary is actually a special <b>summary index</b> that is built and stored on the search head. This summary index is populated by a scheduled report that is <i>based on</i> the report that you'd like to accelerate and which has <b>Enable</b> selected for summary indexing in <b>Settings &gt; Searches and Reports</b>.
</p><p>For example, if the report you want to accelerate uses a <b>transforming commands</b>, you can populate its summary index with a report that swaps the transforming command with a similar "si-" prefix summary indexing transforming command: <code><font size="2">sichart</font></code>, <code><font size="2">sitimechart</font></code>, <code><font size="2">sistats</font></code>, <code><font size="2">sitop</font></code>, and <code><font size="2">sirare</font></code>.  
</p><p>There are two topics on summary indexing setup, both in this manual. 
</p>
<ul><li> <a href="#usesummaryindexing" class="external text">"Use summary indexing for increased reporting efficiency"</a> shows you the easy way of setting up summary indexes, with scheduled searches that use <code><font size="2">si-</font></code> commands. 
</li><li> <a href="#configuresummaryindexes" class="external text">"Configure summary indexes"</a> covers the tricky and difficult method of summary index setup with <code><font size="2">addinfo</font></code>, <code><font size="2">collect</font></code>, and <code><font size="2">overlap</font></code> commands. You should only use this latter method if you're comfortable setting up searches that take aggregated statistics into account. 
</li></ul><p>Summary indexing volume is not counted against your license, although in the event of a license violation, summary indexing will halt like any other non-internal search behavior.
</p>
<h4><font size="3"><b><i> <a name="aboutsummaryindexing_when_should_i_use_summary_indexing.3f"><span class="mw-headline" id="When_should_I_use_summary_indexing.3F">When should I use summary indexing?</span></a></i></b></font></h4>
<p>If the report you're using qualifies for report acceleration, it's almost always preferable to use that method of speeding up the performance of large data volume searches. 
</p><p>You might want to use summary indexing instead of report acceleration if:
</p>
<ul><li> The primary report you want to accelerate includes nonstreamable commands before a transforming command (just as with report acceleration, reports that populate summary indexes must involve transforming commands). 
</li><li> You would like to run any report against a particular summary index, simply by including <code><font size="2">index=&lt;summary_index_name&gt;</font></code> in your search string. (Under report acceleration, Splunk Enterprise automatically decides whether or not a report can run against a specific data summary.)
</li><li> Your raw data rolls more frequently than your reporting window (e.g. your retention policy is 6 months but you want to power a panel in a dashboard from data for the last year).  Summary indexes generally take up less space than the events they aggregate and can be retained separately and for greater durations.
</li></ul><h3> <a name="aboutsummaryindexing_batch_mode_search"><span class="mw-headline" id="Batch_mode_search"> Batch mode search </span></a></h3>
<p>Batch mode search is a feature that improves the performance and reliability of transforming searches. For transforming searches that don't require the events to be time-ordered, running in batch mode means that the search executes bucket-by-bucket (in batches), rather than over time. In certain reporting cases, this means that the transforming search can complete faster. Additionally, batch mode search improves the reliability for long-running distributed searches, which can fail when an indexer goes down while the search is running. In this case, Splunk Enterprise attempts to reconnect to the missing peer and retry the search.
</p><p>Transforming searches that meet the criteria for batch mode search include:
</p>
<ul><li> Generating and transforming searches (stats, chart, etc.) that do not include the <code><font size="2">localize</font></code> or <code><font size="2">transaction</font></code> commands in the search.
</li><li> Searches that are not real-time and not summarizing searches.
</li><li> Non-distributed searches that are not stateful streaming. (A streamstats search is an example of a stateful streaming search.)
</li></ul><p>Batch mode search is invoked from the configuration file, in the <code><font size="2">[search]</font></code> stanza of <code><font size="2">limits.conf</font></code>. Use the search inspector to determine whether or not a transforming search is running in batch mode.
</p><p>For more information, read <a href="#configurebatchmodesearch" class="external text">"Configure batch mode search"</a> in this manual.
</p>
<a name="manageacceleratedsearchsummaries"></a><h2> <a name="manageacceleratedsearchsummaries_manage_report_acceleration"><span class="mw-headline" id="Manage_report_acceleration"> Manage report acceleration</span></a></h2>
<p>Report acceleration is the easiest way to speed up <b>transforming searches</b> and reports that take a long time to complete because they have to cover a large volume of data. You enable acceleration for a transforming search when you save it as a report. You can also accelerate report-based dashboard panels that use a transforming search. 
</p><p>This topic covers various aspects of report acceleration in more detail. It includes: 
</p>
<ul><li> A quick guide to enabling automatic acceleration for a transforming report.
</li><li> Examples of qualifying and nonqualifying reports (only specific kinds of reports qualify for report acceleration).
</li><li> Details on how Splunk Enterprise creates and maintains report acceleration summaries.
</li><li> An overview of the Report acceleration summaries page in Settings, which you can use to review and maintain the data summaries that Splunk Enterprise uses for automatic report acceleration. 
</li></ul><h3> <a name="manageacceleratedsearchsummaries_enabling_report_acceleration"><span class="mw-headline" id="Enabling_report_acceleration">Enabling report acceleration</span></a></h3>
<p>You can enable report acceleration when you create a report, or later, after the report is created.
</p><p>For a more thorough description of this procedure, see "Create and edit reports," in the Reporting Manual.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_enabling_report_acceleration_when_you_create_a_report"><span class="mw-headline" id="Enabling_report_acceleration_when_you_create_a_report">Enabling report acceleration when you create a report</span></a></i></b></font></h4>
<p>To enable report acceleration for a qualifying slow-running search, all you have to do is:
</p><p><b>1.</b> In Search, run the search. 
</p><p><b>2.</b> Click <b>Save As</b> and select <i>Report.</i> This brings up the Save As Report dialog. 
</p><p><b>3.</b> Give your report a <b>Name</b> and optionally, a <b>Description.</b> Click <b>Save</b> to save the search as a report. This takes you to the Your Report Has Been Created dialog. 
</p><p><b>4.</b> Click <b>Acceleration</b> to accelerate your report. This will take you to the Edit Acceleration dialog.
</p><p><b>Note:</b> The Edit Acceleration dialog will say "This report cannot be accelerated" if:
</p>
<ul><li> your permissions don't enable you to accelerate reports. Your <b>role</b> must have the <code><font size="2">schedule_search</font></code> and <code><font size="2">accelerate_search</font></code> <b>capabilities</b>. 
</li><li> your report does not qualify for report acceleration. For more information, see the subtopic <a href="#manageacceleratedsearchsummaries_how_reports_qualify_for_acceleration" class="external text">"How reports qualify for acceleration,"</a> below. 
</li></ul><p><b>5.</b> If your report qualifies for acceleration and your permissions allow for report acceleration, the Edit Acceleration dialog will display a checkbox labeled <b>Accelerate Report</b>. Select it.
</p><p><b>6.</b> The <b>Summary Range</b> field should appear. Select from <i>1 Day</i>, <i>7 Days</i>, <i>1 Month</i>, <i>3 Months</i>, <i>1 Year</i>, or <i>All Time</i> depending on the range of time over which you plan to run the report. For example, if you only plan to run the report over periods of time <i>within</i> the last seven days, choose <i>7 Days.</i> 
</p><p><b>7.</b> Click <b>Save</b> to save your acceleration settings.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_enabling_report_acceleration_for_an_existing_report"><span class="mw-headline" id="Enabling_report_acceleration_for_an_existing_report">Enabling report acceleration for an existing report</span></a></i></b></font></h4>
<p>To enable report acceleration for an existing report, find the report on the Reports page and expand its row to uncover the report detail information. The value for <b>Acceleration</b> will be <i>Disabled</i> if the report hasn't been enabled yet. Click <b>Edit</b> to open the Edit Acceleration dialog. Follow steps 5-7 in the list above to define and save your report acceleration settings.
</p><p><b>Note:</b> The Edit Acceleration dialog will say "This report cannot be accelerated" if your permissions don't enable you to accelerate reports or if the report does not qualify for report acceleration. 
</p><p>Alternatively you can enable report acceleration for an existing report at <b>Settings &gt; Searches and reports.</b>
</p>
<h3> <a name="manageacceleratedsearchsummaries_after_you_enable_acceleration_for_a_report"><span class="mw-headline" id="After_you_enable_acceleration_for_a_report">After you enable acceleration for a report</span></a></h3>
<p>After you enable acceleration for your report, Splunk Enterprise will begin building a report acceleration summary for the report if it determines that the report would benefit from summarization. See the subtopic <a href="#manageacceleratedsearchsummaries_conditions_under_which_splunk_enterprise_cannot_build_or_update_a_summary" class="external text">"Conditions under which Splunk Enterprise cannot build or update a summary,"</a> below, if you enable acceleration for the report and find that its summary is not being constructed (if you go to <b>Settings &gt; Report Acceleration Summaries,</b> you'll notice that the <b>Summary Status</b> is stuck at 0% complete for an extended amount of time). 
</p><p>Once Splunk Enterprise builds the summary, future runs of an accelerated report should complete faster than they did before. See the subtopics below for more information on summaries and how they work.
</p><p><b>Note:</b> When you are running reports in Search, keep in mind that report acceleration only works for reports that have <b>Search Mode</b> set to <i>Smart</i> or <i>Fast.</i> If you select the <i>Verbose</i> search mode for a report that benefits from report acceleration, it will run as slow as it would if no summary existed for it. ('<i>Search Mode</i> does not affect searches powering dashboard panels.) For more information about the <b>Search Mode</b> settings, see "Set search mode to adjust your search experience" in the Search Manual.
</p>
<h3> <a name="manageacceleratedsearchsummaries_how_reports_qualify_for_acceleration"><span class="mw-headline" id="How_reports_qualify_for_acceleration">How reports qualify for acceleration</span></a></h3>
<p>For a report to qualify for acceleration its search string must use a <b>transforming command</b> (such as <code><font size="2">chart</font></code>, <code><font size="2">timechart</font></code>, <code><font size="2">stats</font></code>, and <code><font size="2">top</font></code>). 
</p><p>In addition, if there are any other commands <i>before</i> the first transforming command they must be <b>streamable</b>, which means that they apply a transformation to each event returned by the search. For example, the <code><font size="2">rex</font></code> command is streamable; it is applied to each event returned by the report and extracts fields when they match the regular expression provided with the command. Other common streaming commands include <code><font size="2">search</font></code>, <code><font size="2">where</font></code>, <code><font size="2">eval</font></code>, <code><font size="2">regex</font></code>, <code><font size="2">bin</font></code> (as long as you provide an explicit span), and <code><font size="2">lookup</font></code> (as long as it is <i>not</i> <code><font size="2">local=t</font></code>).
</p><p><b>Note:</b> You can use nonstreaming commands <i>after</i> the first transforming command and still have the report qualify for automatic acceleration. It's just nonstreaming commands <i>before</i> the first transforming command that disqualify the report.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_examples_of_qualifying_search_strings"><span class="mw-headline" id="Examples_of_qualifying_search_strings">Examples of qualifying search strings</span></a></i></b></font></h4>
<p>Here are examples of search strings that qualify for report acceleration:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal | stats count by sourcetype</font></code><br></div> 
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_audit search=* | rex field=search "'(?.*)'" | chart count by user search</font></code><br></div> 
<div class="inlineQuery splunk_search_4_3"><code><font size="2">test foo bar | bin _time span=1d | stats count by _time x y</font></code><br></div> 
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_audit | lookup usertogroup user OUTPUT group | top searches by group</font></code><br></div> 
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_examples_of_nonqualifying_search_strings"><span class="mw-headline" id="Examples_of_nonqualifying_search_strings">Examples of nonqualifying search strings</span></a></i></b></font></h4>
<p>And here are examples of search strings that do <i>not</i> qualify for report acceleration:
</p><p><b>Reason the following search string fails:</b> This is a simple event search, with no transforming command.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal metrics group=per_source_thruput</font></code><br></div> 
<p><b>Reason the following search string fails:</b> <code><font size="2">eventstats</font></code> is not a transforming command.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal sourcetype=splunkd *thruput | eventstats avg(kb) as avgkb by group</font></code><br></div> 
<p><b>Reason the following search string fails:</b> <code><font size="2">transaction</font></code> is not a streaming command. Other non-streaming commands include <code><font size="2">dedup</font></code>, <code><font size="2">head</font></code>, <code><font size="2">tail</font></code>, and any other search command that is not on the list of <b>streaming commands</b>.  
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal | transaction user maxspan=30m | timechart avg(duration) by user</font></code><br></div>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_search_strings_that_qualify_for_report_acceleration_but_won.27t_get_much_out_of_it"><span class="mw-headline" id="Search_strings_that_qualify_for_report_acceleration_but_won.27t_get_much_out_of_it">Search strings that qualify for report acceleration but won't get much out of it</span></a></i></b></font></h4>
<p>In addition, you can have reports that technically qualify for report acceleration, but which may not be helped much by it. This is often the case with reports with high data cardinality--something you'll find when there are two or more transforming commands in the search string and the first transforming command generates many (50k+) output rows. For example:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=* | stats count by id | stats avg(count) as avg, count as distinct_ids</font></code><br></div>
<h3> <a name="manageacceleratedsearchsummaries_understand_report_acceleration_summaries"><span class="mw-headline" id="Understand_report_acceleration_summaries">Understand report acceleration summaries</span></a></h3>
<p>The following subtopics cover various aspects of report summaries to give you a better idea of how they work.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_set_report_acceleration_summary_time_ranges"><span class="mw-headline" id="Set_report_acceleration_summary_time_ranges">Set report acceleration summary time ranges</span></a></i></b></font></h4>
<p>When you enable acceleration for a report, Splunk Enterprise can begin generating a summary that spans a specific range of time, such as the past day, the past seven days, the past month, the past year, and so on. You determine this time range when you choose a value from the <b>Summary Range</b> list. 
</p><p>For example, if you set a summary range of <i>7 days</i>, Splunk Enterprise can create a data summary for the report that approximately covers the past seven days at any point in time. Once Splunk Enterprise finishes building the summary, going forward the report summarization process ensures that the summary always covers the selected range, removing older summary data that passes out of the range. 
</p><p><b>Note:</b> We say that the <b>Summary Range</b> indicates the <i>approximate</i> range of time that a summary spans. At times, summaries will have a store of data that that slightly exceeds their summary range, but they never fail to meet it, unless it's the first time the summary is being built.
</p><p>In the future, when you run the accelerated report over a range that falls within the preceding week, Splunk Enterprise will run the report against its summary rather than the source index (the index the report originally searched against). In most cases the summary will have far less data than the source index, and that--along with the fact that the report summary already contains precomputed results for portions of the search pipeline--means that the report should complete faster than it did on its initial run. 
</p><p>If you run the accelerated report over a period of time that is only partially covered by the summary, the report won't complete quite as fast. This is because Splunk Enterprise has to run at least part of the report over raw data in the main Splunk Enterprise index. For example, if the <b>Summary Range</b> setting for a report is <i>1 week</i> and you run the report over the last 9 days, Splunk Enterprise only gets acceleration benefits for the portion of the report that covers the past 7 days. The portion of the report that runs over days 8 and 9 will run at normal speed. 
</p><p>Keep this in mind when you set the <b>Summary Range</b> value. If you always plan to run a report over time ranges that exceed the past 7 days, but don't extend further out than 30 days, you should select a <b>Summary Range</b> of <i>1 month</i> when you set up report acceleration for that report.
</p><p><b>Note:</b> Splunk Enterprise usually only generates a summary for a report when the number of events in the <b>hot bucket</b> covered by the chosen <b>Summary Range</b> is equal to or greater than 100k. For more information, see the subtopic below titled <a href="#manageacceleratedsearchsummaries_conditions_under_which_splunk_enterprise_cannot_build_or_update_a_summary" class="external text">"Conditions under which Splunk Enterprise cannot build or update a summary."</a>
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_how_splunk_enterprise_builds_report_acceleration_summaries"><span class="mw-headline" id="How_Splunk_Enterprise_builds_report_acceleration_summaries">How Splunk Enterprise builds report acceleration summaries</span></a></i></b></font></h4>
<p>When you enable acceleration for an eligible report and Splunk Enterprise determines that it will build a summary for the report, it begins running the report to populate the summary with data. When the summary is complete, Splunk Enterprise continues running the report on a 10 minute interval to keep the summary up to date. Each update ensures that the entire configured time range is covered without a significant gap in data. This method of summary building also ensures that late-arriving data will be summarized without complication. 
</p><p>It can take Splunk Enterprise some time to build a report summary. The creation time depends on the number of events involved, the overall summary range, and the length of the summary timespans (chunks) in the summary. 
</p><p>You can track progress toward summary completion on the Report Acceleration Summaries page in Settings. On the main page you can check the <b>Summary Status</b> to see what percentage of the summary is complete.
</p><p><b>Note:</b> Just like ordinary <b>scheduled reports</b>, the reports that automatically populate report acceleration summaries on a regular schedule are managed by the report <b>scheduler</b>. By default, the report scheduler is allowed to allocate up to 25% of its total search bandwidth for report acceleration summary creation. 
</p><p>The report scheduler also runs reports that populate report acceleration summaries at the lowest priority. If these "auto-summarization" reports have a scheduling conflict with user-defined alerts, summary-index reports, and regular scheduled reports, the user-defined reports always get run first. This means that you may run into situations where a summary isn't being created or updated because Splunk Enterprise is busy running more prioritized reportes.
</p><p>For more information about the search scheduler see the topic "Configure the priority of scheduled reports," in the Reporting Manual.
</p><p><b>Summary data is divided into chunks with regular timespans</b>
</p><p>As Splunk Enterprise builds and maintains the summary, it breaks the data up into chunks to ensure statistical accuracy, according to a "timespan" that Splunk Enterprise determines automatically based on the overall summary range. For example, if the summary range for a report is <i>1 month</i> then Splunk Enterprise will likely give it a timespan of <i>1d</i> (one day). 
</p><p>A summary timespan represents the smallest time range for which the summary contains statistically accurate data. If you are running a report against a summary that has a one hour timespan, the time range you choose for the report should be evenly divisible by that timespan if you want the report to use the summarized data. When you are dealing with a <i>1h</i> timespan, a report that runs over the past 24 hours would work fine, but a report running over the past 90 minutes might not be able to use the summarized data.
</p><p><b>Summaries can have multiple timespans</b>
</p><p>Splunk Enterprise will give report acceleration summaries multiple timespans if necessary to make them as searchable as possible. For example, a summary with a summary range of <i>3 months</i> can have timespans of <i>1mon</i> and <i>1d</i>. In addition, Splunk Enterprise may assign extra timespans when the summary spans more than one index <b>bucket</b> and the buckets cover very different amounts of time. For example, if a summary spans two buckets, and the first bucket spans two months and the next bucket spans 40 minutes, the summary will have chunks with <i>1d</i> and <i>1m</i> timespans. 
</p><p><b>You can manually set summary timespans (but we don't recommend it)</b>
</p><p>As we mentioned before, Splunk Enterprise determines timespans for summaries automatically, but you can adjust them manually at the report level in <code><font size="2">savedsearches.conf</font></code> (look for the <code><font size="2">auto_summarize.timespan</font></code> parameter). If you do set your summary timespans manually, keep in mind that very small timespans can result in extremely slow summary creation times, especially if the summary range is long. On the other hand, large timespans can result in quick-building summaries that cannot not be used by reports with short time ranges. <i>In almost all cases, for optimal performance and usability it's best to let Splunk Enterprise determine summary timespans.</i>
</p><p><b>The way that Splunk Enterprise gathers data for accelerated reports can result in a lot of files over a very short amount of time</b>
</p><p>Because report acceleration summaries gather information for multiple timespans, many files can be created for the same summary over a short amount of time. If file and folder management is an issue for you, this is something to be aware of.
</p><p>For every accelerated report and search head combination in your system, you get:
</p>
<ul><li> 2 files (data + info) for each 1-day span
</li><li> 2 files (data + info) for each 1-hour span
</li><li> 2 files (data + info) for each 10-minute span
</li><li> SOMETIMES: 2 files (data + info) for each 1-minute span
</li></ul><p>So if you have an accelerated report with a 30-day range and a 10 minute granularity, the result is:
</p><p><code><font size="2">(30x1 + 30x24 + 30x144)x2 = 10,140 files</font></code>
</p><p>If you switch to a 1 minute granularity, the result is:
</p><p><code><font size="2">(30x1 + 30x24 + 30x144 + 30x1440)x2 = 96,540 files</font></code>
</p><p>If you use Deployment Monitor, which ships with 12 accelerated reports by default, an immediate backfill could generate between 122k and 1.2 million files on each indexer in <code><font size="2">$SPLUNK_HOME/var/lib/splunk/_internaldb/summary</font></code>, for each search-head on which it is enabled.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_where_report_acceleration_summaries_are_created_and_stored"><span class="mw-headline" id="Where_report_acceleration_summaries_are_created_and_stored">Where report acceleration summaries are created and stored</span></a></i></b></font></h4>
<p>Splunk Enterprise creates each accelerated report summary on the indexer, parallel to the bucket or buckets that cover the range of time over which the summary spans. For example, for the "index1" index, they reside under <code><font size="2">$SPLUNK_HOME/var/lib/splunk/index1/summary</font></code>. 
</p><p><b>Note:</b> Data model acceleration summaries are stored in the same manner, but in directories labeled <code><font size="2">datamodel_summary</font></code> instead of <code><font size="2">summary</font></code>.
</p><p>Keep in mind that report acceleration summaries are <i>not</i> indexes. <b>Indexer clusters</b> do not replicate report acceleration summaries or otherwise accommodate their presence. If <b>primacy</b> gets reassigned from the original copy of a bucket to another (for example, because the peer holding the primary copy fails), the summary does not move to the peer with new primary copy. Therefore, it becomes unavailable. It will not be available again until the next time Splunk Enterprise attempts to update the summary, finds that it is missing, and regenerates it.
</p><p><b>Note:</b> Until Splunk Enterprise regenerates your summaries, the results of your accelerated reports will be correct, but they will run slower.
</p><p>For more information about how indexer clusters handle report acceleration summaries, see "How search works in an indexer cluster" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_configure_size-based_retention_for_report_acceleration_summaries"><span class="mw-headline" id="Configure_size-based_retention_for_report_acceleration_summaries">Configure size-based retention for report acceleration summaries</span></a></i></b></font></h4>
<p>Do you set size-based retention limits for your indexes so they do not take up too much disk storage space? By default, report acceleration summaries can theoretically take up an unlimited amount of disk space. This can be a problem if you're also locking down the maximum data size of your indexes or index volumes. The good news is that you can optionally configure similar retention limits for your report acceleration summaries. 
</p><p><b>Note:</b> Although report acceleration summaries are unbounded in size by default, they are tied to raw data in your warm and hot index buckets and will age along with it. When events pass out of the hot/warm buckets into cold buckets, they are likewise removed from the related summaries.
</p><p><b>Important:</b> Before attempting to configure size-based retention for your report acceleration summaries, you should first understand how to use volumes to configure limits on index size across indexes, as many of the principles are the same. For more information, see "Configure index size" in <i>Managing Indexers and Clusters</i>.
</p><p>By default, report acceleration summaries live alongside the hot and warm buckets in your index at <code><font size="2">homePath/../summary/</font></code>. In other words, if in <code><font size="2">indexes.conf</font></code> the <code><font size="2">homePath</font></code> for the hot and warm buckets in your index is:
</p>
<div class="samplecode">
<code><font size="2"><br>homePath = /opt/splunk/var/lib/splunk/index1/db<br></font></code>
</div>
<p>Then summaries that map to buckets in that index will be created at:
</p>
<div class="samplecode">
<code><font size="2"><br>homePath/opt/splunk/var/lib/splunk/index1/summary<br></font></code>
</div>
<p>Here are the steps you take to set up size-based retention for the summaries in that index. All of the configurations described are made within <code><font size="2">indexes.conf</font></code>.
</p><p><b>1.</b> Review your volume definitions and identify a volume (or volumes) that will be the home for your report acceleration summary data. 
</p>
<dl><dd> If the right volume doesn't exist, create it.
</dd></dl><dl><dd> If your want to piggyback on a preexisting volume that controls your indexed raw data, you might have that volume reference the filesystem that hosts your hot and warm bucket directories, because your report acceleration summaries will live alongside it.
</dd></dl><dl><dd> However, you could also place your report acceleration summaries in their own filesystem if you want. The only rule here is: You can only reference one filesystem per volume, but you can reference multiple volumes per filesystem.
</dd></dl><p><b>2.</b> For the volume that will be the home for your report acceleration data, add the <code><font size="2">maxVolumeDataSizeMB</font></code> parameter to set the volume's maximum size.
</p>
<dl><dd> This lets you manage size-based retention for report acceleration summary data across your indexes. 
</dd></dl><p><b>3.</b> Update your index definitions. 
</p>
<dl><dd> Set the <code><font size="2">summaryHomePath</font></code> for each index that deals with summary data. Ensure that the path is  referencing the summary data volume that you identified in Step 1. 
</dd></dl><dl><dd> <code><font size="2">summaryHomePath</font></code> overrides the default path for the summaries. Its value should compliment the <code><font size="2">homePath</font></code> for the hot and warm buckets in the indexes. For example, here's the <code><font size="2">summaryHomePath</font></code> that compliments the <code><font size="2">homePath</font></code> value identified above:
</dd></dl><div class="samplecode">
<code><font size="2"><br>summaryHomePath = /opt/splunk/var/lib/splunk/index1/summary<br></font></code>
</div>
<p>This example configuration shows data size limits being set up on a global, per-volume, and per-index basis.
</p>
<div class="samplecode">
<code><font size="2"><br>#########################<br># Global settings<br>#########################<br><br># Inheritable by all indexes: No hot/warm bucket can exceed 1 TB.<br># Individual indexes can override this setting. The global <br># summaryHomePath setting indicates that all indexes that do not explicitly<br># define a summaryHomePath value will write report acceleration summaries <br># to the small_indexes # volume. <br>[global]<br>homePath.maxDataSizeMB = 1000000<br>summaryHomePath = volume:small_indexes/$_index_name/summary<br><br>#########################<br># Volume definitions<br>#########################<br><br># This volume is designed to contain up to 100GB of summary data and other <br># low-volume information.<br>[volume:small_indexes]<br>path = /mnt/small_indexes<br>maxVolumeDataSizeMB = 100000 <br><br># This volume handles everything else. It can contain up to 50 <br># terabytes of data.<br>[volume:large_indexes]<br>path = /mnt/large_indexes<br>maxVolumeDataSizeMB = 50000000<br><br>#########################<br># Index definitions<br>#########################<br><br># The report_acceleration and rare_data indexes together are limited to 100GB, per the <br># small_indexes volume.<br>[report_acceleration]<br>homePath = volume:small_indexes/report_acceleration/db<br>coldPath = volume:small_indexes/report_acceleration/colddb<br>thawedPath = $SPLUNK_DB/summary/thaweddb<br>summaryHomePath = volume:small_indexes/report_acceleration/summary<br>maxHotBuckets = 2<br><br>[rare_data] <br>homePath = volume:small_indexes/rare_data/db<br>coldPath = volume:small_indexes/rare_data/colddb<br>thawedPath = $SPLUNK_DB/rare_data/thaweddb<br>summaryHomePath = volume:small_indexes/rare_data/summary<br>maxHotBuckets = 2<br><br># Splunk constrains the main index and any other large volume indexes that <br># share the large_indexes volume to 50TB, separately from the 100GB of the <br># small_indexes volume. Note that these indexes both use summaryHomePath to <br># direct summary data to the small_indexes volume.<br>[main]<br>homePath = volume:large_indexes/main/db<br>coldPath = volume:large_indexes/main/colddb<br>thawedPath = $SPLUNK_DB/main/thaweddb<br>summaryHomePath = volume:small_indexes/main/summary<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br><br># Some indexes reference the large_indexes volume with summaryHomePath, <br># which means their summaries are created in that volume. Others do not <br># explicitly reference a summaryHomePath, which means that Splunk Enterprise <br># directs their summaries to the small_indexes volume, per the [global] stanza.<br>[idx1_large_vol]<br>homePath=volume:large_indexes/idx1_large_vol/db<br>coldPath=volume:large_indexes/idx1_large_vol/colddb<br>homePath=$SPLUNK_DB/idx1_large/thaweddb<br>summaryHomePath = volume:large_indexes/idx1_large_vol/summary<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br>frozenTimePeriodInSecs = 2592000<br><br>[other_data]<br>homePath=volume:large_indexes/other_data/db<br>coldPath=volume:large_indexes/other_data/colddb<br>homePath=$SPLUNK_DB/other_data/thaweddb<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br></font></code>
</div>
<p>When a report acceleration summary volume reaches its size limit, the Splunk Enterprise volume manager removes the oldest summary in the volume to make room. When the volume manager removes a summary, it places a marker file inside its corresponding bucket. This marker file tells the summary generator not to rebuild the summary.
</p><p>Data model acceleration summaries have a default volume called <code><font size="2">_splunk_summaries</font></code> that is referenced by all indexes for the purpose of data model acceleration summary size-based retention. By default this volume has no <code><font size="2">maxVolumeDataSizeMB</font></code> setting, meaning it has infinite retention.
</p><p>You can use this preexisting volume to manage data model acceleration summaries and report acceleration summaries in one place. You would need to:
</p>
<ul><li> have the <code><font size="2">summaryHomePath</font></code> reference for your report acceleration summaries reference the <code><font size="2">_splunk_summaries</font></code> volume.
</li><li> set a <code><font size="2">maxVolumeDataSizeMB</font></code> value for <code><font size="2">_splunk_summaries</font></code>.
</li></ul><p>For more information about size-based retention for data model acceleration summaries, see <a href="#acceleratedatamodels" class="external text">"Accelerate data models"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_multiple_reports_for_a_single_summary"><span class="mw-headline" id="Multiple_reports_for_a_single_summary">Multiple reports for a single summary</span></a></i></b></font></h4>
<p>A single report summary can be associated with multiple searches when the searches meet the following two conditions. 
</p>
<ul><li> The searches are identical up to and including the first reporting command. 
</li><li> The searches have the same owner and are associated with the same app.   
</li></ul><p>Searches that meet the first condition, but which have different owners or belong to different apps, cannot share the same summary.
</p><p>For example, these two reports use the same report acceleration summary.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* status=2* | stats count by price</font></code><br></div>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* status=2* | stats count by price | eval discount = price/2</font></code><br></div>  
<p>These two reports use different report acceleration summaries.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* status=2* | stats count by price</font></code><br></div>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype=access_* status=2* | timechart by price</font></code><br></div>
<p>Two reports that are identical except for syntax differences that do not cause one to output different results than the other can also use the same summary. 
</p><p>These two searches use the same report acceleration summary.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype = access_* status=2* | fields - clientip, bytes | stats count by price</font></code><br></div>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">sourcetype = access_* status=2* | fields - bytes, clientip | stats count by price</font></code><br></div>
<p>You can also run non-saved searches against the summary, as long as the basic search matches the populating saved search up to the first reporting command and the search time range fits within the summary span. 
</p><p>You can see which searches are associated with your summaries by navigating to <b>Manager &gt; Report Acceleration Summaries</b>. See "<a href="#manageacceleratedsearchsummaries_use_the_report_acceleration_summaries_page" class="external text">Use the Report Acceleration Summaries Page</a>" in this topic.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_conditions_under_which_splunk_enterprise_cannot_build_or_update_a_summary"><span class="mw-headline" id="Conditions_under_which_Splunk_Enterprise_cannot_build_or_update_a_summary">Conditions under which Splunk Enterprise cannot build or update a summary</span></a></i></b></font></h4>
<p>Splunk Enterprise cannot build a summary for a report when the data you want it to summarize meets the following conditions:
</p>
<ul><li> The number of events in the <b>hot bucket</b> covered by the chosen <b>Summary Range</b> is less than than 100k. You see a <b>Summary Status</b> warning that says <i>Not enough data to summarize</i> when this condition exists.
</li><li> Splunk Enterprise estimates that the completed summary will not exceed 10% of the total size of the total bucket size in your deployment. When Splunk Enterprise makes this estimation, it suspends the summary for 24 hours (you will see a <b>Summary Status</b> of <i>Suspended</i>). 
</li></ul><p>You can see the <b>Summary Status</b> for a summary in <b>Settings &gt; Report Acceleration Summaries.</b> 
</p><p>If you define a summary and Splunk Enterprise does not create it because these conditions are met, it continues to periodically check to see if conditions improve. When Splunk Enterprise finds that the summary has 100k or more hot bucket events in its range and will not be overlarge when complete, it begins creating or updating the summary.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_how_can_i_tell_if_a_report_is_using_its_summary.3f"><span class="mw-headline" id="How_can_I_tell_if_a_report_is_using_its_summary.3F">How can I tell if a report is using its summary?</span></a></i></b></font></h4>
<p>The obvious clue that a report is using its summary is if you run it and find that its report performance has improved (it completes faster than it did before).
</p><p>But if that's not enough, or if you aren't sure if there's a performance improvement, you can check the <a href="#viewsearchjobpropertieswiththejobinspector" class="external text">Search Job Inspector</a> for a debug message that indicates whether the report is using a specific report acceleration summary. Here's an example: 
</p>
<div class="samplecode">
<code><font size="2">DEBUG: [thething] Using summaries for search, summary_id=246B0E5B-A8A2-484E-840C-78CB43595A84_search_admin_b7a7b033b6a72b45, maxtimespan=</font></code></div>
<p>In this example, that last string of numbers, <code><font size="2">b7a7b033b6a72b45</font></code>, corresponds to the <b>Summary ID</b> displayed on the Report Acceleration Summaries page.
</p>
<h3> <a name="manageacceleratedsearchsummaries_use_the_report_acceleration_summaries_page"><span class="mw-headline" id="Use_the_Report_Acceleration_Summaries_page">Use the Report Acceleration Summaries page</span></a></h3>
<p>You can review the your report acceleration summaries and even manage various aspects of them with the Report Acceleration Summaries page in Settings. Go to <b>Settings &gt; Report Acceleration Summaries.</b>
</p><p><img alt="Settings report acceleration summaries.png" src="images/6/64/Settings_report_acceleration_summaries.png" width="650" height="363"></p><p>The main Report Acceleration Summaries page enables you to see basic information about the summaries that you have permission to view. 
</p><p>The <b>Summary ID</b> and <b>Normalized Summary ID</b> columns display the unique hashes that Splunk Enterprise assigns to those summaries. The IDs are derived from the remote search string for the report. They are used as part of the directory name that Splunk Enterprise creates for the summary files. Click a summary ID or normalized summary ID to view summary details and perform summary management actions. For more information about this detail view, see the subtopic <a href="#manageacceleratedsearchsummaries_review_summary_details" class="external text">"Review summary details,"</a> below.
</p><p>The <b>Reports Using Summary</b> column lists the saved reports that are associated with each of your summaries. It indicates that each report associated with a particular summary will get report acceleration benefits from that summary. Click on a report title to drill down to the detail page for that report.  
</p><p>Check <b>Summarization Load</b> to get an idea of the effort that Splunk Enterprise has to put into updating the summary. It's calculated by dividing the number of seconds it takes to run the populating report by the interval of the populating report. So if the report runs every 10 minutes (600 seconds) and takes 30 seconds to run, the summarization load is 0.05. If the summarization load is high and the <b>Access Count</b> for the summary shows that the summary is rarely used or hasn't been used in a long time, you might consider deleting the summary to reduce the strain on your system. 
</p><p>The <b>Summary Status</b> column reports on the general state of the summary and tells you when it was last updated with new data. Possible status values are <i>Complete,</i> <i>Pending,</i> <i>Suspended,</i> <i>Not enough data to summarize,</i> or the percentage of the summary that is complete at the moment. If you want to update a summary to the present moment, click its summary ID to go to its detail page  and click <b>Update</b> to kick off a new summary-populating report. 
</p><p>If the <b>Summary Status</b> is <i>Pending</i> it means that the summary may be slightly outdated and the search head is about to schedule a new update job for it.
</p><p>If the <b>Summary Status</b> is <i>Suspended</i> it means that Splunk Enterprise has determined that the report is not worth summarizing because it will create a summary that is too large. Splunk Enterprise projects the size of the summary that a report will create--if it determines that the summary will be larger than 10% of the index buckets it spans, it will suspend the summary for 24 hours. There's no point to creating a summary, for example, if the summary contains 90% of the data in the full index. 
</p><p>You cannot override summary suspension, but you can adjust the length of time that Splunk Enterprise suspends summaries for by changing the value of the <code><font size="2">auto_summarize.suspend_period</font></code> attribute in <code><font size="2">savedsearches.conf,</font></code>
</p><p>If the <b>Summary Status</b> reads <i>Not enough data to summarize,</i> it means that Splunk Enterprise is not currently generating or updating a summary because the reports associated with it are returning less than 100k events from the <b>hot buckets</b> covered by the summary range. For more information, see the subtopic above titled <a href="#manageacceleratedsearchsummaries_conditions_under_which_splunk_enterprise_cannot_build_or_update_a_summary" class="external text">"Conditions under which Splunk Enterprise cannot build or update a summary."</a>
</p>
<h3> <a name="manageacceleratedsearchsummaries_review_summary_details"><span class="mw-headline" id="Review_summary_details">Review summary details</span></a></h3>
<p>You use the summary details page to view detail information about a specific summary and to initiate actions for that summary. You get to this page by clicking a <b>Summary ID</b> on the Report Acceleration Summaries page in Settings.
</p><p><img alt="5.0-Report Acceleration Summaries-Summary Detail.jpg" src="images/f/fb/5.0-Report_Acceleration_Summaries-Summary_Detail.jpg" width="678" height="538"></p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_summary_status"><span class="mw-headline" id="Summary_Status">Summary Status</span></a></i></b></font></h4>
<p>Under <b>Summary Status</b> you'll see basic status information for the summary. It mirrors the <b>Summary Status</b> listed on the Report Acceleration Summaries page (see above) but also provides information about the verification status of the summary. 
</p><p>If you want to update a summary to the present moment click the <b>Update</b> button under <b>Actions</b> to kick off a new summary-populating report.
</p><p>No verification status will appear here if you've never initiated verification for the summary. After you initiate verification this status shows the verification percentage complete. Otherwise this status shows the results of the last attempt at summary verification; the possible values are <i>Verified</i> and <i>Failed to verify,</i> with an indication of how far back in the past this attempt took place.
</p><p>For more information about summary verification, see <a href="#manageacceleratedsearchsummaries_verify_a_summary" class="external text">"Verify a summary,"</a> below.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_reports_using_the_summary"><span class="mw-headline" id="Reports_using_the_summary">Reports using the summary</span></a></i></b></font></h4>
<p>The <b>Reports Using This Summary</b> section lists the reports that are associated with the summary, along with their owner and home app. Click on a report title to drill down to the detail page for that report. Similar reports (reports with search strings that all transform the same root search with different transforming commands, for example) can use the same summary.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_summary_details"><span class="mw-headline" id="Summary_details">Summary details</span></a></i></b></font></h4>
<p>The <b>Details</b> section provides a set of metrics about the summary. 
</p><p><b>Summarization Load</b> and <b>Access Count</b> are mirrored from the main <b>Report Acceleration Summaries</b> page. See the subtopic <a href="#manageacceleratedsearchsummaries_use_the_report_acceleration_summaries_page" class="external text">"Use the Report Acceleration Summaries page,"</a> above, for more information. 
</p><p><b>Size on Disk</b> shows you how much space the summary takes up in terms of storage. You can use this metric along with the <b>Summarization Load</b> and <b>Access Count</b> to determine which summaries ought to be deleted. 
</p><p><b>Note:</b> If the <b>Size</b> value stays at <i>0.00MB</i> it means that Splunk Enterprise is not currently generating this summary because the reports associated with it either don't have enough events (at least 100k hot bucket events are required) or the projected summary size is over 10% of the bucket with which the report is associated. Splunk Enterprise will periodically check this report and automatically create a summary for it when it meets the criteria for summary creation.  
</p><p><b>Summary range</b> is the range of time spanned by the summary, always relative to the present moment. You set this up when you define the report that populates the summary. For more information, see the subtopic <a href="#manageacceleratedsearchsummaries_set_report_acceleration_summary_time_ranges" class="external text">"Set report acceleration summary time ranges,"</a> above.
</p><p><b>Timespans</b> displays the size of the data chunks that make up the summary. A summary timespan represents the smallest time range for which the summary contains statistically accurate data. So if you are running a report against a summary that has a one hour timespan, the time range you choose for the report should be evenly divisible by that timespan if you want to get good results. So if you are dealing with a <i>1h</i> timespan, a report over the past 24 hours would work fine, but a report over the past 90 minutes might be problematic. See the subsection <a href="#manageacceleratedsearchsummaries_how_splunk_enterprise_builds_summaries" class="external text">"How Splunk Enterprise builds summaries,"</a> above, for more information.
</p><p><b>Buckets'</b> shows you how many index <b>buckets</b> the summary spans, and <b>Chunks</b> tells you how many data chunks comprise the summary. Both of these metrics are informational for the most part, though they may aid with troubleshooting issues you may be encountering with your summary.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_verify_a_summary"><span class="mw-headline" id="Verify_a_summary">Verify a summary</span></a></i></b></font></h4>
<p>At some point you may find that an accelerated report seems to be returning results that don't fit with the results the report returned when it was first created. This can happen when certain aspects of the report change without your knowledge, such as a change in the definition of a tag, event type, or field extraction rule used by the report. 
</p><p>If you suspect that this has happened with one of your accelerated reports, go to the detail page for the summary with which the report is associated. You can run a verification process that examines a subset of the summary and verifies that all of the examined data is consistent. If it finds that the data is inconsistent, it notifies you that the verification has failed.
</p><p>For example, say you have a report that uses an <b>event type</b>, <code><font size="2">netsecurity</font></code>, which is associated with a specific kind of network security event. You enable acceleration for this report, and Splunk Enterprise builds a summary for it. At some later point, the definition of the event type <code><font size="2">netsecurity</font></code> is changed, so it finds an entirely different set of events, which means your summary is now being populated by a different set of data than it was before. You notice that the results being returned by the accelerated report seem to be different, so you run the verification process on it from the Report acceleration summaries page in Settings. The summary fails verification, so you begin investigating the root report to find out what happened. 
</p><p>Ideally the verification process should only have to look at a subset of the summary data in order to save time; a full verification of the entire summary will take as long to complete as the building of the summary itself. But in some cases a more thorough verification is required. 
</p><p><img alt="5.0-Report Acceleration Summaries-Verify Summary.jpg" src="images/a/ad/5.0-Report_Acceleration_Summaries-Verify_Summary.jpg" width="398" height="292"></p><p>Clicking <b>Verify</b> opens a <b>Verify Summary</b> dialog box. Verify Summary provides two verification options:
</p>
<ul><li> A <i>Fast verification,</i> which is set to quickly verify a small subset of the summary data at the cost of thoroughness.
</li><li> A <i>Thorough verification,</i> which is set to thoroughly review the summary data at the cost of speed.
</li></ul><p>In both cases, the estimated verification time is provided. 
</p><p>After you click <b>Start</b> to kick off the verification process you can follow its progress on the detail page for your summary under <b>Summary Status</b>. When the verification process completes, this is where you'll be notified whether it succeeded or failed. Either way you can click the verification status to see details about what happened. 
</p><p>When verification fails, the <b>Verification Failed</b> dialog can tell you what went wrong:
</p><p><img alt="5.0-Report Acceleration Summaries-Verification Failed.jpg" src="images/8/8b/5.0-Report_Acceleration_Summaries-Verification_Failed.jpg" width="398" height="251"></p><p>During the verification process, Splunk Enterprise skips hot buckets and buckets that are in the process of building.
</p><p>When a summary fails verification you can review the root search string (or strings) to see if it can be fixed to provide correct results. Once the report is working, click <b>Rebuild</b> to rebuild the summary so it is entirely consistent. Or, if you're fine with the report as-is, just rebuild the report. And if you'd rather start over from scratch, delete the summary and start over with an entirely new report.
</p>
<h4><font size="3"><b><i> <a name="manageacceleratedsearchsummaries_update.2c_rebuild.2c_and_delete_summaries"><span class="mw-headline" id="Update.2C_rebuild.2C_and_delete_summaries">Update, rebuild, and delete summaries</span></a></i></b></font></h4>
<p>Click <b>Update</b> if the <b>Summary Status</b> shows that the summary has not been updated in some time and you would like to make it current. <b>Update</b> kicks off a standard summary update report to pull in events so that it is not missing data from the last few hours (for example). 
</p><p><b>Note:</b> When a summary's <b>Summary Status</b> is <i>Suspended</i>, you cannot use <b>Update</b> to bring it current. 
</p><p>Click <b>Rebuild</b> to make Splunk Enterprise rebuild the index from scratch. You may want to do this in situations where you suspect there has been data loss due to a system crash or similar mishap, or if it failed verification and you've either fixed the underlying report(s) or have decided that the summary is ok with the data it is currently bringing in. 
</p><p>Click <b>Delete</b> to remove the summary from the system (and not regenerate summaries in the future). You may want to do this if the summary is used infrequently and is taking up space that could better be used for something else. You can use the Searches and Reports page in Settings to reenable report acceleration for the report or reports associated with the summary.
</p>
<a name="acceleratedatamodels"></a><h2> <a name="acceleratedatamodels_accelerate_data_models"><span class="mw-headline" id="Accelerate_data_models"> Accelerate data models</span></a></h2>
<p>Data model acceleration is a tool that you can use to speed up data models that represent extremely large datasets. After acceleration, pivots based on accelerated data model objects complete quicker than they did before, as do reports and dashboard panels that are based on those pivots.
</p><p>Data model acceleration performs this magic with the help of Splunk Enterprise's High Performance Analytics Store functionality, which builds data summaries behind the scenes in a manner similar to that of report acceleration. Like report acceleration, data model summaries are easy to enable and disable, and are stored on your indexers parallel to the index buckets that contain the events that are being summarized. 
</p><p>This topic covers:
</p>
<ul><li> The differences between data model acceleration, report acceleration, and summary indexing.
</li><li> How you enable persistent acceleration for data models.
</li><li> How Splunk Enterprise builds data model acceleration summaries.
</li><li> How you can query accelerated data model summaries with the <code><font size="2">tstats</font></code> command.
</li><li> Advanced configurations for persistently accelerated data models.
</li></ul><p>This topic also explains <b>ad hoc data model acceleration</b>. Splunk Enterprise applies ad hoc data model acceleration whenever you build a pivot with an unaccelerated object, even search- and transaction-based objects, which can't be accelerated in a persistent fashion. However, any acceleration benefits you obtain are lost the moment you leave the Pivot Editor or switch objects during a session with the Pivot Editor. These disadvantages do not apply to "persistently" accelerated objects, which will always load with acceleration whenever they're accessed via Pivot. In addition, unlike "persistent" data model acceleration, ad hoc acceleration is <i>not</i> applied to reports or dashboard panels built with Pivot.
</p>
<h3> <a name="acceleratedatamodels_how_data_model_acceleration_differs_from_report_acceleration_and_summary_indexing"><span class="mw-headline" id="How_data_model_acceleration_differs_from_report_acceleration_and_summary_indexing">How data model acceleration differs from report acceleration and summary indexing</span></a></h3>
<p>From a "big picture" perspective, this is how data model acceleration differs from report acceleration and summary indexing:
</p>
<ul><li> Report acceleration and summary indexing <i>speed up individual searches,</i> on a report by report basis. They do this by building collections of precomputed search result aggregates.
</li><li> Data model acceleration speeds up reporting for the entire set of' <b>attributes</b> (fields) that you define in a data model.
</li></ul><p>In other words, data model acceleration creates summaries for the specific set of fields you and your Pivot users want to report on, accelerating the <i>dataset</i> represented by that collection of fields rather than a particular search against that dataset.
</p>
<h3> <a name="acceleratedatamodels_what_is_a_high-performance_analytics_store.3f"><span class="mw-headline" id="What_is_a_high-performance_analytics_store.3F">What is a high-performance analytics store?</span></a></h3>
<p>Data model acceleration summaries take the form of time-series index files, which have the <code><font size="2">.tsidx</font></code> file extension. Each <code><font size="2">.tsidx</font></code> summary contains records of the indexed field::value combos in the selected dataset and all of the index locations of those field::value combos. It's these <code><font size="2">.tsidx</font></code> summaries that make up the high-performance analytics store. Collectively, these summaries are optimized to accelerate a range of analytical searches involving a specific set of fields--the set of fields defined as attributes in the accelerated data model. 
</p><p>An accelerated data model's high-performance analytics store spans a "summary range". This is a range of time that you select when you enable acceleration for the data model. When you run a pivot on an accelerated dataset, the pivot's time range must fall at least partly within this summary range in order to get an acceleration benefit. For example, if you have a data model that accelerates the last month of data but you create a pivot using one of this data model's objects that runs over the past year, the pivot will initially only get acceleration benefits for the portion of the search that runs over the past month. 
</p><p>The <code><font size="2">.tsidx</font></code> files that make up a high-performance analytics store for a single data model are always distributed across one or more of your indexers. This is because Splunk Enterprise creates <code><font size="2">.tsidx</font></code> files on the indexer, parallel to the buckets that contain the events referenced in the file and which cover the range of time that the summary spans.
</p><p><b>Note:</b> The high-performance analytics store created through persistent data model acceleration is different from the summaries created through ad hoc data model acceleration. Ad hoc summaries are always created in a dispatch directory at the search head. For more information about ad hoc data model acceleration, see the subtopic <a href="#acceleratedatamodels_about_ad_hoc_data_model_acceleration" class="external text">"About ad hoc data model acceleration,"</a> below.
</p>
<h3> <a name="acceleratedatamodels_enable_persistent_acceleration_for_a_data_model"><span class="mw-headline" id="Enable_persistent_acceleration_for_a_data_model">Enable persistent acceleration for a data model</span></a></h3>
<p>You use the Edit Acceleration dialog to enable acceleration for a data model. 
</p><p><b>1.</b> Open the Edit Acceleration dialog to enable acceleration for a data model. There are three ways to get to this dialog:
</p>
<ul><li> Navigate to the Data Models management page, find the model you want to accelerate, and click <b>Edit</b> and select <i>Edit Acceleration</i>.
</li><li> Navigate to the Data Models management page, expand the row of the data model you want to accelerate, and click <b>Add</b> for <b>ACCELERATION</b>.
</li><li> Open the Data Model Editor for a data model, click <b>Edit</b> and select <i>Edit Acceleration</i>.
</li></ul><p><b>2.</b> Select <b>Accelerate</b> to to enable acceleration for the data model. 
</p><p><b>3.</b> Choose a <b>Summary Range</b>. 
</p>
<dl><dd> The <b>Summary Range</b> can span <i>1 Day</i>, <i>7 Days</i>, <i>1 Month</i>, <i>3 Months</i>, <i>1 Year</i>, or <i>All Time.</i> It represents the time range over which you plan to run pivots against the accelerated objects in the data model. For example, if you only want to run pivots over periods of time within the last seven days, choose <i>7 Days.</i> For more information about this setting, see the subtopic "About the summary range," below.
</dd></dl><dl><dd> If you require a different summary range than the ones supplied by the <b>Summary Range</b> field, you can configure it for your data model in <code><font size="2">datamodels.conf</font></code>.
</dd></dl><p><b>Note:</b> Smaller time ranges mean smaller <code><font size="2">.tsidx</font></code> files that require less time to build and which take up less space on disc, so you may want to go with shorter ranges when you can.
</p><p>For more details on the summary range setting and how it works, see "About the summary range," below.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_data_model_acceleration_caveats"><span class="mw-headline" id="Data_model_acceleration_caveats">Data model acceleration caveats</span></a></i></b></font></h4>
<p>There are a number of restrictions on the kinds of data model objects that can be accelerated. 
</p>
<ul><li> <b>Data model acceleration only affects event object hierarchies.</b> Object hierarchies based on root search and root transaction objects are not accelerated. 
<ul><li> Pivots that use unaccelerated objects fall back to <code><font size="2">_raw</font></code> data, which means that they initially run slower. However, they can receive some acceleration benefit from ad hoc acceleration. See "About ad hoc data model acceleration" at the end of this topic for more information. 
</li></ul></li><li> <b>Data model acceleration is most efficient if the root event objects being accelerated include in their initial constraint search the index(es) that Splunk Enterprise should search over</b>. A single high-performance analytics store can span across several indexes in multiple indexers. If you know that all of the data that you want to pivot on resides in a particular index or set of indexes, you can speed things up by telling Splunk Enterprise where to look. Otherwise Splunk Enterprise may end up wasting time unnecessarily accelerating data that is not of use to you.
</li></ul><p>For the full list of restrictions and caveats on data model usage see the list in <a href="#managedatamodels_enable_data_model_acceleration" class="external text">"Managing Data Models,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_after_you_enable_acceleration_for_a_data_model"><span class="mw-headline" id="After_you_enable_acceleration_for_a_data_model">After you enable acceleration for a data model</span></a></i></b></font></h4>
<p>After you enable acceleration for your data model, Splunk Enterprise begins building data model acceleration summaries that span the summary range that you've specified. It builds them in indexes with events that contain the fields specified in the data model. These <code><font size="2">.tsidx</font></code> file summaries are stored parallel to their corresponding index buckets in a manner identical to that of report acceleration summaries. 
</p><p>Splunk Enterprise runs a search every 5 minutes to update existing data model summaries. It runs a maintenance process every 30 minutes to remove old, outdated summaries. You can adjust these intervals in <code><font size="2">datamodels.conf</font></code> and <code><font size="2">limits.conf</font></code>, respectively. 
</p><p>A few facts about data model summaries:
</p>
<ul><li> Each bucket in each index in a Splunk Enterprise instance can have one or more data model summaries, one for each accelerated data model for which it has relevant data. These summaries are created by Splunk Enterprise as it collects data. 
</li><li> Splunk Enterprise then scopes summaries by search head (or search head pool id) to account for different extractions that may produce different results for the same search string.
</li><li> There is a directory for each data model that is accelerated along with the app it comes from. Data model accelerations are scoped either to an app or global; data models that are private cannot be accelerated. This prevents individual users from taking up disk space with private data model acceleration summaries. 
</li></ul><p><b>Note:</b> If necessary you can configure the location of data model summaries via <code><font size="2">indexes.conf</font></code>.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_about_the_summary_range"><span class="mw-headline" id="About_the_summary_range">About the summary range</span></a></i></b></font></h4>
<p>For the most part, accelerated data model summary ranges behave in a manner similar to accelerated report summary ranges. 
</p><p>Data model summary ranges span an approximate range of time. If you select a <b>Summary Range</b> of <i>7 days</i>, Splunk Enterprise will build summaries for the data model that each approximately span the past 7 days. Once Splunk Enterprise finishes building the summary, going forward the data model summarization process ensures that the summary always covers the selected range, removing older summary data that passes out of the range.
</p><p><b>Note:</b> We say that the Summary Range indicates the <i>approximate</i> range of time that a summary spans. At times, summaries will have a store of data that that slightly exceeds their summary range, but they never fail to meet it, unless it's the first time the summary is being built.
</p><p>In the future, when you run a pivot using an accelerated object from the data model over a range that falls within the preceding week, Splunk Enterprise will run the pivot against the data model's summary rather than the source index <code><font size="2">_raw</font></code> data again. In most cases the summary will have far less data than the source index, and that means that the pivot should complete faster than it did on its initial run.
</p><p>If you run a pivot over a period of time that is only partially covered by the summary range, the pivot won't complete quite as fast. This is because Splunk Enterprise has to run at least part of the pivot search over raw data in the main Splunk Enterprise index. For example, if the Summary Range setting for a data model is 1 week and you run a pivot using an accelerated object from that data model over the last 9 days, the pivot only gets acceleration benefits for the portion of the report that covers the past 7 days. The portion of the report that runs over days 8 and 9 will run at normal speed. In cases like this, Splunk Enterprise will return the accelerated results from summaries first, and then fill in the gaps at a slower speed.
</p><p>Keep this in mind when you set the <b>Summary Range</b> value. If you always plan to run a report over time ranges that exceed the past 7 days, but don't extend further out than 30 days, you should select a <b>Summary Range</b> of <i>1 month</i> when you set up report acceleration for that report.
</p><p><b>Note:</b> Splunk offers a few advanced settings related to <b>Summary Range</b> that you can use if you have a large Splunk implementation that involves multi-terrabyte datasets. This can lead to situations where the search required to build the initial data model acceleration summary runs too long and/or is resource intensive. For more information, see the subtopic "<a href="#acceleratedatamodels_advanced_configurations_for_persistently_accelerated_data_models" class="external text">Advanced configurations for persistently accelerated data models</a>," below.
</p>
<h3> <a name="acceleratedatamodels_how_splunk_enterprise_builds_data_model_acceleration_summaries"><span class="mw-headline" id="How_Splunk_Enterprise_builds_data_model_acceleration_summaries">How Splunk Enterprise builds data model acceleration summaries</span></a></h3>
<p>As we mentioned earlier, when you enable acceleration for a data model, Splunk Enterprise builds the initial set of <code><font size="2">.tsidx</font></code> file summaries for the data model and then runs scheduled searches in the background every 5 minutes to keep those summaries up to date. Each update ensures that the entire configured time range is covered without a significant gap in data. This method of summary building also ensures that late-arriving data will be summarized without complication.
</p><p>To verify that Splunk Enterprise is scheduling searches to update your data models, in <code><font size="2">log.cfg</font></code> you can set <code><font size="2">category.SavedSplunker=DEBUG</font></code> and then watch <code><font size="2">scheduler.log</font></code> for events like:
</p>
<div class="samplecode">
<code><font size="2"><br>04-24-2013 11:12:02.357 -0700 DEBUG SavedSplunker - Added 1 scheduled searches for accelerated datamodels to the end of ready-to-run list<br></font></code></div>
<p>The speed of summary creation depends on the amount of events involved and the size of the summary range. You can track progress towards summary completion on the Data Models management page. Find the accelerated data model that you want to inspect, expand its row, and review the information that appears under <b>ACCELERATION</b>. 
</p><p><img alt="6.0 dm acceleration metrics.png" src="images/b/bf/6.0_dm_acceleration_metrics.png" width="311" height="188"></p><p><b>Status</b> tells you whether the acceleration summary for the data model is complete. If it is in <i>Building</i> status it will tell you what percentage of the summary is complete. Keep in mind that data model summaries are constantly updating with new data; just because a summary is "complete" now doesn't mean it won't be "building" later. 
</p><p><b>Note:</b> The data model acceleration status is updated occasionally and cached so it may not represent the most up-to-date status.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_data_model_summary_size_on_disk"><span class="mw-headline" id="Data_model_summary_size_on_disk">Data model summary size on disk</span></a></i></b></font></h4>
<p>You can use the data model metrics on the Data Models management page to track the total size of a data model's summary on disk. Summaries do take up space, and sometimes a signficant amount of it, so it's important that you avoid overuse of data model acceleration. For example, you may want to reserve data model acceleration for data models whose pivots are heavily used in dashboard panels. 
</p><p>The amount of space that a data model takes up is related to the number of events that you are collecting for the summary range you've chosen. It can also be negatively affected if the data model includes attributes with high cardinality (that have a  large set of unique values), such as a <code><font size="2">Name</font></code> attribute. 
</p><p>If you are particularly size constrained you may want to test the amount of space a data model acceleration summary will take up by enabling acceleration for a small <b>Summary Range</b> first, and then moving to a larger range if you think you can afford it.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_where_data_model_acceleration_summaries_are_created_and_stored"><span class="mw-headline" id="Where_data_model_acceleration_summaries_are_created_and_stored">Where data model acceleration summaries are created and stored</span></a></i></b></font></h4>
<p>Splunk Enterprise creates each data model acceleration summary on the indexer, parallel to the bucket or buckets that cover the range of time over which the summary spans. For example, for the "index1" index, they reside under <code><font size="2">$SPLUNK_HOME/var/lib/splunk/index1/datamodel_summary</font></code>. 
</p><p><b>Note:</b> Report acceleration summaries are stored in the same manner, but in directories labeled <code><font size="2">summary</font></code> instead of <code><font size="2">datamodel_summary</font></code>.
</p><p>Keep in mind that data model acceleration summaries are <i>not</i> indexes. <b>Indexer clusters</b> do not replicate data model acceleration summaries or otherwise accommodate their presence. If <b>primacy</b> gets reassigned from the original copy of a bucket to another (for example, because the peer holding the primary copy fails), the summary does not move to the peer with new primary copy. Therefore, it becomes unavailable. It will not be available again until the next time Splunk Enterprise attempts to update the summary, finds that it is missing, and regenerates it.
</p><p><b>Note:</b> Until Splunk Enterprise regenerates your summaries, the results of your accelerated data models will be correct, but they will run slower.
</p><p>For more information about how indexer clusters handle data model acceleration summaries, see "How search works in an indexer cluster" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p>
<h3> <a name="acceleratedatamodels_configure_size-based_retention_for_data_model_summaries"><span class="mw-headline" id="Configure_size-based_retention_for_data_model_summaries">Configure size-based retention for data model summaries</span></a></h3>
<p>Do you set size-based retention limits for your indexes so they do not take up too much disk storage space? By default, data model acceleration summaries can take up an unlimited amount of disk space. This can be a problem if you're also locking down the maximum data size of your indexes or index volumes. The good news is that you can optionally configure similar retention limits for your data model acceleration summaries. 
</p><p><b>Note:</b> Before attempting to configure size-based retention for your data model acceleration summaries, you should first understand how to use volumes to configure limits on index size across indexes, as many of the principles are the same. For more information, see "Configure index size" in <i>Managing Indexers and Clusters</i>.
</p><p>By default, data model acceleration summaries reside in a predefined volume titled <code><font size="2">_splunk_summaries</font></code> at the following path:  
</p>
<div class="samplecode">
<code><font size="2"><br>&nbsp;$SPLUNK_DB/&lt;index_name&gt;/datamodel_summary/&lt;bucket_id&gt;/&lt;search_head_or_pool_id&gt;/DM_&lt;datamodel_app&gt;_&lt;datamodel_name&gt;<br></font></code></div>
<p>This volume initially has no maximum size specification, which means that it has infinite retention. 
</p><p>Also by default, the <code><font size="2">tstatsHomePath</font></code> parameter is specified only once as a global setting in <code><font size="2">indexes.conf</font></code>. Its path is inherited by all indexes. In <code><font size="2">etc/system/default/indexes.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[global]<br>[....]<br>tstatsHomePath = volume:_splunk_summaries/$_index_name/datamodel_summary<br>[....]<br></font></code></div>
<p>You can override this default behavior by specifying <code><font size="2">tstatsHomePath</font></code> for a specific index and pointing to a different volume that you have defined. You can also add size limits to any volume (including <code><font size="2">_splunk_summaries</font></code>) by setting a <code><font size="2">maxVolumeDataSizeMB</font></code> parameter in the volume configuration. 
</p><p>Here are the steps you take to set up size-based retention for data model acceleration summaries. All of the configurations described are made within <code><font size="2">indexes.conf</font></code>.
</p><p><b>1.</b> (Optional) If you want to have data model acceleration summary results go into volumes other than <code><font size="2">_splunk_summaries</font></code>, create them.
</p><p><b>2.</b> Add <code><font size="2">maxVolumeDataSizeMB</font></code> parameters to the volume or volumes that 
will be the home for your data model acceleration summary data, such as <code><font size="2">_splunk_summaries</font></code>. 
</p>
<dl><dd> This parameter manages size-based retention for data model acceleration summaries across your indexers.  
</dd></dl><p><b>3.</b> Update your index definitions. 
</p>
<dl><dd> Set a <code><font size="2">tstatsHomePath</font></code> parameter for each index that deals with data model acceleration summary data. Ensure that the path is pointing to the data model acceleration summary data volume that you identified in Step 2. 
</dd></dl><dl><dd> If you defined multiple volumes for your data model acceleration summaries, make sure that the <code><font size="2">tstatsHomePath</font></code> settings for your indexes point to the appropriate volumes.
</dd></dl><p>This example configuration shows data size limits being set up for data model acceleration summaries on the <code><font size="2">_splunk_summaries</font></code> volume. 
</p>
<div class="samplecode">
<code><font size="2"><br>#########################<br># Volume definitions<br>#########################<br><br># This volume can contain up to 100GB of summary data, per the <br># &lt;code&gt;maxVolumeDataSizeMB&lt;/code&gt; setting.<br>[volume:_splunk_summaries]<br>path = $SPLUNK_DB<br>maxVolumeDataSizeMB = 100000<br><br>#########################<br># Index definitions<br>#########################<br><br># Using the _tstatsHomePath parameter, the _splunk_summaries volume <br># manages overall data model acceleration size across each of these <br># indexes, limiting it to 100GB. <br>[main]<br>homePath &nbsp;&nbsp;= $SPLUNK_DB/defaultdb/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/defaultdb/colddb<br>thawedPath = $SPLUNK_DB/defaultdb/thaweddb<br>tstatsHomePath = volume:_splunk_summaries/defaultdb/datamodel_summary<br>maxMemMB = 20<br>maxConcurrentOptimizes = 6<br>maxHotIdleSecs = 86400<br>maxHotBuckets = 10<br>maxDataSize = auto_high_volume<br><br>[history]<br>homePath &nbsp;&nbsp;= $SPLUNK_DB/historydb/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/historydb/colddb<br>thawedPath = $SPLUNK_DB/historydb/thaweddb<br>tstatsHomePath = volume:_splunk_summaries/historydb/datamodel_summary<br>maxDataSize = 10<br>frozenTimePeriodInSecs = 604800<br><br>[dm_acceleration]<br>homePath &nbsp;&nbsp;= $SPLUNK_DB/dm_accelerationdb/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/dm_accelerationdb/colddb<br>thawedPath = $SPLUNK_DB/dm_accelerationdb/thaweddb<br>tstatsHomePath = volume:_splunk_summaries/dm_accelerationdb/datamodel_summary<br><br>[_internal]<br>homePath &nbsp;&nbsp;= $SPLUNK_DB/_internaldb/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/_internaldb/colddb<br>thawedPath = $SPLUNK_DB/_internaldb/thaweddb<br>tstatsHomePath = volume:_splunk_summaries/_internaldb/datamodel_summary<br></font></code></div>
<p>When a data model acceleration summary volume reaches its size limit, the Splunk Enterprise volume manager removes the oldest summary in the volume to make room. When the volume manager removes a summary, it places a marker file inside its corresponding bucket. This marker file tells the summary generator not to rebuild the summary.
</p><p><b>Note:</b> You can configure size-based retention for report acceleration summaries in much the same way that you do for data model acceleration summaries. The primary difference is that there is no default volume for report acceleration summaries. If you are trying to manage both kinds of summaries you may decide it is easiest to have both summaries use the default <code><font size="2">_splunk_summaries</font></code> volume. For more information about this strategy for managing size-based retention of both acceleration summary types, see <a href="#manageacceleratedsearchsummaries" class="external text">"Manage report acceleration"</a> in this manual.
</p>
<h3> <a name="acceleratedatamodels_query_data_model_acceleration_summaries"><span class="mw-headline" id="Query_data_model_acceleration_summaries">Query data model acceleration summaries</span></a></h3>
<p>You can query the high-performance analytics store for a specific accelerated data model in Search with the <code><font size="2">tstats</font></code> command. 
</p><p><code><font size="2">tstats</font></code> can sort through the full set of <code><font size="2">.tsidx</font></code> file summaries that belong to your accelerated data model even when they are distributed among multiple indexes. 
</p><p>This can be a way to quickly run a stats-based search against a particular data model just to see if it's capturing the data you expect for the summary range you've selected.
</p><p>To do this, you identify the data model using <code><font size="2">FROM datamodel=&lt;datamodel-name&gt;</font></code>:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">| tstats avg(foo) FROM datamodel=buttercup_games WHERE bar=value2 baz&gt;5</font></code><br></div>
<p>The above query returns the average of the field <code><font size="2">foo</font></code> in the "Buttercup Games" data model acceleration summaries, specifically where <code><font size="2">bar</font></code> is <code><font size="2">value2</font></code> and the value of <code><font size="2">baz</font></code> is greater than <code><font size="2">5</font></code>. 
</p><p><b>Note:</b> You don't have to specify the app of the data model as Splunk Enterprise takes this from the search context (the app you are in). However you cannot query an accelerated data model in App B from App A unless the data model in App B is shared globally. 
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_using_the_summariesonly_argument"><span class="mw-headline" id="Using_the_summariesonly_argument">Using the summariesonly argument</span></a></i></b></font></h4>
<p>The <code><font size="2">summariesonly</font></code> argument of the <code><font size="2">tstats</font></code> command enables you to get specific information about data model summaries. 
</p><p>This example uses the <code><font size="2">summariesonly</font></code> argument to get the time range of the summary for an accelerated data model named <code><font size="2">mydm</font></code>.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">| tstats summariesonly=t min(_time) as min, max(_time) as max from datamodel=mydm | eval prettymin=strftime(min, "%c") | eval prettymax=strftime(max, "%c")</font></code><br></div>
<p>This example uses <code><font size="2">summariesonly</font></code> in conjunction with <code><font size="2">timechart</font></code> to reveal what data has been summarized over a selected time range for an accelerated data model titled <code><font size="2">mydm</font></code>.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">| tstats summariesonly=t prestats=t count from datamodel=mydm by _time span=1h | timechart span=1h count</font></code><br></div>
<p>For more about the <code><font size="2">tstats</font></code> command, <b>including the usage of <code><font size="2">tstats</font></code> to query normal indexed data,</b> see the entry for <code><font size="2">tstats</font></code> in the Search Reference.
</p>
<h3> <a name="acceleratedatamodels_advanced_configurations_for_persistently_accelerated_data_models"><span class="mw-headline" id="Advanced_configurations_for_persistently_accelerated_data_models">Advanced configurations for persistently accelerated data models</span></a></h3>
<p>There are a few very specific situations that may require you to set up advanced configurations for your persistently accelerated data models in <code><font size="2">datamodels.conf</font></code>. 
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_when_summary-populating_searches_take_too_long_to_run"><span class="mw-headline" id="When_summary-populating_searches_take_too_long_to_run">When summary-populating searches take too long to run</span></a></i></b></font></h4>
<p>If your Splunk Enterprise implementation processes an extremely large amount of data on a regular basis you may find that the initial creation of persistent data model acceleration summaries is resource intensive. The searches that build these summaries may run too long, causing them to fail to summarize incoming events. To deal with this situation, Splunk Enterprise gives you two configuration parameters, both in <code><font size="2">datamodels.conf</font></code>. These parameters are <code><font size="2">max_time</font></code> and <code><font size="2">backfill_time</font></code>.
</p><p><b>Important:</b> Most Splunk Enterprise users do not need to adjust these settings. The default <code><font size="2">max_time</font></code> setting of 1 hour should ensure that long-running summary creation searches do not impede the addition of new events to the summary. We advise that you not change these advanced summary range configurations unless you know it is the only solution to your summary creation issues. 
</p>
<h5> <a name="acceleratedatamodels_change_the_maximum_period_of_time_that_a_summary-populating_search_can_run"><span class="mw-headline" id="Change_the_maximum_period_of_time_that_a_summary-populating_search_can_run">Change the maximum period of time that a summary-populating search can run</span></a></h5>
<p>The <code><font size="2">max_time</font></code> causes summary populating searches to quit after a specified amount of time has passed. After a summary-populating search stops, Splunk Enterprise runs a search to catch all of the events that have come in since the initial summary-populating search began, and then it continues adding the summary where the last summary-populating search left off. The <code><font size="2">max_time</font></code> parameter is set to 3600 seconds (60 minutes) by default, a setting that should ensure proper summary creation for the majority of Splunk Enterprise instances. 
</p><p>For example: You have enabled acceleration for a data model, and you want its summary to retain events for the past three months. Because your organization indexes large amounts of data, the search that initially creates this summary should take about four hours to complete. Unfortunately you can't let the search run interrupted for that amount of time because it might fail to index some of the new events that come in while that four-hour search is in process. 
</p><p>The <code><font size="2">max_time</font></code> parameter stops the search after an hour, and another search takes its place to pull in the new events that have come in during that time. It then continues running to add events from the last three months to the summary. This second search also stops after an hour and the process repeats until the summary is complete.
</p><p><b>Note:</b> The <code><font size="2">max_time</font></code> parameter is an approximate time limit. After the 60 minutes elapses, Splunk Enterprise has to finish summarizing the current bucket before kicking off the summary search. This prevents wasted work. 
</p>
<h5> <a name="acceleratedatamodels_set_a_backfill_time_range_that_is_shorter_than_the_summary_time_range"><span class="mw-headline" id="Set_a_backfill_time_range_that_is_shorter_than_the_summary_time_range">Set a backfill time range that is shorter than the summary time range</span></a></h5>
<p>If you are indexing a tremendous amount of data with your Splunk implementation and you don't want to adjust the <code><font size="2">max_time</font></code> range for a slow-running summary-populating search, you have an alternative option: the <code><font size="2">backfill_time</font></code> parameter. 
</p><p>The <code><font size="2">backfill_time</font></code> parameter creates a second "backfill time range" that you set within the summary range. Splunk Enterprise will build a partial summary that initially only covers this shorter time range. After that, the summary will expand with each new event summarized until it reaches the limit of the larger summary time range. At that point the full summary will be complete and Splunk will stop retaining events that age out of the summary range. 
</p><p>For example, say you want to set your <b>Summary Range</b> to <i>1 Month</i> but you know that your system would be taxed by a search that built a summary for that time range. To deal with this, you set <code><font size="2">acceleration.backfill_time = -7d</font></code> to have Splunk Enterprise run a search that creates a partial summary that initially just covers the past week. After that limit is reached, Splunk Enterprise would only add new events to the summary, causing the range of time covered by the summary to expand. But the full summary would still only retain events for one month, so once the partial summary expands to the full <b>Summary Range</b> of the past month, it starts dropping its oldest events, just like an ordinary data model summary does. 
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_when_you_do_not_want_persistently_accelerated_data_models_to_be_rebuilt_automatically"><span class="mw-headline" id="When_you_do_not_want_persistently_accelerated_data_models_to_be_rebuilt_automatically">When you do not want persistently accelerated data models to be rebuilt automatically</span></a></i></b></font></h4>
<p>By default Splunk Enterprise automatically rebuilds persistently accelerated data models whenever it finds that those models are outdated. Data models can become outdated when the search stored in the data model configuration in <code><font size="2">savesearches.conf</font></code> no longer matches the search for the actual data model. This can happen if the JSON file for an accelerated model is edited on disk without first disabling the model's acceleration.
</p><p>In very specific cases you may want to disable this feature for specific accelerated data models, so that those data models are not rebuilt automatically when they become out of date. Instead it will be up to admins to initiate the rebuilds manually. Admins can manually rebuild a data model through the Data Model Manager page, by expanding the row for the affected data model and clicking <b>Rebuild</b>. For more about the Data Model Manager page, see "<a href="#managedatamodels" class="external text">Manage data models</a>" in this manual.
</p><p>To disable automatic rebuilds for a specific persistently accelerated data model, open <code><font size="2">datamodels.conf</font></code>, find the stanza for the data model, and set <code><font size="2">acceleration.manual_rebuilds = true</font></code>
</p>
<h3> <a name="acceleratedatamodels_about_ad_hoc_data_model_acceleration"><span class="mw-headline" id="About_ad_hoc_data_model_acceleration">About ad hoc data model acceleration</span></a></h3>
<p>Even when you're building a pivot that is based on a data model object that is not accelerated in a persistent fashion, that pivot can benefit from what we call "ad hoc" data model acceleration. In these cases, Splunk Enterprise builds a summary in a search head dispatch directory when you work with an object to build a pivot in the Pivot Editor. 
</p><p>The search head begins building the ad-hoc data model acceleration summary after you select an object and enter the pivot editor. You can follow the progress of the ad hoc summary construction with the progress bar: 
</p><p><img alt="6.0 pivot progressbar.png" src="images/9/9c/6.0_pivot_progressbar.png" width="339" height="90"></p><p>When the progress bar reads <b>Complete,</b> the ad hoc summary is built, and the search head uses it to return pivot results faster going forward. But this summary only lasts while you work with the object in the Pivot Editor. If you leave the editor and return, or switch to another object and then return to the first one, the search head will need to rebuild the ad hoc summary. 
</p><p>Ad hoc data model acceleration summaries complete faster when they collect data for a shorter range of time. You can change this range for root event objects, root transaction objects, and their children by resetting the time <b>Filter</b> in the Pivot Editor. See "About ad hoc data model acceleration summary time ranges," below, for more information.
</p><p>Ad hoc data model acceleration works for all object types, including root search objects and root transaction objects. Its main disadvantage against persistent data model acceleration is that with persistent data model acceleration, the summary is always there, keeping pivot performance speedy, until acceleration is disabled for the data model. With ad hoc data model acceleration, you have to wait for the summary to be rebuilt each time you enter the Pivot Editor.
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_about_ad_hoc_data_model_acceleration_summary_time_ranges"><span class="mw-headline" id="About_ad_hoc_data_model_acceleration_summary_time_ranges">About ad hoc data model acceleration summary time ranges</span></a></i></b></font></h4>
<p>The search head always tries to make ad hoc data model acceleration summaries fit the range set by the time <b>Filter</b> in the Pivot Editor. When you first enter the Pivot Editor for an object, the pivot time range is set to <i>All Time</i>. If your object represents a large dataset this can mean that the initial pivot will complete slowly as it builds the ad hoc summary behind the scenes. 
</p><p>When you give the pivot a time range other than <i>All Time</i>, the search head builds an ad hoc summary that fits that range as efficiently as possible. For any given data model object, the search head completes an ad hoc summary for a pivot with a short time range quicker than it completes when that same pivot has a longer time range.
</p><p>The search head only rebuilds the ad hoc summary from start to finish if you replace the current time range with a new time range that has a different "latest" time. This is because the search head builds each ad hoc summary backwards, from its latest time to its earliest time. If you keep the latest time the same but change the earliest time the search head at most will work to collect any extra data that is required.
</p><p><b>Note:</b> Root search objects and their child objects are a special case here as they do not have time range filters in Pivot (they do not extract <code><font size="2">_time</font></code> as an attribute). Pivots based on these objects always build summaries for all of the events returned by the search. However, you can design the root search object's search string so it includes "earliest" and "latest" dates, which restricts the dataset represented by the root search object and its children. 
</p>
<h4><font size="3"><b><i> <a name="acceleratedatamodels_how_ad_hoc_data_model_acceleration_differs_from_persistent_data_model_acceleration"><span class="mw-headline" id="How_ad_hoc_data_model_acceleration_differs_from_persistent_data_model_acceleration">How ad hoc data model acceleration differs from persistent data model acceleration</span></a></i></b></font></h4>
<p>Here's a summary of the ways in which ad hoc data model acceleration differs from persistent data model acceleration:
</p>
<ul><li> <b>Ad hoc data model acceleration takes place on the search head rather than the indexer.</b> This enables it to accelerate all three object types (event, search, and transaction).
</li><li> <b>Splunk Enterprise creates ad hoc data model acceleration summaries in dispatch directories at the search head.</b> It creates and stores persistent data model acceleration summaries in your indexes alongside index buckets.
</li><li> <b>Splunk Enterprise deletes ad hoc data model acceleration summaries when you leave the Pivot Editor or change the object you are working on while you are in the Pivot Editor.</b> When you return to the Pivot Editor for the same object, the search head must rebuild the ad hoc summary. You cannot preserve ad hoc data model summaries for later use.
<ul><li> Pivot job IDs are retained in the pivot URL, so if you quickly use the back button after leaving Pivot (or return to the pivot job with a permalink) you may be able to use the ad-hoc summary for that job without waiting for a rebuild. The search head deletes ad hoc data model acceleration summaries from the dispatch directory a few minutes after you leave Pivot or switch to a different model within Pivot.
</li></ul></li><li> <b>Ad hoc acceleration does not apply to reports or dashboard panels that are based on pivots.</b> If you want pivot-based reports and dashboard panels to benefit from data model acceleration, base them on objects from persistently accelerated event object hierarchies.
</li><li> <b>Ad hoc data model acceleration can potentially create more load on your search head than persistent data model acceleration creates on your indexers.</b> This is because the search head creates a separate ad hoc data model acceleration summary for each user that accesses a specific data model object in Pivot that is not persistently accelerated. On the other hand, summaries for persistently accelerated data model objects are shared by each user of the associated data model. This data model summary reuse results in less work for your indexers.
</li></ul><a name="usesummaryindexing"></a><h2> <a name="usesummaryindexing_use_summary_indexing_for_increased_reporting_efficiency"><span class="mw-headline" id="Use_summary_indexing_for_increased_reporting_efficiency"> Use summary indexing for increased reporting efficiency</span></a></h2>
<p>Use <b>summary indexing</b> to efficiently report on large volumes of data. With summary indexing, you set up a search that extracts the precise information you want, on a frequent basis. Each time Splunk Enterprise runs this search it saves the results into a summary index that you designate. You can then run searches and reports on this significantly smaller (and thus seemingly "faster") summary index. And what's more, these reports will be statistically accurate because of the frequency of the index-populating search (for example, if you want to manually run searches that cover the past seven days, you might run them on a summary index that is updated on an hourly basis). 
</p><p>Summary indexing allows the cost of a computationally expensive report to be spread over time. In the example we've been discussing, the hourly search to populate the summary index with the previous hour's worth of data would take a fraction of a minute. Generating the complete report without the benefit of summary indexing would take approximately 168 (7 days * 24 hrs/day) times longer. 
</p><p>Perhaps an even more important advantage of summary indexing is its ability to amortize costs over different reports, as well as for the same report over a different but overlapping time range. The same summary data generated on a Tuesday can be used for a report of the previous 7 days done on the Wednesday, Thursday, or the following Monday. It could also be used for a monthly report that needed the average response size per day. 
</p><p><b>Note:</b> Summary indexing volume is not counted against your license, even if you have multiple summary indexes. However, in the event of a license violation, summary indexing will halt like any other non-internal search behavior.
</p>
<h3> <a name="usesummaryindexing_summary_indexing_use_cases"><span class="mw-headline" id="Summary_indexing_use_cases"> Summary indexing use cases </span></a></h3>
<p><b>Example #1 - Run reports over long time ranges for large datasets more efficiently:</b> You're using Splunk Enterprise at a company that indexes tens of millions of events--or more--per day. You want to set up a dashboard for your employees that, among other things, displays a report that shows the number of page views and visitors each of your Web sites had over the past 30 days, broken out by site.
</p><p>You could run this report on your primary data volume, but its runtime would be quite long, because Splunk Enterprise has to sort through a huge number of events that are totally unrelated to web traffic in order to extract the desired data. But that's not all--the fact that the report is included in a popular dashboard means it'll be run frequently, and this could significantly extend its average runtime, leading to a lot of frustrated users. 
</p><p>But if you use summary indexing, you can set up a saved search that collects website page view and visitor information into a designated summary index on a weekly, daily, or even hourly basis. You'll then run your month-end report on this smaller summary index, and the report should complete far faster than it would otherwise because it is searching on a smaller and better-focused dataset.
</p><p><b>Example #2 - Building rolling reports:</b> Say you want to run a report that shows a running count of an aggregated statistic over a long period of time--a running count of downloads of a file from a Web site you manage, for example.
</p><p>First, schedule a saved search to return the total number of downloads over a specified slice of time. Then, use summary indexing to have Splunk Enterprise save the results of that search into a summary index. You can then run a report any time you want on the data in the summary index to obtain the latest count of the total number of downloads.
</p><p>For another view, you can watch this Splunk Enterprise developer video about the theory and practice of summary indexing.
</p>
<h3> <a name="usesummaryindexing_use_the_summary_indexing_reporting_commands"><span class="mw-headline" id="Use_the_summary_indexing_reporting_commands"> Use the summary indexing reporting commands </span></a></h3>
<p>If you are new to summary indexing, use the summary indexing reporting commands (<code><font size="2">sichart</font></code>, <code><font size="2">sitimechart</font></code>, <code><font size="2">sistats</font></code>, <code><font size="2">sitop</font></code>, and <code><font size="2">sirare</font></code>) when you define the search that will populate the summary index. If you use these commands you can use the same search string that you use for the search that you eventually run on the summary index, with the exception that you use regular reporting commands in the latter search. 
</p><p><b>Note:</b> You do not have to use the si- summary index search commands if you are proficient with the "old-school" way of creating summary-index-populating searches. If you create summary indexes using those methods and they work for you there's no need to update them. In fact, they may be more efficient: there are performance impacts related to the use of the si- commands, because they create slightly larger indexes than the "manual" method does. 
</p><p>In most cases the impact is insignificant, but you may notice a difference if the summary indexes you are creating are themselves fairly large. You may also notice performance issues if you're setting up several searches to report against an index populated by an si- command search. 
</p><p>See the following section if you're interested in designing summary indexes without the help of si- search commands.
</p>
<h4><font size="3"><b><i> <a name="usesummaryindexing_define_index-populating_searches_without_the_special_commands"><span class="mw-headline" id="Define_index-populating_searches_without_the_special_commands"> Define index-populating searches without the special commands </span></a></i></b></font></h4>
<p>In previous versions of Splunk Enterprise you had to be very careful about how you designed the searches that you used to populate your summary index, especially if the search you wanted to run on the finished summary index involved aggregate statistics, because it meant that you had to carefully set up the "index-populating" search in a way that did not provide incorrect results. For example, if you wanted to run a search on the finished summary index that gave you average response times broken out by server, you'd want to set up a summary-index-populating search that:
</p>
<ul><li> is scheduled to run on a more frequent basis than the search you plan to run against the summary index
</li><li> samples a larger amount of data than the search you plan to run against the summary index.
</li><li> contains additional search commands that ensure that the index-populating search is generating a weighted average (only necessary if you are looking for an average in the first place)..
</li></ul><p>The summary index reporting commands take care of the last two points for you--they automatically determine the adjustments that need to be made so that your summary index is populated with data that does not produce statistically inaccurate results. However, you still should arrange for the summary-index-populating search to run on a more frequent basis than the search that you later run against the summary index.
</p><p>Interested in setting up summary indexes without the si- commands? Find out about the <code><font size="2">addinfo</font></code>, <code><font size="2">collect</font></code>, and <code><font size="2">overlap</font></code> commands, learn how to devise searches that provide weighted averages, and review an example of summary index configuration via <code><font size="2">savedsearches.conf</font></code>  in the topic <a href="#configuresummaryindexes" class="external text">"Configure summary indexes,"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="usesummaryindexing_summary_indexing_reporting_command_usage_example"><span class="mw-headline" id="Summary_indexing_reporting_command_usage_example"> Summary indexing reporting command usage example </span></a></i></b></font></h4>
<p>Let's say you've been running the following search, with a time range of the past year:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">eventtype=firewall | top src_ip </font></code><br></div>
<p>This search gives you the top source ips for the past year, but it takes forever to run because it scans across your entire index each time. 
</p><p>What you need to do is create a summary index that is composed of the top source IPs from the "firewall" event type. You can use the following search to build that summary index. You would schedule it to run on a daily basis, collecting the top <code><font size="2">src_ip</font></code> values for only the previous 24 hours each time. The results of each daily search are added to an index named "summary":
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">eventtype=firewall | sitop src_ip </font></code><br></div>
<p><b>Note:</b> Summary-index-populating searches are statistically more accurate if you schedule them to run and sample information on a more frequent basis than the searches you plan to run against the finished summary index. So in this example, because we plan to run searches that cover a timespan of a year, we set up a summary-index-populating search that samples information on a daily basis. 
</p><p><b>Important:</b> When you define summary-index-populating searches, do not pipe other search operators after the main summary indexing reporting command. In other words, don't include additional <code><font size="2">| eval</font></code> commands and the like. Save the extra search operators for the searches you run <i>against</i> the summary indexes, not the search you use to populate it.
</p><p><b>Important:</b> The results from a summary-indexing optimized search are stored in a special format that cannot be modified before the final transformation is performed. This means that if you populate a summary index with <code><font size="2">... | sistats &lt;args&gt;</font></code>, the only valid retrieval of the data is: <code><font size="2">index=&lt;summary&gt; source=&lt;saved search name&gt; | stats &lt;args&gt;</font></code>. The search against the summary index cannot create or modify fields before the <code><font size="2">| stats &lt;args&gt;</font></code> command.
</p><p>Now, let's say you save this search with the name "Summary - firewall top src_ip" (all saved summary-index-populating searches should have names that identify them as such). After your summary index is populated with results, search and report against that summary index using a search that specifies the summary index and the name of the search that you used to populate it. For example, this is the search you would use to get the top source_ips over the past year: 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=summary search_name="summary - firewall top src_ip" |top src_ip </font></code><br></div>
<p>Because this search specifies the search name, it filters out other data that have been placed in the summary index by other summary indexing searches. This search should run fairly quickly, even if the time range is a year or more.
</p><p><b>Note:</b> If you are running a search against a summary index that queries for events with a specific <code><font size="2">sourcetype</font></code> value, be aware that you need to use <code><font size="2">orig_sourcetype</font></code> instead. So instead of running a search against a summary index like <code><font size="2">...|stats timechart avg(ip) by sourcetype</font></code>, use <code><font size="2">...|stats timechart avg(ip) by orig_sourcetype</font></code>. 
</p><p>Why do you have to do this? When events are gathered into a summary index, Splunk Enterprise changes their <code><font size="2">sourcetype</font></code> values to "stash" and moves the original sourcetype values to <code><font size="2">orig_sourcetype</font></code>.
</p>
<h3> <a name="usesummaryindexing_set_up_summary_index_searches_in_splunk_web"><span class="mw-headline" id="Set_up_summary_index_searches_in_Splunk_Web"> Set up summary index searches in Splunk Web </span></a></h3>
<p>You can set up summary index searches through Splunk Web. Summary indexing is an alert option for scheduled reports. Once you determine the report that you want to use to populate a summary index, follow these steps:
</p><p><b>1.</b> Navigate to <b>Settings &gt; Searches, Reports, and Alerts</b>. 
</p><p><b>2.</b> Select the name of a report (or click <b>New</b> to create a new report).  
</p><p><b>3.</b> Under Schedule and alert, select <b>Schedule this search</b> if the report isn't already scheduled. 
</p>
<dl><dd> You must select <b>Schedule this search</b> to see the report scheduling options.
</dd></dl><p><b>4.</b> Schedule the report to run on an appropriate interval. 
</p>
<dl><dd> Searches that populate summary indexes should run on a fairly frequent basis in order to create statistically accurate final reports. If the report you're running against the summary index is gathering information for the past week, you should have the summary report run on an hourly basis, collecting information for each hour. If you're running reports over the past year's worth of data, you might have the summary index collect data on a daily basis for the past day. For more information, see "Schedule reports" in the Reporting Manual.
</dd></dl><dl><dd><b>Note:</b> Be sure to schedule the report so that there are no data gaps and overlaps. For more on this <a href="#usesummaryindexing_schedule_the_populating_search_to_avoid_data_gaps_and_overlaps" class="external text">see the subtopic on this issue</a>, below.
</dd></dl><p><b>5.</b> Under Alert, set <b>Condition</b> to <i>always</i>.
</p><p><b>6.</b> Set <b>Alert mode</b> to <i>Once per search.</i> 
</p>
<dl><dd> This ensures that the alert will be triggered each time the report runs.
</dd></dl><p><img alt="5.0-enable sum indexing.jpg" src="images/e/e0/5.0-enable_sum_indexing.jpg" width="405" height="204"></p><p><b>7.</b> Under Summary indexing, select <b>Enable</b>.
</p>
<dl><dd> When you select <b>Enable</b> Splunk Enterprise automatically sets the alert <b>Condition</b> to <i>always</i> and the <b>Alert mode</b> to <i>Once per search.</i> You won't be able to select other values for these fields without disabling summary indexing.
</dd></dl><p><b>8.</b> Select the name of the summary index that the report populates from the <b>Select the summary index</b> list. 
</p>
<dl><dd> The default summary index is named <i>summary</i>. The list only displays indexes to which you have permission to write. 
</dd></dl><p><b>9.</b> (Optional) You may need to create additional summary indexes if you plan to run a variety of summary index reports. 
</p>
<dl><dd> For information about creating new indexes, see "Set up multiple indexes" in the Managing Indexers and Clusters manual. It's a good idea to create indexes that are dedicated to the collection of summary data.
</dd></dl><p><b>10.</b> (Optional) Under <b>Add fields</b>, you can add field/value pairs to the summary index definition. 
</p>
<dl><dd> These key/value pairs are annotated to each event that gets summary indexed. This makes it easier to find them with later searches. For example, you could add the name of the report populating the summary index (<i>report</i>=<i>summary_firewall_top_src_ip</i>) or the name of the index that the report populates (<i>index</i>=<i>summary</i>), and then search on those terms later.
</dd></dl><dl><dd> <b>Note:</b> You can also add field/value pairs to the summary index configuration in <code><font size="2">savedsearches.conf</font></code>. For more information, see <a href="#configuresummaryindexes" class="external text">"Configure summary indexes"</a> in the Knowledge Manager manual.
</dd></dl><p>For more information about saving searches as reports and alerts, see: "Create and edit reports" (in the Reporting Manual)  and "Create an alert" (in the Alerting Manual).
</p>
<h4><font size="3"><b><i> <a name="usesummaryindexing_schedule_the_populating_report_to_avoid_data_gaps_and_overlaps"><span class="mw-headline" id="Schedule_the_populating_report_to_avoid_data_gaps_and_overlaps"> Schedule the populating report to avoid data gaps and overlaps </span></a></i></b></font></h4>
<p>To minimize data gaps and overlaps you should be sure to set appropriate intervals and delays in the schedules of reports you use to populate summary indexes.
</p><p><i>Gaps</i> in a summary index are periods of time when a summary index fails to index events. Gaps can occur if: 
</p>
<ul><li> <b>The summary-index-populating report takes too long to run and runs past the next scheduled run time.</b> For example, if you were to schedule the report that populates the summary to run every 5 minutes when that report typically takes around 7 minutes to run, you would have problems, because the search won't run again when it's still running a preceding report.
</li><li> <b>You have forced the summary-index-populating report to use real-time scheduling.</b> You do this by mistakenly changing the report definition in <code><font size="2">savedsearches.conf</font></code> so that the <code><font size="2">realtime_schedule</font></code> attribute is set to <code><font size="2">1</font></code>, enabling real-time scheduling. This setting can result in data collection gaps if you are concurrently running several reports. When you define a summary-index-populating scheduled report in Splunk Web by selecting <b>Enable</b> for summary indexing and saving the report, Splunk Enterprise automatically sets <code><font size="2">realtime_schedule</font></code> to <code><font size="2">0</font></code>, to ensure that the report never skips a scheduled run. For more information see "Configure the priority of scheduled reports", in the Reporting Manual.
</li><li> <b><code><font size="2">splunkd</font></code> goes down.</b> If Splunk Enterprise can't index events, you will have gaps in your summary indexes. 
</li></ul><p><i>Overlaps</i> are events in a summary index (from the same report) that share the same timestamp. Overlapping events skew reports and statistics created from summary indexes. Overlaps can occur if you set the time range of a report to be longer than the frequency of the schedule of the report. In other words, don't arrange for a report that runs hourly to gather data for the past 90 minutes.
</p><p><b>Note:</b> If you think you have gaps or overlaps in your summary index data, Splunk Enterprise provides methods of detecting them and either backfilling them (in the case of gaps) or deleting the overlapping events. For more information, see <a href="#managesummaryindexgapsandoverlaps" class="external text">"Manage summary index gaps and overlaps"</a> in this manual.
</p>
<h3> <a name="usesummaryindexing_how_summary_indexing_works"><span class="mw-headline" id="How_summary_indexing_works"> How summary indexing works </span></a></h3>
<p>In Splunk Web, summary indexing is an alert option for scheduled saved searches. When you run a saved search with summary indexing turned on, its search results are temporarily stored in a file (<code><font size="2">$SPLUNK_HOME/var/spool/splunk/&lt;savedsearch_name&gt;_&lt;random-number&gt;.stash</font></code>). From the file, Splunk Enterprise uses the addinfo command to add general information about the current search and the fields you specify during configuration to each result. Splunk Enterprise then indexes the resulting event data in the summary index that you've designated for it (<code><font size="2">index=summary</font></code> by default).
</p><p><b>Note:</b> Use the addinfo command to add fields containing general information about the current search to the search results going into a summary index. General information added about the search helps you run reports on results you place in a summary index.
</p>
<h3> <a name="usesummaryindexing_summary_indexing_of_data_without_timestamps"><span class="mw-headline" id="Summary_indexing_of_data_without_timestamps"> Summary indexing of data without timestamps </span></a></h3>
<p>To set the time for summary index events, Splunk Enterprise uses the following information, in this order of precedence:
</p><p><b>1.</b> The <code><font size="2">_time</font></code> value of the event being summarized.
</p><p><b>2.</b> The earliest (or minimum) time of the scheduled search that populates the summary index. For example, if the summary-index-populating search covers the two minutes preceding each launch of its search, its earliest time is -2m.
</p><p><b>3.</b> The current system time (in the case of an "all time" search, where no "earliest" value is specified)
</p><p>In the majority of cases, your events will have timestamps, so the first method of discerning the summary index timestamp holds. But if you are summarizing data that doesn't contain an <code><font size="2">_time</font></code> field (such as data from a lookup), the resulting events will have the timestamp of the earliest time of the summary-index-populating search.   
</p><p>For example, if you summarize the lookup "asset_table" every night at midnight, and the asset table does not contain an <code><font size="2">_time</font></code> column, tonight's summary will have an <code><font size="2">_time</font></code> value equal to the earliest time of the search. If I have set the time range of the search to be between <code><font size="2">-24h</font></code> and <code><font size="2">+0s</font></code>, each summarized event will have an <code><font size="2">_time</font></code> value of <code><font size="2">now()-86400</font></code> (that's the start time of the search minus 86,400 seconds, or 24 hours). This means that every event without an <code><font size="2">_time</font></code> field value that is found by this summary-index-populating search will be given the exact same <code><font size="2">_time</font></code> value: the search's earliest time.
</p><p>The best practice for summarizing data without a time stamp is to manually create an <code><font size="2">_time</font></code> value as part of your search. Following on from the example above:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">|inputlookup asset_table | eval _time=now()</font></code><br></div>
<h3> <a name="usesummaryindexing_fields_added_to_summary-indexed_data_by_the_si-_summary_indexing_commands"><span class="mw-headline" id="Fields_added_to_summary-indexed_data_by_the_si-_summary_indexing_commands">Fields added to summary-indexed data by the si- summary indexing commands</span></a></h3>
<p><b>Caution:</b> Use of these fields and their encoded data by any search commands other than the <code><font size="2">si*</font></code> summary indexing commands is unsupported. The format and content of these fields can change at any time without warning.
</p><p>When you run searches with the <code><font size="2">si*</font></code> commands in order to populate a summary index, Splunk Enterprise adds a set of special fields to the summary index data that all begin with <code><font size="2">psrsvd</font></code>, such as <code><font size="2">psrsvd_ct_bytes</font></code> and <code><font size="2">psrsvd_v</font></code> and so on. When you run a search against the summary index with reporting commands like <code><font size="2">chart</font></code>, <code><font size="2">timechart</font></code>, and <code><font size="2">stats</font></code>, Splunk Enterprise uses the <code><font size="2">psrsvd*</font></code> fields to calculate results for tables and charts that are statistically correct. <code><font size="2">psrsvd</font></code> stands for "prestats reserved."
</p><p>Most <code><font size="2">psrsvd</font></code> types present information about a specific field in the original (pre-summary indexing) file in the dataset, altough some <code><font size="2">psrsvd</font></code> types are not scoped to a single field. The general pattern is <code><font size="2">psrsvd_[type]_[fieldname]</font></code>. For example, <code><font size="2">psrsvd_ct_bytes</font></code> presents count information for the <code><font size="2">bytes</font></code> field. 
</p><p>Here is a list of the available <code><font size="2">psrsvd</font></code> types:
</p>
<ul><li> <code><font size="2">ct</font></code> = count
</li><li> <code><font size="2">gc</font></code> = group count (the count for a stats "grouping," not scoped to a single field.
</li><li> <code><font size="2">nc</font></code> = numerical count (number of numerical values)
</li><li> <code><font size="2">nn</font></code> = minimum numerical value
</li><li> <code><font size="2">nx</font></code> = maximum numerical value
</li><li> <code><font size="2">sm</font></code> = sum
</li><li> <code><font size="2">sn</font></code> = minimum lexicographical value
</li><li> <code><font size="2">ss</font></code> = sum of squares
</li><li> <code><font size="2">sx</font></code> = maximum lexicographical value
</li><li> <code><font size="2">v</font></code> = version (not scoped to a single field)
</li><li> <code><font size="2">vm</font></code> = value map (all distinct values for the field and the number of times they appear)
</li><li> <code><font size="2">vt</font></code> = value type (contains the precision of the associated field)
</li></ul><a name="managesummaryindexgapsandoverlaps"></a><h2> <a name="managesummaryindexgapsandoverlaps_manage_summary_index_gaps"><span class="mw-headline" id="Manage_summary_index_gaps"> Manage summary index gaps</span></a></h2>
<p>The accuracy of your summary index searches can be compromised if the summary indexes involved have gaps in their collected data.
</p><p>Gaps in summary index data can come about for a number of reasons:
</p>
<ul><li> <b>A summary index initially only contains events from the point that you start data collection:</b> Don't lose sight of the fact that summary indexes won't have data from before the summary index collection start date--unless you arrange to put it in there yourself with the backfill script. 
</li><li> <b><code><font size="2">splunkd</font></code> outages:</b> If <code><font size="2">splunkd</font></code> goes down for a significant amount of time, there's a good chance you'll get gaps in your summary index data, depending on when the searches that populate the index are scheduled to run.
</li><li> <b>Searches that run longer than their scheduled intervals:</b> If the search you're using to populate the scheduled search runs longer than the interval that you've scheduled it to run on, then you're likely to end up with gaps because Splunk Enterprise won't run a scheduled search again when a preceding search is still running. For example, if you were to schedule the index-populating search to run every five minutes, you'll have a gap in the index data collection if the search ever takes more than five minutes to run.  
</li></ul><p><b>Note:</b> For general information about creating and maintaining summary indexes, see <a href="#usesummaryindexing" class="external text">"Use summary indexing for increased reporting efficiency"</a> in the Knowledge Manager manual.
</p>
<h3> <a name="managesummaryindexgapsandoverlaps_use_the_backfill_script_to_add_other_data_or_fill_summary_index_gaps"><span class="mw-headline" id="Use_the_backfill_script_to_add_other_data_or_fill_summary_index_gaps">Use the backfill script to add other data or fill summary index gaps</span></a></h3>
<p>The <code><font size="2">fill_summary_index.py</font></code> script backfills gaps in summary index collection by running the saved searches that populate the summary index as they would have been executed at their regularly scheduled times for a given time range. In other words, even though your new summary index only started collecting data at the start of this week, if necessary you can use <code><font size="2">fill_summary_index.py</font></code> to fill the summary index with data from the past month.
</p><p>In addition, when you run <code><font size="2">fill_summary_index.py</font></code> you can specify an App and schedule backfill actions for a list of summary index searches associated with that App, or simply choose to backfill all saved searches associated with the App.
</p><p>When you enter the <code><font size="2">fill_summary_index.py</font></code> commands through the CLI, you must provide the backfill time range by indicating an "earliest time" and "latest time" for the backfill operation. You can indicate the precise times either by using relative time identifiers (such as <code><font size="2">-3d@d</font></code> for "3 days ago at midnight") or by using UTC epoch numbers. The script automatically computes the times during this range when the summary index search would have been run. 
</p><p><b>NOTE:</b> To ensure that the <code><font size="2">fill_summary_index.py</font></code> script only executes summary index searches at times that correspond to missing data, you must use <code><font size="2">-dedup true</font></code> when you invoke it.
</p><p>The <code><font size="2">fill_summary_index.py</font></code> script requires that you provide necessary authentication (username and password). If you know the valid Splunk Enterprise key when you invoke the script, you can pass it in via the <code><font size="2">-sk</font></code> option. 
</p><p>The script is designed to prompt you for any required information that you fail to provide in the command line, including the names of the summary index searches, the authentication information, and the time range.
</p>
<h4><font size="3"><b><i> <a name="managesummaryindexgapsandoverlaps_examples_of_fill_summary_index.py_invocation"><span class="mw-headline" id="Examples_of_fill_summary_index.py_invocation"> Examples of fill_summary_index.py invocation </span></a></i></b></font></h4>
<p><b>If this is your situation:</b>
</p><p>You need to backfill all of the summary index searches for the splunkdotcom App for the past month--but you also need to skip any searches that already have data in the summary index:
</p><p><b>Then you'd enter this into the CLI:</b>
</p><p><code><font size="2">./splunk cmd python fill_summary_index.py -app splunkdotcom -name "*" -et -mon@mon -lt @mon -dedup true -auth admin:changeme</font></code>
</p><p><b>If this is your situation:</b>
</p><p>You need to backfill the <code><font size="2">my_daily_search</font></code> summary index search for the past year, running no more than 8 concurrent searches at any given time (to reduce impact on Splunk Enterprise performance while the system collects the backfill data). You do <i>not</i> want the script to skip searches that already have data in the summary index. The <code><font size="2">my_daily_search</font></code> summary index search is owned by the "admin" role. 
</p><p><b>Then you'd enter this into the CLI:</b>
</p><p><code><font size="2">./splunk cmd python fill_summary_index.py -app search -name my_daily_search -et -y -lt now -j 8 -owner admin -auth admin:changeme</font></code>
</p><p><b>Note:</b> You need to specify the <code><font size="2">-owner</font></code> option for searches that are owned by a specific user or role.
</p>
<h4><font size="3"><b><i> <a name="managesummaryindexgapsandoverlaps_what_to_do_if_fill_summary_index.py_is_interrupted_while_running"><span class="mw-headline" id="What_to_do_if_fill_summary_index.py_is_interrupted_while_running"> What to do if fill_summary_index.py is interrupted while running</span></a></i></b></font></h4>
<p>In the app that you are invoking fill_summary_index.py from (default: 'search'), there will be a 'log' directory.  In this directory, there will be an empty temp file named 'fsidx*lock'.
</p><p>Delete the 'fsidx*lock' file and you will be able to restart fill_summary_index.py.
</p>
<h4><font size="3"><b><i> <a name="managesummaryindexgapsandoverlaps_fill_summary_index.py_usage_and_commands"><span class="mw-headline" id="fill_summary_index.py_usage_and_commands"> fill_summary_index.py usage and commands </span></a></i></b></font></h4>
<p>In the CLI, start by entering:
</p><p><code><font size="2">python fill_summary_index.py</font></code> 
</p><p>...and add the required and optional fields from the table below. 
</p><p><b>Note:</b> <code><font size="2">&lt;boolean&gt;</font></code> options accept the values <code><font size="2">1</font></code>, <code><font size="2">t</font></code>, <code><font size="2">true</font></code>, or <code><font size="2">yes</font></code> for "true" and <code><font size="2">0</font></code>, <code><font size="2">f</font></code>, <code><font size="2">false</font></code>, or <code><font size="2">no</font></code> for "false."
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0"> Field </th><th bgcolor="#C0C0C0"> Value
</th></tr><tr><td valign="center" align="left"><code><font size="2">-et &lt;string&gt;</font></code>
</td><td valign="center" align="left">Earliest time (required). Either a UTC time or a relative time string.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-lt &lt;string&gt;</font></code>
</td><td valign="center" align="left">Latest time (required). Either a UTC time or a relative time string.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-app &lt;string&gt;</font></code>
</td><td valign="center" align="left">The app context to use (defaults to <code><font size="2">None</font></code>).
</td></tr><tr><td valign="center" align="left"><code><font size="2">-name &lt;string&gt;</font></code>
</td><td valign="center" align="left">Specify a single saved search name. Can specify multiple times to provide multiple names. Use the wildcard symbol ("*") to specify all enabled, scheduled saved searches that have a summary index action.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-names &lt;string&gt;</font></code>
</td><td valign="center" align="left">Specify a comma seperated list of saved search names.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-namefile &lt;filename&gt;</font></code>
</td><td valign="center" align="left">Specify a file with a list of saved search names, one per line. Lines beginning with a <code><font size="2">#</font></code> are considered comments and ignored.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-owner &lt;string&gt;</font></code>
</td><td valign="center" align="left">The user context to use (defaults to "None").
</td></tr><tr><td valign="center" align="left"><code><font size="2">-index &lt;string&gt;</font></code>
</td><td valign="center" align="left">Identifies the summary index that the saved search populates. If the index is not provided, the backfill script tries to determine it automatically. If this attempt at auto index detection fails, the index defaults to "summary".
</td></tr><tr><td valign="center" align="left"><code><font size="2">-auth &lt;string&gt;</font></code>
</td><td valign="center" align="left">The authentication string expects either <code><font size="2">&lt;username&gt;</font></code> or <code><font size="2">&lt;username&gt;:&lt;password&gt;</font></code>.  If only a username is provided, the script requests the password interactively.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-sleep &lt;float&gt;</font></code>
</td><td valign="center" align="left">Number of seconds to sleep between each search. Default is 5 seconds.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-j &lt;int&gt;</font></code>
</td><td valign="center" align="left">Maximum number of concurrent searches to run (default is 1).
</td></tr><tr><td valign="center" align="left"><code><font size="2">-dedup &lt;boolean&gt;</font></code>
</td><td valign="center" align="left">When this option is set to true, the script does not run saved searches for a scheduled timespan if data already exists in the summary index for that timespan. This option is set to false by default.<br><br><b>Note:</b> This option has no connection to the <code><font size="2">dedup</font></code> command in the search language. The script does not have the ability to perform event-level data analysis. It cannot determine whether certain events are duplicates of others.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-nolocal &lt;boolean&gt;</font></code>
</td><td valign="center" align="left">Specifies that the summary indexes are not on the search head but are on the indexes instead, if you are working with a distributed environment. To be used in conjunction with <code><font size="2">-dedup</font></code>.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-showprogress &lt;boolean&gt;</font></code>
</td><td valign="center" align="left">When this option is set to true, the script periodically shows the done progress for each currently running search that it spawns. If this option is unused, its default is false.
</td></tr><tr><th colspan="2" bgcolor="#C0C0C0">Advanced options: these should not be used in almost all cases.
</th></tr><tr><td valign="center" align="left"><code><font size="2">-trigger &lt;boolean&gt;</font></code>
</td><td valign="center" align="left">When this option is set to false, the script runs each search but does not trigger the summary indexing action. If this option is unused its default is true.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-dedupsearch &lt;string&gt;</font></code>
</td><td valign="center" align="left">Indicates the search to be used to determine if data corresponding to a particular saved search at a specific scheduled times is present.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-distdedupsearch &lt;string&gt;</font></code>
</td><td valign="center" align="left">Same as <code><font size="2">-dedupsearch</font></code> except that this is a distributed search string. It does not limit its scope to the search head. It looks for summary data on the indexers as well.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-namefield &lt;string&gt;</font></code>
</td><td valign="center" align="left">Indicates the field in the summary index data that contains the name of the saved search that generated that data.
</td></tr><tr><td valign="center" align="left"><code><font size="2">-timefield &lt;string&gt;</font></code>
</td><td valign="center" align="left">Indicates the field in the summary index data that contains the scheduled time of the saved search that generated that data.
</td></tr></table><a name="configuresummaryindexes"></a><h2> <a name="configuresummaryindexes_configure_summary_indexes"><span class="mw-headline" id="Configure_summary_indexes"> Configure summary indexes</span></a></h2>
<p>For a general overview of summary indexing and instructions for setting up summary indexing through Splunk Web, see the topic <a href="#usesummaryindexing" class="external text">"Use summary indexing for increased reporting efficiency"</a> in the Knowledge Manager manual.
</p><p>You can't manually configure a summary index for a saved report in <code><font size="2">savedsearches.conf</font></code> until it is set up as a scheduled report that runs on a regular interval, triggers each time it is run, and has the <b>Enable summary indexing</b> alert option selected. 
</p><p>In addition, you need to enter the name of the summary index that the report will populate. You do this through the detail page for the report in <b>Settings &gt; Searches and Reports</b> after selecting <b>Enable summary indexing</b>. The <i>Summary</i> index is the default summary index (the index that Splunk Enterprise uses if you do not indicate another one).
</p><p>If you plan to run a variety of summary index reports you may need to create additional summary indexes. For information about creating new indexes, see "Set up multiple indexes" in the Managing Indexers and Clusters manual. It's a good idea to create indexes that are dedicated to the collection of summary data. 
</p><p>Summary indexing volume is not counted against your license, even if you have several summary indexes. In the event of a license violation, summary indexing will halt like any other non-internal search behavior.
</p><p><b>Note:</b> If you enter the name of an index that does not exist, Splunk Enterprise runs the report on the schedule you've defined, but it does not save the report data to a summary index.
</p><p>For more information about creating and managing reports, see "Create and edit reports" in this manual. 
</p><p>For more information about defining a report that can populate a summary index, see the subtopic on setting up summary index reports in Splunk Web in <a href="#usesummaryindexing_set_up_summary_index_searches_in_splunk_web" class="external text">"Use summary indexing for in increased reporting efficiency,"</a> in this manual.
</p><p><b>Note:</b> When you define the report that you'll use to build your index, most of the time you should use the <a href="#usesummaryindexing_use_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a> in the report's search string. These commands are prefixed with "si-": <code><font size="2">sichart</font></code>, <code><font size="2">sitimechart</font></code>, <code><font size="2">sistats</font></code>, <code><font size="2">sitop</font></code>, and <code><font size="2">sirare</font></code>. The reports you create with them should be versions of the report that you'll eventually use to query the completed summary index. 
</p><p>The summary index transforming commands automatically take into account the issues that are covered in "Considerations for summary index report definition" below, such as scheduling shorter time ranges for the populating report, and setting the populating report to take a larger sample. You only have to worry about these issues if the report you are using to build your index does <i>not</i> include summary index transforming commands. 
</p><p>If you do not use the summary index transforming commands, you can use the <code><font size="2">addinfo</font></code> and <code><font size="2">collect</font></code> search commands to create a report that Splunk Enterprise saves and schedules, and which populates a pre-created summary index. For more information about that method, see "Manually populate the summary index" in this topic. 
</p>
<h3> <a name="configuresummaryindexes_customize_summary_indexing_for_a_scheduled_report"><span class="mw-headline" id="Customize_summary_indexing_for_a_scheduled_report"> Customize summary indexing for a scheduled report </span></a></h3>
<p>When you use Splunk Web to enable summary indexing for a scheduled and summary-index-enabled report, Splunk Enterprise automatically generates a stanza in <code><font size="2">$SPLUNK_HOME/etc/system/local/savedsearches.conf</font></code>. You can customize summary indexing for the report by editing this stanza. 
</p><p>If you've used Splunk Web to save and schedule a report, but haven't used Splunk Web to enable the summary index for the report, you can easily enable summary indexing for the report through <code><font size="2">savedsearches.conf</font></code> as long as you have a new index for it to populate. For more information about manual index configuration, see, see the topic  "About managing indexes" in the Managing Indexers and Clusters manual.
</p>
<div class="samplecode">
<code><font size="2"><br>[ &lt;name&gt; ]<br>action.summary_index = 0 | 1<br>action.summary_index._name = &lt;index&gt;<br>action.summary_index.&lt;field&gt; = &lt;value&gt;<br></font></code></div>
<ul><li> <code><font size="2">[&lt;name&gt;]</font></code>: Splunk Enterprise names the stanza based on the name of the scheduled report that you enabled for summary indexing.
</li><li> <code><font size="2">action.summary_index = 0 | 1</font></code>: Set to 1 to enable summary indexing. Set to 0 to disable summary indexing.
</li><li> <code><font size="2">action.summary_index._name = &lt;index&gt;</font></code> - This displays the name of the summary index populated by this report. If you've created a specific summary index for this report, enter its name in <code><font size="2">&lt;index&gt;</font></code>. Defaults to <code><font size="2">summary</font></code>, the summary index that is delivered with Splunk Enterprise.
</li><li> <code><font size="2">action.summary_index.&lt;field&gt; = &lt;value&gt;</font></code>: Specify a field/value pair to add to every event that gets summary indexed by this report. You can define multiple field/value pairs for a single summary index report. 
</li></ul><p>This field/value pair acts as a "tag" of sorts that makes it easier for you to identify the events that go into the summary index when you are running reports against the greater population of event data. This key is optional but we recommend that you never set up a summary index without at least one field/value pair. 
</p><p>For example, add the name of the report that is populating the summary index (<code><font size="2">action.summary_index.report = summary_firewall_top_src_ip</font></code>), or the name of the index that the report populates (<code><font size="2">action.summary_index.index = search</font></code>).
</p>
<h3> <a name="configuresummaryindexes_search_commands_useful_to_summary_indexing"><span class="mw-headline" id="Search_commands_useful_to_summary_indexing"> Search commands useful to summary indexing </span></a></h3>
<p>Summary indexing utilizes of a set of specialized transforming commands which you need to use if you are manually creating your summary indexes without the help of the Splunk Web interface or the <a href="#usesummaryindexing_using_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a>. 
</p>
<ul><li> <code><font size="2">addinfo</font></code>: Summary indexing uses <code><font size="2">addinfo</font></code> to add fields containing general information about the current report to the report results going into a summary index.  Add <code><font size="2">| addinfo</font></code> to any report to see what results will look like if they are indexed into a summary index. 
</li><li> <code><font size="2">collect</font></code>: Summary indexing uses <code><font size="2">collect</font></code> to index report results into the summary index. Use <code><font size="2">| collect</font></code> to index any report results into another index (using <code><font size="2">collect</font></code> command options). 
</li><li> <code><font size="2">overlap</font></code>: Use <code><font size="2">overlap</font></code> to identify gaps and overlaps in a summary index. <code><font size="2">overlap</font></code> finds events of the same <code><font size="2">query_id</font></code> in a summary index with overlapping timestamp values or identifies periods of time where there are missing events.
</li></ul><h3> <a name="configuresummaryindexes_manually_configure_a_report_to_populate_a_summary_index"><span class="mw-headline" id="Manually_configure_a_report_to_populate_a_summary_index"> Manually configure a report to populate a summary index </span></a></h3>
<p>If you want to configure summary indexing without using the report options dialog in Splunk Web and the <a href="#usesummaryindexing_using_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a>, you must first configure a summary index just like you would any other index via <code><font size="2">indexes.conf</font></code>. For more information about manual index configuration, see, see the topic "About managing indexes" in the Managing Indexers and Clusters manual.
</p><p><b>Important:</b> You must restart Splunk Enterprise for changes in <code><font size="2">indexes.conf</font></code> to take effect.
</p><p><b>1.</b> Design a search string that you want to summarize results from in Splunk Web. 
</p>
<ul><li> Be sure to limit the time range of your report. The number of results that the report generates needs to fit within the maximum report result limits you have set for reporting. 
</li><li> Make sure to choose a time interval that works for your data, such as 10 minutes, 2 hours, or 1 day. (For more information about using Splunk Web to schedule report intervals, see the topic "Schedule reports" in the Reporting Manual.)
</li></ul><p><b>2.</b> Use the <code><font size="2">addinfo</font></code> search command. Append <code><font size="2">| addinfo</font></code> to the end of the report's search string.  
</p>
<ul><li> This command adds information about the report to events that the collect command requires in order to place them into a summary index.
</li><li> You can always add <code><font size="2">| addinfo</font></code> to any search string to preview what its results will look like in a summary index.
</li></ul><p><b>3.</b> Add the <code><font size="2">collect</font></code> search command to the report's search string.  Append <code><font size="2">|collect index=&lt;index_name&gt; addtime=t marker="report_name=\"&lt;summary_report_name&gt;\""</font></code> to the end of the search string. 
</p>
<ul><li> Replace <code><font size="2">index_name</font></code> with the name of the summary index.
</li><li> Replace <code><font size="2">summary_report_name</font></code> with a key to find the results of this report in the index.
</li><li> A <code><font size="2">summary_report_name</font></code> *must* be set if you wish to use the overlap search command on the generated events.
</li></ul><p><b>Note:</b> For the general case we recommend that you use the provided <code><font size="2">summary_index</font></code> alert action.  Configuring via <code><font size="2">addinfo</font></code> and <code><font size="2">collect</font></code> requires some redundant steps that are not needed when you generate summary index events from scheduled reports. Manual configuration remains necessary when you backfill a summary index for timeranges which have already transpired.
</p>
<h3> <a name="configuresummaryindexes_considerations_for_summary_index_report_definition"><span class="mw-headline" id="Considerations_for_summary_index_report_definition"> Considerations for summary index report definition </span></a></h3>
<p>If for some reason you're going to set up a summary-index-populating report that <i>does not use</i> the <a href="#usesummaryindexing_using_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a>, you should take a few moments to plan out your approach. With summary indexing, the egg comes before the chicken. Use the report that you actually want to run and review the results of to help define the report you use to populate the summary index. 
</p><p>Many summary-searching reports involve aggregated statistics--for example, a report where you are searching for the top 10 ip addresses associated with firewall offenses over the past day--when the main index accrues millions of events per day. 
</p><p>If you populate the summary index with the results of the same report that you run <i>on</i> the summary index, you'll likely get results that are statistically inaccurate. You should follow these rules when defining the report that populates your summary index to improve the accuracy of aggregated statistics generated from summary index reports.
</p>
<h4><font size="3"><b><i> <a name="configuresummaryindexes_schedule_a_shorter_time_range_for_the_populating_report"><span class="mw-headline" id="Schedule_a_shorter_time_range_for_the_populating_report"> Schedule a shorter time range for the populating report </span></a></i></b></font></h4>
<p>The report that populates your summary index should be scheduled on a shorter (and therefore more frequent) interval than that of the report that you eventually run against the index. You should go for the smallest time range possible. For example, if you need to generate a daily "top" report, then the report populating the summary index should take its sample on an hourly basis. 
</p>
<h4><font size="3"><b><i> <a name="configuresummaryindexes_set_the_populating_report_to_take_a_larger_sample"><span class="mw-headline" id="Set_the_populating_report_to_take_a_larger_sample"> Set the populating report to take a larger sample </span></a></i></b></font></h4>
<p>The report populating the summary index should seek out a significantly larger sample than the report that you want to run on the summary index.<b> So, for example, if you plan to search the summary index for the <i>daily top 10</i> offending ip addresses, you would set up a report to populate the summary index with the <i>hourly top 100</i> offending ip addresses.  </b>
</p><p>This approach has two benefits--it ensures a higher amount of statistical accuracy for the top 10 report (due to the larger and more-frequently-taken overall sample) and it gives you a bit of wiggle room if you decide you'd rather report on the top 20 or 30 offending ips.
</p><p>The <a href="#usesummaryindexing_using_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a> automatically take a sample that is larger than the report that you'll run to query the completed summary index, thus creating summary indexes with event data that is not incorrectly skewed. If you do not use those commands, you can use the head command to select a larger sample for the summary-index-populating report than the report that you run over the summary index. In other words, you would have <code><font size="2">| head=100</font></code> for the hourly summary-index-populating report, and <code><font size="2">| head=10</font></code> for the daily report over the completed summary index.
</p>
<h4><font size="3"><b><i> <a name="configuresummaryindexes_set_up_your_report_to_get_a_weighted_average"><span class="mw-headline" id="Set_up_your_report_to_get_a_weighted_average"> Set up your report to get a weighted average </span></a></i></b></font></h4>
<p>If your summary-index-populating report involves averages, and you are not using the <a href="#usesummaryindexing_using_the_summary_indexing_reporting_commands" class="external text">summary indexing transforming commands</a>, you need to set that report up to get a weighted average.
</p><p>For example, say you want to build hourly, daily, or weekly reports of average response times. To do this, you'd generate the "daily average" by averaging the "hourly averages" together. Unfortunately, the daily average becomes skewed if there aren't the same number of events in each "hourly average". You can get the correct "daily average" by using a weighted average function. 
</p><p>The following expression calculates the daily average response time correctly with a weighted average by using the <code><font size="2">stats</font></code> and <code><font size="2">eval</font></code> commands in conjunction with the <code><font size="2">sum</font></code> statistical aggregator. In this example, the <code><font size="2">eval</font></code> command creates a <code><font size="2">daily_average</font></code> field, which is the result of dividing the average response time sum by the average response time count.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">| stats sum(hourly_resp_time_sum) as resp_time_sum, sum(hourly_resp_time_count) as resp_time_count | eval daily_average= resp_time_sum/resp_time_count | .....</font></code><br></div>
<h4><font size="3"><b><i> <a name="configuresummaryindexes_schedule_the_summary-index-populating_report_to_avoid_data_gaps_and_overlaps"><span class="mw-headline" id="Schedule_the_summary-index-populating_report_to_avoid_data_gaps_and_overlaps"> Schedule the summary-index-populating report to avoid data gaps and overlaps </span></a></i></b></font></h4>
<p>Along with the above two rules, to minimize data gaps and overlaps you should also be sure to set appropriate intervals and delays in the schedules of reports you use to populate summary indexes. 
</p><p><b>Gaps</b> in a summary index are periods of time when a summary index fails to index events. Gaps can occur if: 
</p>
<ul><li> <code><font size="2">splunkd</font></code> goes down.
</li><li> the scheduled saved report (the one being summary indexed) takes too long to run and runs past the next scheduled run time. For example, if you were to schedule the report that populates the summary to run every 5 minutes when that report typically takes around 7 minutes to run, you would have problems, because the report won't run again when it's still running a preceding report.
</li></ul><p><i>Overlaps</i> are events in a summary index (from the same report) that share the same timestamp. Overlapping events skew reports and statistics created from summary indexes. Overlaps can occur if you set the time range of a saved report to be longer than the frequency of the schedule of the report, or if you manually run summary indexing using the collect command.
</p>
<h3> <a name="configuresummaryindexes_example_of_a_summary_index_configuration"><span class="mw-headline" id="Example_of_a_summary_index_configuration"> Example of a summary index configuration </span></a></h3>
<p>This example shows a configuration for a summary index of Apache server statistics as it might appear in <code><font size="2">savedsearches.conf</font></code>. The keys listed below enable summary indexing for the "Apache Method Summary" report.
</p><p><b>Note:</b> If you set <code><font size="2">action_summary.index=1</font></code>, you don't need to have the <code><font size="2">addinfo</font></code> or <code><font size="2">collect</font></code> commands in the report's search string.
</p>
<div class="samplecode">
<code><font size="2"><br>#name of the report = Apache Method Summary<br>[Apache Method Summary]<br># sets the report to run at each interval<br>counttype = always<br># enable the report schedule<br>enableSched = 1<br># report interval in cron notation (this means "every 5 minutes")<br>schedule = */5****<br># id of user for report<br>userid = jsmith<br># search string for summary index<br>search = index=apache_raw startminutesago=30 endminutesago=25 | extract auto=false | stats count by method<br># enable summary indexing<br>action.summary_index = 1<br>#name of summary index to which report results are added<br>action.summary_index._name = summary &nbsp;&nbsp;<br># add these keys to each event<br>action.summary_index.report = "count by method"<br></font></code></div>
<h3> <a name="configuresummaryindexes_other_configuration_files_affected_by_summary_indexing"><span class="mw-headline" id="Other_configuration_files_affected_by_summary_indexing"> Other configuration files affected by summary indexing </span></a></h3>
<p>In addition to the settings you configure in  savedsearches.conf, there are also settings for summary indexing in indexes.conf and  alert_actions.conf.
</p><p>Indexes.conf specifies index configuration for the summary index. 
Alert_actions.conf controls the alert actions (including summary indexing) associated with reports.
</p><p><b>Caution:</b> Do not edit settings in <code><font size="2">alert_actions.conf</font></code> without explicit instructions from Splunk Enterprise staff.
</p>
<a name="configurebatchmodesearch"></a><h2> <a name="configurebatchmodesearch_configure_batch_mode_search"><span class="mw-headline" id="Configure_batch_mode_search"> Configure batch mode search</span></a></h2>
<p>Running in batch mode means that the search executes one <b>bucket</b> at a time (in batches), rather than over time. For certain <b>reporting searches</b> that don't require the events to be time-ordered, this means that the reporting search can complete faster. Additionally, batch mode search improves the reliability for long-running <b>distributed searches</b>, which can fail when an <b>indexer</b> goes down while the search is running. In this case, Splunk Enterprise attempts to complete the search by reconnecting to the missing <b>peer</b> or redistributing the search across the rest of the peers.
</p>
<h3> <a name="configurebatchmodesearch_requirements_for_batch_mode_search"><span class="mw-headline" id="Requirements_for_batch_mode_search"> Requirements for batch mode search </span></a></h3>
<p>Batch mode search does not work with <b>real-time</b> and summarizing searches.
</p><p>There are restrictions on the types of reporting and transforming searches that can be run in batch mode. 
</p>
<ul><li> The search needs to be generating and can include transforming commands such as stats, chart, etc. However, it cannot include commands such as localize and transaction.
</li><li> If it is a non-distributed search, it cannot use commands that require events to be in time descending order. For example, it cannot include commands like streamstats, head, tail, etc.
</li></ul><p>Searches should run automatically in batch mode if they meet the requirements. This mode is turned on by default in limits.conf, with <code><font size="2">allow_batch_mode = true</font></code>. This is discussed in more detail in the next section of this topic.
</p><p>You can use the search job inspector to confirm whether or not a search is running in batch mode. Batch mode search is indicated by the boolean parameter <code><font size="2">isBatchModeSearch</font></code>. For more information about, read about <a href="#viewsearchjobpropertieswiththejobinspector" class="external text">"View search job properties with Search Job Inspector"</a> in the Knowledge Manager Manual.
</p>
<h3> <a name="configurebatchmodesearch_configuring_batch_mode_search"><span class="mw-headline" id="Configuring_batch_mode_search"> Configuring batch mode search </span></a></h3>
<p>Batch mode search is enabled in the <code><font size="2">limits.conf</font></code> configuration file under the <code><font size="2">[search]</font></code> stanza:
</p>
<div class="samplecode">
<code><font size="2"><br>[search]<br>allow_batch_mode = &lt;bool&gt;<br>batch_search_max_index_values = &lt;int&gt;<br></font></code></div>
<ul><li> If <code><font size="2">allow_batch_mode = false</font></code> , your transforming searches will never run in batch mode. Defaults to true.
</li><li> When allow_batch_mode = true, use the <code><font size="2">batch_search_max_index_values</font></code> to limit the number of events that are read from the index file (bucket). These entries are small, approximately 72 bytes; however, batch mode is more efficient when it can read more entries at once. Defaults to 1000000 (or 10M). 
</li></ul><p>You can lower batch_search_max_index_values, to 1000000 (1M) for example, to decrease the memory usage during searches. Note that setting this value to a smaller number can lead to slower search performance. A balance needs to be struck between more efficient searching in batch mode and running out of memory on the system with concurrently running searches.
</p><p>Other settings in limits.conf control the periodicity of retries to search peers in the event of failures, such as connection errors. The interval exists between failure and first retry, as well as successive retries in the event of further failures.
</p>
<div class="samplecode">
<code><font size="2"><br>batch_retry_min_interval = &lt;int&gt;<br>batch_retry_max_interval = &lt;int&gt;<br>batch_retry_scaling = &lt;double&gt;<br>batch_wait_after_end = &lt;int&gt;<br></font></code></div>
<ul><li> Use the <code><font size="2">batch_retry_min_interval</font></code> and <code><font size="2">batch_retry_max_interval</font></code> parameters to specify the minimum or maximum interval (in seconds) to wait before Splunk Enterprise attempts to retry the search on a failed peer. The minimum interval defaults to 5 seconds. The maximum interval defaults to 300 seconds. 
</li><li> After a retry attempt fails increase the time to wait before another retry by a scaling factor, <code><font size="2">batch_retry_scaling</font></code>, which takes a value greater than 1.0. Defaults to 1.5.
</li><li> Batch mode considers the search ended (finished) when all peers without communication failure have explicitly indicated that they have delivered the complete answer. After the search is completed, Splunk Enterprise will wait the period (in seconds) specified by <code><font size="2">batch_wait_after_end</font></code> before it attempts a retry with the lost-connection peer. Defaults to 900 seconds.
</li></ul><h3> <a name="configurebatchmodesearch_restarting_a_fractured_search"><span class="mw-headline" id="Restarting_a_fractured_search"> Restarting a fractured search </span></a></h3>
<p>Restarting of the search peer is handled differently depending on whether the peer is clustered or not.
</p>
<ul><li> In the non-clustered case, if a peer is lost, Splunk Enterprise will attempt to reconnect to the peer. Once the connection is re-established, the search resumes until completion. 
</li><li> In a clustered case, Splunk Enterprise waits for the cluster master to spawn a new generation.
</li></ul></body><script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

        <script src="js/index.js"></script></html>
