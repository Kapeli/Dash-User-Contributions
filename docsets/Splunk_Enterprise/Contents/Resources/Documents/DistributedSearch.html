<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="http://ogp.me/ns/fb#" charset="utf-8"><head><meta charset="UTF-8"><title></title>
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" type="text/css" href="css/main.css">
<link rel="stylesheet" href="css/style.css">
<style>
html,body {
margin: 0px;
padding: 10px;
width: 210mm;
max-width: 210mm;
overflow-x: hidden;
}
pre {
	width: 100%;
	overflow-x: hidden;
}
</style>
 <script src="js/prefixfree.min.js"></script>
</head><body><h1>Overview of distributed search</h1><a name="whatisdistributedsearch"></a><div class="all-questions"><h2> <a name="whatisdistributedsearch_about_distributed_search"><span class="mw-headline" id="About_distributed_search"> About distributed search</span></a></h2>
<p>Before reading this manual, see the <i>Distributed Deployment Manual.</i> That manual describes the fundamentals of Splunk Enterprise distributed deployment and shows how distributed search contributes to the overall deployment.
</p><p><b>Distributed search</b> provides a way to scale your deployment by separating the search management and presentation layer from the indexing and search retrieval layer.
</p>
<h3> <a name="whatisdistributedsearch_use_cases"><span class="mw-headline" id="Use_cases"> Use cases</span></a></h3>
<p>These are some of the key use cases for distributed search:
</p>
<ul><li> <b>Horizontal scaling for enhanced performance.</b> Distributed search facilitates horizontal scaling by providing a way to distribute the indexing and searching loads across multiple Splunk Enterprise instances, making it possible to index and search large quantities of data. 
</li></ul><ul><li> <b>Access control.</b> You can use distributed search to control access to indexed data. For example, some users, such as security personnel, might need access to data across the enterprise, while others need access to data only in their functional area.
</li></ul><ul><li> <b>Managing geo-dispersed data.</b> Distributed search allows local offices to access their own data, while maintaining centralized access at the corporate level. For example, users in Chicago and San Francisco can look just at their local data, while users at headquarters in New York can search the local data, as well as the data in Chicago and San Francisco.
</li></ul><h3> <a name="whatisdistributedsearch_distributed_search_components"><span class="mw-headline" id="Distributed_search_components"> Distributed search components</span></a></h3>
<p>With distributed search, a Splunk Enterprise instance called a <b>search head</b> sends search requests to a group of <b>indexers</b>, or <b>search peers</b>, which perform the actual searches on their indexes. The search head then merges the results back to the user. Here is a basic distributed search scenario, with one dedicated search head managing searches across several indexers:
</p><p><img alt="Horizontal scaling 60.png" src="images/b/b5/Horizontal_scaling_60.png" width="700" height="554"></p>
<h3> <a name="whatisdistributedsearch_types_of_distributed_search"><span class="mw-headline" id="Types_of_distributed_search">Types of distributed search </span></a></h3>
<p>There are several fundamental options for deploying a distributed search environment:
</p>
<ul><li> Use one or more independent search heads to search across the search peers. These search heads can be either dedicated or dual-purpose.
</li><li> Deploy multiple search heads in a search head cluster. The search heads in the cluster share resources, configurations, and jobs. This offers a way to scale your deployment transparently to your users. 
</li><li> Deploy search heads as part of an indexer cluster. Among other advantages, an indexer cluster promotes data availability and data recovery. The search heads in an indexer cluster can be either dedicated, independent search heads or members of a search head cluster.
</li></ul><p>The rest of this topic provides an introduction to these topologies.
</p>
<h4><font size="3"><b><i> <a name="whatisdistributedsearch_dedicated_and_dual-purpose_search_heads"><span class="mw-headline" id="Dedicated_and_dual-purpose_search_heads"> Dedicated and dual-purpose search heads</span></a></i></b></font></h4>
<p>The typical distributed search deployment uses a dedicated search head; that is, a search head dedicated to running searches. A dedicated search head does not index external data. 
</p><p>You can also configure one or more of your instances to function as both search peer and search head. These dual-purpose search heads can take the place of, or be in addition to, a dedicated search head. See <a href="#somesearchscenarios" class="external text">"Some search scenarios"</a> for examples of several distributed search topologies involving both dedicated and non-dedicated search heads.
</p>
<h4><font size="3"><b><i> <a name="whatisdistributedsearch_search_head_clusters"><span class="mw-headline" id="Search_head_clusters">Search head clusters </span></a></i></b></font></h4>
<p>A <b>search head cluster</b> is a group of search heads that work together to provide scalability and high availability. It serves as a central resource for searching across a set of search peers.
</p><p>The search heads in a cluster are, for most purposes, interchangeable. All search heads have access to the same set of search peers. They can also run or access the same searches, dashboards, knowledge objects, and so on.
</p><p>A search head cluster is the recommended topology when you need to run multiple search heads across the same set of search peers. The cluster coordinates the activity of the search heads, allocates jobs based on the current loads, and ensures that all the search heads have access to the same set of knowledge objects. 
</p><p>See <a href="#aboutshc" class="external text">"About search head clustering."</a>
</p>
<h4><font size="3"><b><i> <a name="whatisdistributedsearch_indexer_clusters_and_search_heads"><span class="mw-headline" id="Indexer_clusters_and_search_heads"> Indexer clusters and search heads</span></a></i></b></font></h4>
<p><b>Indexer clusters</b> also use search heads to search across the set of indexers, or <b>peer nodes</b>. The search heads in an indexer cluster can be either independent dedicated search heads or members of a search head cluster.
</p><p>You deploy and configure search heads very differently when they are part of an indexer cluster:
</p>
<ul><li> For information on using independent dedicated search heads with indexer clusters, see "Configure the search head" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</li></ul><ul><li> Fo information on using search head clusters with indexer clusters, read <a href="#shcandindexercluster" class="external text">"Integrate the search head cluster with an indexer cluster"</a>.
</li></ul><a name="somesearchscenarios"></a><h2> <a name="somesearchscenarios_some_search_scenarios"><span class="mw-headline" id="Some_search_scenarios"> Some search scenarios</span></a></h2>
<p>Distributed search offers a variety of ways to split functionality across a set of Splunk Enterprise instances. You can deploy dedicated search heads and dedicated indexers, or indexers that double as search heads, or a combination of both.  This topic illustrates a few possible scenarios.
</p><p><b>Note:</b> The terms <b>indexer</b> and <b>search peer</b> are used interchangeably. 
</p><p>This diagram shows a simple distributed search scenario for horizontal scaling, with one dedicated search head searching across three indexers:
</p><p><img alt="Horizontal scaling 60.png" src="images/b/b5/Horizontal_scaling_60.png" width="700" height="554"></p>
<code><font size="2"><br><br></font></code>
<p>In this diagram showing a distributed search scenario for access control, a dedicated search head in the Security department has visibility into all the indexing search peers. Each search peer also has the ability to search its own data. In addition, the department A search peer has access to both its data and the data of department B:
</p><p><img alt="Access control 60.png" src="images/8/8d/Access_control_60.png" width="700" height="554"></p>
<code><font size="2"><br><br></font></code>
<p>This diagram shows load-balanced forwarders inputting data across the set of indexers. In addition to a dedicated search head, each indexer doubles as a search head. All the search heads can search across the entire set of indexers:
</p><p><img alt="30 admin13 forwardreceive-dsearch 60.png" src="images/a/ab/30_admin13_forwardreceive-dsearch_60.png" width="700" height="554"></p><p>For information on load balancing, see "Set up load balancing" in the <i>Forwarding Data</i> manual.
</p>
<a name="whatsearchheadssend"></a><h2> <a name="whatsearchheadssend_what_search_heads_send_to_search_peers"><span class="mw-headline" id="What_search_heads_send_to_search_peers"> What search heads send to search peers </span></a></h2>
<p>When initiating a distributed search, the search head replicates and distributes its <b>knowledge objects</b> to its <b>search peers</b>, or indexers. Knowledge objects include saved searches, event types, and other entities used in searching across indexes. The search head needs to distribute this material to its search peers so that they  can properly execute queries on its behalf. This set of knowledge objects is called the <b>knowledge bundle.</b>
</p>
<h3> <a name="whatsearchheadssend_what_the_knowledge_bundle_contains"><span class="mw-headline" id="What_the_knowledge_bundle_contains">What the knowledge bundle contains</span></a></h3>
<p>The search peers use the search head's knowledge bundle to execute queries on its behalf. When executing a distributed search, the peers are ignorant of any local knowledge objects. They have access only to the objects in the search head's knowledge bundle.
</p><p>Bundles typically contain a subset of files (configuration files and assets) from <code><font size="2">$SPLUNK_HOME/etc/system</font></code>, <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and <code><font size="2">$SPLUNK_HOME/etc/users</font></code>.
</p><p>The process of distributing knowledge bundles means that peers by default receive nearly the entire contents of the search head's <b>apps</b>. If an app contains large binaries that do not need to be shared with the peers, you can eliminate them from the bundle and thus reduce the bundle size. See <a href="#limittheknowledgebundlesize" class="external text">"Modify the knowledge bundle"</a>.
</p>
<h3> <a name="whatsearchheadssend_location_of_the_knowledge_bundle"><span class="mw-headline" id="Location_of_the_knowledge_bundle">Location of the knowledge bundle </span></a></h3>
<p>On the search head, the knowledge bundles resides under the <code><font size="2">$SPLUNK_HOME/var/run</font></code> directory. The bundles have the extension <code><font size="2">.bundle</font></code> for full bundles or <code><font size="2">.delta</font></code> for delta bundles. They are tar files, so you can run <code><font size="2">tar tvf</font></code> against them to see the contents.
</p><p>The knowledge bundle gets distributed to the <code><font size="2">$SPLUNK_HOME/var/run/searchpeers</font></code> directory on each search peer. Because the knowledge bundle reside at a different location on the search peers than on the search head, search scripts should not hardcode paths to resources.
</p>
<h3> <a name="whatsearchheadssend_user_authorization"><span class="mw-headline" id="User_authorization"> User authorization </span></a></h3>
<p>All authorization for a distributed search originates from the search head. At the time it sends the search request to its search peers, the search head also distributes the authorization information. It tells the search peers the name of the user running the search, the user's role, and the location of the distributed <code><font size="2">authorize.conf</font></code> file containing the authorization information.
</p>
<h1>Deploy distributed search</h1><a name="overviewofconfiguration"></a><h2> <a name="overviewofconfiguration_deploy_a_distributed_search_environment"><span class="mw-headline" id="Deploy_a_distributed_search_environment"> Deploy a distributed search environment</span></a></h2>
<p><b>Important:</b> The topics in this chapter explain how to deploy a non-clustered <b>distributed search</b> topology. For information on deploying a search head cluster instead, read the chapter <a href="#shcsystemrequirements" class="external text">"Deploy search head clustering."</a>
</p><p>The basic configuration to enable distributed search is simple. You just designate one or more Splunk Enterprise instances as <b>search heads</b> and establish connections from each search head to a set of <b>search peers</b>, or <b>indexers</b>. 
</p><p>This is the type of topology that this topic specifically addresses:
</p><p><img alt="Horizontal scaling 60.png" src="images/b/b5/Horizontal_scaling_60.png" width="700" height="554"></p><p>The search head interfaces with the user and manages searches across the set of indexers. The indexers index incoming data and search the data, as directed by the search head.
</p><p>The deployment process is similar for any variants on this topology, such as multiple search heads or non-dedicated search heads that also serve as indexers.
</p>
<h3> <a name="overviewofconfiguration_deploy_distributed_search"><span class="mw-headline" id="Deploy_distributed_search"> Deploy distributed search </span></a></h3>
<p>To set up a simple distributed search topology, consisting of a single dedicated search head and several search peers, perform these steps:
</p><p><b>1.</b> Identify your requirements. See <a href="#distsearchsystemrequirements" class="external text">"System requirements and other deployment considerations for distributed search"</a>.
</p><p><b>2.</b> Designate a Splunk Enterprise instance as the search head. Since distributed search is enabled automatically on every full Splunk Enterprise instance, you do not actually perform any action in this step, aside from choosing the instance that you want to be your search head. 
</p><p>For a dedicated search head, choose an existing instance that is not indexing external data or install a new instance. For installation information, see the topic in the <i>Installation Manual</i> specific to your operating system.
</p><p><b>3.</b> Establish connections from the search head to all the search peers that you want it to search across.  This is the key step in the procedure. See <a href="#configuredistributedsearch" class="external text">"Add search peers to the search head"</a>.
</p><p><b>4.</b> Add data inputs to the search peers. You add inputs in the same way as for any indexer, either directly on the search peer or through forwarders connecting to the search peer. See the <i>Getting Data In</i> manual for information on data inputs.
</p><p><b>5.</b> Forward the search head's internal data to the search peers. See  <a href="#forwardsearchheaddata" class="external text">"Best practice: Forward search head data to the indexer layer"</a>.
</p><p><b>6.</b> Log in to the search head and perform a search that runs across all the search peers, such as a search for *. Examine the <code><font size="2">splunk_server</font></code> field in the results. Verify that all the search peers are listed in that field.
</p><p><b>7.</b> See the <i>Securing Splunk Enterprise</i>
manual for information on setting up authentication.
</p><p>To set up a more complex topology, deploy additional search heads and search peers as needed.
</p>
<h3> <a name="overviewofconfiguration_deploy_non-dedicated_search_heads"><span class="mw-headline" id="Deploy_non-dedicated_search_heads">Deploy non-dedicated search heads </span></a></h3>
<p>In some cases, you might need to deploy instances that serve as both search heads and search peers. You configure these dual-purpose search heads in the same way as dedicated search heads, by establishing connections to all external search peers.
</p><p>See <a href="#somesearchscenarios" class="external text">"Some search scenarios"</a> for examples of distributed search topologies that use both dedicated and non-dedicated search heads.
</p>
<h3> <a name="overviewofconfiguration_deploy_search_heads_in_indexer_clusters"><span class="mw-headline" id="Deploy_search_heads_in_indexer_clusters"> Deploy search heads in indexer clusters </span></a></h3>
<p>Splunk <b>indexer clusters</b> use search heads to search across their set of indexers, or <b>peer nodes</b>. You deploy search heads very differently when they are part of an indexer cluster. To learn about deploying search heads in indexer clusters, read "Enable the search head" in the <i>Managing Indexers and Clusters  of Indexers</i> manual.
</p>
<a name="distsearchsystemrequirements"></a><h2> <a name="distsearchsystemrequirements_system_requirements_and_other_deployment_considerations_for_distributed_search"><span class="mw-headline" id="System_requirements_and_other_deployment_considerations_for_distributed_search"> System requirements and other deployment considerations for distributed search</span></a></h2>
<p>This topic describes the key considerations when deploying a basic distributed search topology with search heads that function independently of each other. If instead you are deploying a search head cluster, see <a href="#shcsystemrequirements" class="external text">"System requirements and other deployment considerations for search head clusters."</a>
</p>
<h3> <a name="distsearchsystemrequirements_hardware_requirements_for_distributed_search_instances"><span class="mw-headline" id="Hardware_requirements_for_distributed_search_instances">Hardware requirements for distributed search instances</span></a></h3>
<p>For information on the hardware requirements for search heads and search peers (indexers), see "Reference hardware" in the <i>Capacity Planning Manual</i>. 
</p>
<h3> <a name="distsearchsystemrequirements_splunk_enterprise_version_compatibility"><span class="mw-headline" id="Splunk_Enterprise_version_compatibility"> Splunk Enterprise version compatibility </span></a></h3>
<p>It is best to upgrade search heads and search peers at the same time, to take full advantage of the latest search capabilities. If you cannot do so, follow these version compatibility guidelines.
</p>
<h4><font size="3"><b><i> <a name="distsearchsystemrequirements_compatibility_between_search_heads_and_search_peers"><span class="mw-headline" id="Compatibility_between_search_heads_and_search_peers"> Compatibility between search heads and search peers </span></a></i></b></font></h4>
<p>6.x search heads are compatible with 6.x and 5.x search peers. The search head must be at the same or a higher level than the search peers:
</p>
<ul><li> A 6.x search head is compatible with a 5.x search peer.
</li><li> A  5.x search head is not compatible with a 6.x search peer.
</li></ul><p>These guidelines are valid for both standalone search heads and search heads that are participating in a search head cluster. 
</p><p><b>Important:</b> Search heads participating in indexer clusters have different compatibility restrictions. See "Splunk Enterprise version compatibility" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p>
<h4><font size="3"><b><i> <a name="distsearchsystemrequirements_mixed-version_distributed_search_compatibility"><span class="mw-headline" id="Mixed-version_distributed_search_compatibility"> Mixed-version distributed search compatibility  </span></a></i></b></font></h4>
<p>You can run a 6.x search head against 5.x search peers, but there are a few compatibility issues to be aware of.  To take full advantage of the 6.x feature set, it is recommended that you upgrade both search head(s) and search peers at the same time.
</p><p>This section describes the compatibility issues.
</p>
<h5> <a name="distsearchsystemrequirements_6.x_features_in_a_mixed-version_deployment"><span class="mw-headline" id="6.x_features_in_a_mixed-version_deployment">6.x features in a mixed-version deployment</span></a></h5>
<p>When running a 6.x search head against 5.x search peers, note the following:
</p>
<ul><li> You can use data models on the search head but only without report acceleration.
</li><li> You can use Pivot on the search head.
</li><li> You can run predictive analytics (the <code><font size="2">predict</font></code> command) on the search head.
</li></ul><h3> <a name="distsearchsystemrequirements_licenses_for_distributed_search"><span class="mw-headline" id="Licenses_for_distributed_search">Licenses for distributed search</span></a></h3>
<p>Each instance in a distributed search deployment must have access to a license pool. This is true for both search heads and search peers. See "Licenses for search heads" in <i>Admin Manual</i>.
</p>
<h3> <a name="distsearchsystemrequirements_synchronize_system_clocks_across_the_distributed_search_environment"><span class="mw-headline" id="Synchronize_system_clocks_across_the_distributed_search_environment"> Synchronize system clocks across the distributed search environment </span></a></h3>
<p>It is important that you synchronize the system clocks on all machines, virtual or physical, that are running Splunk Enterprise instances participating in distributed search. Specifically, this means your search heads and search peers. In the case of search head pooling or mounted bundles, this also includes the shared storage hardware. Otherwise, various issues can arise, such as bundle replication failures, search failures, or premature expiration of search artifacts.
</p><p>The synchronization method you use depends on your specific set of machines. Consult the system documentation for the particular machines and operating systems on which you are running Splunk Enterprise. For most environments, Network Time Protocol (NTP) is the best approach.
</p>
<a name="configuredistributedsearch"></a><h2> <a name="configuredistributedsearch_add_search_peers_to_the_search_head"><span class="mw-headline" id="Add_search_peers_to_the_search_head"> Add search peers to the search head</span></a></h2>
<p>To activate <b>distributed search</b>, you add <b> search peers</b>, or <b>indexers</b>, to a Splunk Enterprise instance that you designate as a <b>search head</b>. You do this by specifying each search peer manually. 
</p><p>This topic describes how to connect a search head to a set of search peers. To deploy multiple search heads, repeat the procedure for each search head. 
</p><p><b>Important:</b> Clusters establish connectivity between search heads and search peers differently from the procedures described in this topic:  
</p>
<ul><li> <b>Indexer clusters</b> automatically establish the connection between their search heads and indexers, or <b>peer nodes</b>. To learn how to configure search heads in indexer clusters, read "Configure the search head" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</li><li> <b>Search head clusters</b> have certain restrictions that you must consider when connecting search heads to search peers. See <a href="#connectclustersearchheadstosearchpeers" class="external text">"Connect the search heads in clusters to search peers"</a>.
</li></ul><h3> <a name="configuredistributedsearch_configuration_overview"><span class="mw-headline" id="Configuration_overview"> Configuration overview</span></a></h3>
<p>To set up the connection between a search head and its search peers, configure the search head through one of these methods:
</p>
<ul><li> Splunk Web 
</li><li> Splunk CLI 
</li><li> The <code><font size="2">distsearch.conf</font></code> configuration file
</li></ul><p>Splunk Web is the simplest method for most purposes.
</p><p>The configuration occurs on the search head. For most deployments, no configuration is necessary on the search peers. Access to the peers is controlled through public key authentication.
</p>
<h3> <a name="configuredistributedsearch_prerequistites"><span class="mw-headline" id="Prerequistites">Prerequistites</span></a></h3>
<p>Before an indexer can function as a search peer, you must change its password from the default "changeme". Otherwise, the search head will not be able to authenticate against it.
</p>
<h3> <a name="configuredistributedsearch_use_splunk_web"><span class="mw-headline" id="Use_Splunk_Web">Use Splunk Web</span></a></h3>
<h4><font size="3"><b><i> <a name="configuredistributedsearch_specify_the_search_peers"><span class="mw-headline" id="Specify_the_search_peers">Specify the search peers</span></a></i></b></font></h4>
<p>To specify the search peers:
</p><p><b>1.</b> Log into Splunk Web on the search head and click <b>Settings</b> at the top of the page.
</p><p><b>2.</b> Click <b>Distributed search</b> in the Distributed Environment area.
</p><p><b>3.</b> Click <b>Search peers</b>.
</p><p><b>4.</b> On the <b>Search peers</b> page, select <b>New</b>.
</p><p><b>5.</b> Specify the search peer, along with any authentication settings.
</p><p><b>6.</b> Click <b>Save</b>.
</p><p><b>7.</b> Repeat for each of the search head's search peers.
</p>
<h4><font size="3"><b><i> <a name="configuredistributedsearch_configure_miscellaneous_distributed_search_settings"><span class="mw-headline" id="Configure_miscellaneous_distributed_search_settings">Configure miscellaneous distributed search settings</span></a></i></b></font></h4>
<p>To configure other settings:
</p><p><b>1.</b> Log into Splunk Web on the search head and click <b>Settings</b> at the top of the page.
</p><p><b>2.</b> Click <b>Distributed search</b> in the Distributed Environment area.
</p><p><b>3.</b> Click <b>Distributed search setup</b>.
</p><p><b>5.</b> Change any settings as needed.
</p><p><b>6.</b> Click <b>Save</b>. 
</p>
<h3> <a name="configuredistributedsearch_use_the_cli"><span class="mw-headline" id="Use_the_CLI"> Use the CLI </span></a></h3>
<p>To specify the search peers:
</p><p><b>1.</b> Navigate to the <code><font size="2">$SPLUNK_HOME/bin/</font></code> directory on the search head.
</p><p><b>2.</b> Invoke the <code><font size="2">splunk add search-server</font></code> command for each search peer you want to add. 
</p><p>For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add search-server -host 10.10.10.10:8089 -auth admin:password -remoteUsername admin -remotePassword passremote<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> Use the <code><font size="2">-host</font></code> flag to specify the IP address and management port for the search peer. 
</li><li> Provide credentials for both the local (search head) and remote (search peer) instances. Use the <code><font size="2">-auth</font></code> flag for the local credentials and the <code><font size="2">-remoteUsername</font></code> and <code><font size="2">-remotePassword</font></code> flags for the remote credentials.  The remote credentials must be for an admin-level user on the search peer.
</li></ul><h3> <a name="configuredistributedsearch_edit_distsearch.conf"><span class="mw-headline" id="Edit_distsearch.conf">Edit distsearch.conf</span></a></h3>
<p>The settings available through Splunk Web provide sufficient options for most configurations. Some advanced configuration settings, however, are only available by directly editing <code><font size="2">distsearch.conf</font></code>. This section discusses only the configuration settings necessary for connecting search heads to search peers. For information on the advanced configuration options, see the distsearch.conf spec file.
</p>
<h4><font size="3"><b><i> <a name="configuredistributedsearch_add_the_search_peers"><span class="mw-headline" id="Add_the_search_peers">Add the search peers</span></a></i></b></font></h4>
<p>To connect the search peers:
</p><p><b>1.</b> Create or edit a <code><font size="2">distsearch.conf</font></code> file on the search head.
</p><p><b>2.</b> Add the set of search peers to the <code><font size="2">[distributedSearch]</font></code> stanza as a set of comma-separated values (IP addresses with management ports). For example:
</p>
<code><font size="2"><br>[distributedSearch]<br>servers = 192.168.1.1:8089,192.168.1.2:8089<br></font></code>
<p><b>3.</b> Restart the search head.
</p>
<h4><font size="3"><b><i> <a name="configuredistributedsearch_distribute_the_key_files"><span class="mw-headline" id="Distribute_the_key_files"> Distribute the key files </span></a></i></b></font></h4>
<p>If you add search peers via Splunk Web or the CLI, Splunk Enterprise automatically configures authentication. However, if you add peers by editing <code><font size="2">distsearch.conf</font></code>, you must distribute the key files manually.  After adding the search peers and restarting the search head, as described above:
</p><p><b>1.</b> Copy the file <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/trusted.pem</font></code> from the search head to <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/&lt;searchhead_name&gt;/trusted.pem</font></code> on each search peer.
</p><p>The <code><font size="2">&lt;searchhead_name&gt;</font></code> is the search head's <code><font size="2">serverName</font></code>, specified in server.conf.  
</p><p><b>2.</b> Restart each search peer.
</p>
<h4><font size="3"><b><i> <a name="configuredistributedsearch_authentication_of_multiple_search_heads_from_a_single_peer"><span class="mw-headline" id="Authentication_of_multiple_search_heads_from_a_single_peer"> Authentication of multiple search heads from a single peer</span></a></i></b></font></h4>
<p>Multiple search heads can search across a single peer. The peer must store a copy of each search head's certificate.
</p><p>The search peer stores the search head keys in directories with the specification <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/&lt;searchhead_name&gt;</font></code>.
</p><p>For example, if you have two search heads, named A and B, and they both need to search one particular search peer, do the following: 
</p><p><b>1.</b> On the search peer, create the directories <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/A/</font></code> and <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/B/</font></code>.
</p><p><b>2.</b> Copy A's <code><font size="2">trusted.pem</font></code> file to <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/A/</font></code> and B's <code><font size="2">trusted.pem</font></code> to <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/B/</font></code>.
</p><p><b>3.</b> Restart the search peer.
</p>
<h3> <a name="configuredistributedsearch_group_the_search_peers"><span class="mw-headline" id="Group_the_search_peers"> Group the search peers </span></a></h3>
<p>You can group search peers into distributed search groups.  This allows you to target searches to subsets of search peers. See <a href="#distributedsearchgroups" class="external text">"Create distributed search groups"</a>.
</p>
<a name="forwardsearchheaddata"></a><h2> <a name="forwardsearchheaddata_best_practice:_forward_search_head_data_to_the_indexer_layer"><span class="mw-headline" id="Best_practice:_Forward_search_head_data_to_the_indexer_layer"> Best practice: Forward search head data to the indexer layer</span></a></h2>
<p>It is considered a best practice to forward all search head internal data to the search peer (indexer) layer.  This has several advantages:
</p>
<ul><li> It accumulates all data in one place. This simplifies the process of managing your data: You only need to manage your indexes and data at one level, the indexer level.
</li><li> It enables diagnostics for the search head if it goes down.  The data leading up to the failure is accumulated on the indexers, where another search head can later access it.
</li><li> By forwarding the results of summary index searches to the indexer level, all search heads have access to them.  Otherwise, they're only available to the search head that generates them.
</li></ul><h3> <a name="forwardsearchheaddata_forward_search_head_data"><span class="mw-headline" id="Forward_search_head_data">Forward search head data</span></a></h3>
<p>The preferred approach is to forward the data directly to the indexers, without indexing separately on the search head. You do this by configuring the search head as a forwarder.  These are the main steps:
</p><p><b>1.</b> <b>Make sure that all necessary indexes exist on the indexers.</b> For example, the S.o.S app uses a scripted input that puts data into a custom index. If you install S.o.S on the search head, you need to also install the S.o.S Add-on on the indexers, to provide the indexers with the necessary index settings for the data the app generates.  On the other hand, since <code><font size="2">_audit</font></code> and <code><font size="2">_internal</font></code> exist on indexers as well as search heads, you do not need to create separate versions of those indexes to hold the corresponding search head data.
</p><p><b>2.</b> <b>Configure the search head as a forwarder.</b> Create an <code><font size="2">outputs.conf</font></code> file on the search head that configures the search head for load-balanced forwarding across the set of search peers (indexers). You must also turn off indexing on the search head, so that the search head does not both retain the data locally as well as forward it to the search peers.
</p><p>Here is an example <code><font size="2">outputs.conf</font></code> file:
</p>
<div class="samplecode">
<code><font size="2"><br># Turn off indexing on the search head<br>[indexAndForward]<br>index = false<br>&nbsp;<br>[tcpout]<br>defaultGroup = my_search_peers <br>forwardedindex.filter.disable = true &nbsp;<br>indexAndForward = false <br>&nbsp;<br>[tcpout:my_search_peers]<br>server=10.10.10.1:9997,10.10.10.2:9997,10.10.10.3:9997<br>autoLB = true<br></font></code>
</div>
<p>This example assumes that each indexer's receiving port is set to 9997.
</p><p>For details on configuring <code><font size="2">outputs.conf</font></code>, read "Configure forwarders with outputs.conf" in the Forwarding Data manual.
</p>
<h3> <a name="forwardsearchheaddata_forward_data_from_search_head_cluster_members"><span class="mw-headline" id="Forward_data_from_search_head_cluster_members"> Forward data from search head cluster members</span></a></h3>
<p>You perform the same configuration steps to forward data from search head cluster members to their set of search peers. However, you must ensure that all members use the same <code><font size="2">outputs.conf</font></code> file. To do so, do not edit the file on the individual search heads. Instead, use the deployer to propagate the file across the cluster. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</p>
<h1>Manage distributed search</h1><a name="limittheknowledgebundlesize"></a><h2> <a name="limittheknowledgebundlesize_modify_the_knowledge_bundle"><span class="mw-headline" id="Modify_the_knowledge_bundle"> Modify the knowledge bundle</span></a></h2>
<p>The <b>knowledge bundle</b> is the data that the search head replicates and distributes to each search peer to enable its searches. For information on the contents and purpose of this bundle, see <a href="#whatsearchheadssend" class="external text">"What search heads send to search peers"</a>.
</p><p>The knowledge bundle consists of a set of files that the search peers ordinarily need in order to perform their searches.  You can, if necessary, modify this set of files.  The main reasons for modifying the set of files are if:
</p>
<ul><li> <b>As an app developer,</b> you want to customize the files for the needs of your app. This case usually involves manipulating the replication whitelist. You can also use a replication blacklist for this purpose.
</li></ul><ul><li> <b>As an admin,</b> you need to limit the size of the knowledge bundle. This case is somewhat unusual, because Splunk Enterprise uses delta-based replication to keep the bundle compact, with the search head usually only replicating the changed portion of the bundle to its search peers. This case requires that you identify unnecessary files and filter them out with a replication blacklist. It is also possible, although less common, to use a whitelist for this purpose.
</li></ul><p>See distsearch.conf in the <i>Admin Manual</i> for details on the settings discussed in this topic.
</p>
<h3> <a name="limittheknowledgebundlesize_customize_the_bundle_for_an_app"><span class="mw-headline" id="Customize_the_bundle_for_an_app"> Customize the bundle for an app </span></a></h3>
<p>The system looks at two stanzas in <code><font size="2">distsearch.conf</font></code> to determine which <code><font size="2">*.conf</font></code> files to include in the bundle, in this order:
</p><p><b>1.</b> <code><font size="2">[replicationWhitelist]</font></code>
</p><p><b>2.</b> <code><font size="2">[replicationSettings:refineConf]</font></code>
</p><p>You typically only need to edit the <code><font size="2">[replicationSettings:refineConf]</font></code> stanza to customize the bundle for your app, but, under rare circumstances, you might also need to modify the <code><font size="2">[replicationWhitelist]</font></code> stanza. 
</p><p>Since the system starts by examining the <code><font size="2">[replicationWhitelist]</font></code> stanza, this discussion does too. 
</p>
<h4><font size="3"><b><i> <a name="limittheknowledgebundlesize_edit_the_replicationwhitelist_stanza"><span class="mw-headline" id="Edit_the_replicationWhitelist_stanza">Edit the replicationWhitelist stanza</span></a></i></b></font></h4>
<p>The <code><font size="2">[replicationWhitelist]</font></code> stanza in the system default version of <code><font size="2">distsearch.conf</font></code> whitelists all the <code><font size="2">*.conf</font></code> files that are specified in the <code><font size="2">[replicationSettings:refineConf]</font></code> stanza. Therefore, to add or delete a <code><font size="2">*.conf</font></code> file from the bundle, do not modify this stanza. Instead, change the set of files specified in the <code><font size="2">[replicationSettings:refineConf]</font></code> stanza, as described in the next section, "Edit the replicationSettings:refineConf stanza."  
</p><p>The main reason for modifying the <code><font size="2">[replicationWhitelist]</font></code> stanza is to include in the bundle some type of special file for use in a custom search command. This is an unusual circumstance.
</p><p>If you do need to alter the whitelist, you can override the system default whitelist by creating a version of the <code><font size="2">[replicationWhitelist]</font></code> stanza in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;appname&gt;/default/distsearch.conf</font></code>:
</p>
<code><font size="2"><br>[replicationWhitelist]<br>&lt;name&gt; = &lt;whitelist_regex&gt; <br>...<br></font></code>
<p>The knowledge bundle will include all files that both satisfy the whitelist regex and are specified in <code><font size="2">[replicationSettings:refineConf]</font></code>. If multiple regex's are specified, the bundle will include the union of those files. 
</p><p>In this example, the knowledge bundle will include all files with extensions of either ".conf" or ".spec":
</p>
<code><font size="2"><br>[replicationWhitelist]<br>allConf = *.conf<br>allSpec = *.spec<br></font></code>
<p>The names, such as allConf and allSpec, are used only for layering. That is, if you have both a global and a local copy of <code><font size="2">distsearch.conf</font></code>, the local copy can be configured so that it overrides only one of the regex's. For instance, assume that the example shown above is the global copy and that you then specify a whitelist in your local copy like this:
</p>
<code><font size="2"><br>[replicationWhitelist]<br>allConf = *.foo.conf<br></font></code>
<p>The two conf files will be layered, with the local copy taking precedence. Thus, the search head will distribute only files that satisfy these two regex's:
</p>
<code><font size="2"><br>allConf = *.foo.conf<br>allSpec = *.spec<br></font></code>
<p>For more information on attribute layering in configuration files, see "Attribute precedence" in the Admin manual.
</p><p><b>Caution:</b> Replication whitelists are applied globally across all conf data, and are not limited to any particular app, regardless of where they are defined.  Be careful to pull in only your intended files.
</p>
<h4><font size="3"><b><i> <a name="limittheknowledgebundlesize_edit_the_replicationsettings:refineconf_stanza"><span class="mw-headline" id="Edit_the_replicationSettings:refineConf_stanza">Edit the replicationSettings:refineConf stanza </span></a></i></b></font></h4>
<p>The <code><font size="2">[replicationSettings:refineConf]</font></code> stanza in <code><font size="2">distsearch.conf</font></code> specifies the <code><font size="2">*.conf</font></code> files and <code><font size="2">*.meta</font></code> stanzas that get included in the knowledge bundle. If you want to modify the set of files in the bundle, add or delete them from this stanza.
</p><p>The system default <code><font size="2">distsearch.conf</font></code> file includes a version of this stanza that specifies the <code><font size="2">*.conf</font></code> files that are normally included in the knowledge bundle: 
</p>
<code><font size="2"><br>[replicationSettings:refineConf]<br># Replicate these specific *.conf files and their associated *.meta stanzas.<br>replicate.app &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.authorize &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.collections &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.commands &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.eventtypes &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.fields &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.segmenters &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.literals &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.lookups &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.multikv &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.props &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.tags &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.transforms &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= true<br>replicate.transactiontypes &nbsp;= true<br></font></code> 
<p>If you want to replicate a <code><font size="2">.conf</font></code> file that is not in the system default version of the <code><font size="2">[replicationSettings:refineConf]</font></code> stanza, create a version of the stanza in  <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;appname&gt;/default/distsearch.conf</font></code> and specify the <code><font size="2">*.conf</font></code> file there. Similarly, you can remove files from the bundle by setting them to "false" in this stanza.
</p>
<h3> <a name="limittheknowledgebundlesize_limit_the_size_of_the_knowledge_bundle"><span class="mw-headline" id="Limit_the_size_of_the_knowledge_bundle">Limit the size of the knowledge bundle </span></a></h3>
<p>You can also create a replication blacklist, using the <code><font size="2">[replicationBlacklist]</font></code> stanza. This is most useful for limiting the size of the knowledge bundle, particularly in the case of very large files that do not need to be replicated to the search peers. The blacklist takes precedence over any whitelist. 
</p><p><b>Caution:</b> Replication blacklists are applied globally across all conf data, and are not limited to any particular app, regardless of where they are defined.  If you are defining an app-specific blacklist, be careful to constrain it to match only files that your application will not need.
</p>
<a name="managedistributedservernames"></a><h2> <a name="managedistributedservernames_manage_distributed_server_names"><span class="mw-headline" id="Manage_distributed_server_names"> Manage distributed server names</span></a></h2>
<p>The name of each search head and search peer is determined by its <code><font size="2">serverName</font></code> attribute, specified in server.conf. The <code><font size="2">serverName</font></code> attribute defaults to the server's machine name. 
</p><p>In distributed search, all search heads and search peers in the group must have unique names. The <code><font size="2">serverName</font></code> has three specific uses in distributed search:
</p>
<ul><li> <b>For authenticating search heads.</b> When search peers are authenticating a search head, they look for the search head's key file in <code><font size="2">/etc/auth/distServerKeys/&lt;searchhead_name&gt;/trusted.pem</font></code>. 
</li><li> <b>For identifying search peers in search queries.</b> <code><font size="2">serverName</font></code> is the value of the <code><font size="2">splunk_server</font></code> field that you specify when you want to query a specific node. See "Retrieve events from indexes and distributed search peers" in the Search manual. 
</li><li> <b>For identifying search peers in search results.</b> <code><font size="2">serverName</font></code> gets reported back in the <code><font size="2">splunk_server</font></code> field.
</li></ul><p><b>Note:</b> <code><font size="2">serverName</font></code> is <i>not</i> used when adding search peers to a search head. In that case, you identify the search peers through their domain names or IP addresses.
</p><p>The only reason to change <code><font size="2">serverName</font></code> is if you have multiple instances of Splunk Enterprise residing on a single machine, and they're participating in the same distributed search group. In that case, you'll need to change <code><font size="2">serverName</font></code> to distinguish them.
</p>
<a name="distributedsearchgroups"></a><h2> <a name="distributedsearchgroups_create_distributed_search_groups"><span class="mw-headline" id="Create_distributed_search_groups"> Create distributed search groups</span></a></h2>
<p>You can group your search peers to facilitate searching on a subset of them.  Groups of search peers are known as "distributed search groups."  You specify distributed search groups in the <code><font size="2">distsearch.conf</font></code> file.
</p><p>For example, say you have a set of search peers in New York and another set in San Francisco, and you want to perform searches across peers in just a single location.  You can do this by creating two search groups, NYC and SF.
</p><p>In <code><font size="2">distsearch.conf</font></code>, create these stanzas:
</p>
<div class="samplecode">
<code><font size="2"><br>[distributedSearch]<br># This stanza lists the full set of search peers.<br>servers = 192.168.1.1:8089, 192.168.1.2:8089, 175.143.1.1:8089, 175.143.1.2:8089, 175.143.1.3:8089<br><br>[distributedSearch:NYC]<br># This stanza lists the set of search peers in New York.<br>default = false<br>servers = 192.168.1.1:8089, 192.168.1.2:8089<br><br>[distributedSearch:SF]<br># This stanza lists the set of search peers in San Francisco.<br>default = false<br>servers = 175.143.1.1:8089, 175.143.1.2:8089, 175.143.1.3:8089<br></font></code>
</div>
<p>This example creates two search groups, NYC and SF, which you can then specify in searches.
</p><p>Note the following:
</p>
<ul><li> The <code><font size="2">servers</font></code> attribute lists groups of search peers by IP address and management port.
</li><li> The servers list for each search group must be a subset of the list in the general <code><font size="2">[distributedSearch]</font></code> stanza. 
</li><li> The group lists can overlap.  For example, you can add a third group named "Primary_Indexers" that contains some peers from each location.
</li><li> If you set a group's <code><font size="2">default</font></code> attribute to "true," the peers in that group will be the ones queried when the search does not specify a search group.  Otherwise, if you set all groups to "false," the full set of search peers in the <code><font size="2">[distributedSearch]</font></code> stanza will be queried when the search does not specify a search group.
</li></ul><p>To use a search group in a search, specify the search group like this:
</p>
<div class="samplecode">
<code><font size="2"><br>sourcetype=access_combined status=200 action=purchase splunk_server_group=NYC | stats count by product</font></code>
</div>
<p>This search runs against only the peers in the NYC location.
</p><p><b>Note:</b> This feature is not valid for indexer clustering. In indexer clustering, the cluster replicates data arbitrarily across the set of search peers, or "peer nodes". You cannot know whether any particular set of data will reside on any particular peer.
</p>
<a name="removeasearchpeer"></a><h2> <a name="removeasearchpeer_remove_a_search_peer"><span class="mw-headline" id="Remove_a_search_peer"> Remove a search peer</span></a></h2>
<p>You can remove a search peer from a search head through Splunk Web or the CLI. As you might expect, doing so merely removes the search head's knowledge of that search peer; it does not affect the peer itself.
</p>
<h3> <a name="removeasearchpeer_remove_a_search_peer_via_splunk_web"><span class="mw-headline" id="Remove_a_search_peer_via_Splunk_Web"> Remove a search peer via Splunk Web </span></a></h3>
<p>You can remove a search peer from a search head through the <b>Distributed search</b> page on the search head's Splunk Web. 
</p><p><b>Note:</b> This only removes the search peer entry from the search head; it does not remove the search head key from the search peer. In most cases, this is not a problem and no further action is needed.
</p>
<h3> <a name="removeasearchpeer_remove_a_search_peer_via_the_cli"><span class="mw-headline" id="Remove_a_search_peer_via_the_CLI"> Remove a search peer via the CLI </span></a></h3>
<p>On the search head, run the <code><font size="2">splunk remove search-server</font></code> command to remove a search peer from the search head. 
</p><p>Note the following:
</p>
<ul><li> Use the <code><font size="2">-auth</font></code> flag to provide credentials for the search head only. 
</li><li> Use the <code><font size="2">-url</font></code> flag to specify the peer's location and <code><font size="2">splunkd</font></code> management port. By default, the management port is <code><font size="2">8089</font></code>, although it might be different for your deployment.  
</li></ul><p>This example removes the search peer <code><font size="2">10.10.10.10:8089</font></code>:
</p>
<code><font size="2"><br>splunk remove search-server -auth admin:password -url 10.10.10.10:8089<br></font></code>
<p>A message indicating success appears after the peer is removed.
</p>
<h3> <a name="removeasearchpeer_disable_the_trust_relationship"><span class="mw-headline" id="Disable_the_trust_relationship"> Disable the trust relationship </span></a></h3>
<p>As an additional step, you can disable the trust relationship between the search peer and the search head. To do this, delete the <code><font size="2">trusted.pem</font></code> file from <code><font size="2">$SPLUNK_HOME/etc/auth/distServerKeys/&lt;searchhead_name&gt;</font></code> on the search peer. 
</p><p><b>Note:</b> The <code><font size="2">&lt;searchhead_name&gt;</font></code> is the search head's <code><font size="2">serverName</font></code>, as described in <a href="#managedistributedservernames" class="external text">"Manage distributed server names"</a>.
</p><p>This step is usually unnecessary.
</p>
<h1>Overview of search head clustering</h1><a name="aboutshc"></a><h2> <a name="aboutshc_about_search_head_clustering"><span class="mw-headline" id="About_search_head_clustering"> About search head clustering</span></a></h2>
<p>A <b>search head cluster</b> is a group of Splunk Enterprise <b>search heads</b> that serves as a central resource for searching. The <b>members</b> of a search head cluster are essentially interchangeable. You can run the same searches, view the same dashboards, and access the same search results from any member of the cluster. 
</p><p>To achieve this interchangeability, the search heads in the cluster must share configurations and apps, <b>search artifacts</b>, and job scheduling. Search head clusters automatically propagate most of these shareable resources among the members. 
</p>
<h3> <a name="aboutshc_benefits_of_a_search_head_cluster"><span class="mw-headline" id="Benefits_of_a_search_head_cluster"> Benefits of a search head cluster</span></a></h3>
<p>Search head clusters provide these key benefits:
</p>
<ul><li> <b>Horizontal scaling.</b> As the number of users and the search load increases, you can add new search heads to the cluster. By combining a search head cluster with a third-party load balancer placed between users and the cluster, the topology can be transparent to the users.
</li><li> <b>High availability.</b> If a search head goes down, you can run the same set of searches and access the same set of search results from&Acirc;&nbsp;any other search head in the cluster.
</li><li> <b>No single point of failure.</b> The search head cluster uses a dynamic <b>captain</b> to manage the cluster. If the captain goes down, another member automatically takes over management of the cluster.
</li></ul><h3> <a name="aboutshc_cluster_architecture"><span class="mw-headline" id="Cluster_architecture"> Cluster architecture </span></a></h3>
<p>A search head cluster consists of a group of networked search heads, called cluster members. One cluster member, the  captain, coordinates all cluster-wide activities. If the member serving as captain goes down, another member takes its place.
</p><p>The members share:
</p>
<ul><li> <b>Job scheduling.</b> The cluster manages job scheduling centrally, allocating each scheduled search to the optimal member, usually the member with the least load.
</li><li> <b>Search artifacts.</b> The cluster replicates search artifacts and makes them available to all members.
</li><li> <b>Configurations.</b> The cluster requires that all members share the same set of configurations.  For runtime updates to knowledge objects, such as updates to dashboards or reports, the cluster replicates configurations automatically to all members. For apps and some other configurations, the user must push configurations to the cluster members by means of the <b>deployer</b>, a Splunk Enterprise instance that resides outside the cluster.
</li></ul><p>See <a href="#shcarchitecture" class="external text">"Search head clustering architecture."</a>
</p>
<h3> <a name="aboutshc_how_to_set_up_the_cluster"><span class="mw-headline" id="How_to_set_up_the_cluster"> How to set up the cluster</span></a></h3>
<p>You set up a cluster by configuring and deploying the cluster's search heads.  The process is similar to how you set up search heads in any distributed search environment. The main difference is that you also need to configure the search heads as cluster members.
</p><p>See the chapter <a href="#shcsystemrequirements" class="external text">"Deploy search head clustering"</a>.
</p>
<h3> <a name="aboutshc_how_the_user_accesses_the_cluster"><span class="mw-headline" id="How_the_user_accesses_the_cluster"> How the user accesses the cluster </span></a></h3>
<p>Users access the cluster the same way that they access any search head. They point their browser at any search head that is a member of the cluster.  Because cluster members share jobs, search artifacts, and configurations, it does not matter which search head a user accesses.  The user has access to the same set of dashboards, searches, and so on.
</p><p>To achieve the goals of high availability and load balancing, Splunk recommends that you put a load balancer in front of the cluster. That way, the load balancer can assign the user to any search head in the cluster and balance the user load across the cluster members. If one search head goes down, the load balancer can reassign the user to any remaining search head.
</p>
<h3> <a name="aboutshc_search_head_clusters_and_indexer_clusters"><span class="mw-headline" id="Search_head_clusters_and_indexer_clusters"> Search head clusters and indexer clusters </span></a></h3>
<p>Search head clusters are different from <b>indexer clusters</b>. The primary purpose of indexer clusters is to provide highly available data through coordinated groups of indexers. Indexer clusters always include one or more associated search heads to access the data on the indexers. These search heads might be, but are not necessarily, members of a search head cluster. 
</p><p>For information on search heads in indexer clusters, see the chapter "Configure the search head" in the <i>Managing Indexers and Clusters of Indexers</i> manual. 
</p><p>For information on adding a search head cluster to an indexer cluster, see the topic <a href="#shcandindexercluster" class="external text">"Integrate the search head cluster with an indexer cluster"</a> in this manual.
</p>
<a name="shcarchitecture"></a><h2> <a name="shcarchitecture_search_head_clustering_architecture"><span class="mw-headline" id="Search_head_clustering_architecture"> Search head clustering architecture</span></a></h2>
<p>A search head cluster is a group of Splunk Enterprise search heads that serves as a central resource for searching. 
</p>
<h3> <a name="shcarchitecture_parts_of_a_search_head_cluster"><span class="mw-headline" id="Parts_of_a_search_head_cluster"> Parts of a search head cluster </span></a></h3>
<p>A <b>search head cluster</b> consists of a group of <b>search heads</b> that share configurations, job scheduling, and <b>search artifacts</b>. The search heads are known as the cluster <b>members</b>. 
</p><p>One cluster member has the role of <b>captain</b>, which means that it coordinates job scheduling and replication activities among all the members. It also serves as a search head like any other member, running search jobs, serving results, and so on. Over time, the role of captain can shift among the cluster members.
</p><p>In addition to the set of search head members that constitute the actual cluster, a functioning cluster requires several other components:
</p>
<ul><li> <b>The deployer.</b> This is a Splunk Enterprise instance that distributes apps and other configurations to the cluster members.  It stands outside the cluster and cannot run on the same instance as a cluster member.  It can, however, under some circumstances, reside on the same instance as some other Splunk Enterprise components, such as a <b>deployment server</b> or an <b>indexer cluster</b> master node.  See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</li><li> <b>Search peers.</b>  These are the indexers that cluster members run their searches across.  The search peers can be either independent indexers or nodes in an indexer cluster. See <a href="#connectclustersearchheadstosearchpeers" class="external text">"Connect the search heads in clusters to search peers."</a>
</li><li> <b>Load balancer.</b> This is third-party software or hardware optionally residing between the users and the cluster members. With a load balancer in place, users can access the set of search heads through a single interface, without needing to specify a particular search head. See <a href="#useshcwithloadbalancers" class="external text">"Use a load balancer with search head clustering."</a>
</li></ul><p>Here is a diagram of a small search head cluster, consisting of three members:
</p><p><img alt="Searchhead cluster.png" src="images/9/96/Searchhead_cluster.png" width="700" height="447"></p><p>This diagram shows the key cluster-related components and interactions:
</p>
<ul><li> One member serves as the captain, directing various activities within the cluster.  
</li><li> The members communicate among themselves to schedule jobs, replicate artifacts, update configurations, and coordinate other activities within the cluster.
</li><li> The members communicate with search peers to fulfill search requests. 
</li><li> Users optionally access the search heads through a third-party load balancer. 
</li><li> A deployer sits outside the cluster and distributes updates to the cluster members.
</li></ul><h3> <a name="shcarchitecture_search_head_cluster_captain"><span class="mw-headline" id="Search_head_cluster_captain"> Search head cluster captain </span></a></h3>
<p>The captain is a cluster member with additional responsibilities, beyond the search activities common to all cluster members. It serves to coordinate the activities of the cluster. Any member can perform the role of captain, but the cluster has just one captain at any time. Over time, if failures occur, the captain changes and a new member gets elected to the role.
</p>
<h4><font size="3"><b><i> <a name="shcarchitecture_role_of_the_captain"><span class="mw-headline" id="Role_of_the_captain"> Role of the captain </span></a></i></b></font></h4>
<p>The captain is a cluster member and in that capacity it performs the search activities typical of any cluster member, serving both ad hoc and scheduled searches. If necessary, you can limit the captain's search activities so that it performs only ad hoc searches and not scheduled searches.  See <a href="#adhocclustermember_configure_the_captain_to_run_ad_hoc_searches_only" class="external text">"Configure the captain to run ad hoc searches only"</a>.
</p><p>The captain also coordinates activities among all cluster members. Its responsibilities include:
</p>
<ul><li> Scheduling jobs. It assigns jobs to members, including itself, based on relative current loads.
</li><li> Coordinating alerts and alert suppressions across the cluster. The captain tracks each alert but the member running an initiating search fires it.
</li><li> Pushing the <b>knowledge bundle</b> to search peers. 
</li><li> Coordinating artifact replication. The captain ensures that search artifacts get replicated as necessary to fulfill the <b>replication factor</b>. See <a href="#chooseshcreplicationfactor" class="external text">"Choose the replication factor for the search head cluster"</a>.
</li><li> Replicating configuration updates. The captain replicates any runtime changes to knowledge objects on one cluster member to all other members. This includes, for example, changes or additions to saved searches, lookup tables, and dashboards. See <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates"</a>.
</li></ul><h4><font size="3"><b><i> <a name="shcarchitecture_captain_election"><span class="mw-headline" id="Captain_election"> Captain election </span></a></i></b></font></h4>
<p>A search head cluster uses a dynamic captain. This means that the member serving as captain can change over the life of the cluster. Any member has the ability to function as captain. When necessary, the cluster holds an election, which can result in a new member taking over the role of captain.
</p><p>Captain election occurs when:
</p>
<ul><li> The current captain fails or restarts.
</li><li> The cluster performs a rolling restart, for example, after the deployer updates the cluster members. During a rolling restart, the captain also restarts, triggering election. See <a href="#restartshc" class="external text">"Restart the search head cluster"</a>.
</li><li> A network partition occurs, causing one or more members to get cut from the rest of the search head cluster. Subsequent healing of the network partition triggers another, separate captain election.
</li><li> The current captain steps down, because it does not detect that a majority of members are participating in the cluster.
</li></ul><p><b>Note:</b> The mere failure or restart of a non-captain cluster member, without an associated network partition, does not trigger captain election.
</p><p>To become captain, a member needs a majority vote of all members. For example, in a seven-member cluster, election requires four votes.  Similarly, a six-member cluster also requires four votes. 
</p><p>The majority must be a majority of all members, not just of the members currently running.  So, if four members of a seven-member cluster fail, the cluster cannot elect a new captain, because the remaining three members are fewer than the required majority of four.
</p><p>The election process involves timers set randomly on all the members. The member whose timer runs out first stands for election and asks the other members to vote for it. Usually, the other members comply and that member becomes the new captain. 
</p><p>It typically takes one to two minutes after a triggering event occurs to elect a new captain. During that time, there is no functioning captain, and the search heads are aware only of their local environment. The election takes this amount of time because each member waits for a minimum timeout period before trying to become captain. These timeouts are configurable.  
</p><p>The cluster might re-elect the member that was the previous captain, if that member is still running. There is no bias either for or against this occurring.
</p><p>Once a member is elected as captain, it takes over the duties of captaincy.
</p><p><b>Note:</b> A majority of members must be running and participating in the cluster at all times.  If the captain does not detect a majority of members, it steps down, relinquishing its authority. An election for a new captain will subsequently occur, but without a majority of participating members, it will not succeed.
</p>
<h4><font size="3"><b><i> <a name="shcarchitecture_consequences_of_a_non-functioning_cluster"><span class="mw-headline" id="Consequences_of_a_non-functioning_cluster">Consequences of a non-functioning cluster</span></a></i></b></font></h4>
<p>If the cluster lacks a majority of members and therefore cannot elect a captain, the members will continue to function as independent search heads.  However, they will only be able to service ad hoc searches. Scheduled searches and alerts will not run, because, in a cluster, the scheduling function is relegated to the captain. In addition, configurations and search artifacts will not be replicated during this time.
</p>
<h4><font size="3"><b><i> <a name="shcarchitecture_captain_election_process_has_deployment_implications"><span class="mw-headline" id="Captain_election_process_has_deployment_implications">Captain election process has deployment implications</span></a></i></b></font></h4>
<p>The need of a majority vote for a successful election has these deployment implications:
</p>
<ul><li> A cluster should consist of a minimum of three members. A two-member cluster cannot tolerate any node failure.  Failure of either member will prevent the cluster from electing a captain and continuing to function. Captain election requires majority (51%) assent of all members, which, in the case of a two-member cluster, means that both nodes must be running. You therefore forfeit the high availability benefits of a search head cluster if you limit it to two members.
</li></ul><ul><li> If you are deploying the cluster across multiple sites, your primary site must contain a majority of the nodes. If there is a network disruption between the sites, only the site with a majority can elect a new captain. See <a href="#runtimeissues_site_failure_can_prevent_captain_election" class="external text">"Site failure can prevent captain election."</a>
</li></ul><h3> <a name="shcarchitecture_how_the_cluster_handles_search_artifacts"><span class="mw-headline" id="How_the_cluster_handles_search_artifacts"> How the cluster handles search artifacts </span></a></h3>
<p>The cluster replicates most search artifacts, also known as search results, to multiple cluster members. If a member needs to access an artifact, it accesses a local copy, if possible. Otherwise, it uses proxying to access the artifact.
</p>
<h4><font size="3"><b><i> <a name="shcarchitecture_artifact_replication"><span class="mw-headline" id="Artifact_replication"> Artifact replication </span></a></i></b></font></h4>
<p>The cluster maintains multiple copies of artifacts resulting from scheduled saved searches. The replication factor determines the number of copies that the cluster maintains of each artifact. For example, if the replication factor is three, the cluster maintains three copies of each artifact: one on the member that originated the artifact, and two on other members. 
</p><p>The captain coordinates the replication of artifacts to cluster members. As with any search head, clustered or not, when a search is complete, its search artifact is placed in the dispatch directory of the member originating the search. The captain then directs the artifact's replication process, in which copies stream between members until copies exist on the replication factor number of members, including the originating member.  
</p><p>The set of members receiving copies can change from artifact to artifact. That is, two artifacts from the same originating member might have their replicated copies on different members. 
</p><p>The captain maintains the artifact registry, with information on the locations of copies of each artifact. When the registry changes, the captain sends the delta to each member.
</p><p>If a member goes down, thus causing the cluster to lose some artifact copies, the captain coordinates fix-up activities, with the goal of returning the cluster to a state where each artifact has the replication factor number of copies.
</p><p>Search artifacts are contained in the dispatch directory, located under <code><font size="2">$SPLUNK_HOME/var/run/splunk/dispatch</font></code>. Each dispatch subdirectory contains one search artifact. It is these subdirectories that the cluster replicates.
</p>
<h4><font size="3"><b><i> <a name="shcarchitecture_artifact_proxying"><span class="mw-headline" id="Artifact_proxying"> Artifact proxying </span></a></i></b></font></h4>
<p>The cluster only replicates search artifacts resulting from scheduled saved searches. It does not replicate results from these other search types:
</p>
<ul><li> Scheduled real-time searches
</li><li> Ad hoc searches of any kind (realtime or historical)
</li></ul><p>Instead, the cluster proxies these results, if they are requested by a non-originating search head. They appear on the requesting member after a short delay.
</p><p>In addition, if a member needs an artifact from a scheduled saved search but does not itself have a local copy of that artifact, it proxies the results from a member that does have a copy. At the same time, the cluster replicates a copy of that artifact to the requesting member, so that it has a local copy for any future requests. Because of this process, some artifacts might have more than the replication factor number of copies.
</p>
<h3> <a name="shcarchitecture_distribution_of_configuration_changes"><span class="mw-headline" id="Distribution_of_configuration_changes"> Distribution of configuration changes </span></a></h3>
<p>With a few exceptions, all cluster members must use the same set of configurations. For example, if a user edits a dashboard on one member, the updates must somehow propagate to all the other members. Similarly, if you distribute an app, you must distribute it to all members. Search head clustering has methods to ensure that configurations stay in sync across the cluster.
</p><p>There are two types of configuration changes, based on how they are distributed to cluster members:
</p>
<ul><li> <b>Replicated changes.</b> The cluster automatically replicates any runtime knowledge object changes on one member to all other members.
</li><li> <b>Deployed changes.</b> The cluster relies on an external instance, the deployer, to push apps and other non-runtime configuration changes to the set of members. You must initiate each push of changes from the deployer.
</li></ul><p>See <a href="#howconfigurationworksinshc" class="external text">"How configuration changes propagate across the search head cluster"</a>.
</p>
<h3> <a name="shcarchitecture_job_scheduling"><span class="mw-headline" id="Job_scheduling"> Job scheduling </span></a></h3>
<p>The captain schedules saved search jobs, allocating them to the various cluster members according to load-based heuristics. Essentially, it attempts to assign each job to the member currently with the least search load.
</p><p>If a job fails on one member, the captain reassigns it to a different member. The captain reassigns the job only once, as multiple failures are unlikely to be resolvable without intervention on the part of the user. For example, a job with a bad search string will fail no matter how many times the cluster attempts to run it.
</p><p>You can designate a member as "ad hoc only." In that case, the captain will not schedule jobs on it. You can also designate the captain functionality as "ad hoc only."  The captain then will never schedule jobs on itself. Since the role of captain can move among members, this setting ensures that captain functionality does not compete with scheduled searches. See <a href="#adhocclustermember" class="external text">"Configure a cluster member to run ad hoc searches only."</a>
</p><p><b>Note:</b> The captain does not have insight into the actual CPU load on each member's machine. It assumes that all machines in the cluster are provisioned homogeneously, with the same number and type of cores, and so forth.
</p>
<h3> <a name="shcarchitecture_search_head_clustering_and_kv_store"><span class="mw-headline" id="Search_head_clustering_and_KV_store"> Search head clustering and KV store </span></a></h3>
<p>KV store can reside on a search head cluster.  However, the search head cluster does not coordinate replication of KV store data or otherwise involve itself in the operation of the KV store. For information on KV store, see "About KV store" in the <i>Admin Manual.</i>
</p>
<h1>Deploy search head clustering</h1><a name="shcsystemrequirements"></a><h2> <a name="shcsystemrequirements_system_requirements_and_other_deployment_considerations_for_search_head_clusters"><span class="mw-headline" id="System_requirements_and_other_deployment_considerations_for_search_head_clusters"> System requirements and other deployment considerations for search head clusters</span></a></h2>
<p>The <b>members</b> of a search head cluster have most of the same system requirements as any non-clustered search head. This topic details requirements specific to a search head cluster.
</p>
<h3> <a name="shcsystemrequirements_summary_of_key_requirements"><span class="mw-headline" id="Summary_of_key_requirements">Summary of key requirements</span></a></h3>
<p>These are the main issues to note regarding provisioning of cluster members:
</p>
<ul><li> Each member must run on its own machine or virtual machine, and all machines must run the same operating system.
</li><li> All members must run on the same version of Splunk Enterprise.
</li><li> All members must be connected over a high-speed network.
</li><li> You must deploy at least as many members as either the replication factor or three, whichever is greater.
</li></ul><p>In addition to the cluster members, you need a deployer to distribute updates to the members. The deployer must run on a non-member instance. In some cases, it can run on the same instance as a <b>deployment server</b> or an <b>indexer cluster</b> master node.
</p><p>See the remainder of this topic for details on these and other issues.
</p>
<h3> <a name="shcsystemrequirements_hardware_and_operating_system_requirements"><span class="mw-headline" id="Hardware_and_operating_system_requirements"> Hardware and operating system requirements </span></a></h3>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_machine_requirements_for_cluster_members"><span class="mw-headline" id="Machine_requirements_for_cluster_members">Machine requirements for cluster members</span></a></i></b></font></h4>
<p>Each member must run on its own, separate machine or virtual machine. 
</p><p>The hardware requirements for the machine are essentially the same as for any Splunk Enterprise search head. See "Reference hardware" in the <i>Capacity Planning Manual</i>. The main difference is the need for increased storage to accommodate a larger dispatch directory. See <a href="#shcsystemrequirements_storage_considerations" class="external text">"Storage considerations"</a>.
</p><p>Splunk recommends that you use homogeneous machines with identical hardware specifications for all cluster members. The reason is because the <b>cluster captain</b> assigns scheduled jobs to members based on their current job loads. When it does this, it does not have insight into the actual processing power of each member's machine. Instead, it assumes that each machine is provisioned equally.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_operating_system_requirements_for_cluster_members"><span class="mw-headline" id="Operating_system_requirements_for_cluster_members">Operating system requirements for cluster members</span></a></i></b></font></h4>
<p>All search head cluster members and the deployer must run on the same operating system. 
</p><p>If the search head cluster is connected to an indexer cluster, then the indexer cluster instances must run on the same operating system as the search head cluster members.
</p><p>Search head clustering is available on the following operating systems:
</p>
<ul><li> Linux
</li><li> Solaris
</li></ul><p>Splunk does not currently support search head clustering on Windows systems.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_storage_considerations"><span class="mw-headline" id="Storage_considerations"> Storage considerations</span></a></i></b></font></h4>
<p>When determining the storage requirements for your clustered search heads, you need to consider the increased capacity necessary to handle replicated copies of <b>search artifacts</b>. 
</p><p>For the purpose of developing storage estimates, you can observe the size over time of dispatch directories on the search heads in your non-clustered environment, if any, before you migrate to a cluster. Total up the size of dispatch directories across all the non-clustered search heads and then make adjustments to account for the cluster-specific factors.  
</p><p>The most important factor to take into consideration is the <b>replication factor</b>. For example, if you have a replication factor of 3, you will need approximately triple the amount of the total pre-cluster storage, distributed equally among the cluster members.
</p><p>Other factors can further increase the cluster storage needs. One key factor is the need to plan for node failure. If a member goes down, causing its set of artifacts (original and replicated) to disappear from the cluster, fix-up activities take place to ensure that each artifact once again has its full complement of copies, matching the replication factor. During fix-up, the copies that were resident on the failed member get replicated among the remaining members, increasing the size of each member's dispatch directory.
</p><p>Other issues can also increase storage on a per-member basis. For example, the cluster does not guarantee an absolutely equal distribution of replicated copies across the members. In addition, the cluster can hold more than the replication factor number of some search artifacts. See <a href="#shcarchitecture_how_the_cluster_handles_search_artifacts" class="external text">"How the cluster handles search artifacts."</a> 
</p><p>As a best practice, equip each member machine with substantially more storage than the estimated need. This allows both for future growth and for temporarily increased need resulting from downed cluster members. The cluster will stop running searches if any of its members runs out of disk space.
</p>
<h3> <a name="shcsystemrequirements_splunk_enterprise_instance_requirements"><span class="mw-headline" id="Splunk_Enterprise_instance_requirements"> Splunk Enterprise instance requirements </span></a></h3>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_splunk_enterprise_version_compatibility"><span class="mw-headline" id="Splunk_Enterprise_version_compatibility">Splunk Enterprise version compatibility</span></a></i></b></font></h4>
<p>You can implement search head clustering on any group of Splunk Enterprise instances, version 6.2 or above. All cluster members must run on the same version of Splunk Enterprise.
</p><p>Search head clusters can run against 5.x or 6.x <b>search peers</b>. For details on version compatibility between search heads and search peers, see <a href="#distsearchsystemrequirements_version_compatibility" class="external text">"Version compatibility."</a> 
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_licensing_requirements"><span class="mw-headline" id="Licensing_requirements">Licensing requirements</span></a></i></b></font></h4>
<p>Licensing needs are the same as for any search head.  See "Types of Splunk Enterprise licenses" in the <i>Admin Manual</i>. 
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_required_number_of_instances"><span class="mw-headline" id="Required_number_of_instances">Required number of instances </span></a></i></b></font></h4>
<p>The cluster must contain at a minimum the number of members needed to fulfill both of these requirements:
</p>
<ul><li> Three members, so that the cluster can continue to function if one member goes down. See <a href="#shcarchitecture_captain_election_process_has_deployment_implications" class="external text">"Captain election process has deployment implications."</a>
</li><li> The replication factor number of instances. See <a href="#chooseshcreplicationfactor" class="external text">"Choose the replication factor for the search head cluster."</a> 
</li></ul><p>For example, if your replication factor is either 2 or 3, you need at least three instances.  If your replication factor is 5, you need at least five instances.
</p><p>You can optionally add more members to boost search and user capacity. 
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_search_head_clusters_running_across_multiple_sites"><span class="mw-headline" id="Search_head_clusters_running_across_multiple_sites">Search head clusters running across multiple sites</span></a></i></b></font></h4>
<p>Running a cluster across multiple sites is not currently supported. Search head clusters have been tested only with all members running on a single site.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_cluster_member_cannot_be_a_search_peer"><span class="mw-headline" id="Cluster_member_cannot_be_a_search_peer"> Cluster member cannot be a search peer </span></a></i></b></font></h4>
<p>A cluster member cannot be the search peer of another search head. For the recommended approach to accessing cluster member data, see <a href="#forwardsearchheaddata" class="external text">"Best practice: Forward search head data to the indexer layer."</a>
</p>
<h3> <a name="shcsystemrequirements_network_requirements"><span class="mw-headline" id="Network_requirements"> Network requirements </span></a></h3>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_network_provisioning"><span class="mw-headline" id="Network_provisioning"> Network provisioning </span></a></i></b></font></h4>
<p>All members must reside on a high speed network where each member can access every other member. 
</p><p>The members do not necessarily need to be on the same subnet, or even in the same data center, if you have a fast connection between the data centers.  You can adjust the various search head clustering timeout settings in server.conf. For help in configuring timeout settings, contact Splunk Professional Services.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_ports_that_the_cluster_members_use"><span class="mw-headline" id="Ports_that_the_cluster_members_use">Ports that the cluster members use</span></a></i></b></font></h4>
<p>These ports must be available on each member:
</p>
<ul><li> The management port (by default, 8089) must be available to all other members. 
</li><li> The http port (by default, 8000) must be available to any browsers accessing data from the member.
</li><li> The KV store port (by default, 8191) must be available to all other members. You can use the CLI command <code><font size="2">splunk show kvstore-port</font></code> to identify the port number.
</li><li> The <b>replication port</b> must be available to all other members.
</li></ul><p>These ports must be in your firewall's list of allowed ports.
</p><p><b>Caution:</b> Do not change the management port on any of the members while they are participating in the cluster. If you need to change the management port, you must first remove the member from the cluster.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_synchronize_system_clocks_across_the_distributed_search_environment"><span class="mw-headline" id="Synchronize_system_clocks_across_the_distributed_search_environment"> Synchronize system clocks across the distributed search environment </span></a></i></b></font></h4>
<p>It is important that you synchronize the system clocks on all machines, virtual or physical, that are running Splunk Enterprise instances participating in distributed search. Specifically, this means your cluster members and search peers. Otherwise, various issues can arise, such as search failures, premature expiration of search artifacts, or problems with alerts.
</p><p>The synchronization method you use depends on your specific set of machines. Consult the system documentation for the particular machines and operating systems on which you are running Splunk Enterprise. For most environments, Network Time Protocol (NTP) is the best approach.
</p>
<h3> <a name="shcsystemrequirements_deployer_requirements"><span class="mw-headline" id="Deployer_requirements"> Deployer requirements </span></a></h3>
<p>You need a Splunk Enterprise instance that functions as the <b>deployer</b>.  The deployer updates member configurations. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates"</a>. 
</p><p>Deployer functionality is only for use with search head clustering, but it is built into all Splunk Enterprise instances running version 6.2 or above. You have several options as to the instance on which you run the deployer:
</p>
<ul><li> If you have a deployment server that is servicing only a small number of deployment clients (no more than 50), you can run the deployer on the same instance as the deployment server. The deployer and deployment server functionalities can interfere with each other at larger client counts. See "Deployment server provisioning" in the <i>Updating Splunk Enterprise Instances</i> manual.
</li></ul><ul><li> If you do not have a deployment server, or the deployment server is servicing more than 50 clients, but you are running an indexer cluster, you might be able to run the deployer on the same instance as the indexer cluster master node. Whether this is an option depends on the master's load. See "Additional roles for the master node" in the <i>Managing Indexers and Clusters of Indexers</i> manual for information on cluster master load limits.
</li></ul><ul><li> If you have neither a deployment server with a small number of clients nor an indexer cluster master with an acceptable load, run the deployer on a dedicated Splunk Enterprise instance.
</li></ul><p><b>Important:</b> Do not locate deployer functionality on a search head cluster member. The deployer must be a separate instance from any cluster member.
</p><p>A deployer can service only a single search head cluster. If you have multiple clusters, you must use a separate deployer for each one. The deployers must run on separate instances.
</p>
<h3> <a name="shcsystemrequirements_other_considerations"><span class="mw-headline" id="Other_considerations"> Other considerations </span></a></h3>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_deployment_server_and_search_head_clusters"><span class="mw-headline" id="Deployment_server_and_search_head_clusters">Deployment server and search head clusters </span></a></i></b></font></h4>
<p>Do not use deployment server to update cluster members.
</p><p>The deployment server is not supported as a means to distribute configurations or apps to cluster members. To distribute configurations across the set of members, you must use the search head cluster deployer. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates"</a>.
</p>
<h4><font size="3"><b><i> <a name="shcsystemrequirements_search_head_clustering_and_search_head_pooling"><span class="mw-headline" id="Search_head_clustering_and_search_head_pooling"> Search head clustering and search head pooling </span></a></i></b></font></h4>
<p>You cannot enable search head clustering on an instance that is part of a <b>search head pool</b>. For information on migrating, see <a href="#migratefromsearchheadpooling" class="external text">"Migrate from a search head pool to a search head cluster"</a>.
</p>
<a name="shcdeploymentoverview"></a><h2> <a name="shcdeploymentoverview_deploy_a_search_head_cluster"><span class="mw-headline" id="Deploy_a_search_head_cluster"> Deploy a search head cluster </span></a></h2>
<p>This topic covers the key steps needed to configure and start a search head cluster.
</p>
<h3> <a name="shcdeploymentoverview_parts_of_a_search_head_cluster"><span class="mw-headline" id="Parts_of_a_search_head_cluster"> Parts of a search head cluster </span></a></h3>
<p>A search head cluster consists of a group of <b>search heads</b> that share configurations, job scheduling, and <b>search artifacts</b>. The search heads are known as the <b>cluster members</b>. 
</p><p>One cluster member has the role of <b>captain</b>, which means that it coordinates job and replication activities among all the members. It also serves as a search head like any other member, running search jobs, serving results, and so on. Over time, the role of captain can shift among the cluster members.
</p><p>In addition to the set of search head members that constitute the actual cluster, a functioning cluster requires several other components:
</p>
<ul><li> <b>The deployer.</b> This is a Splunk Enterprise instance that distributes apps and other configurations to the cluster members.  It stands outside the cluster and cannot run on the same instance as a cluster member.  It can, however, under some circumstances, reside on the same instance as other Splunk Enterprise components, such as a deployment server or an indexer cluster master node.
</li><li> <b>Search peers</b>. These are the indexers that cluster members run their searches across.  The search peers can be either independent indexers or nodes in an indexer cluster.
</li><li> <b>Load balancer.</b> This is third-party software or hardware optionally residing between the users and the cluster members. With a load balancer in place, users can access the set of search heads through a single interface, without needing to specify a particular one.
</li></ul><p>This diagram of a small search head cluster, consisting of three members, illustrates the various components and their relationships:
</p><p><img alt="Searchhead cluster.png" src="images/9/96/Searchhead_cluster.png" width="700" height="447"></p><p>This topic focuses on setting up the cluster members and the deployer.  Other topics in this chapter describe how to configure search peers, connect with an indexer cluster, and add a load balancer.
</p>
<h3> <a name="shcdeploymentoverview_deploy_the_cluster"><span class="mw-headline" id="Deploy_the_cluster"> Deploy the cluster </span></a></h3>
<p>These are the key steps in deploying clusters:
</p><p><b>1.</b> Identify your requirements.
</p><p><b>2.</b> Set up the deployer.
</p><p><b>3.</b> Install the Splunk Enterprise instances.
</p><p><b>4.</b> Initialize cluster members.
</p><p><b>5.</b> Bring up the cluster captain.
</p><p><b>6.</b> Perform post-deployment set-up.
</p>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_1._identify_your_requirements"><span class="mw-headline" id="1._Identify_your_requirements"> 1. Identify your requirements </span></a></i></b></font></h4>
<p><b>a.</b> Determine the cluster size, that is, the number of search heads that you want to include in it. It usually makes sense to put all your search heads in a single cluster. Factors that influence cluster size include the anticipated search load and number of concurrent users, and your availability and failover needs. See <a href="#aboutshc" class="external text">"About search head clustering"</a>.
</p><p><b>b.</b> Decide what <b>replication factor</b> you want to implement. The replication factor is the number of copies of search artifacts that the cluster maintains. Your optimal replication factor depends on factors specific to your environment, but essentially involves a trade-off between failure tolerance and storage capacity. A higher replication factor means that more copies of the search artifacts will reside on more cluster members, so your cluster can tolerate more member failures without needing to use a proxy to access the artifacts. But it also means that you will need more storage to handle the additional copies. See <a href="#chooseshcreplicationfactor" class="external text">"Choose the replication factor for the search head cluster."</a>
</p><p><b>c.</b> Determine whether the search head cluster will be running against a group of standalone indexers or an indexer cluster. For information on indexer clusters, see "About indexer clusters and index replication" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p><p><b>d.</b> Study the topic <a href="#shcsystemrequirements" class="external text">"System requirements and other deployment considerations for search head clusters"</a> for information on other key issues.
</p>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_2._set_up_the_deployer"><span class="mw-headline" id="2._Set_up_the_deployer"> 2. Set up the deployer </span></a></i></b></font></h4>
<p>It is recommended that you select the deployer now, as part of cluster set-up, because you need a deployer in place before you can distribute apps and updated configurations to the cluster members.  
</p><p><b>a.</b> Choose a Splunk Enterprise instance for the deployer functionality.  
</p><p>This instance cannot be a member of the search head cluster, but, under some circumstances, it can be a Splunk Enterprise instance in use for other purposes. If necessary, install a new Splunk Enterprise instance to serve as the deployer. See <a href="#shcsystemrequirements_deployer_requirements" class="external text">"Deployer requirements"</a>. 
</p><p>If you have multiple clusters, you must use a separate deployer for each cluster, unless you are deploying identical configurations across all the clusters. See <a href="#propagateshcconfigurationchanges_deploy_to_multiple_clusters" class="external text">"Deploy to multiple clusters."</a> 
</p><p>Deployer functionality is automatically enabled on all Splunk Enterprise instances. The only configuration step is to specify the deployer's security key, as described in the next step.  Later in this procedure, you point the cluster members at this deployer instance, so that they have access to it. 
</p><p>For information on how to use the deployer to distribute apps to cluster members, see <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a> 
</p><p><b>b.</b> Configure the deployer's security key.
</p><p>See <a href="#setsecretkey" class="external text">"Set a security key for the search head cluster."</a>
</p><p>The deployer uses the security key to authenticate communication with the cluster members. The cluster members also use it to authenticate with each other. The key is optional, but if you use it, you must set it to the same value on all cluster members and the deployer. You can set the key on the cluster members when you initialize them.
</p><p><b>Important:</b>  Splunk strongly recommends that you set a security key.
</p><p>To set the key on the deployer, specify the <code><font size="2">pass4SymmKey</font></code> attribute in either the <code><font size="2">[general]</font></code> or the <code><font size="2">[shclustering]</font></code> stanza of the deployer's <code><font size="2">server.conf</font></code> file.  For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>pass4SymmKey = yoursecuritykey<br></font></code>
</div>
<p>You must restart the deployer for the key to take effect.
</p>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_3._install_the_splunk_enterprise_instances"><span class="mw-headline" id="3._Install_the_Splunk_Enterprise_instances">3. Install the Splunk Enterprise instances </span></a></i></b></font></h4>
<p>Install the Splunk Enterprise instances that will serve as cluster members. For information on the minimum number of members necessary, see <a href="#shcsystemrequirements_required_number_of_instances" class="external text">"Required number of instances."</a>
</p><p><b>Caution:</b> Always use new instances. The process of adding an instance to a search head cluster overwrites any configurations or apps currently resident on the instance.
</p><p>For information on how to install Splunk Enterprise, read the <i>Installation Manual</i>.
</p><p><b>Important:</b> You must change the admin password on each instance. The CLI commands that you use to configure the cluster will not operate on instances with the default password.
</p>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_4._initialize_cluster_members"><span class="mw-headline" id="4._Initialize_cluster_members">4. Initialize cluster members </span></a></i></b></font></h4>
<p>For each instance that you want to include in the cluster, run the <code><font size="2">splunk init shcluster-config</font></code> command and restart the instance:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk init shcluster-config -auth &lt;username&gt;:&lt;password&gt; -mgmt_uri &lt;URI&gt;:&lt;management_port&gt; -replication_port &lt;replication_port&gt; -replication_factor &lt;n&gt; -conf_deploy_fetch_url &lt;URL&gt;:&lt;management_port&gt; -secret &lt;security_key&gt;<br><br>splunk restart <br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> You can only execute this command on an instance that is up and running.
</li><li> The <code><font size="2">-auth</font></code> parameter specifies your current login credentials for this instance. This parameter is required.
</li><li> The <code><font size="2">-mgmt_uri</font></code> parameter specifies the URI and management port for this instance.  You must use the fully qualified domain name. This parameter is required.
</li><li> The <code><font size="2">-replication_port </font></code> parameter specifies the port that the instance uses to listen for search artifacts streamed from the other cluster members. You can specify any available, unused port as the replication port. Do not reuse the instance's management or receiving ports. This parameter is required.
</li><li> The <code><font size="2">-replication_factor</font></code> parameter determines the number of copies of each search artifact that the cluster maintains.  All cluster members must use the same replication factor. This parameter is optional. If not explicitly set, the replication factor defaults to 3.
</li><li> The <code><font size="2">-conf_deploy_fetch_url</font></code> parameter specifies the URL and management port for the deployer instance. This parameter is optional during initialization, but you do need to set it before you can use the deployer functionality. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a> 
</li><li> The <code><font size="2">-secret</font></code> parameter specifies the security key that authenticates communication between the cluster members and between each member and the deployer. This parameter is optional, but if you configure it for one member, you must configure it for all. The key must be the same across all cluster members and the deployer.  See <a href="#setsecretkey" class="external text">"Set a security key for the search head cluster."</a><br><br><b>Important:</b>  Splunk strongly recommends that you set a security key.
</li></ul><p>For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk init shcluster-config -auth admin:changed -mgmt_uri https://sh1.example.com:8089 -replication_port 34567 -replication_factor 2 -conf_deploy_fetch_url https://10.160.31.200:8089 -secret mykey <br><br>splunk restart <br></font></code>
</div>
<p><b>Caution:</b> To add more members after you bootstrap the captain in step 5, you must follow the procedures in <a href="#addaclustermember" class="external text">"Add a cluster member"</a>.
</p>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_5._bring_up_the_cluster_captain"><span class="mw-headline" id="5._Bring_up_the_cluster_captain">5. Bring up the cluster captain </span></a></i></b></font></h4>
<p><b>a.</b> Select one of the initialized instances to be the first cluster captain. It does not matter which instance you select for this role. 
</p><p><b>b.</b> Run the <code><font size="2">splunk bootstrap shcluster-captain</font></code> command on the selected instance:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk bootstrap shcluster-captain -servers_list "&lt;URI&gt;:&lt;management_port&gt;,&lt;URI&gt;:&lt;management_port&gt;,..." -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> This command designates the specified instance as the first cluster captain.
</li><li> Run this command on only a single instance and only once in the lifetime of a cluster.
</li><li> The <code><font size="2">-servers_list</font></code> parameter contains a comma-separated list of the cluster members, including the member that you are running the command on. The members are identified by URI and management port. This parameter is required.
</li><li> <b>Important:</b> The URIs that you specify in <code><font size="2">-servers_list</font></code> must be exactly the same as the ones that you specified earlier when you initialized each member, in the <code><font size="2">-mgmt_uri</font></code> parameter. You cannot, for example, use <code><font size="2">https://foo.example.com:8089</font></code> during initialization and <code><font size="2">https://foo.subdomain.example.com:8089</font></code> here, even if they resolve to the same node.
</li></ul><p>Here is an example of the bootstrap command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk bootstrap shcluster-captain -servers_list "https://sh1.example.com:8089,https://sh2.example.com:8089,https://sh3.example.com:8089,https://sh4.example.com:8089" -auth admin:changed<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="shcdeploymentoverview_6._perform_post-deployment_set-up"><span class="mw-headline" id="6._Perform_post-deployment_set-up">6. Perform post-deployment set-up</span></a></i></b></font></h4>
<p>To complete set-up, perform these additional steps, as necessary:
</p><p><b>a.</b> <b>Integrate the search head cluster into an indexer cluster.</b>  This step is optional. See <a href="#shcandindexercluster" class="external text">"Integrate the search head cluster with an indexer cluster."</a>
</p><p><b>b.</b> <b>Connect the search heads to their search peers.</b> This step is required. See <a href="#connectclustersearchheadstosearchpeers" class="external text">"Connect the search heads in clusters to search peers"</a>.
</p><p><b>c.</b> <b>Add users.</b> This step is required. See <a href="#adduserstotheshc" class="external text">"Add users to the search head cluster"</a>.
</p><p><b>d.</b> <b>Install a load balancer in front of the search heads.</b> This step is optional. See <a href="#useshcwithloadbalancers" class="external text">"Use a load balancer with search head clustering."</a>
</p><p><b>e.</b> <b>Use the deployer to distribute apps and configuration updates to the search heads.</b> This step is required whenever you need to upgrade your set of configurations. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</p><p><br></p>
<h3> <a name="shcdeploymentoverview_check_search_head_cluster_status"><span class="mw-headline" id="Check_search_head_cluster_status">Check search head cluster status</span></a></h3>
<p>To check the overall status of your search head cluster, run this command from any of the members:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk show shcluster-status -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>The command returns basic information on the captain and the cluster members.  It indicates the status of each member, up or down.
</p>
<a name="shcandindexercluster"></a><h2> <a name="shcandindexercluster_integrate_the_search_head_cluster_with_an_indexer_cluster"><span class="mw-headline" id="Integrate_the_search_head_cluster_with_an_indexer_cluster"> Integrate the search head cluster with an indexer cluster</span></a></h2>
<p>To integrate a search head cluster with an <b>indexer cluster</b>, you must configure each member of the search head cluster as a search head on the indexer cluster. Once you have done that, the search heads will get their list of <b>search peers</b> from the master node of the indexer cluster.
</p><p>In this diagram, a search head cluster performs searches across a single-site indexer cluster:
</p><p><img alt="SH cluster with Indexer Cluster.png" src="images/a/ab/SH_cluster_with_Indexer_Cluster.png" width="700" height="584"></p>
<h3> <a name="shcandindexercluster_integrate_with_an_indexer_cluster"><span class="mw-headline" id="Integrate_with_an_indexer_cluster"> Integrate with an indexer cluster </span></a></h3>
<p>Configure each search head cluster member as a search head on the indexer cluster. Use the CLI <code><font size="2">splunk edit cluster-config</font></code> command. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode searchhead -master_uri https://10.152.31.202:8089 -secret newsecret123 <br><br>splunk restart<br></font></code>
</div>
<p>This example specifies:
</p>
<ul><li> The instance is a search head in an indexer cluster.
</li><li> The master node of the indexer cluster resides at <code><font size="2">10.152.31.202:8089</font></code>.
</li><li> The security key is "newsecret123". You must use the same security key across all nodes in both the indexer cluster and the search head cluster.
</li></ul><p>You must do this for each member of the search head cluster.
</p><p>This is all you need for the basic configuration.  The search heads now run their searches against the peer nodes in the indexer cluster.
</p>
<h3> <a name="shcandindexercluster_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>For more information on configuration of search heads on indexer clusters, see the chapter "Configure the search head" in the <i>Managing Indexers and Clusters of Indexers</i> manual.  That chapter also includes configuration for more complex scenarios, such as hybrid searching, where the search heads search across both indexer clusters and non-clustered indexers.
</p>
<a name="connectclustersearchheadstosearchpeers"></a><h2> <a name="connectclustersearchheadstosearchpeers_connect_the_search_heads_in_clusters_to_search_peers"><span class="mw-headline" id="Connect_the_search_heads_in_clusters_to_search_peers"> Connect the search heads in clusters to search peers</span></a></h2>
<p>Before the search heads in the cluster can run searches, they need to know the identity of their indexers, or <b>search peers</b>. All members of a cluster must have access to the same set of search peers.  
</p><p>How the search heads find out about their search peers depends on whether the search head cluster is part of an <b>indexer cluster</b>. There are two scenarios to consider:
</p>
<ul><li> The search head cluster will be running against an indexer cluster.
</li><li> The search head cluster will be running against individual, non-clustered indexers.
</li></ul><p><b>Important:</b> Cluster members cannot distribute searches to other cluster members. In other words, a cluster member cannot be a search peer of the cluster. 
</p>
<h3> <a name="connectclustersearchheadstosearchpeers_search_head_cluster_with_indexer_cluster"><span class="mw-headline" id="Search_head_cluster_with_indexer_cluster"> Search head cluster with indexer cluster </span></a></h3>
<p>If the search head cluster is connected to an indexer cluster, the master node on the indexer cluster provides the search heads with a list of peer nodes to search against.
</p><p>Once you configure the search head cluster members so that they participate in the indexer cluster, you do not need to perform any further configuration for the search heads to know their search peers. See <a href="#shcandindexercluster" class="external text">"Integrate the search head cluster with an indexer cluster"</a>. 
</p><p>Even if you do not need the benefits of index replication, you can still take advantage of this simple approach to configuring the set of search peers. Just incorporate your set of indexers into an indexer cluster with a replication factor of 1. This topology also provides numerous other benefits from a management perspective. See "Use indexer clusters to scale indexing" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p>
<h3> <a name="connectclustersearchheadstosearchpeers_search_head_cluster_with_non-clustered_indexers"><span class="mw-headline" id="Search_head_cluster_with_non-clustered_indexers">Search head cluster with non-clustered indexers</span></a></h3>
<p>If you do not have an indexer cluster, you must add the search peers individually to each search head.  The easiest way to do this is through the CLI. Perform the following procedure on each search head:
</p><p><b>1.</b> Navigate to the <code><font size="2">$SPLUNK_HOME/bin/</font></code> directory on the search head.
</p><p><b>2.</b> Invoke the <code><font size="2">splunk add search-server</font></code> command for each search peer that you want to add:
</p>
 <div class="samplecode">
<code><font size="2"><br>splunk add search-server -host &lt;URI&gt;:&lt;management_port&gt; -auth &lt;user&gt;:&lt;password&gt; -remoteUsername &lt;user&gt; -remotePassword &lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> Use the <code><font size="2">-host</font></code> flag to specify the search peer's URI and management port. 
</li><li> Provide credentials for both the local (search head) and remote (search peer) instances. Use the <code><font size="2">-auth</font></code> flag for the local credentials and the <code><font size="2">-remoteUsername</font></code> and <code><font size="2">-remotePassword</font></code> flags for the remote credentials.  The remote credentials must be for an admin-level user on the search peer.
</li></ul><p>In this example, the search peer has an IP address of <code><font size="2">10.10.10.10</font></code>, a management port of <code><font size="2">8089</font></code>, and an <code><font size="2">admin</font></code> user with a password of <code><font size="2">passremote</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add search-server -host 10.10.10.10:8089 -auth admin:mypassword -remoteUsername admin -remotePassword passremote<br></font></code>
</div>
<p><b>3.</b> Restart the search head after adding all search peers.
</p><p>You must repeat this procedure on each search head.  
</p><p><b>Caution:</b> All search heads must use the same set of search peers.
</p><p>You can also add search peers through Splunk Web.  To do this, you must first unhide the hidden settings, as described in <a href="#howconfigurationworksinshc_the_settings_menu" class="external text">"The Settings menu."</a> Then follow the instructions in <a href="#configuredistributedsearch" class="external text">"Add search peers to the search head."</a>
</p>
<h3> <a name="connectclustersearchheadstosearchpeers_forward_search_head_data_to_the_search_peers"><span class="mw-headline" id="Forward_search_head_data_to_the_search_peers">Forward search head data to the search peers</span></a></h3>
<p>It is considered a best practice to forward all search head internal data to the search peer (indexer) layer. After you connect the search heads to the search peers, follow the instructions in <a href="#forwardsearchheaddata" class="external text">"Best practice: Forward search head data to the indexer layer."</a>
</p>
<a name="adduserstotheshc"></a><h2> <a name="adduserstotheshc_add_users_to_the_search_head_cluster"><span class="mw-headline" id="Add_users_to_the_search_head_cluster"> Add users to the search head cluster</span></a></h2>
<p>To add users to the search head cluster, use either LDAP or Splunk Enterprise built-in authentication.
</p>
<h3> <a name="adduserstotheshc_use_ldap_to_add_users"><span class="mw-headline" id="Use_LDAP_to_add_users"> Use LDAP to add users </span></a></h3>
<p>To add users through LDAP:
</p><p><b>1.</b> Edit a copy of authentication.conf for your LDAP environment. See "Configure LDAP with the configuration file" in the <i>Securing Splunk Enterprise</i> manual.
</p><p><b>2.</b>  On a separate test instance, ensure that the authentication functions properly.
</p><p><b>3.</b> Place the edited copy of the file on the deployer. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a> As explained in that topic, you must place standalone files like this one in a subdirectory under <code><font size="2">$SPLUNK_HOME/etc/shcluster/apps</font></code>.
</p><p><b>4.</b> Push the deployer's configuration bundle, including this file, to the cluster members. See <a href="#propagateshcconfigurationchanges_push_the_configuration_bundle" class="external text">"Push the configuration bundle."</a>
</p>
<h3> <a name="adduserstotheshc_use_splunk_enterprise_built-in_authentication_to_add_members"><span class="mw-headline" id="Use_Splunk_Enterprise_built-in_authentication_to_add_members">Use Splunk Enterprise built-in authentication to add members</span></a></h3>
<p>To use the built-in authentication method, you must add each user to each cluster member. This is necessary to generate the <code><font size="2">$SPLUNK_HOME/etc/passwd</font></code> file on each member.
It is recommended that you script this:
</p><p><b>1.</b> Create a script that adds each user through the <code><font size="2">splunk add user</font></code> CLI command. See "Configure users with the CLI" in the <i>Securing Splunk Enterprise</i> manual.
</p><p><b>2.</b> Run the script on each cluster member.
</p>
<a name="useshcwithloadbalancers"></a><h2> <a name="useshcwithloadbalancers_use_a_load_balancer_with_search_head_clustering"><span class="mw-headline" id="Use_a_load_balancer_with_search_head_clustering"> Use a load balancer with search head clustering</span></a></h2>
<p>Splunk recommends that you run a third-party hardware or software load balancer in front of your set of clustered search heads. That way, users can access the set of search heads through a single interface, without needing to specify a particular one. 
</p><p>There are a variety of third-party load balancers available that you can use for this purpose. Select a load balancer that employs layer-7 (application-level) processing. 
</p><p>Configure the load balancer so that user sessions are "sticky" or "persistent." This ensures that the user remains on a single search head throughout their session.
</p>
<a name="migratefromsearchheadpooling"></a><h2> <a name="migratefromsearchheadpooling_migrate_from_a_search_head_pool_to_a_search_head_cluster"><span class="mw-headline" id="Migrate_from_a_search_head_pool_to_a_search_head_cluster"> Migrate from a search head pool to a search head cluster</span></a></h2>
<p>You can migrate the settings from a search head pool to a search head cluster. You cannot migrate the search head instances themselves, however. You must use new instances when enabling search head cluster members.
</p><p>To migrate the settings from a search head pool to the search head cluster, you copy the shared directories in the search head pool to the deployer instance. You then use the deployer instance to propagate those directories to the cluster members.
</p><p>The  migration procedure varies somewhat depending on whether you are migrating to a new cluster or to a cluster that is already running.  
</p>
<h3> <a name="migratefromsearchheadpooling_important_points_to_consider"><span class="mw-headline" id="Important_points_to_consider">Important points to consider</span></a></h3>
<p>There are a few points to keep in mind when migrating your settings from a search head pool to a search head cluster.
</p>
<h4><font size="3"><b><i> <a name="migratefromsearchheadpooling_do_not_migrate_default_apps"><span class="mw-headline" id="Do_not_migrate_default_apps"> Do not migrate default apps </span></a></i></b></font></h4>
<p>When you migrate apps to the search head cluster, do not migrate any default apps, that is, apps that ship with Splunk Enterprise, such as the search app. If you push default apps to cluster members, you overwrite the version of those apps residing on the members, and you do not want to do this. 
</p><p>It is, however, fine to migrate any private objects associated with the default apps. Private objects are located under the search head pool's <code><font size="2">etc/users</font></code>, not under <code><font size="2">etc/apps</font></code>.
</p>
<h4><font size="3"><b><i> <a name="migratefromsearchheadpooling_migrated_settings_get_placed_in_default_directories"><span class="mw-headline" id="Migrated_settings_get_placed_in_default_directories"> Migrated settings get placed in default directories </span></a></i></b></font></h4>
<p>The deployer puts all migrated settings into default directories on the cluster members. This includes any runtime changes that were made while the apps were running on the search head pool. 
</p><p>Because users cannot change settings in default directories, this means that users cannot perform certain runtime operations on migrated entities:
</p>
<ul><li> Delete. Users cannot delete any migrated entities.
</li><li> Move. Users cannot move these settings from one app to another.
</li><li> Change sharing level. Users cannot change sharing levels. For example, a user cannot change sharing from private to app-level.
</li></ul><p>Users can override existing attributes by editing entities in place. Runtime changes get put in the local directories on the cluster members. Local directories override default directories, so the changes override the default settings.
</p><p>For more information on where deployed settings reside on the cluster members, see See  <a href="#propagateshcconfigurationchanges_location_on_the_cluster_members" class="external text">"Location on the cluster members."</a>
</p><p><b>Note:</b> Splunk does not support migration of per-user search history files.
</p>
<h3> <a name="migratefromsearchheadpooling_migrate_to_a_new_search_head_cluster"><span class="mw-headline" id="Migrate_to_a_new_search_head_cluster"> Migrate to a new search head cluster </span></a></h3>
<p>To migrate settings from a search head pool to a new search head cluster:
</p><p><b>1.</b> Follow the procedure for deploying any new search head cluster. Specify the deployer location at the time that you initialize the cluster members. See  <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster."</a> 
</p><p><b>Caution:</b> You must deploy new instances. You cannot reuse existing search heads. 
</p><p><b>2.</b> Copy the <code><font size="2">etc/apps</font></code> and <code><font size="2">etc/users</font></code> directories on the shared storage location in the search head pool to the distribution directory on the deployer instance.  The distribution directory is located at <code><font size="2">$SPLUNK_HOME/etc/shcluster</font></code>.   
</p><p>For details on the distribution directory file structure, see <a href="#propagateshcconfigurationchanges_what_the_configuration_bundle_contains" class="external text">"What the configuration bundle contains."</a>
</p><p><b>3.</b> If <code><font size="2">$SPLUNK_HOME/etc/shcluster/apps</font></code> contains any default apps, such as the search app, you must delete them now. Do not push them to the cluster members. If you do, they will overwrite the versions of those apps already on the members.
</p><p><b>4.</b> Use the deployer to push the configuration bundle to the cluster members. See <a href="#propagateshcconfigurationchanges_push_the_configuration_bundle" class="external text">"Push the configuration bundle."</a>
</p><p><b>Note:</b> If you point the cluster members at the same set of search peers previously used by the search head pool, the cluster will need to rebuild any report acceleration summaries or data model summaries resident on the search peers. It does this automatically. It does not, however, automatically remove the old set of summaries.
</p>
<h3> <a name="migratefromsearchheadpooling_migrate_to_an_existing_search_head_cluster"><span class="mw-headline" id="Migrate_to_an_existing_search_head_cluster"> Migrate to an existing search head cluster </span></a></h3>
<p>To migrate settings from a search head pool to an existing search head cluster:
</p><p><b>1.</b> Copy the <code><font size="2">/etc/apps</font></code> and <code><font size="2">/etc/users</font></code> directories on the shared storage location in the search head pool to a temporary directory where you can edit them.
</p><p><b>2.</b> In the temporary directory, delete these subdirctories:
</p>
<ul><li> Any default apps, such as the search app. Do not push default apps to the cluster members. If you do, they will overwrite the versions of those apps already on the members.
</li><li> Any apps already existing in the deployer's distribution directory. Otherwise, the versions from the search head pool will overwrite the versions already on the members.
</li></ul><p><b>3.</b> Copy the remaining subdirectories from the temporary location to the distribution directory on the deployer, located at <code><font size="2">$SPLUNK_HOME/etc/shcluster</font></code>. Leave any subdirectories already in the distribution directory unchanged. 
</p><p>For details on the distribution directory file structure, see <a href="#propagateshcconfigurationchanges_what_the_configuration_bundle_contains" class="external text">"What the configuration bundle contains."</a> 
</p><p><b>4.</b> Use the deployer to push the configuration bundle, including the migrated settings, to the cluster members. See <a href="#propagateshcconfigurationchanges_push_the_configuration_bundle" class="external text">"Push the configuration bundle."</a>
</p>
<h3> <a name="migratefromsearchheadpooling_search_head_clustering_and_mounted_bundles"><span class="mw-headline" id="Search_head_clustering_and_mounted_bundles"> Search head clustering and mounted bundles </span></a></h3>
<p>For most types of deployments, including search head clustering, Splunk recommends that you use normal bundle replication, rather than mounted bundles with shared storage. 
</p><p>As a result of changes to bundle replication made in the 5.0 timeframe, such as the introduction of delta-based replication and improvements in streaming, the practical use case for mounted bundles is now extremely limited. In most cases, mounted bundles make little difference in the amount of network traffic or the speed at which bundle changes get distributed to the search peers.  At the same time, they add significant management complexity, particularly when combined with shared storage.  Because of delta-based replication, even if your configurations contain large files, normal bundle replication entails little ongoing replication cost, as long as those files rarely change.
</p>
<a name="migratefromstandalonesearchheads"></a><h2> <a name="migratefromstandalonesearchheads_migrate_from_a_standalone_search_head_to_a_search_head_cluster"><span class="mw-headline" id="Migrate_from_a_standalone_search_head_to_a_search_head_cluster"> Migrate from a standalone search head to a search head cluster</span></a></h2>
<p>You can migrate settings from an existing standalone search head to all members in a search head cluster. 
</p><p><b>Important:</b> You cannot migrate the search head instance itself, only its settings. You can only add clean, new Splunk Enterprise instances to a search head cluster.
</p>
<h3> <a name="migratefromstandalonesearchheads_important_points_to_consider"><span class="mw-headline" id="Important_points_to_consider">Important points to consider</span></a></h3>
<p>There are a few points to keep in mind when migrating your settings to a search head cluster.
</p>
<h4><font size="3"><b><i> <a name="migratefromstandalonesearchheads_do_not_migrate_default_apps"><span class="mw-headline" id="Do_not_migrate_default_apps"> Do not migrate default apps </span></a></i></b></font></h4>
<p>When you migrate apps to the search head cluster, do not migrate any default apps, that is, apps that ship with Splunk Enterprise, such as the search app. If you push default apps to cluster members, you overwrite the version of those apps residing on the members, and you do not want to do this. 
</p><p>It is, however, fine to migrate any private objects associated with the default apps. Private objects are located under the <code><font size="2">etc/users</font></code> directory, not under <code><font size="2">etc/apps</font></code>.
</p>
<h4><font size="3"><b><i> <a name="migratefromstandalonesearchheads_migrated_settings_get_placed_in_default_directories"><span class="mw-headline" id="Migrated_settings_get_placed_in_default_directories"> Migrated settings get placed in default directories </span></a></i></b></font></h4>
<p>The deployer puts all migrated settings into default directories on the cluster members. This includes any runtime changes that were made while the apps were running on the standalone search head. 
</p><p>Because users cannot change settings in default directories, this means that users cannot perform certain runtime operations on migrated entities:
</p>
<ul><li> Delete. Users cannot delete any migrated entities.
</li><li> Move. Users cannot move these settings from one app to another.
</li><li> Change sharing level. Users cannot change sharing levels. For example, a user cannot change sharing from private to app-level.
</li></ul><p>Users can override existing attributes by editing entities in place. Runtime changes get put in the local directories on the cluster members. Local directories override default directories, so the changes override the default settings.
</p><p>For more information on where deployed settings reside on the cluster members, see See  <a href="#propagateshcconfigurationchanges_location_on_the_cluster_members" class="external text">"Location on the cluster members."</a>
</p><p><b>Note:</b> Splunk does not support migration of per-user search history files.
</p>
<h3> <a name="migratefromstandalonesearchheads_migrate_settings_to_a_search_head_cluster"><span class="mw-headline" id="Migrate_settings_to_a_search_head_cluster"> Migrate settings to a search head cluster</span></a></h3>
<p>To migrate settings:
</p><p><b>1.</b> Copy the <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and <code><font size="2">$SPLUNK_HOME/etc/users</font></code> directories on the standalone search head to a temporary directory where you can edit them.
</p><p><b>2.</b> In the temporary directory, delete these subdirctories:
</p>
<ul><li> Any default apps, such as the search app. Do not push default apps to the cluster members. If you do, they will overwrite the versions of those apps already on the members.
</li><li> Any apps already existing in the deployer's distribution directory. Otherwise, the versions from the standalone search head will overwrite the versions already on the members.
</li></ul><p><b>3.</b> Copy the remaining subdirectories from the temporary location to the distribution directory on the deployer.  The distribution directory is located at <code><font size="2">$SPLUNK_HOME/etc/shcluster</font></code>. Leave any subdirectories already in the distribution directory unchanged.
</p><p>For details on the distribution directory file structure, see <a href="#propagateshcconfigurationchanges_what_the_configuration_bundle_contains" class="external text">"What the configuration bundle contains."</a> 
</p><p><b>4.</b> If you need to add new cluster members, you must deploy clean instances. You cannot reuse the existing search head. For information on adding cluster members, see  <a href="#addaclustermember" class="external text">"Add a cluster member."</a>
</p><p><b>5.</b> Use the deployer to push the configuration bundle, including the migrated settings, to the cluster members. See <a href="#propagateshcconfigurationchanges_push_the_configuration_bundle" class="external text">"Push the configuration bundle."</a>
</p><p><b>Important:</b> If you point the cluster members at the same set of search peers previously used by the standalone search head, the cluster will need to rebuild any report acceleration summaries or data model summaries resident on the search peers. It does this automatically. It does not, however, automatically remove the old set of summaries.
</p>
<a name="upgradeashc"></a><h2> <a name="upgradeashc_upgrade_a_search_head_cluster"><span class="mw-headline" id="Upgrade_a_search_head_cluster"> Upgrade a search head cluster</span></a></h2>
<p>This topic covers the set of search head cluster upgrade scenarios.  Currently, the possible scenarios consist of:
</p>
<ul><li> Upgrade to a new maintenance release
</li></ul><h3> <a name="upgradeashc_upgrade_to_a_new_maintenance_release"><span class="mw-headline" id="Upgrade_to_a_new_maintenance_release"> Upgrade to a new maintenance release </span></a></h3>
<p>To upgrade a search head cluster to a new maintenance release (for example, from 6.2.1 to 6.2.2), perform these steps:
</p><p><b>1.</b> Stop all cluster members. 
</p><p><b>2.</b> Upgrade all members.
</p><p><b>3.</b> Stop the deployer.
</p><p><b>4.</b> Upgrade the deployer.
</p><p><b>5.</b> Restart the deployer.
</p><p><b>6.</b> Restart the members.
</p><p><b>7.</b> Wait one to two minutes for captain election to complete. The cluster will then begin functioning.
</p><p>Note the following:
</p>
<ul><li> All cluster members must be running the same version of Splunk Enterprise (down to the maintenance level).
</li><li> You can run search head cluster members against 5.x or 6.x search peers, so it is not necessary to upgrade the indexers at the same time. See <a href="#shcsystemrequirements_splunk_enterprise_version_compatibility" class="external text">"Splunk Enterprise version compatibility."</a>
</li></ul><h1>Configure search head clustering</h1><a name="shcconfigurationoverview"></a><h2> <a name="shcconfigurationoverview_configure_the_search_head_cluster"><span class="mw-headline" id="Configure_the_search_head_cluster"> Configure the search head cluster </span></a></h2>
<p>This topic describes how to configure the behavior of the search head cluster itself. It does not describe how to configure the search-time environment of the cluster members, such as the set of saved searches, dashboards, and apps that the members have access to. For information on configuring the search-time environment, see the chapter <a href="#howconfigurationworksinshc" class="external text">"Update search head cluster members"</a>.
</p><p>The members store their cluster configurations in their local <code><font size="2">server.conf</font></code> files, located under <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>.  See the server.conf specification file for details on all available configuration attributes.
</p>
<h3> <a name="shcconfigurationoverview_key_information"><span class="mw-headline" id="Key_information"> Key information </span></a></h3>
<p>Remember these key points while reading this topic:
</p>
<ul><li> The essential configuration occurs when you initialize each member during the deployment process.  
</li><li> Search head clustering has a large number of configuration settings available.  With a few exceptions, you should not change these settings from their initial or default values without guidance from Splunk Support.
</li><li> You must maintain identical settings across all members, except as noted.
</li><li> When you do change a setting across all members, you must restart all the members at approximately the same time.
</li></ul><h3> <a name="shcconfigurationoverview_initialization-time_configurations"><span class="mw-headline" id="Initialization-time_configurations"> Initialization-time configurations </span></a></h3>
<p>You can set all essential configurations during the deployment process, when you initialize each member. These are the key configuration attributes that you can or must set for each cluster member during initialization:
</p>
<ul><li> The member's URI. See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster"</a>.
</li><li> The member's replication port. See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster"</a>.
</li><li> The cluster's replication factor. See <a href="#chooseshcreplicationfactor" class="external text">"Choose the replication factor for the search head cluster"</a>.
</li><li> The cluster's security key. See <a href="#setsecretkey" class="external text">"Set a security key for the search head cluster"</a>.
</li><li> The deployer location. See <a href="#propagateshcconfigurationchanges_point_the_cluster_members_to_the_deployer" class="external text">"Point the cluster members to the deployer"</a>.
</li></ul><p><b>Caution:</b> It is strongly recommended that you set all these attributes during initialization and do not later change them. See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster"</a>.
</p>
<h3> <a name="shcconfigurationoverview_post-initialization_configuration_changes"><span class="mw-headline" id="Post-initialization_configuration_changes"> Post-initialization configuration changes </span></a></h3>
<p>The main configuration changes that you can safely perform on your own, post-initialization, are the ad hoc search settings. There are two of these: one for specifying whether a particular member should run ad hoc searches only, and another for specifying whether the member currently functioning as the captain should run ad hoc searches only. The captain will not assign scheduled searches to ad hoc members. See <a href="#adhocclustermember" class="external text">"Configure a cluster member to run ad hoc searches only"</a>.
</p><p><b>Caution:</b> Do not edit the <code><font size="2">id</font></code> attribute in the <code><font size="2">[shclustering]</font></code> stanza. The system sets it automatically. This attribute must conform to the requirements for a valid GUID.
</p>
<h3> <a name="shcconfigurationoverview_maintain_the_same_configuration_settings_across_all_members"><span class="mw-headline" id="Maintain_the_same_configuration_settings_across_all_members"> Maintain the same configuration settings across all members</span></a></h3>
<p>The <code><font size="2">server.conf</font></code> attributes for search head clustering must have the same values across all members, with these exceptions:
</p>
<ul><li> <code><font size="2">mgmt_uri</font></code>
</li><li> <code><font size="2">adhoc_searchhead</font></code>
</li><li> <code><font size="2"> [replication_port://&lt;port&gt;]</font></code>
</li></ul><p>If any configuration values other than these ones vary from member to member, then the behavior of the cluster will change depending on which member is currently serving as captain. You do not want that to occur.
</p>
<h3> <a name="shcconfigurationoverview_configuration_methods"><span class="mw-headline" id="Configuration_methods"> Configuration methods </span></a></h3>
<p>Most of the configuration occurs during initial cluster deployment, through the CLI <code><font size="2">splunk init</font></code> command. To perform further configuration later, you have two choices:
</p>
<ul><li> Use the CLI <code><font size="2">splunk edit shcluster-config</font></code> command.
</li></ul><ul><li> Edit the <code><font size="2">[shclustering]</font></code> stanza in <code><font size="2">server.conf</font></code> directly.
</li></ul><p>It is generally simpler to use the CLI.
</p><p><b>Caution:</b> You must make the same configuration changes on all members and then restart them all at approximately the same time. Because of the importance of maintaining identical settings across all members, do not use the <code><font size="2">splunk rolling-restart</font></code> command to restart, except when changing the <code><font size="2">captain_is_adhoc_searchhead</font></code> attribute, as described in <a href="#adhocclustermember" class="external text">"Configure a cluster member to run ad hoc searches only"</a>. Instead, run the <code><font size="2">splunk restart</font></code> command on each member.
</p>
<h4><font size="3"><b><i> <a name="shcconfigurationoverview_configure_search_head_clustering_with_the_cli"><span class="mw-headline" id="Configure_search_head_clustering_with_the_CLI"> Configure search head clustering with the CLI </span></a></i></b></font></h4>
<p>You can use the CLI <code><font size="2">splunk edit shcluster-config</font></code> command to make edits to the <code><font size="2">[shclustering]</font></code> stanza in <code><font size="2">server.conf</font></code>. Specify each attribute and its configured value as a key value pair. 
</p><p>For example, to edit the <code><font size="2">adhoc_searchhead</font></code> attribute:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit shcluster-config -adhoc_searchhead true -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>The CLI confirms that the operation was successful and instructs you to restart <code><font size="2">splunkd</font></code>. 
</p><p>Note the following:
</p>
<ul><li> You can use this command to edit any attribute in the <code><font size="2">[shclustering]</font></code> stanza except the <code><font size="2">disabled</font></code> attribute, which turns search head clustering on and off.
</li><li> You can only use this command on a member that has already been initialized. For initial configuration, use <code><font size="2">splunk init shcluster-config</font></code>.
</li></ul><h4><font size="3"><b><i> <a name="shcconfigurationoverview_configure_search_head_clustering_by_editing_server.conf"><span class="mw-headline" id="Configure_search_head_clustering_by_editing_server.conf"> Configure search head clustering by editing server.conf </span></a></i></b></font></h4>
<p>You can also change attributes by directly editing <code><font size="2">server.conf</font></code>. The search head clustering attributes are located in the <code><font size="2">[shclustering]</font></code> stanza, with one exception: To modify the replication port, use the <code><font size="2">[replication_port]</font></code> stanza.
</p>
<a name="chooseshcreplicationfactor"></a><h2> <a name="chooseshcreplicationfactor_choose_the_replication_factor_for_the_search_head_cluster"><span class="mw-headline" id="Choose_the_replication_factor_for_the_search_head_cluster"> Choose the replication factor for the search head cluster</span></a></h2>
<p>The replication factor determines the number of copies of each search artifact, or search result, that the cluster maintains. Replication occurs only for artifacts from scheduled saved searches. The cluster does not replicate results from ad hoc searches or realtime searches.
</p>
<h3> <a name="chooseshcreplicationfactor_effect_of_the_replication_factor"><span class="mw-headline" id="Effect_of_the_replication_factor"> Effect of the replication factor </span></a></h3>
<p>The cluster can tolerate a failure of (replication factor - 1) members without losing any search artifacts. For example, if you want to ensure that your system can handle the failure of two members without losing search artifacts, you must configure a replication factor of 3, which means that the cluster stores three copies of each search artifact, with each copy on a different member. If two members go down, the artifact is still available on a third member. 
</p><p>The default value for the replication factor is 3. This number is sufficient for most purposes.
</p><p>Even with a large cluster of, say, 50 search heads, you do not need a commensurately large replication factor. As long as you do not lose the replication factor number of members, at least one copy of each search artifact still exists somewhere on the cluster and is accessible to all cluster members. Any search head in the cluster can access any search artifact by proxying from a search head storing a copy of that artifact. The proxying operation is fast and unlikely to impede access to search results from any search head.
</p><p><b>Note:</b> The replication factor determines only the number of copies of search artifacts that the cluster maintains. It does not affect the replication of runtime configuration changes, such as new saved searches. Those changes get replicated to all cluster members by a different process.  If you have 50 search heads, each of those 50 gets a copy of such configuration changes. See <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates."</a>
</p>
<h3> <a name="chooseshcreplicationfactor_replication_factor_configuration"><span class="mw-headline" id="Replication_factor_configuration"> Replication factor configuration </span></a></h3>
<p>All cluster members must use the same replication factor. The <code><font size="2">server.conf</font></code> attribute that determines the replication factor is <code><font size="2">replication_factor</font></code>. 
</p><p>You specify the replication factor during deployment of the cluster, as part of member initialization.  See <a href="#shcdeploymentoverview_4._initialize_cluster_members" class="external text">"Initialize cluster members."</a>
</p><p>You can change the replication factor post-deployment, if necessary, but it is recommended that you consult Splunk Support before doing so.  If you change the replication factor on one member, you must change it on all members. For information on modifying configuration values, see <a href="#shcconfigurationoverview" class="external text">"Configure the search head cluster."</a>
</p>
<h3> <a name="chooseshcreplicationfactor_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>For information on how the cluster replicates search artifacts, see <a href="#shcarchitecture_how_the_cluster_handles_search_artifacts" class="external text">"How the cluster handles search artifacts."</a>  That subtopic describes several key points about artifact replication, among them:
</p>
<ul><li> In some cases, the cluster might replicate more than the replication factor number of a search artifact. 
</li><li> Artifact proxying, along with additional replication, occurs if a member without a copy of the artifact needs access to it.
</li><li> If a member goes down, the cluster replaces the artifact copies that were being stored on that member.
</li></ul><p>See <a href="#viewshcstatus_list_search_artifacts" class="external text">"List search artifacts"</a>  to learn how to view the set of artifacts in the cluster and on individual members.
</p>
<a name="setsecretkey"></a><h2> <a name="setsecretkey_set_a_security_key_for_the_search_head_cluster"><span class="mw-headline" id="Set_a_security_key_for_the_search_head_cluster"> Set a security key for the search head cluster</span></a></h2>
<p>You can set a security key to authenticate communication between all cluster members, as well as between members and the deployer instance. 
</p><p>For an overview of search head clustering configuration, see <a href="#shcconfigurationoverview" class="external text">"Configure the search head cluster"</a>.
</p>
<h3> <a name="setsecretkey_security_key_must_be_identical_across_all_nodes"><span class="mw-headline" id="Security_key_must_be_identical_across_all_nodes">Security key must be identical across all nodes </span></a></h3>
<p>If you set the key on one cluster member, you must also set the key to the same value on all other members and the deployer. 
</p><p>If the search head cluster is part of an indexer cluster, the same key must be used across both cluster types.
</p>
<h3> <a name="setsecretkey_best_practice:_set_the_security_key_during_deployment"><span class="mw-headline" id="Best_practice:_Set_the_security_key_during_deployment"> Best practice: Set the security key during deployment</span></a></h3>
<p>Splunk strongly recommends that you set the key during deployment, using the <code><font size="2">-secret</font></code> parameter with the <code><font size="2">splunk init shcluster-config</font></code> CLI command. See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster"</a>.
</p>
<h3> <a name="setsecretkey_set_the_security_key_post-deployment"><span class="mw-headline" id="Set_the_security_key_post-deployment"> Set the security key post-deployment</span></a></h3>
<p>You can set the security key post-deployment by configuring the <code><font size="2">pass4SymmKey</font></code> attribute in <code><font size="2">server.conf</font></code> on each cluster member. Put the attribute under the <code><font size="2">[shclustering]</font></code> or <code><font size="2">[general]</font></code> stanza.  For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>pass4SymmKey = yoursecuritykey<br></font></code>
</div>
<p>If the search head cluster is part of an indexer cluster, set the key in the <code><font size="2">[general]</font></code> stanza, so that the instance uses the same key in its two roles of both a search head cluster member and an indexer cluster node.
</p><p>You must restart each instance for the key to take effect. For more information on post-deployment configuration, see <a href="#shcconfigurationoverview_configuration_methods" class="external text">"Configuration methods."</a>
</p>
<h3> <a name="setsecretkey_keep_a_copy_of_the_security_key"><span class="mw-headline" id="Keep_a_copy_of_the_security_key"> Keep a copy of the security key</span></a></h3>
<p>You should save a copy of the key in a safe place. Once an instance starts running, the security key changes from clear text to encrypted form, and it is no longer recoverable from <code><font size="2">server.conf</font></code>. If you later want to add a new member, you will need to use the clear text version to set the key.
</p>
<h1>Update search head cluster members</h1><a name="howconfigurationworksinshc"></a><h2> <a name="howconfigurationworksinshc_how_configuration_changes_propagate_across_the_search_head_cluster"><span class="mw-headline" id="How_configuration_changes_propagate_across_the_search_head_cluster"> How configuration changes propagate across the search head cluster</span></a></h2>
<h3> <a name="howconfigurationworksinshc_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> "Administer Splunk Enterprise with configuration files" in the <i>Admin Manual.</i> The topics in that chapter provide important background information on configuration files.
</li></ul><h3> <a name="howconfigurationworksinshc_the_importance_of_configuration_files_in_a_search_head_cluster"><span class="mw-headline" id="The_importance_of_configuration_files_in_a_search_head_cluster"> The importance of configuration files in a search head cluster </span></a></h3>
<p>Settings in configuration files control the functionality of a search head, including the set of <b>knowledge objects</b>. For example, there are configuration files for <b>saved searches</b>, <b>event types</b>, and <b>workflow actions</b>.  Other configuration files provide the settings for non-search functionality, such as data inputs and indexing. See "List of configuration files" in the <i>Admin Manual.</i>
</p><p>Besides the configuration files, other files are important to search-time functionality.  For example, static <b>lookup</b> tables, <b>dashboards</b>, and <b>data models</b> use various files as part of their definition.
</p><p>For a search head cluster to function properly, its members must all use the same set of search-related configurations. For example, all search heads in the cluster need access to the same set of saved searches. They must therefore use the same <code><font size="2">savedsearches.conf</font></code> settings.
</p><p>Apps must also be identical across all search heads in a cluster. An app is essentially just a set of configurations.
</p>
<h3> <a name="howconfigurationworksinshc_how_configuration_changes_propagate_in_a_search_head_cluster"><span class="mw-headline" id="How_configuration_changes_propagate_in_a_search_head_cluster"> How configuration changes propagate in a search head cluster</span></a></h3>
<p>A search head cluster uses two means to ensure that configurations are identical across its members: automatic replication and the deployer. 
</p>
<h4><font size="3"><b><i> <a name="howconfigurationworksinshc_replicated_changes"><span class="mw-headline" id="Replicated_changes"> Replicated changes </span></a></i></b></font></h4>
<p>The cluster automatically replicates any runtime knowledge object changes on one cluster member to all other members. This includes, for example, changes or additions to saved searches, lookup tables, and dashboards. For example, when a user in Splunk Web defines a field extraction, the cluster replicates that field extraction to all other search heads in the cluster.  
</p><p>See <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates."</a>
</p>
<h4><font size="3"><b><i> <a name="howconfigurationworksinshc_deployed_changes"><span class="mw-headline" id="Deployed_changes"> Deployed changes </span></a></i></b></font></h4>
<p>The cluster does not replicate all configuration changes, only changes made at runtime through Splunk Web, the CLI, or the REST API.  For other configuration changes and additions, you must explicitly push the changes to all cluster members. You do this through a special Splunk Enterprise instance called the <b>deployer</b>.
</p><p>Examples of changes that require use of the deployer include any configuration files that you edit directly. For example, if you make a change in <code><font size="2">limits.conf</font></code>, you must push the change through the deployer. Similarly, if you directly edit a knowledge object configuration file, like <code><font size="2">savedsearches.conf</font></code>, you must use the deployer to distribute it to cluster members. In addition, you must use the deployer to push new or upgraded apps to the cluster members. 
</p><p>See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</p>
<h3> <a name="howconfigurationworksinshc_the_settings_menu"><span class="mw-headline" id="The_Settings_menu"> The Settings menu </span></a></h3>
<p>The Settings menu in Splunk Web organizes settings into several groups, including one called Knowledge, which contains the knowledge object settings. Search head clustering hides all non-Knowledge groups in each member's Settings menu by default. For example, it hides settings for data inputs and the distributed environment. You can unhide the hidden categories, if necessary.
</p><p>The reason for hiding non-Knowledge settings is that the cluster only replicates changes made to settings in the Knowledge category. If you make a change on one member to a setting in a non-Knowledge category, the cluster does not automatically replicate that change to the other members.  
</p><p>If you need to access a non-Knowledge setting on a member, you can unhide the hidden settings:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right corner of Splunk Web. A list of settings, limited to the Knowledge group, appears.
</p><p><b>2.</b> Click the <b>Show All Settings</b> button at the end of the list. A dialog box reminds you that hidden settings will not be replicated.  
</p><p><b>3.</b> To continue, click <b>Show</b> in the dialog box. The full list of settings, dependent on your role permissions, appears.
</p><p>The settings are now unhidden for all users with permission to view them; typically, all admin users. To rehide the settings, you must restart the instance.
</p><p><b>Important:</b> If you make a change to a non-Knowledge setting, the changed configuration will exist only on the cluster member where you made the change. If you want other members to get that change as well, you must use the deployer to push the underlying configuration file for that setting.
</p>
<a name="howconfrepoworksinshc"></a><h2> <a name="howconfrepoworksinshc_configuration_updates_that_the_cluster_replicates"><span class="mw-headline" id="Configuration_updates_that_the_cluster_replicates"> Configuration updates that the cluster replicates</span></a></h2>
<p>The cluster automatically replicates any runtime <b>knowledge object</b> changes on one cluster member to all other members. This includes, for example, changes or additions to <b>saved searches</b>, <b>lookup</b> tables, and <b>dashboards</b>. For example, when a user in Splunk Web defines a <b>field extraction</b>, the cluster replicates that field extraction to all search heads in the cluster.  
</p><p><b>Note:</b> The cluster replicates configuration changes to all cluster members, independent of the cluster's replication factor, which applies only to search artifact replication.
</p><p>A whitelist determines the types of changes that get replicated.  You can configure the list to exclude certain types.
</p>
<h3> <a name="howconfrepoworksinshc_how_replication_works"><span class="mw-headline" id="How_replication_works"> How replication works </span></a></h3>
<p>When a user makes a configuration change to a cluster member search head, the member saves the change to a file, or set of files, locally and also sends the change to the captain. Approximately every five seconds, each cluster member contacts the captain and pulls any changes that have arrived since the last time it pulled changes. Each cluster member then applies the changes locally.
</p><p>For example, assume a user on one cluster member uses Splunk Web to create a new field extraction. Splunk Web saves the field extraction in local files on that member. The member then sends the file changes to the captain.  When each cluster member next contacts the captain, it pulls the changes, along with any other recent changes, and applies them locally. Within a few seconds, all cluster members have the new field extraction.
</p><p><b>Note:</b> Files replicated and updated this way are semantically and functionally equivalent across the set of cluster members. The files might not be identical on all members, however.  For example, depending on circumstances such as the order in which changes reach the captain, it is possible that an updated setting in <code><font size="2">props.conf</font></code> could appear in different locations within the file on different members.
</p>
<h3> <a name="howconfrepoworksinshc_the_changes_that_the_cluster_replicates"><span class="mw-headline" id="The_changes_that_the_cluster_replicates"> The changes that the cluster replicates</span></a></h3>
<p>The cluster replicates changes to knowledge objects. 
</p><p>Replication operates under these constraints:
</p>
<ul><li> The cluster only replicates changes made through specific configuration methods.
</li><li> A whitelist determines the type of changes that the cluster replicates. 
</li></ul><h4><font size="3"><b><i> <a name="howconfrepoworksinshc_configuration_methods_that_trigger_replication"><span class="mw-headline" id="Configuration_methods_that_trigger_replication">Configuration methods that trigger replication </span></a></i></b></font></h4>
<p>The cluster replicates changes made through these methods:
</p>
<ul><li> Splunk Web
</li><li> The Splunk CLI
</li><li> The REST API
</li></ul><p>The cluster does not replicate any configuration changes that you make manually, such as direct edits to configuration files.
</p><p>For example, if a user creates a saved search in Splunk Web on a cluster member, the cluster replicates that saved search to all cluster members.  However, if you, as the administrator, add a saved search by directly editing the <code><font size="2">savedsearches.conf</font></code> file on one cluster member, the cluster does not replicate that saved search to the other cluster members.  You must use the <b>deployer</b> to push that saved search to all cluster members.
</p>
<h4><font size="3"><b><i> <a name="howconfrepoworksinshc_the_replication_white_list"><span class="mw-headline" id="The_replication_white_list"> The replication white list </span></a></i></b></font></h4>
<p>The cluster uses a whitelist to determine what changes to replicate. This whitelist is configured through the set of <code><font size="2">conf_replication_include</font></code> attributes in the default version of <code><font size="2">server.conf</font></code>, located in <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>. 
</p><p>You can add or remove items from that list by editing the members' <code><font size="2">server.conf</font></code> files under <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>.  If you change the whitelist, you must make the same changes on all cluster members.
</p><p>For a comprehensive list of items in the whitelist, consult the default version of <code><font size="2">server.conf</font></code>. This is the approximate set of whitelisted items:
</p>
<code><font size="2"><br>alert_actions <br>commands <br>datamodels <br>event_renderers <br>eventtypes <br>fields <br>history <br>html <br>literals <br>lookups <br>macros <br>manager <br>models <br>multikv <br>nav <br>panels <br>props <br>quickstart <br>savedsearches <br>searchbnf <br>searchscripts <br>segmenters <br>tags <br>times <br>transforms <br>transactiontypes <br>ui-prefs <br>user-prefs <br>views <br>viewstates <br>workflow_actions<br></font></code>
<p>The cluster replicates changes to all files underlying the whitelist items.  In addition to configuration files themselves, this includes dashboard and nav XML, lookup table files, data model JSON files, and so on. The cluster also replicates permissions stored in *.meta files.
</p><p>These are examples of the types of files replicated for various whitelist items: 
</p>
<code><font size="2"><br># escape-hatch HTML views<br>conf_replication_include.html = true<br># lookup table files<br>conf_replication_include.lookups = true<br># manager XML<br>conf_replication_include.manager = true<br># datamodel JSON files<br>conf_replication_include.models &nbsp;= true<br># nav XML<br>conf_replication_include.nav = true<br># view XML<br>conf_replication_include.views = true<br></font></code>
<h3> <a name="howconfrepoworksinshc_the_changes_that_the_cluster_ignores"><span class="mw-headline" id="The_changes_that_the_cluster_ignores"> The changes that the cluster ignores </span></a></h3>
<p>The cluster ignores configuration changes for any items that are not on the whitelist. Examples include index-time settings, such as those that define data inputs or indexes. 
</p><p>In addition, the cluster only replicates changes that are made through Splunk Web, the Splunk CLI, or the REST API. If you directly edit a configuration file, the cluster does not replicate it. Instead, you must use the deployer to distribute the file to all cluster members. 
</p><p>The cluster also does not replicate newly installed or upgraded apps to cluster members.
</p><p>For information on how to distribute such configuration changes through the deployer, see <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</p>
<h3> <a name="howconfrepoworksinshc_when_replication_happens"><span class="mw-headline" id="When_replication_happens"> When replication happens</span></a></h3>
<p>The purpose of replication is to keep search-related configurations in sync across all cluster members. To ensure this happens, replication occurs at various times, depending on the state of the member:
</p>
<ul><li> <b>Each active cluster member</b> contacts the captain every five seconds and pulls any changes that have arrived since the last time it pulled changes. <br><br><b>Note:</b> You can tune the interval at which members contact the captain by modifying the <code><font size="2">conf_replication_period</font></code> attribute in <code><font size="2">server.conf</font></code>.
</li></ul><ul><li> <b>When a new member joins the cluster</b>, it contacts the captain and downloads a tarball containing the current set of replicated configurations, including all changes that have been made over the life of the cluster. It applies the tarball locally.
</li></ul><ul><li> <b>When a member rejoins the cluster</b>. First, follow the procedure outlined in <a href="#addaclustermember_add_a_member_that_was_previously_removed_from_the_cluster" class="external text">"Add a member that was previously removed from the cluster"</a>, cleaning the instance before you re-add it to the cluster. The member then contacts the captain and downloads the tarball, the same way that a new member does.<br><br>For cases where the member went down unexpectedly, see <a href="#handlememberfailure" class="external text">"Handle failure of a search head cluster member"</a>.  In some cases, the member can download the set of intervening changes and apply them. In other cases, you might need to run the <code><font size="2">splunk resync shcluster-replicated-config</font></code> command to apply the tarball.
</li></ul><a name="propagateshcconfigurationchanges"></a><h2> <a name="propagateshcconfigurationchanges_use_the_deployer_to_distribute_apps_and_configuration_updates"><span class="mw-headline" id="Use_the_deployer_to_distribute_apps_and_configuration_updates"> Use the deployer to distribute apps and configuration updates </span></a></h2>
<p>The deployer is a Splunk Enterprise instance that you use to distribute apps and certain other configuration updates to search head cluster members. The set of updates that the deployer distributes is called the <b>configuration bundle</b>.  
</p><p>The deployer distributes the configuration bundle in response to your command. The deployer also distributes any updates whenever a member joins or rejoins the cluster.
</p><p><b>Caution:</b> You must use the deployer, not the deployment server, to distribute apps to cluster members. Use of the  deployer eliminates the possibility of conflict with the run-time updates that the cluster replicates automatically by means of the mechanism described in <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates."</a> 
</p>
<h3> <a name="propagateshcconfigurationchanges_what_configurations_does_the_deployer_manage.3f"><span class="mw-headline" id="What_configurations_does_the_deployer_manage.3F"> What configurations does the deployer manage? </span></a></h3>
<p>You use the deployer primarily to distribute non-runtime configuration changes. 
</p><p>You do not use the deployer to distribute runtime search-related configuration changes. Instead, the cluster automatically replicates such changes to all cluster members. For example, if a user creates a saved search on one member, the cluster replicates it automatically to all other members. See <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates."</a>  To distribute other updates, you need the deployer. 
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_the_types_of_updates_that_the_deployer_handles"><span class="mw-headline" id="The_types_of_updates_that_the_deployer_handles">The types of updates that the deployer handles </span></a></i></b></font></h4>
<p>These are the types of updates that require the deployer:
</p>
<ul><li> New or upgraded apps.
</li><li> Configuration files that you edit directly.
</li><li> All non-search-related updates, even those that can be configured through the CLI or Splunk Web, such as updates to <code><font size="2">indexes.conf</font></code> or <code><font size="2">inputs.conf</font></code>.
</li></ul><p><b>Note:</b> You use the deployer to deploy configuration updates only.  You cannot use it for initial configuration of the search head cluster or for version upgrades to the Splunk Enterprise instances that the members run on.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_app_upgrades_and_runtime_changes"><span class="mw-headline" id="App_upgrades_and_runtime_changes"> App upgrades and runtime changes </span></a></i></b></font></h4>
<p>Because of how configuration file precedence works, changes that users make to apps at runtime get maintained in the app through subsequents upgrades. 
</p><p>Say, for example, that you deploy the 1.0 version of some app,&Acirc;&nbsp;and then a user modifies the app's dashboards. When you later deploy the 1.1 version of the app, the user modifications will persist in the 1.1 version of the app.
</p><p>As explained in <a href="#howconfrepoworksinshc" class="external text">"Configuration updates that the cluster replicates,"</a> the cluster replicates any runtime changes to all members. Those runtime changes do not get subsequently uploaded to the deployer, but because of the way configuration layering works, those changes have precedence over the configurations in the unmodified apps distributed by the deployer. To understand this issue in detail, read the rest of this topic, as well as the topic "Configuration file precedence" in the <i>Admin Manual.</i>
</p>
<h3> <a name="propagateshcconfigurationchanges_when_does_the_deployer_distribute_configurations_to_the_members.3f"><span class="mw-headline" id="When_does_the_deployer_distribute_configurations_to_the_members.3F">When does the deployer distribute configurations to the members? </span></a></h3>
<p>The deployer distributes the configuration bundle to the cluster members under these circumstances:
</p>
<ul><li> When you invoke the <code><font size="2">splunk apply shcluster-bundle</font></code> command, the deployer pushes any new or changed configurations to the members. See <a href="#propagateshcconfigurationchanges_deploy_a_configuration_bundle" class="external text">"Deploy a configuration bundle."</a>
</li><li> When a member joins or rejoins the cluster, it checks the deployer for updates. A member also checks for updates whenever it restarts. If any updates are available, it pulls them from the deployer.
</li></ul><h3> <a name="propagateshcconfigurationchanges_configure_the_deployer"><span class="mw-headline" id="Configure_the_deployer">Configure the deployer </span></a></h3>
<p><b>Note:</b> The actions in this subsection are integrated into the procedure for deploying the search head cluster, described in the topic <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster."</a> If you set up the deployer during initial deployment of the search head cluster, you can skip this section.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_choose_an_instance_to_be_the_deployer"><span class="mw-headline" id="Choose_an_instance_to_be_the_deployer">Choose an instance to be the deployer</span></a></i></b></font></h4>
<p>Each search head cluster needs one deployer.  The deployer must run on a Splunk Enterprise instance outside the search head cluster.
</p><p>Depending on the specific components of your Splunk Enterprise environment, the deployer might be able to run on an existing Splunk Enterprise instance with other responsibilities, such as a deployment server or the master node of an indexer cluster.  Otherwise, you can run it on a dedicated instance. See <a href="#shcsystemrequirements_deployer_requirements" class="external text">"Deployer requirements"</a>.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_deploy_to_multiple_clusters"><span class="mw-headline" id="Deploy_to_multiple_clusters">Deploy to multiple clusters </span></a></i></b></font></h4>
<p>The deployer sends the same configuration bundle to all cluster members that it services. Therefore, if you have multiple search head clusters, you can use the same deployer for all the clusters only if the clusters employ exactly the same configurations, apps, and so on. 
</p><p>If you anticipate that your clusters might need different configurations over time,  set up a separate deployer for each cluster. 
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_set_a_secret_key_on_the_deployer"><span class="mw-headline" id="Set_a_secret_key_on_the_deployer">Set a secret key on the deployer </span></a></i></b></font></h4>
<p>If the search head cluster members are using a secret key, you must also set the same key on the deployer. The deployer uses this key to authenticate communication with the cluster members. To set the key, specify the <code><font size="2">pass4SymmKey</font></code> attribute in either the <code><font size="2">[general]</font></code> or the <code><font size="2">[shclustering]</font></code> stanza of the deployer's <code><font size="2">server.conf</font></code> file. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>pass4SymmKey = yoursecretkey<br></font></code>
</div>
<p>The key must be the same for all cluster members and the deployer. You can set the key on the cluster members during initialization.
</p><p>You must restart the instance for the key to take effect.
</p><p><b>Note:</b> If there is a mismatch between the value of <code><font size="2">pass4SymmKey</font></code> on the cluster members and on the deployer (for example, you set it on the members but neglect to set it on the deployer), you will get an error message when the deployer attempts to push the configuration bundle.  The message will resemble this:
</p>
<div class="samplecode">
<code><font size="2"><br>Error while deploying apps to first member: ConfDeploymentException: Error while fetching apps baseline on target=https://testitls1l:8089: Non-200/201 status_code=401; {"messages":[{"type":"WARN","text":"call not properly authenticated"}]}<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_point_the_cluster_members_to_the_deployer"><span class="mw-headline" id="Point_the_cluster_members_to_the_deployer">Point the cluster members to the deployer </span></a></i></b></font></h4>
<p>Each cluster member needs to know the location of the deployer. Splunk recommends that you specify the deployer location during member initialization. See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster."</a>
</p><p>If you do not set the deployer location at initialization time, you must add the location to each member's <code><font size="2">server.conf</font></code> file before using the deployer:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>conf_deploy_fetch_url = &lt;URL&gt;:&lt;management_port&gt; <br></font></code>
</div>
<p>The <code><font size="2">conf_deploy_fetch_url</font></code> attribute specifies the URL and management port for the deployer instance.
</p><p>If you later add a new member to the cluster, you must set <code><font size="2">conf_deploy_fetch_url</font></code> on the member before adding it to the cluster, so it can immediately contact the deployer for the current configuration bundle, if any.
</p>
<h3> <a name="propagateshcconfigurationchanges_what_the_configuration_bundle_contains"><span class="mw-headline" id="What_the_configuration_bundle_contains"> What the configuration bundle contains </span></a></h3>
<p>The configuration bundle is the set of files that the deployer distributes to the cluster members. It can contain apps or other groups of configuration files. You determine what it contains. You place the files for the configuration bundle in a designated location on the deployer. 
</p><p>The deployer pushes the configuration bundle to the members as a set of tarballs, one for each app.
</p><p><b>Caution:</b> If you attempt to push a very large tarball (&gt;200 MB), the operation might fail due to various timeouts. Delete some of the contents from the tarball's app, if possible, and try again.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_location_on_the_deployer"><span class="mw-headline" id="Location_on_the_deployer">Location on the deployer</span></a></i></b></font></h4>
<p>On the deployer, the configuration bundle resides under the <code><font size="2">$SPLUNK_HOME/etc/shcluster</font></code> directory. The set of files under that directory constitutes the configuration bundle. 
</p><p>The directory has this structure:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/shcluster/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;apps/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;users/<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> Put each app in its own subdirectory under <code><font size="2">/apps</font></code>. You must untar the app.
</li><li> To push files to users, put the files under the <code><font size="2">/users</font></code> subdirectories where you want them to reside on the members. 
</li><li> The deployer will push the content under <code><font size="2">/shcluster/users</font></code> only if the content includes at least one configuration file. For example, if you place a private lookup table or view under some user subdirectory, the deployer will push it only if there is also at least one configuration file somewhere under <code><font size="2">/shcluster/users</font></code>.
</li><li> All files placed under both default and local subdirectories get merged into default subdirectories on the members, post-deployment. This holds true for both app and user subdirectories. See <a href="#propagateshcconfigurationchanges_location_on_the_cluster_members" class="external text">"Location on the cluster members."</a> 
</li><li> The configuration bundle must contain all previously pushed apps, as well as any new ones.  If you delete an app from the bundle, the next time you push the bundle, the app will get deleted from the cluster members.
</li><li> To update an app on the cluster members, put the updated version in the configuration bundle.
</li><li> To delete an app that you previously pushed, remove it from the configuration bundle. When you next push the bundle, each member will delete it from its own file system.
</li><li> The deployer only pushes the contents of subdirectories under <code><font size="2">shcluster</font></code>. It does not push any standalone files directly under <code><font size="2">shcluster</font></code>. For example, it will not push the file <code><font size="2">/shcluster/file1</font></code>. To deploy standalone files, create a new apps directory under <code><font size="2">/apps</font></code> and put the files in the local subdirectory. For example, put <code><font size="2">file1</font></code> under <code><font size="2">$SPLUNK_HOME/etc/shcluster/apps/newapp/local</font></code>.
</li></ul><p>When the deployer pushes the bundle, it pushes the full contents of all apps that have changed since the last push. Even if the only change to an app is a single file, it pushes the entire app. If an app has not changed, the deployer does not push it again.
</p><p><b>Caution:</b>  If an app in the configuration bundle has the same name as a default app on the cluster members, it will overwrite that app. For example, if you create an app called "search" in the configuration bundle, it will overwrite the default search app that ships with Splunk Enterprise. It is highly unlikely that you want this to happen.
</p><p><b>Note:</b> The <code><font size="2">shcluster</font></code> location is only for files that you want to distribute to cluster members. The deployer does not use the files in that directory for its own configuration needs.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_location_on_the_cluster_members"><span class="mw-headline" id="Location_on_the_cluster_members"> Location on the cluster members </span></a></i></b></font></h4>
<p>On the cluster members, the deployed apps and files reside under <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and <code><font size="2">$SPLUNK_HOME/etc/users</font></code>.
</p><p><b>Important:</b> The deployer never deploys files to the members' local app directories, <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local</font></code>. Instead, it deploys both local and default settings in the configuration bundle to the members' default app directories, <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/default</font></code>. This ensures that deployed settings never overwrite local or replicated runtime settings on the members. Otherwise, for example, app upgrades would wipe out runtime changes.
</p><p>Similarly, the deployer deploys user files to members' default user directories, not to their local user directories. For example, if you place a user file such as <code><font size="2">$SPLUNK_HOME/etc/shcluster/users/admin/search/local/savedsearches.conf</font></code> on the deployer and then deploy it to the members, it resides in <code><font size="2">$SPLUNK_HOME/etc/users/admin/search/default/savedsearches.conf</font></code> on each member.
</p><p>During the staging process that occurs prior to pushing the configuration bundle, the deployer copies the configuration  bundle to a staging area, where it merges all settings from files in <code><font size="2">/shcluster/apps/&lt;appname&gt;/local</font></code> into corresponding files in <code><font size="2">/shcluster/apps/&lt;appname&gt;/default</font></code>. Settings from the local directory take precedence over any corresponding default settings. For example, if you have a <code><font size="2">/newapp/local/inputs.conf</font></code> file, the deployer takes the settings from that file and merges them with any settings in <code><font size="2">/newapp/default/inputs.conf</font></code>. If a particular attribute is defined in both places, the merged file retains the definition from the local directory. The deployer then pushes only the merged default file.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_what_exactly_does_the_deployer_send_to_the_members.3f"><span class="mw-headline" id="What_exactly_does_the_deployer_send_to_the_members.3F">What exactly does the deployer send to the members?</span></a></i></b></font></h4>
<p>The deployer pushes the configuration bundle to the members as a set of tarballs, one for each app, plus one for the entire <code><font size="2">$SPLUNK_HOME/etc/shcluster/users</font></code> directory.
</p><p>On the initial push to a set of new members, the deployer distributes the entire configuration bundle to each member. On subsequent pushes, it distributes only new apps and any apps that have changed since the last push. If even a single file has changed in an app, the deployer redistributes the entire app. It does not redistribute unchanged apps.
</p><p>For the purposes of determining what to push, the deployer treats the <code><font size="2">$SPLUNK_HOME/etc/shcluster/users</font></code> directory like a single app. So if you change a single file within a user directory on the deployer, the deployer will redeploy every user directory. This is because the <code><font size="2">users</font></code> directory is typically modified and redeployed only during upgrade or migration, unlike the <code><font size="2">apps</font></code> directory, which might see regular updates during the lifetime of the cluster.
</p>
<h3> <a name="propagateshcconfigurationchanges_deploy_a_configuration_bundle"><span class="mw-headline" id="Deploy_a_configuration_bundle">Deploy a configuration bundle </span></a></h3>
<p>To deploy a configuration bundle, you push the bundle from the deployer to the cluster members.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_push_the_configuration_bundle"><span class="mw-headline" id="Push_the_configuration_bundle"> Push the configuration bundle </span></a></i></b></font></h4>
<p>To push the configuration bundle to the cluster members:
</p><p><b>1.</b> Put the apps and other configuration changes in subdirectories under <code><font size="2">shcluster/</font></code> on the deployer. 
</p><p><b>2.</b> Untar any app.
</p><p><b>3.</b> Run the <code><font size="2">splunk apply shcluster-bundle</font></code> command on the deployer:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply shcluster-bundle -target &lt;URI&gt;:&lt;management_port&gt; -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> The <code><font size="2">-target</font></code> parameter specifies the URI and management port for any member of the cluster, for example, <code><font size="2">https://10.0.1.14:8089</font></code>. You specify only one cluster member but the deployer pushes to all members.  This parameter is required.
</li><li> The <code><font size="2">-auth</font></code> parameter specifies credentials for the deployer instance. 
</li></ul><p>In response to <code><font size="2">splunk apply shcluster-bundle</font></code>, the deployer displays this message:
</p>
<div class="samplecode">
<code><font size="2"><br>Warning: Depending on the configuration changes being pushed, this command<br>might initiate a rolling-restart of the cluster members. Please refer to the<br>documentation for the details. &nbsp;Do you wish to continue? [y/n]:<br></font></code>
</div>
<p>For information on which configuration changes trigger restart, see <code><font size="2">$SPLUNK_HOME/etc/system/default/app.conf</font></code>.  It lists the configuration files that do not trigger restart when changed. All other configuration changes trigger restart.
</p><p><b>4.</b> To proceed, respond to the message with <code><font size="2">y</font></code>. 
</p><p><b>Note:</b> You can eliminate the message by appending the flag <code><font size="2">--answer-yes</font></code> to the <code><font size="2">splunk apply shcluster-bundle</font></code> command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply shcluster-bundle --answer-yes -target &lt;URI&gt;:&lt;management_port&gt; -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>This is useful if you are including the command in a script or otherwise automating the process.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_how_the_cluster_applies_the_configuration_bundle"><span class="mw-headline" id="How_the_cluster_applies_the_configuration_bundle">How the cluster applies the configuration bundle </span></a></i></b></font></h4>
<p>The deployer and the cluster members execute the command as follows:
</p><p><b>1.</b> The deployer stages the configuration bundle in a separate location on its file system (<code><font size="2">$SPLUNK_HOME/var/run/splunk/deploy</font></code>) and then pushes it to each cluster member. The configuration bundle typically consists of several tarballs, one for each app. 
</p><p><b>2.</b> Each cluster member then applies the changes contained in the bundle locally.  If&Acirc;&nbsp;a rolling restart is determined necessary,  approximately 10% of the members then restart at a time, until all have restarted.
</p><p>During a rolling restart, all members, including the current captain, restart. Restart of the captain triggers the election process, which can result in a new captain.  After the final member restarts, it requires approximately 60 seconds for the cluster to stabilize. During this interval, error messages might appear. You can ignore these messages. They should desist after 60 seconds. For more information on the rolling restart process, see <a href="#restartshc" class="external text">"Restart the search head cluster."</a>
</p>
<h3> <a name="propagateshcconfigurationchanges_maintain_lookup_files_across_app_upgrades"><span class="mw-headline" id="Maintain_lookup_files_across_app_upgrades"> Maintain lookup files across app upgrades </span></a></h3>
<p>Any app that uses lookup tables typically ships with stubs for the table files.  Once the app is in use on the search head, the tables get populated as an effect of runtime processes, such as searches.  When you later upgrade the app, by default the populated lookup tables get overwritten by the stub files from the latest version of the app, causing you to lose the data in the tables.
</p><p>To avoid this problem, you can stipulate that the stub files in upgraded apps not overwrite any table files of the same name already on the cluster members. Run the <code><font size="2">splunk apply shcluster-bundle</font></code> command on the deployer, setting the <code><font size="2">-preserve-lookups</font></code> flag to "true":
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply shcluster-bundle -target &lt;URI&gt;:&lt;management_port&gt; -preserve-lookups true -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> The default for <code><font size="2">-preserve-lookups</font></code> is "false". In other words, by default, the populated lookup tables are overwritten on upgrade.
</li></ul><p><b>Note:</b> To ensure that a stub persists on members only if there is no existing table file of the same name already on the members, this feature can temporarily rename a table file with a <code><font size="2">.default</font></code> extension. (So, for example, <code><font size="2">lookup1.csv</font></code> becomes <code><font size="2">lookup1.csv.default</font></code>.) Therefore, if you have been manually renaming table files with a <code><font size="2">.default</font></code> extension, you might run into problems when using this feature. You should contact Support before proceeding.
</p>
<h3> <a name="propagateshcconfigurationchanges_consequence_and_remediation_of_deployer_failure"><span class="mw-headline" id="Consequence_and_remediation_of_deployer_failure"> Consequence and remediation of deployer failure </span></a></h3>
<p>The deployer distributes the configuration bundle to the cluster members under these circumstances:
</p>
<ul><li> When you invoke the <code><font size="2">splunk apply shcluster-bundle</font></code> command, the deployer pushes the configurations to the members. 
</li><li> When a member joins or rejoins the cluster, it checks the deployer for updates. A member also checks for updates whenever it restarts. If any updates are available, it pulls them from the deployer. 
</li></ul><p>This means that if the deployer is down:
</p>
<ul><li> You cannot push new configurations to the members. 
</li><li> A member that joins or rejoins the cluster, or restarts, cannot pull the latest configuration bundle.
</li></ul><p>The implications of the deployer being down depend, therefore, on the state of the cluster members. These are the main cases to consider:
</p>
<ul><li> The deployer is down but the set of cluster members remains stable.
</li><li> The deployer is down and a member attempts to join or rejoin the cluster.
</li></ul><h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_the_deployer_is_down_but_the_set_of_cluster_members_remains_stable"><span class="mw-headline" id="The_deployer_is_down_but_the_set_of_cluster_members_remains_stable">The deployer is down but the set of cluster members remains stable</span></a></i></b></font></h4>
<p>If no member joins or rejoins the cluster while the deployer is down, there are no important consequences to the functioning of the cluster. All member configurations remain in sync and the cluster continues to operate normally. The only consequence is the obvious one, that you cannot push new configurations to the members during this time.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_the_deployer_is_down_and_a_member_attempts_to_join_or_rejoin_the_cluster"><span class="mw-headline" id="The_deployer_is_down_and_a_member_attempts_to_join_or_rejoin_the_cluster"> The deployer is down and a member attempts to join or rejoin the cluster </span></a></i></b></font></h4>
<p>In the case of a member attempting to join or rejoin the cluster while the deployer is down, there is the possibility that the configuration on that member will be out-of-sync with the configuration on the other cluster members: 
</p>
<ul><li> A new member will not be able to pull the current configuration bundle. 
</li><li> A member that left the cluster before the deployer failed and rejoined the cluster after the deployer failed will not be able to pull any updates made to the bundle during the time that the member was down and the deployer was still running.
</li></ul><p>In these circumstances, the joining/rejoining member will have a different set of configurations from the other cluster members. Depending on the nature of the bundle changes, this can cause the joining member to behave differently from the other members.  It can even lead to failure of the entire cluster.  Therefore, you must make sure that this circumstance does not develop.
</p>
<h4><font size="3"><b><i> <a name="propagateshcconfigurationchanges_how_to_remedy_deployer_failure"><span class="mw-headline" id="How_to_remedy_deployer_failure">How to remedy deployer failure </span></a></i></b></font></h4>
<p>Remediation is two-fold:
</p><p><b>1.</b> Prevent any member from joining or rejoining the cluster during deployer failure, unless you can be certain that the set of configurations on the joining member is identical to that on the other members (for example, if the rejoining member went down subsequent to the deployer failure).
</p><p><b>2.</b> Bring up a new deployer:
</p><p><b>a.</b> Create a new deployer instance.
</p><p><b>b.</b> Restore the contents of <code><font size="2">$SPLUNK_HOME/etc/shcluster</font></code> to the new instance from backup.
</p><p><b>c.</b> If necessary, update the <code><font size="2">conf_deploy_fetch_url</font></code> values on all search head cluster members.
</p><p><b>d.</b> Push the restored bundle contents to all members by running the <code><font size="2">splunk apply shcluster-bundle</font></code> command.
</p>
<h1>Manage search head clustering</h1><a name="addaclustermember"></a><h2> <a name="addaclustermember_add_a_cluster_member"><span class="mw-headline" id="Add_a_cluster_member"> Add a cluster member</span></a></h2>
<p>There are several categories of members that you might need to add to a cluster:
</p>
<ul><li> <b>A new member.</b> In this case, you want to expand the cluster by adding a new member.
</li><li> <b>A member that was previously removed from the cluster.</b> In this case, you removed the member with the <code><font size="2">splunk remove</font></code> command and now want to add it back.
</li><li> <b>A member that left the cluster without being removed from it.</b> This can happen if, for example, the instance shut down unexpectedly.
</li></ul><p>This topic treats each of these categories separately through a set of high-level procedures, each of which references one or more detailed steps.
</p>
<h3> <a name="addaclustermember_add_a_new_member"><span class="mw-headline" id="Add_a_new_member"> Add a new member </span></a></h3>
<p>These procedures are for Splunk Enterprise instances that have not previously been part of the cluster.
</p><p><b>Important:</b> It is recommended that you always use newly installed instances.
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_add_a_newly_installed_instance"><span class="mw-headline" id="Add_a_newly_installed_instance">Add a newly installed instance</span></a></i></b></font></h4>
<p>To add a newly installed Splunk Enterprise instance, which has not previously functioned as a search head:
</p><p><b>1.</b> Initialize the instance. See <a href="#addaclustermember_initialize_the_instance" class="external text">"Initialize the instance."</a>
</p><p><b>2.</b> Add the instance to the cluster. See <a href="#addaclustermember_add_the_instance" class="external text">"Add the instance."</a>
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_add_an_existing_instance"><span class="mw-headline" id="Add_an_existing_instance">Add an existing instance</span></a></i></b></font></h4>
<p>To add an existing Splunk Enterprise instance:
</p><p><b>1.</b> If the instance was formerly a member of another search head cluster, remove and disable the member from that cluster before adding it to this cluster.  See <a href="#removeaclustermember" class="external text">"Remove a cluster member."</a>
</p><p><b>2.</b> Clean the instance to remove any existing configurations that could interfere with the cluster. See <a href="#addaclustermember_clean_the_instance" class="external text">"Clean the instance."</a>
</p><p><b>3.</b> Initialize the instance. See <a href="#addaclustermember_initialize_the_instance" class="external text">"Initialize the instance."</a>
</p><p><b>4.</b> Add the instance to the cluster. See <a href="#addaclustermember_add_the_instance" class="external text">"Add the instance."</a>
</p>
<h3> <a name="addaclustermember_add_a_member_that_was_previously_removed_from_the_cluster"><span class="mw-headline" id="Add_a_member_that_was_previously_removed_from_the_cluster"> Add a member that was previously removed from the cluster </span></a></h3>
<p>These procedures are for Splunk Enterprise instances that were previously members of this cluster but were removed from it with the <code><font size="2">splunk remove shcluster-member</font></code> command. See <a href="#removeaclustermember" class="external text">"Remove a cluster member."</a> 
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_add_a_removed_member"><span class="mw-headline" id="Add_a_removed_member">Add a removed member</span></a></i></b></font></h4>
<p>To add a removed member:
</p><p><b>1.</b> Clean the instance to remove any existing configurations that could interfere with the cluster. See <a href="#addaclustermember_clean_the_instance" class="external text">"Clean the instance."</a>
</p><p><b>2.</b> Add the instance to the cluster. <a href="#addaclustermember_add_the_instance" class="external text">"Add the instance."</a>
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_add_a_member_that_was_both_removed_and_disabled"><span class="mw-headline" id="Add_a_member_that_was_both_removed_and_disabled">Add a member that was both removed and disabled </span></a></i></b></font></h4>
<p>To add a member that was both removed and disabled:
</p><p><b>1.</b> Clean the instance to remove any existing configurations that could interfere with the cluster. See <a href="#addaclustermember_clean_the_instance" class="external text">"Clean the instance."</a>
</p><p><b>2.</b> Initialize the instance. See <a href="#addaclustermember_initialize_the_instance" class="external text">"Initialize the instance."</a>
</p><p><b>3.</b> Add the instance to the cluster. <a href="#addaclustermember_add_the_instance" class="external text">"Add the instance."</a>
</p>
<h3> <a name="addaclustermember_add_a_member_that_left_the_cluster_without_being_removed_from_it"><span class="mw-headline" id="Add_a_member_that_left_the_cluster_without_being_removed_from_it"> Add a member that left the cluster without being removed from it </span></a></h3>
<p>For members that left the cluster without being explicitly removed from it:
</p><p><b>1.</b> Start the instance with the <code><font size="2">splunk start</font></code> command.
</p><p><b>2.</b> Depending on how long the member has been down, you might need to run the <code><font size="2">splunk resync</font></code> command to download the current set of configurations. 
</p><p>See <a href="#handlememberfailure" class="external text">"Handle failure of a cluster member"</a> for information on the <code><font size="2">splunk resync</font></code> command, along with a discussion of other issues related to dealing with a failed member.
</p><p>A typical reason for a member falling into this category is a temporary failure of the cluster member.
</p>
<h3> <a name="addaclustermember_detailed_steps"><span class="mw-headline" id="Detailed_steps"> Detailed steps </span></a></h3>
<p>The high-level procedures for adding a cluster member reference the detailed steps in this section.   Depending on the particular situation you are handling, you might need to use only a subset of these steps.  See the specific high-level procedures in this topic to determine which of these steps your situation requires.
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_clean_the_instance"><span class="mw-headline" id="Clean_the_instance"> Clean the instance </span></a></i></b></font></h4>
<p><b>Note:</b> This step is not necessary if you are adding a new instance that uses only the default set of configurations. 
</p><p>If you are adding an existing instance to the cluster, you must first stop the instance and run the <code><font size="2">splunk clean all</font></code> command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br><br>splunk clean all<br><br>splunk start<br></font></code>
</div>
<p>The <code><font size="2">splunk clean all</font></code> command deletes configuration updates that could interfere with the goal of maintaining the necessary identical configurations and apps across all cluster members. It does not delete any existing settings under the <code><font size="2">[shclustering]</font></code> stanza in <code><font size="2">server.conf</font></code>.
</p><p><b>Caution:</b> This step deletes most previously configured settings on the instance. 
</p><p>For a discussion of configurations that must be shared by all members, see <a href="#howconfigurationworksinshc" class="external text">"How configuration changes propagate across the search head cluster."</a>
</p><p>For more information on the <code><font size="2">splunk clean</font></code> command, access the online CLI help:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk help clean<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="addaclustermember_initialize_the_instance"><span class="mw-headline" id="Initialize_the_instance"> Initialize the instance </span></a></i></b></font></h4>
<p>If the member is new to the cluster, you must initialize it before adding it to the cluster:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk init shcluster-config -auth &lt;username&gt;:&lt;password&gt; -mgmt_uri &lt;URI&gt;:&lt;management_port&gt; -replication_port &lt;replication_port&gt; -replication_factor &lt;n&gt; -conf_deploy_fetch_url &lt;URL&gt;:&lt;management_port&gt; -secret security_key -shcluster_label &lt;label&gt;<br><br>splunk restart <br></font></code>
</div>
<p><b>Note the following:</b>
</p>
<ul><li> See <a href="#shcdeploymentoverview" class="external text">"Deploy a search head cluster"</a> for details on the <code><font size="2">splunk init shcluster-config</font></code> command, including the meaning of the various parameters.
</li><li> The <code><font size="2">conf_deploy_fetch_url</font></code> parameter specifies the URL and management port for the deployer instance. You must set it when adding a new member to an existing cluster, so that the member can immediately contact the deployer for the latest configuration bundle, if any. See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</li></ul><p>This step is for new members only. Do not run it on members rejoining the cluster.
</p>
<h4><font size="3"><b><i> <a name="addaclustermember_add_the_instance"><span class="mw-headline" id="Add_the_instance"> Add the instance </span></a></i></b></font></h4>
<p>The final step is to add the instance to the cluster. You can run the <code><font size="2">splunk add shcluster-member</font></code> command either on the new member or from any current member of the cluster. The command requires different parameters depending on where you run it from.
</p><p><b>When running the splunk add command on the new member itself, use this version of the command:</b>
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add shcluster-member -current_member_uri &lt;URI&gt;:&lt;management_port&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">current_member_uri</font></code> is the management URI of any current member of the cluster that this node is joining.  This parameter allows the new node to communicate with the cluster.
</li></ul><p><b>When running the splunk add command from a current cluster member, use this version of the command:</b>
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add shcluster-member -new_member_uri &lt;URI&gt;:&lt;management_port&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">new_member_uri</font></code> is the management URI of the new member that you are adding to the cluster.  This parameter must be identical to the <code><font size="2">-mgmt_uri</font></code> value you specified when you initialized this member.
</li></ul><h3> <a name="addaclustermember_post-add_activity"><span class="mw-headline" id="Post-add_activity">Post-add activity</span></a></h3>
<p>After the member joins or rejoins the cluster, it applies all replicated and deployed configuration updates:
</p><p><b>1.</b> It contacts the deployer to get the configuration bundle. 
</p><p><b>2.</b> It contacts the captain and downloads the replicated configuration tarball. 
</p><p>See <a href="#howconfigurationworksinshc" class="external text">"How configuration changes propagate across the search head cluster."</a>
</p>
<a name="removeaclustermember"></a><h2> <a name="removeaclustermember_remove_a_cluster_member"><span class="mw-headline" id="Remove_a_cluster_member"> Remove a cluster member</span></a></h2>
<p>To remove a member from a cluster, run the <code><font size="2">splunk remove shcluster-member</font></code> command on any cluster member. 
</p><p><b>Important:</b> You must use the procedure documented here to remove a member from the cluster. Do not just stop the member.
</p><p>To rejoin the member to the cluster later, see <a href="#addaclustermember_add_a_member_that_was_previously_removed_from_the_cluster" class="external text">"Add a member that was previously removed from the cluster."</a>  The exact procedure depends on whether you merely removed the member from the cluster or both removed and disabled the member.
</p>
<h3> <a name="removeaclustermember_remove_the_member"><span class="mw-headline" id="Remove_the_member"> Remove the member </span></a></h3>
<p><b>Caution:</b>  Do not stop the member before removing it from the cluster.
</p><p><b>1.</b> Remove the member.
</p><p>If you run the splunk remove command on the member being removed, use this version:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk remove shcluster-member<br></font></code>
</div>
<p>If you run the splunk remove command from another member, use this version:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk remove shcluster-member -mgmt_uri &lt;URI&gt;:&lt;management_port&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">mgmt_uri</font></code> is the management URI of the member being removed from the cluster.
</li></ul><p><b>2.</b> Stop the member.
</p><p>After removing the member, wait about two minutes for configurations to be updated across the cluster, and then stop the instance:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br></font></code>
</div>
<p>By stopping the instance, you prevent error messages about the removed member from appearing on the captain. 
</p>
<h3> <a name="removeaclustermember_disable_the_member"><span class="mw-headline" id="Disable_the_member"> Disable the member </span></a></h3>
<p>If you intend to keep the instance alive for use in some other capacity, you must next disable the member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk disable shcluster-config<br></font></code>
</div>
<p>This command disables search head clustering functionality entirely.
</p><p><b>Important:</b> Disable the member only after removing the member with <code><font size="2">splunk remove shcluster-member</font></code>.
</p>
<a name="adhocclustermember"></a><h2> <a name="adhocclustermember_configure_a_cluster_member_to_run_ad_hoc_searches_only"><span class="mw-headline" id="Configure_a_cluster_member_to_run_ad_hoc_searches_only"> Configure a cluster member to run ad hoc searches only</span></a></h2>
<p>A search head in a cluster typically services both ad hoc search requests from users and scheduled searches assigned by the captain. You can limit a cluster member to ad hoc search requests only. If you designate a member as an ad hoc search head, the captain will not assign it any scheduled searches.
</p><p>You can designate an ad hoc search head in two ways:
</p>
<ul><li> You can specify that a specific member run only ad hoc searches at all times.
</li></ul><ul><li> You can specify that a member run only ad hoc searches while it is the captain.
</li></ul><p><b>Note:</b> Although you can specify that a member run only ad hoc searches, you cannot specify that it run only scheduled searches. Any cluster member can always run an ad hoc search. You can, of course, prevent user access to a search head through any number of means.
</p>
<h3> <a name="adhocclustermember_configure_a_member_to_run_ad_hoc_searches_only"><span class="mw-headline" id="Configure_a_member_to_run_ad_hoc_searches_only"> Configure a member to run ad hoc searches only </span></a></h3>
<p>Depending on your specific deployment, you might want to reserve certain search heads for ad hoc use only.  Ad hoc search heads will never run scheduled searches. To specify an ad hoc search head, set the <code><font size="2">adhoc_searchhead</font></code> attribute in the member's <code><font size="2">server.conf</font></code> file:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>adhoc_searchhead = true<br></font></code>
</div>
<p>You must restart the instance for the change to take effect.
</p>
<h3> <a name="adhocclustermember_configure_the_captain_to_run_ad_hoc_searches_only"><span class="mw-headline" id="Configure_the_captain_to_run_ad_hoc_searches_only"> Configure the captain to run ad hoc searches only </span></a></h3>
<p>You can designate the captain member as an ad hoc search head. This prevent members from running scheduled searches while they are serving as captain, so that the captain can dedicate its resources to controlling the activities of the cluster. When the captain role moves to another member, then the previous captain will resume running scheduled searches and the new captain will now run ad hoc searches only.
</p><p><b>Important:</b> You should make this change on all cluster members, so that the behavior stays the same no matter which member is functioning as captain.
</p><p>To designate the captain as an ad hoc search head, set the <code><font size="2">captain_is_adhoc_searchhead</font></code> attribute in <code><font size="2">server.conf</font></code> on each member:
</p>
<div class="samplecode">
<code><font size="2"><br>[shclustering]<br>captain_is_adhoc_searchhead = true<br></font></code>
</div>
<p>You must restart each member for the change to take effect. Unlike most configuration changes related to search head clustering, you can use the <code><font size="2">splunk rolling-restart</font></code> command to restart all members. See <a href="#restartshc" class="external text">"Restart the search head cluster"</a>.
</p><p>For an overview of search head clustering configuration, see <a href="#shcconfigurationoverview" class="external text">"Configure the search head cluster"</a>.
</p>
<a name="viewshcstatus"></a><h2> <a name="viewshcstatus_view_the_status_of_a_search_head_cluster"><span class="mw-headline" id="View_the_status_of_a_search_head_cluster"> View the status of a search head cluster</span></a></h2>
<p>A number of CLI commands provide status information on the search head cluster.
</p>
<h3> <a name="viewshcstatus_show_cluster_status"><span class="mw-headline" id="Show_cluster_status">Show cluster status</span></a></h3>
<p>To check the overall status of your search head cluster, run this command from any member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk show shcluster-status -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>The command returns basic information on the captain and the cluster members.  It indicates the status of each member: up, down, detention, restarting.
</p><p>Note the following:
</p>
<ul><li> <b>Detention.</b> A cluster member enters detention when it runs out of disk space. While in detention, the captain will not assign scheduled searches or artifact copies to it. To remediate, you must increase the disk space available to the instance.
</li></ul><ul><li> <b>Down.</b> When a member leaves the cluster (typically, because of some failure), it enters the down state. The captain notes this and reports the member's status as "down" when you run this command. However, if captaincy then transfers, the new captain does not have any record of the member leaving the cluster. When you run this command on the new captain, it does not return any information about the downed member. To reliably view all members, including those that are currently down, instead run <code><font size="2">splunk list shcluster-members</font></code>.  See <a href="#viewshcstatus_list_cluster_members" class="external text">"List cluster members."</a>
</li></ul><h3> <a name="viewshcstatus_show_member_configuration"><span class="mw-headline" id="Show_member_configuration"> Show member configuration </span></a></h3>
<p>To check the configuration of a cluster member, run this command on the member itself:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-config -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Alternatively, you can run this variant on another member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-config -uri &lt;URI&gt;:&lt;management_port&gt; -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> The <code><font size="2">-uri</font></code> parameter specifies the URI and management port for the member whose configuration you want to check.
</li></ul><h3> <a name="viewshcstatus_list_cluster_members"><span class="mw-headline" id="List_cluster_members"> List cluster members </span></a></h3>
<p>To get a list of all cluster members, run this command from any member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-members -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>This command returns all members of the cluster, along with their configurations.
</p>
<h3> <a name="viewshcstatus_list_member_information"><span class="mw-headline" id="List_member_information">List member information </span></a></h3>
<p>To list information about a member, run this command on the member itself:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-member-info -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Alternatively, you can run this variant on another member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-member-info -uri &lt;URI&gt;:&lt;management_port&gt; -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> The <code><font size="2">-uri</font></code> parameter specifies the URI and management port for the member whose configuration you want to know.
</li></ul><h3> <a name="viewshcstatus_list_search_artifacts"><span class="mw-headline" id="List_search_artifacts"> List search artifacts</span></a></h3>
<p>To list the set of artifacts stored on the cluster, run this command on the captain:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-artifacts<br></font></code>
</div>
<p>To list the set of artifacts stored on a particular member, run this command on the member itself:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-member-artifacts<br></font></code>
</div>
<h3> <a name="viewshcstatus_list_scheduler_jobs"><span class="mw-headline" id="List_scheduler_jobs">List scheduler jobs </span></a></h3>
<p>To list the set of scheduler jobs, run this command on the captain:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list shcluster-scheduler-jobs -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>

<a name="handlememberfailure"></a><h2> <a name="handlememberfailure_handle_failure_of_a_search_head_cluster_member"><span class="mw-headline" id="Handle_failure_of_a_search_head_cluster_member"> Handle failure of a search head cluster member</span></a></h2>
<p>When a member fails, the cluster can usually absorb the failure and continue to function normally.
</p><p>When a failed member restarts and rejoins the cluster, the cluster can frequently complete the process automatically. In some cases, however, your intervention is necessary.
</p>
<h3> <a name="handlememberfailure_when_a_member_fails"><span class="mw-headline" id="When_a_member_fails">When a member fails </span></a></h3>
<p>If a search head cluster member fails for any reason and leaves the cluster unexpectedly, the cluster can usually continue to function without interruption:
</p>
<ul><li> The cluster's high availability features ensure that the cluster can continue to function as long as a majority (at least 51%) of the members are still running. For example, if you have a cluster configured with seven members, the cluster will function as long as four or more members remain up. If a majority of members fail, the cluster cannot successfully elect a new captain, which results in failure of the entire cluster. See <a href="#shcarchitecture_search_head_cluster_captain" class="external text">"Search head cluster captain."</a>
</li></ul><ul><li> All search artifacts resident on the failed member remain available through other search heads, as long as the number of machines that fail is less than the replication factor. If the number of failed members equals or exceeds the replication factor, it is likely that some search artifacts will no longer be available to the remaining members.
</li></ul><ul><li> If the failed member was serving as captain, the remaining nodes elect another member as captain.  Since members share configurations, the new captain is immediately fully functional. 
</li></ul><ul><li> If you are employing a load balancer in front of the search heads, the load balancer should automatically reroute users on the failed member to an available search head.
</li></ul><h3> <a name="handlememberfailure_when_the_member_rejoins_the_cluster"><span class="mw-headline" id="When_the_member_rejoins_the_cluster">When the member rejoins the cluster</span></a></h3>
<p>A failed member automatically rejoins the cluster, if its instance successfully restarts. When this occurs, its configurations require immediate updating so that they match those of the other cluster members. The member needs updates for two sets of configurations:
</p>
<ul><li> The replicated changes, which it gets from the captain. See <a href="#handlememberfailure_updating_the_replicated_changes" class="external text">"Updating the replicated changes."</a>
</li></ul><ul><li> The deployed changes, which it gets from the deployer. See <a href="#handlememberfailure_updating_the_deployed_changes" class="external text">"Updating the deployed changes."</a>
</li></ul><p>See <a href="#howconfigurationworksinshc" class="external text">"How configuration changes propagate across the search head cluster"</a> for information on how configurations are shared among cluster members.
</p>
<h3> <a name="handlememberfailure_updating_the_replicated_changes"><span class="mw-headline" id="Updating_the_replicated_changes"> Updating the replicated changes </span></a></h3>
<p>When the member rejoins the cluster, it contacts the captain to request the set of intervening replicated changes. What happens next depends on whether the member and the captain still share a common commit in their change histories:
</p>
<ul><li> If the captain and the member still share a common commit, the member automatically downloads the intervening changes from the captain and applies them to its pre-offline configuration. The member also pushes its intervening changes, if any, to the captain, which replicates them to the other members.
</li></ul><ul><li> If the captain and member do not share a common commit, they cannot properly sync without your intervention. To update the member's configuration, you must instruct the member to download the entire configuration tarball from the captain, as described in <a href="#handlememberfailure_how_the_update_proceeds" class="external text">"How the update proceeds."</a> The tarball overwrites the member's existing set of configurations, causing it to lose any local changes.
</li></ul><p>Changes are purged from the change history over time, based on configurable purge limits.
</p>
<h4><font size="3"><b><i> <a name="handlememberfailure_purge_limits"><span class="mw-headline" id="Purge_limits">Purge limits</span></a></i></b></font></h4>
<p>The purging of the configuration change history is determined by these attributes in <code><font size="2">server.conf</font></code>:
</p>
<ul><li> <code><font size="2">conf_replication_purge.eligibile_count</font></code>. Its default is 20,000 changes.
</li><li> <code><font size="2">conf_replication_purge.eligibile_age</font></code>. Its default is one day.
</li></ul><p>When both limits have been exceeded on a member, the member begins to purge the change history, starting with the oldest changes. 
</p><p>For more information on purge limit attributes, see the server.conf specification file.
</p>
<h4><font size="3"><b><i> <a name="handlememberfailure_how_the_update_proceeds"><span class="mw-headline" id="How_the_update_proceeds">How the update proceeds</span></a></i></b></font></h4>
<p>Upon rejoining the cluster, the member attempts to apply the set of intervening replicated changes from the captain. If the set exceeds the purge limits and the member and captain no longer share a common commit, a banner message appears on the member's UI, with text similar to the following:
</p>
<div class="samplecode">
<code><font size="2"><br>Error pulling configurations from the search head cluster captain; consider performing a destructive configuration resync on this search head cluster member.<br></font></code>
</div>
<p>If this message appears, it means that the member is unable to update its configuration through the configuration change delta and must apply the entire configuration tarball. It does not do this automatically.  Instead, it waits for your intervention.  
</p><p>You must then initiate the process of downloading and applying the tarball by running this CLI command on the member:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk resync shcluster-replicated-config<br></font></code>
</div>
<p>You do not need to restart the member after running this command.
</p><p><b>Caution:</b> This command causes an overwrite of the member's entire set of search-related configurations, resulting in the loss of any local changes.
</p>
<h3> <a name="handlememberfailure_updating_the_deployed_changes"><span class="mw-headline" id="Updating_the_deployed_changes"> Updating the deployed changes </span></a></h3>
<p>When the member rejoins the cluster, it automatically contacts the deployer for the latest configuration bundle.  The member then applies any changes or additions that have been made since it last downloaded the bundle.
</p><p>See <a href="#propagateshcconfigurationchanges" class="external text">"Use the deployer to distribute apps and configuration updates."</a>
</p>
<a name="restartshc"></a><h2> <a name="restartshc_restart_the_search_head_cluster"><span class="mw-headline" id="Restart_the_search_head_cluster"> Restart the search head cluster</span></a></h2>
<p>You can restart the entire cluster with the <code><font size="2">splunk rolling-restart</font></code> command.  The command performs a phased restart of all cluster members, so that the cluster as a whole can continue to perform its functions during the restart process. 
</p><p>The deployer also automatically initiates a rolling restart, when necessary, after distributing a configuration bundle to the members. For details on this process, see <a href="#propagateshcconfigurationchanges_push_the_configuration_bundle" class="external text">"Push the configuration bundle"</a>.  
</p><p><b>Caution:</b> In most cases, when changing configuration settings in the <code><font size="2">[shcluster]</font></code> stanza of <code><font size="2">server.conf</font></code>, you must restart all members at approximately the same time, in order to maintain identical settings across all members. For this reason, do not use the <code><font size="2">splunk rolling-restart</font></code> command to restart the members after such configuration changes, except when configuring the <code><font size="2">captain_is_adhoc_searchhead</font></code> attribute.  Instead, run the <code><font size="2">splunk restart</font></code> command on each member. See <a href="#shcconfigurationoverview" class="external text">"Configure the search head cluster"</a>.
</p>
<h3> <a name="restartshc_initiate_a_rolling_restart"><span class="mw-headline" id="Initiate_a_rolling_restart"> Initiate a rolling restart</span></a></h3>
<p>Invoke the <code><font size="2">splunk rolling-restart</font></code> command from the captain:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rolling-restart shcluster-members<br></font></code>
</div>
<h3> <a name="restartshc_how_rolling_restart_works"><span class="mw-headline" id="How_rolling_restart_works">How rolling restart works</span></a></h3>
<p>The rolling restart works like this: The captain issues a restart message to approximately 10%, by default, of the members at a time. Once those members restart and contact the captain, the captain then issues a restart message to another 10% of the members, and so on, until all the members, including the captain, have restarted. 
</p><p><b>Note:</b> If there are fewer than 10 members in the cluster, the captain issues the restart to one member at a time.  
</p><p>The captain is the final member to restart. Restart of the captain triggers the election process, which can result in a new captain.
</p><p>After all members have restarted, it requires approximately 60 seconds for the cluster to stabilize. During this interval, error messages might appear. You can ignore these messages. They should desist within 60 seconds.
</p><p><b>Note:</b> During a rolling restart, there is no guarantee that all knowledge objects will be available to all members.
</p>
<h3> <a name="restartshc_configure_the_number_of_members_that_restart_simultaneously"><span class="mw-headline" id="Configure_the_number_of_members_that_restart_simultaneously">Configure the number of members that restart simultaneously</span></a></h3>
<p>By default, the captain issues the restart command to 10% of the members at a time. However, the percentage is configurable through the <code><font size="2">percent_peers_to_restart</font></code> attribute in the <code><font size="2">[shcluster]</font></code> stanza of <code><font size="2">server.conf</font></code>. For convenience, you can configure this attribute with the CLI <code><font size="2">splunk edit shcluster-config</font></code> command. For example, to change the restart behavior so that the captain restarts 20% of the peers at a time, use this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit shcluster-config -percent_peers_to_restart 20<br></font></code>
</div>
<p><b>Caution:</b> Do not set the value to greater than 20%.  Otherwise, issues can arise during the captain election process.
</p><p>After changing the <code><font size="2">percent_peers_to_restart</font></code> attribute, you still need to run the <code><font size="2">splunk rolling-restart</font></code> command to initiate the actual restart.
</p>
<h3> <a name="restartshc_monitor_the_restart_process"><span class="mw-headline" id="Monitor_the_restart_process">Monitor the restart process</span></a></h3>
<p>To check the progress of the rolling restart, run this command from any of the members:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk show shcluster-status -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
</div>
<p>This command returns, among other values, a <code><font size="2">rolling_restart_flag</font></code> that indicates whether a rolling-restart is in progress (1) or not (0).
</p><p><b>Note:</b> This command is not operative during the final step of the restart process, when the captain itself, which tracks this information, is restarting.  During that time, you might see an error message that begins, "In handler 'shclusterstatus': Node is not captain...."  Wait approximately 60 seconds for the restart process to complete and retry the command.
</p>
<h1>Troubleshoot search head clustering</h1><a name="shcdeploymentissues"></a><h2> <a name="shcdeploymentissues_deployment_issues"><span class="mw-headline" id="Deployment_issues"> Deployment issues</span></a></h2>
<h3> <a name="shcdeploymentissues_crash_when_adding_new_member"><span class="mw-headline" id="Crash_when_adding_new_member"> Crash when adding new member </span></a></h3>
<p>If a member crashes when you add it to a cluster, determine whether the instance was previously a member of another cluster. If that is the case, you probably did not properly remove it from its previous cluster.  
</p><p>It is recommended that you always use new instances when adding members to a cluster, but if you choose to re-use an instance, you must follow the instructions in <a href="#addaclustermember_add_a_new_member" class="external text">"Add a new member."</a>
</p>
<a name="runtimeissues"></a><h2> <a name="runtimeissues_runtime_considerations"><span class="mw-headline" id="Runtime_considerations"> Runtime considerations</span></a></h2>
<h3> <a name="runtimeissues_delays_due_to_coordination_between_cluster_members"><span class="mw-headline" id="Delays_due_to_coordination_between_cluster_members">Delays due to coordination between cluster members</span></a></h3>
<p>Coordination between the captain and other cluster members sometimes creates latency of up to 1.5 minutes. For example, when you save a search job, Splunk Web might not update the job's state for a short period of time. Similarly, it can take a minute or more for the captain to orchestrate the complete deletion of jobs.
</p><p>In addition, when an event triggers the election of a new captain, there will be an interval of one to two minutes while the election completes. During this time, search heads can service only ad hoc job requests.
</p>
<h3> <a name="runtimeissues_limit_to_number_of_active_alerts"><span class="mw-headline" id="Limit_to_number_of_active_alerts">Limit to number of active alerts</span></a></h3>
<p>The search head cluster can handle approximately 5000 active, unexpired alerts. To stay within this boundary, use alert throttling or limit alert retention time. See the <i>Alerting Manual.</i> 
</p>
<h3> <a name="runtimeissues_site_failure_can_prevent_captain_election"><span class="mw-headline" id="Site_failure_can_prevent_captain_election">Site failure can prevent captain election</span></a></h3>
<p>If the cluster is deployed across two sites and the site with a majority of members goes down or is otherwise inaccessible, the cluster cannot elect a new captain. 
</p><p>Therefore, in the case of a two-site cluster, it is vital that you put the majority of your members on the site that you consider primary.
</p>
<h4><font size="3"><b><i> <a name="runtimeissues_why_the_majority_of_members_must_be_on_the_primary_site"><span class="mw-headline" id="Why_the_majority_of_members_must_be_on_the_primary_site">Why the majority of members must be on the primary site</span></a></i></b></font></h4>
<p>If you are deploying the cluster across two sites, put a majority of the cluster members on the site that you consider primary. This ensures that the cluster can continue to function as long as that site is running.
</p><p>Under certain circumstances, such as when a member leaves or joins the cluster, the cluster holds an election in which it chooses a new captain. The success of this election process requires that a majority of all cluster members agree on the new captain. Therefore, the proper functioning of the cluster requires that a majority of members be running at all times. See <a href="#shcarchitecture_captain_election" class="external text">"Captain election."</a>
</p><p>In the case of a cluster running across two sites, if one site fails, the remaining site can elect a new captain only if it holds a majority of members. Similarly, if there is a network disruption between the sites, only the site with a majority can elect a new captain. By assigning the majority of members to your primary site, you maximize its availability.
</p>
<h4><font size="3"><b><i> <a name="runtimeissues_what_happens_when_the_site_with_the_majority_fails"><span class="mw-headline" id="What_happens_when_the_site_with_the_majority_fails">What happens when the site with the majority fails </span></a></i></b></font></h4>
<p>If the site with a majority of members fails or is otherwise inaccessible, the remaining members on the minority site cannot elect a new captain. Captain election requires the vote of a majority of members, but only a minority of members are running. The cluster does not function.
</p><p>To remediate, you must either resurrect the downed majority site or bring down the remaining cluster members and reconfigure the cluster.
</p>
<h4><font size="3"><b><i> <a name="runtimeissues_consequences_of_a_non-functioning_cluster"><span class="mw-headline" id="Consequences_of_a_non-functioning_cluster">Consequences of a non-functioning cluster</span></a></i></b></font></h4>
<p>If the cluster cannot elect a captain, its members will continue to function as independent search heads.  However, they will only be able to service ad hoc searches. Scheduled searches and alerts will not run, because, in a cluster, the scheduling function is relegated to the captain. In addition, configurations and search artifacts will not be replicated during this time.
</p>
<h4><font size="3"><b><i> <a name="runtimeissues_what_happens_when_there_is_a_network_interruption_between_sites"><span class="mw-headline" id="What_happens_when_there_is_a_network_interruption_between_sites">What happens when there is a network interruption between sites </span></a></i></b></font></h4>
<p>If the network between sites fails, the members on each site will attempt to elect a captain.  However, only a site that holds a majority of the total members will succeed. That site can continue to function as the cluster indefinitely. Once the other sites reconnect, their members will rejoin the cluster.
</p>
<h4><font size="3"><b><i> <a name="runtimeissues_clusters_with_more_than_two_sites"><span class="mw-headline" id="Clusters_with_more_than_two_sites">Clusters with more than two sites </span></a></i></b></font></h4>
<p>If there are more than two sites, the cluster can function only if a majority of members across the sites are still able to communicate and elect a captain. For example, if you have site1 with five members, site2 with eight members, and site3 with four members, the cluster can survive the loss of any one site, because you will still have a majority of members (at least nine) among the remaining two sites.  However, if you have site1 with six members, site2 with two members, and site3 with three members, the cluster can only function as long as site1 remains alive, because you need at least six members to constitute a majority.
</p>
<h1>Search head pooling</h1><a name="configuresearchheadpooling"></a><h2> <a name="configuresearchheadpooling_overview_of_search_head_pooling"><span class="mw-headline" id="Overview_of_search_head_pooling"> Overview of search head pooling</span></a></h2>
<table border="1" cellpadding="5" cellspacing="0"><tr bgcolor="#D9EAED"><th valign="top" bgcolor="#C0C0C0"> This feature has been deprecated.
</th></tr><tr><td valign="center" align="left"> This feature has been deprecated as of Splunk Enterprise version 6.2. This means that although it continues to function, it might be removed in a future version.
<p>As an alternative, you can deploy search head clustering. See <a href="#aboutshc" class="external text">"About search head clustering"</a>.
</p><p>For a list of all deprecated features, see the topic "Deprecated features" in the Release Notes.
</p>
</td></tr></table><table cellpadding="10" cellspacing="0" border="1" width="100%"><tr><td valign="center" align="left"><b>Important:</b> Search head pooling is an advanced feature.  It's recommended that you contact the Splunk sales team to discuss your deployment before attempting to implement it.
</td></tr></table><p>You can set up multiple search heads so that they share configuration and user data. This is known as <b>search head pooling</b>. The main reason for having multiple search heads is to facilitate horizontal scaling when you have large numbers of users searching across the same data. Search head pooling can also reduce the impact if a search head becomes unavailable. This diagram provides an overview of a typical deployment with search head pooling:
</p><p><img alt="Search head pooling 1 60.png" src="images/2/26/Search_head_pooling_1_60.png" width="700" height="552"></p><p>You enable search head pooling on each search head that you want to be included in the pool, so that they can share configuration and user data. Once search head pooling has been enabled, these categories of objects will be available as common resources across all search heads in the pool:
</p>
<ul><li> <b>configuration data</b> -- <b>configuration files</b> containing settings for  <b>saved searches</b> and other  <b>knowledge objects</b>.
</li><li> <b>search artifacts</b>, records of specific search runs.
</li><li> <b>scheduler state</b>, so that only one search head in the pool runs a particular <b>scheduled report</b>.
</li></ul><p>For example, if you create and save a search on one search head, all the other search heads in the pool will automatically have access to it. 
</p><p>Search head pooling makes all files in <code><font size="2">$SPLUNK_HOME/etc/{apps,users}</font></code> available for sharing. This includes *.conf files, *.meta files, view files, search scripts, lookup tables, etc.
</p>
<h3> <a name="configuresearchheadpooling_key_implementation_issues"><span class="mw-headline" id="Key_implementation_issues">Key implementation issues</span></a></h3>
<p>Note the following:
</p>
<ul><li> Most shared storage solutions don't perform well across a WAN. Since search head pooling requires low-latency shared storage capable of serving a high number of operations per second, implementing search head pooling across a WAN is not supported.
</li></ul><ul><li> All search heads in a pool must be running the same version of Splunk Enterprise. Be sure to upgrade all of them at once. See "Upgrade your distributed deployment" in the Distributed Deployment Manual for details.
</li></ul><ul><li> The purpose of search head pooling is to simplify the management of groups of <i>dedicated</i> search heads. Do not implement it on groups of indexers doubling as search heads. That is an unsupported configuration. <i>Search head pooling has a significant effect on indexing performance</i>.
</li></ul><ul><li> The search heads in a pool cannot be search peers of each other.
</li></ul><h3> <a name="configuresearchheadpooling_search_head_pooling_and_knowledge_bundles"><span class="mw-headline" id="Search_head_pooling_and_knowledge_bundles">Search head pooling and knowledge bundles</span></a></h3>
<p>The set of data that a search head distributes to its search peers is known as the <b>knowledge bundle.</b> For details, see <a href="#whatsearchheadssend" class="external text">"What search heads send to search peers"</a>.
</p><p>By default, only one search head in a search head pool sends the knowledge bundle to the set of search peers. Also, if search heads in a pool are also search peers of each other, they will not send bundles to each other, since they can access the bundles in the pool.This is an optimization introduced in version 4.3.2 but made the default in version 5.0. It is controllable by means of the <code><font size="2">useSHPBundleReplication</font></code> attribute in distsearch.conf.
</p><p>As a further optimization, you can mount knowledge bundles on shared storage, as described in <a href="#mounttheknowledgebundle" class="external text">"About mounted bundles"</a>. By doing so, you eliminate the need to distribute the bundle to the search peers. For information on how to combine search head pooling with mounted knowledge bundles, read the section in that topic called <a href="#mountedbundlesandsearchheadpooling" class="external text">"Use mounted bundles with search head pooling"</a>.
</p>
<h3> <a name="configuresearchheadpooling_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>See the other topics in this chapter for more information on search head pooling:
</p>
<ul><li> <a href="#createasearchheadpool" class="external text">"Create a search head pool"</a>
</li><li> <a href="#usealoadbalancerwiththesearchheadpool" class="external text">"Use a load balancer with the search head pool"</a>
</li><li> <a href="#otherpoolingoperations" class="external text">"Other pooling operations"</a>
</li><li> <a href="#manageconfigurationchanges" class="external text">"Manage configuration changes"</a>
</li><li> <a href="#deploymentserverandsearchheadpooling" class="external text">"Deployment server and search head pooling"</a>
</li><li> <a href="#selecttimingforconfigurationrefresh" class="external text">"Select timing for configuration refresh"</a>
</li></ul><h3> <a name="configuresearchheadpooling_answers"><span class="mw-headline" id="Answers">Answers</span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has about search head pooling.
</p>
<a name="createasearchheadpool"></a><h2> <a name="createasearchheadpool_create_a_search_head_pool"><span class="mw-headline" id="Create_a_search_head_pool"> Create a search head pool</span></a></h2>
<p>To create a pool of search heads, follow these steps:
</p><p><b>1.</b> Set up a shared storage location accessible to each search head.
</p><p><b>2.</b> Configure each individual search head.
</p><p><b>3.</b> Stop the search heads.
</p><p><b>4.</b> Enable pooling on each search head.
</p><p><b>5.</b> Copy user and app directories to the shared storage location.
</p><p><b>6.</b> Restart the search heads.
</p><p>The steps are described below in detail:
</p>
<h3> <a name="createasearchheadpool_1._set_up_a_shared_storage_location_accessible_to_each_search_head"><span class="mw-headline" id="1._Set_up_a_shared_storage_location_accessible_to_each_search_head"> 1. Set up a shared storage location accessible to each search head </span></a></h3>
<p>So that each search head in a pool can share configurations and artifacts, they need to access a common set of files via shared storage:
</p>
<ul><li> <b>On *nix platforms,</b> set up an NFS mount.
</li></ul><ul><li> <b>On Windows,</b> set up a CIFS (SMB) share.
</li></ul><p><b>Important:</b> The Splunk user account needs read/write access to the shared storage location. <i>When installing a search head on Windows, be sure to install it as a user with read/write access to shared storage. The Local System user does not have this access.</i> For more information, see "Choose the user Splunk should run as" in the Installation manual.
</p>
<h3> <a name="createasearchheadpool_2._configure_each_search_head"><span class="mw-headline" id="2._Configure_each_search_head"> 2. Configure each search head </span></a></h3>
<p><b>a.</b> Set up each search head individually, specifying the search peers in the usual fashion. See <a href="#configuredistributedsearch" class="external text">"Add search peers to the search head"</a>. 
</p><p><b>b.</b> Make sure that each search head has a unique <code><font size="2">serverName</font></code> attribute, configured in server.conf. See <a href="#managedistributedservernames" class="external text">"Manage distributed server names"</a> for detailed information on this requirement. If the search head does not have a unique <code><font size="2">serverName</font></code>, a warning will be generated at start-up. See <a href="#searchheadpoolingconfigurationissues_warning_about_unique_servername_attribute" class="external text">"Warning about unique serverName attribute"</a> for details.
</p><p><b>c.</b> Specify the necessary authentication. You have two choices:
</p>
<ul><li> Specify user authentication on each search head separately. A valid user on one search head is not automatically a user on another search head in the pool. You can use LDAP to centrally manage user authentication, as described in "Set up user authentication with LDAP".
</li></ul><ul><li> Place a common authentication configuration on shared storage, to be used by all pool members.  You must restart the pool members after any change to the authentication.
</li></ul><p><b>Note:</b> Any authentication change made on an individual pool member (for example, via Splunk Web) overrides <i>for that pool member only</i> any configuration on shared storage. You should, therefore, generally avoid making authentication changes through Splunk Web if a common configuration already exists on shared storage.
</p>
<h3> <a name="createasearchheadpool_3._stop_the_search_heads"><span class="mw-headline" id="3._Stop_the_search_heads"> 3. Stop the search heads </span></a></h3>
<p>Before enabling pooling, you must stop <code><font size="2">splunkd</font></code>. Do this for each search head in the pool.
</p>
<h3> <a name="createasearchheadpool_4._enable_pooling_on_each_search_head"><span class="mw-headline" id="4._Enable_pooling_on_each_search_head"> 4. Enable pooling on each search head </span></a></h3>
<p>Use the CLI command <code><font size="2">splunk pooling enable</font></code> to enable pooling on a search head. The command sets certain values in <code><font size="2">server.conf</font></code>. It also creates subdirectories within the shared storage location and validates that Splunk Enterprise can create and move files within them.
</p><p>Here's the command syntax:
</p>
<code><font size="2"> &nbsp;<code><font size="2">splunk pooling enable &lt;path_to_shared_storage&gt; [--debug]</font></code><br></font></code>
<p>Note:
</p>
<ul><li> On NFS, <code><font size="2">&lt;path_to_shared_storage&gt;</font></code> should be the NFS's share mountpoint.
</li><li> On Windows, <code><font size="2">&lt;path_to_shared_storage&gt;</font></code> should be the UNC path of the CIFS/SMB share.
</li><li> The <code><font size="2">--debug</font></code> parameter causes the command to log additional information to <code><font size="2">btool.log</font></code>.
</li></ul><p>Execute this command on each search head in the pool. 
</p><p>The command sets values in the <code><font size="2">[pooling]</font></code> stanza of the  <code><font size="2">server.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>.  
</p><p>You can also directly edit the <code><font size="2">[pooling]</font></code> stanza of <code><font size="2">server.conf</font></code>. For detailed information on <code><font size="2">server.conf</font></code>, look here. 
</p><p><b>Important:</b> The <code><font size="2">[pooling]</font></code> stanza must be placed in the <code><font size="2">server.conf</font></code> file directly under <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>. This means that you cannot deploy the <code><font size="2">[pooling]</font></code> stanza via an app, either on local disk or on shared storage.  For details see the server.conf spec file.
</p>
<h3> <a name="createasearchheadpool_5._copy_user_and_app_directories_to_the_shared_storage_location"><span class="mw-headline" id="5._Copy_user_and_app_directories_to_the_shared_storage_location"> 5. Copy user and app directories to the shared storage location </span></a></h3>
<p>Copy the contents of the <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and <code><font size="2">$SPLUNK_HOME/etc/users</font></code> directories on an existing search head into the empty <code><font size="2">/etc/apps</font></code> and <code><font size="2">/etc/users</font></code> directories in the shared storage location. Those directories were created in step 4 and reside under the <code><font size="2">&lt;path_to_shared_storage&gt;</font></code> that you specified at that time. 
</p><p>For example, if your NFS mount is at <code><font size="2">/tmp/nfs</font></code>, copy the apps subdirectories that match this pattern:
</p>
<code><font size="2"><br>$SPLUNK_HOME/etc/apps/*<br></font></code>
<p>into
</p>
<code><font size="2"><br>/tmp/nfs/etc/apps<br></font></code>
<p>This results in a set of subdirectories like:
</p>
<code><font size="2"><br>/tmp/nfs/etc/apps/search<br>/tmp/nfs/etc/apps/launcher<br>/tmp/nfs/etc/apps/unix<br>[...]<br></font></code>
<p>Similarly, copy the user subdirectories:
</p>
<code><font size="2"><br>$SPLUNK_HOME/etc/users/*<br></font></code>
<p>into
</p>
<code><font size="2"><br>/tmp/nfs/etc/users<br></font></code>
<p><b>Important:</b> You can choose to copy over just a subset of apps and user subdirectories; however, be sure to move them to the precise locations described above. 
</p>
<h3> <a name="createasearchheadpool_6._restart_the_search_heads"><span class="mw-headline" id="6._Restart_the_search_heads"> 6. Restart the search heads </span></a></h3>
<p>After running the <code><font size="2">splunk pooling enable</font></code> command, restart <code><font size="2">splunkd</font></code>. Do this for each search head in the pool.
</p>
<a name="usealoadbalancerwiththesearchheadpool"></a><h2> <a name="usealoadbalancerwiththesearchheadpool_use_a_load_balancer_with_the_search_head_pool"><span class="mw-headline" id="Use_a_load_balancer_with_the_search_head_pool"> Use a load balancer with the search head pool</span></a></h2>
<p>You will probably want to run a load balancer in front of your search heads. That way, users can access the pool of search heads through a single interface, without needing to specify a particular one. 
</p><p>Another reason for using a load balancer is to ensure access to search artifacts and results if one of the search heads goes down. Ordinarily, RSS and email alerts provide links to the search head where the search originated. If that search head goes down (and there's no load balancer), the artifacts and results become inaccessible. However, if you've got a load balancer in front, you can set the alerts so that they reference the load balancer instead of a particular search head. 
</p>
<h3> <a name="usealoadbalancerwiththesearchheadpool_configure_the_load_balancer"><span class="mw-headline" id="Configure_the_load_balancer">Configure the load balancer </span></a></h3>
<p>There are a couple issues to note when selecting and configuring the load balancer:
</p>
<ul><li> The load balancer must employ layer-7 (application-level) processing. 
</li></ul><ul><li> Configure the load balancer so that user sessions are "sticky" or "persistent". This ensures that the user remains on a single search head throughout their session. 
</li></ul><h3> <a name="usealoadbalancerwiththesearchheadpool_generate_alert_links_to_the_load_balancer"><span class="mw-headline" id="Generate_alert_links_to_the_load_balancer"> Generate alert links to the load balancer </span></a></h3>
<p>To generate alert links to the load balancer, you must edit <code><font size="2">alert_actions.conf</font></code>:
</p><p><b>1.</b> Copy <code><font size="2">alert_actions.conf</font></code> from a search head to the appropriate app directory in the shared storage location. In most cases, this will be <code><font size="2">/&lt;path_to_shared_storage&gt;/etc/apps/search/local</font></code>.
</p><p><b>2.</b> Edit the <code><font size="2">hostname</font></code> attribute to point to the load balancer:
</p>
<code><font size="2"><br>hostname = &lt;proxy host&gt;:&lt;port&gt;<br></font></code>
<p>For details, see alert_actions.conf in the Admin manual.
</p><p>The alert links should now point to the load balancer, not the individual search heads.
</p>
<a name="otherpoolingoperations"></a><h2> <a name="otherpoolingoperations_other_pooling_operations"><span class="mw-headline" id="Other_pooling_operations"> Other pooling operations</span></a></h2>
<p>Besides the <code><font size="2">splunk pooling enable</font></code> CLI command, there are several other commands that are important for managing search head pooling:
</p>
<ul><li> <code><font size="2">splunk pooling validate</font></code>
</li><li> <code><font size="2">splunk pooling disable</font></code>
</li><li> <code><font size="2">splunk pooling display</font></code>
</li></ul><p>You must stop <code><font size="2">splunkd</font></code> before running <code><font size="2">splunk pooling enable</font></code> or <code><font size="2">splunk pooling disable</font></code>. However, you can run <code><font size="2">splunk pooling validate</font></code> and <code><font size="2">splunk pooling display</font></code> while <code><font size="2">splunkd</font></code> is either stopped or running.
</p>
<h3> <a name="otherpoolingoperations_validate_that_each_search_head_has_access_to_shared_resources"><span class="mw-headline" id="Validate_that_each_search_head_has_access_to_shared_resources">Validate that each search head has access to shared resources </span></a></h3>
<p>The <code><font size="2">splunk pooling enable</font></code> command validates search head access when you initially set up search head pooling. If you ever need to revalidate the search head's access to shared resources (for example, if you change the NFS configuration), you can run the <code><font size="2">splunk pooling validate</font></code> CLI command:
</p>
<code><font size="2"> &nbsp;<code><font size="2">splunk pooling validate [--debug]</font></code><br></font></code>
<h3> <a name="otherpoolingoperations_disable_search_head_pooling"><span class="mw-headline" id="Disable_search_head_pooling"> Disable search head pooling </span></a></h3>
<p>You can disable search head pooling with this CLI command:
</p>
<code><font size="2"> &nbsp;<code><font size="2">splunk pooling disable [--debug]</font></code><br></font></code>
<p>Run this command for each search head that you need to disable. 
</p><p><b>Important:</b> Before running the <code><font size="2">splunk pooling disable</font></code> command, you must stop <code><font size="2">splunkd</font></code>.
After running the command, you should restart <code><font size="2">splunkd</font></code>.
</p>
<h3> <a name="otherpoolingoperations_display_pooling_status"><span class="mw-headline" id="Display_pooling_status"> Display pooling status </span></a></h3>
<p>You can use the <code><font size="2">splunk pooling display</font></code> CLI command to determine whether pooling is enabled on a search head:
</p>
<code><font size="2"> &nbsp;<code><font size="2">splunk pooling display</font></code><br></font></code>
<p>This example shows how the system response varies depending on whether pooling is enabled:
</p>
<code><font size="2"><br>$ splunk pooling enable /foo/bar<br>$ splunk pooling display<br>Search head pooling is enabled with shared storage at: /foo/bar<br>$ splunk pooling disable<br>$ splunk pooling display<br>Search head pooling is disabled<br></font></code>

<a name="manageconfigurationchanges"></a><h2> <a name="manageconfigurationchanges_manage_configuration_changes"><span class="mw-headline" id="Manage_configuration_changes"> Manage configuration changes</span></a></h2>
<p><b>Important:</b> Once pooling is enabled on a search head, you must notify the search head whenever you directly edit a configuration file.
</p><p>Specifically, if you add a stanza to any configuration file in a <code><font size="2">local</font></code> directory, you must run the following command:
</p>
<code><font size="2"><br>splunk btool fix-dangling<br></font></code>
<p><b>Note:</b> This is not necessary if you make changes by means of Splunk Web or the CLI.
</p>
<a name="deploymentserverandsearchheadpooling"></a><h2> <a name="deploymentserverandsearchheadpooling_deployment_server_and_search_head_pooling"><span class="mw-headline" id="Deployment_server_and_search_head_pooling"> Deployment server and search head pooling</span></a></h2>
<p>With search head pooling, all search heads access a single set of configurations, so you don't need to use a <b>deployment server</b> or a third party deployment management tool like Puppet to push updates to multiple search heads. However, you might still want to use a deployment tool with search head pooling, in order to consolidate configuration operations across all Splunk Enterprise instances. 
</p><p>If you want to use the deployment server to manage your search head configuration, note the following:
</p><p><b>1.</b> Designate one of the search heads as a deployment client by creating a <code><font size="2">deploymentclient.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code> and specifying its deployment server. You only need to designate one search head as a deployment client.
</p><p><b>2.</b> In <code><font size="2">deploymentclient.conf</font></code>, set the <code><font size="2">repositoryLocation</font></code> attribute to the search head's shared storage mountpoint. You must also set <code><font size="2">serverRepositoryLocationPolicy=rejectAlways</font></code>, so that the locally set <code><font size="2">repositoryLocation</font></code> gets used as the download location.
</p><p><b>3.</b> In <code><font size="2">serverclass.conf</font></code> on the deployment server, define a server class for the search head client. 
</p><p>For detailed information on the deployment server, see "About deployment server" in the <i>Updating Splunk Enterprise Instances</i> manual.
</p>
<a name="selecttimingforconfigurationrefresh"></a><h2> <a name="selecttimingforconfigurationrefresh_select_timing_for_configuration_refresh"><span class="mw-headline" id="Select_timing_for_configuration_refresh"> Select timing for configuration refresh</span></a></h2>
<p>In version 5.0.2 and earlier, the defaults for synchronizing from the storage location were set to very frequent intervals. This could lead to excessive time spent reading configuration changes from the pool, particularly in deployments with large numbers of users (in the hundreds or thousands).
</p><p>The default settings have been changed to less frequent intervals starting with 5.0.3. In <code><font size="2">server.conf</font></code>, the following settings affect configuration refresh timing:
</p>
<code><font size="2"><br># 5.0.3 defaults<br>[pooling]<br>poll.interval.rebuild = 1m<br>poll.interval.check = 1m<br></font></code>
<p>The previous defaults for these settings were 2s and 5s, respectively.
</p><p>With the old default values, a change made on one search head would become available on another search head at most seven seconds later. There is usually no need for updates to be propagated that quickly. By changing the settings to values of one minute, the load on the shared storage system is greatly reduced. Depending on your business needs, you might be able to set these values to even longer intervals.
</p>
<a name="upgradeasearchheadpool"></a><h2> <a name="upgradeasearchheadpool_upgrade_a_search_head_pool"><span class="mw-headline" id="Upgrade_a_search_head_pool"> Upgrade a search head pool</span></a></h2>
<p>All search heads in a pool must be running the same version of Splunk Enterprise. 
</p><p>For the upgrade procedure, see "Upgrade your distributed deployment" in the <i>Distributed Deployment Manual</i>.  Read this procedure carefully before attempting to upgrade your search head pool. You must follow the steps precisely to ensure that the pool remains fully functional.
</p>
<h1>Mount the knowledge bundle</h1><a name="mounttheknowledgebundle"></a><h2> <a name="mounttheknowledgebundle_about_mounted_bundles"><span class="mw-headline" id="About_mounted_bundles">About mounted bundles</span></a></h2>
<p><b>Important:</b>  For most deployments, Splunk recommends that you use normal bundle replication, not mounted bundles with shared storage. As a result of changes to bundle replication made in the 5.0 timeframe, such as the introduction of delta-based replication and improvements in streaming, the practical use case for mounted bundles is now extremely limited. In most cases, mounted bundles make little difference in the amount of network traffic or the speed at which bundle changes get distributed to the search peers.  At the same time, they add significant management complexity, particularly when combined with shared storage.  Because of delta-based replication, even if your configurations contain large files, normal bundle replication entails little ongoing replication cost, as long as those files rarely change.
</p><p>The set of data that a search head distributes to its search peers is called the <b>knowledge bundle.</b> The bundle contents reside in the search head's <code><font size="2">$SPLUNK_HOME/etc/{apps,users,system}</font></code> subdirectories. For information on the contents and purpose of this bundle, see <a href="#whatsearchheadssend" class="external text">"What search heads send to search peers"</a>.
</p><p>By default, the search head replicates and distributes the knowledge bundle to each search peer. You can instead tell the search peers to mount the knowledge bundle's directory location, eliminating the need for bundle replication. When you mount a knowledge bundle on shared storage, it's referred to as a <b>mounted bundle</b>.
</p><p><b>Caution:</b> Most shared storage solutions don't work well across a WAN. Since mounted bundles require shared storage, you generally should not implement them across a WAN.
</p>
<h3> <a name="mounttheknowledgebundle_mounted_bundle_architectures"><span class="mw-headline" id="Mounted_bundle_architectures"> Mounted bundle architectures </span></a></h3>
<p>Depending on your search head configuration, there are a number of ways to set up mounted bundles. These are some of the typical ones:
</p>
<ul><li> <b>For a single search head.</b> Mount the knowledge bundle on shared storage. All the search peers then access the bundle to process search requests. This diagram illustrates a single search head with a mounted bundle on shared storage:
</li></ul><p><img alt="Mounted bundles 3 60.png" src="images/b/b4/Mounted_bundles_3_60.png" width="700" height="554"></p>
<ul><li> <b>For multiple non-clustered search heads.</b> Maintain the knowledge bundle(s) on each search head's local storage. In this diagram, each search head maintains its own bundle, which each search peer mounts and accesses individually:
</li></ul><p><img alt="Mounted bundles 2 60.png" src="images/e/e8/Mounted_bundles_2_60.png" width="700" height="554"></p><p>In each case, the search peers need access to each search head's <code><font size="2">$SPLUNK_HOME/etc/{apps,users,system}</font></code> subdirectories. 
</p><p>The search peers use the mounted directories only when fulfilling the search head's search requests. For indexing and other purposes not directly related to distributed search, the search peers will use their own, local <code><font size="2">apps</font></code>, <code><font size="2">users</font></code>, and <code><font size="2">system</font></code> directories, the same as any other indexer.
</p>
<a name="configuremountedbundles"></a><h2> <a name="configuremountedbundles_configure_mounted_bundles"><span class="mw-headline" id="Configure_mounted_bundles"> Configure mounted bundles</span></a></h2>
<p>To set up mounted bundles, you need to configure both the search head and its search peers. The procedures described here assume the bundles are on shared storage, but they do not need to be. They just need to be in some location that both the search head and its search peers can access.
</p><p><b>Note:</b> It's best not to locate mounted bundles in the search head's local <code><font size="2">$SPLUNK_HOME</font></code> path.
</p>
<h3> <a name="configuremountedbundles_configure_the_search_head"><span class="mw-headline" id="Configure_the_search_head">Configure the search head</span></a></h3>
<p>Here are the steps you take on the search head:
</p><p><b>1.</b> Mount the bundle subdirectories (<code><font size="2">$SPLUNK_HOME/etc/{apps,users,system}</font></code>) on shared storage. The simplest way to do this is to mount the search head's entire <code><font size="2">$SPLUNK_HOME/etc</font></code> directory:
</p>
<ul><li> <b>On *nix platforms,</b> set up an NFS mount.
</li></ul><ul><li> <b>On Windows,</b> set up a CIFS (SMB) share.
</li></ul><p><b>Important:</b> The search head's Splunk user account needs read/write access to the shared storage location. The search peers must have only read access to the bundle subdirectories, to avoid file-lock issues.  Search peers do not need to update any files in the shared storage location.
</p><p><b>2.</b> In the <code><font size="2">distsearch.conf</font></code> file on the search head, set:
</p>
<code><font size="2"><br>shareBundles=false<br></font></code> 
<p>This stops the search head from replicating bundles to the search peers.
</p><p><b>3.</b> Restart the search head.
</p>
<h3> <a name="configuremountedbundles_configure_the_search_peers"><span class="mw-headline" id="Configure_the_search_peers">Configure the search peers</span></a></h3>
<p>For each search peer, follow these steps to access the mounted bundle:
</p><p><b>1.</b> Mount the bundle directory on the search peer.
</p><p><b>2.</b> Create a <code><font size="2">distsearch.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code> on the search peer. For each search head that the peer is connected to, create a <code><font size="2">[searchhead:&lt;searchhead-splunk-server-name&gt;]</font></code> stanza, with these attributes:
</p>
<code><font size="2"><br>[searchhead:&lt;searchhead-splunk-server-name&gt;]<br>mounted_bundles=true<br>bundles_location=&lt;path_to_bundles&gt;<br></font></code>
<p>Note the following:
</p>
<ul><li> The search peer's configuration file must contain only the <code><font size="2">[searchhead:&lt;searchhead-splunk-server-name&gt;]</font></code> stanza(s). The other stanzas in <code><font size="2">distsearch.conf</font></code> are for search heads only.
</li></ul><ul><li> To identify the <code><font size="2">&lt;searchhead-splunk-server-name&gt;</font></code>, run this command on the search head:
</li></ul><code><font size="2"><br>&nbsp;&nbsp;&nbsp;splunk show servername <br></font></code>
<ul><li> The <code><font size="2">&lt;path_to_bundles&gt;</font></code> needs to specify the mountpoint on the search peer, not on the search head. For example, say <code><font size="2">$SPLUNK_HOME</font></code> on your search head is <code><font size="2">/opt/splunk</font></code>, and you export <code><font size="2">/opt/splunk/etc</font></code> via NFS. Then, on the search peer, you mount that NFS share at <code><font size="2">/mnt/splunk-head</font></code>. The value of <code><font size="2">&lt;path_to_bundles&gt;</font></code> should be <code><font size="2">/mnt/splunk-head</font></code>, <b>not</b> <code><font size="2">/opt/splunk</font></code>.
</li></ul><p><b>Important:</b> If multiple search heads will be distributing searches to this search peer, you must create a separate stanza on the search peer for each of them. 
</p><p><b>3.</b> Restart the search peer.
</p><p><b>Note:</b> You can optionally set up symbolic links to the bundle subdirectories (<code><font size="2">apps,users,system</font></code>) to ensure that the search peer has access only to the necessary subdirectories in the search head's <code><font size="2">/etc</font></code> directory. See the following example for details on how to do this.
</p>
<h3> <a name="configuremountedbundles_example_configuration"><span class="mw-headline" id="Example_configuration">Example configuration</span></a></h3>
<p>Here's an example of how to set up mounted bundles on shared storage:
</p>
<h4><font size="3"><b><i> <a name="configuremountedbundles_search_head"><span class="mw-headline" id="Search_head">Search head</span></a></i></b></font></h4>
<p>On a search head whose Splunk Enterprise server name is "searcher01":
</p><p><b>1.</b> Mount the search head's <code><font size="2">$SPLUNK_HOME/etc</font></code> directory to shared storage with read/write access.
</p><p><b>2.</b> In the <code><font size="2">distsearch.conf</font></code> file on the search head, set:
</p>
<code><font size="2"><br>[distributedSearch]<br>...<br>shareBundles = false<br></font></code> 
<p><b>3.</b> Restart the search head.
</p>
<h4><font size="3"><b><i> <a name="configuremountedbundles_search_peers"><span class="mw-headline" id="Search_peers">Search peers</span></a></i></b></font></h4>
<p>For each search peer:
</p><p><b>1.</b> Mount the search head's <code><font size="2">$SPLUNK_HOME/etc</font></code> directory on the search peer to:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;/mnt/searcher01<br></font></code>
<p><b>2.</b> (Optional.) Create a directory that consists of symbolic links to the bundle subdirectories:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/searcher01<br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/searcher01/system -&gt; /mnt/searcher01/system<br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/searcher01/users -&gt; /mnt/searcher01/users<br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/searcher01/apps -&gt; /mnt/searcher01/apps<br></font></code>
<p><b>Note:</b> This optional step is useful for ensuring that the peer has access only to the necessary subdirectories.
</p><p><b>3.</b> Create a <code><font size="2">distsearch.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code> on the search peer, with this stanza:
</p>
<code><font size="2"><br>[searchhead:searcher01]<br>mounted_bundles = true<br>bundles_location = /opt/shared_bundles/searcher01<br></font></code>
<p><b>4.</b> Restart the search peer.
</p><p><b>5.</b> Repeat the process for each search peer.
</p>
<a name="mountedbundlesandsearchheadpooling"></a><h2> <a name="mountedbundlesandsearchheadpooling_use_mounted_bundles_with_search_head_pooling"><span class="mw-headline" id="Use_mounted_bundles_with_search_head_pooling"> Use mounted bundles with search head pooling</span></a></h2>
<table border="1" cellpadding="5" cellspacing="0"><tr bgcolor="#D9EAED"><th valign="top" bgcolor="#C0C0C0"> This feature has been deprecated.
</th></tr><tr><td valign="center" align="left"> Search head pooling has been deprecated as of Splunk Enterprise version 6.2. This means that although it continues to function, it might be removed in a future version.
<p>As an alternative, you can deploy search head clustering. See <a href="#aboutshc" class="external text">"About search head clustering".</a> Search head clustering does not support mounted bundles.
</p><p>For a list of all deprecated features, see the topic "Deprecated features" in the Release Notes.
</p>
</td></tr></table><p><br>
The process for configuring mounted bundles is basically no different if you're using search head pooling to manage multiple search heads. A few things to keep in mind:
</p>
<ul><li> Use the same shared storage location for both the search head pool and the mounted bundles. Search head pooling uses a subset of the directories required for mounted bundles.
</li><li> Search head pooling itself only requires that you mount the <code><font size="2">$SPLUNK_HOME/etc/{apps,users}</font></code> directories. However, when using mounted bundles, you must also provide a mounted <code><font size="2">$SPLUNK_HOME/etc/system</font></code> directory. This doesn't create any conflict among the search heads, as they will always use their own versions of the <code><font size="2">system</font></code> directory and ignore the mounted version.
</li><li> The search peers must create separate stanzas in <code><font size="2">distsearch.conf</font></code> for each search head in the pool. The <code><font size="2">bundles_location</font></code> in each of those stanzas must be identical.
</li></ul><p>See <a href="#configuresearchheadpooling" class="external text">"Configure search head pooling"</a> for information on setting up a search head pool.
</p>
<h3> <a name="mountedbundlesandsearchheadpooling_example_configuration:_search_head_pooling_with_mounted_bundles"><span class="mw-headline" id="Example_configuration:_Search_head_pooling_with_mounted_bundles">Example configuration: Search head pooling with mounted bundles</span></a></h3>
<p>This example shows how to combine search head pooling and mounted bundles in one system.  There are two main sections to the example:
</p><p><b>1.</b> Set up a search head pool consisting of two search heads. In this part, you also mount the bundles.
</p><p><b>2.</b> Set up the search peers so that they can access bundles from the search head pool. 
</p><p>The example assumes you're using an NFS mount for the shared storage location.
</p>
<h4><font size="3"><b><i> <a name="mountedbundlesandsearchheadpooling_part_1:_set_up_the_search_head_pool"><span class="mw-headline" id="Part_1:_Set_up_the_search_head_pool">Part 1: Set up the search head pool</span></a></i></b></font></h4>
<p>Before configuring the pool, perform these preliminary steps:
</p><p><b>1.</b> Enable two Splunk Enterprise instances as search heads. This example assumes that the instances are named "searcher01" and "searcher02". 
</p><p><b>2.</b> Set up a shared storage location accessible to each search head. This example assumes that you set up an NFS mountpoint, specified on the search heads as <code><font size="2">/mnt/search-head-pooling</font></code>. 
</p><p>For detailed information on these steps, see <a href="#configuresearchheadpooling_create_a_pool_of_search_heads" class="external text">"Create a pool of search heads"</a>.
</p><p><b>Now, configure the search head pool:</b>
</p><p><b>1.</b> <b>On each search head,</b> stop <code><font size="2">splunkd</font></code>:
</p>
<code><font size="2"><br>splunk stop splunkd<br></font></code>
<p><b>2.</b> <b>On each search head,</b> enable search head pooling. In this example, you're using an NFS mount of <code><font size="2">/mnt/search-head-pooling </font></code> as your shared storage location:
</p>
<code><font size="2"><br>splunk pooling enable /mnt/search-head-pooling [--debug]<br></font></code>
<p>Among other things, this step creates empty <code><font size="2">/etc/apps</font></code> and <code><font size="2">/etc/users</font></code> directories under <code><font size="2">/mnt/search-head-pooling</font></code>. Step 3 uses those directories.
</p><p><b>3.</b> Copy the contents of the <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and <code><font size="2">$SPLUNK_HOME/etc/users</font></code> directories <b>on one of the search heads</b> into the <code><font size="2">/etc/apps</font></code> and <code><font size="2">/etc/users</font></code> subdirectories under <code><font size="2">/mnt/search-head-pooling</font></code>:
</p>
<code><font size="2"><br>cp -r $SPLUNK_HOME/etc/apps/* /mnt/search-head-pooling/etc/apps<br><br>cp -r $SPLUNK_HOME/etc/users/* /mnt/search-head-pooling/etc/users<br></font></code>
<p><b>4.</b> Copy one search head's <code><font size="2">$SPLUNK_HOME/etc/system</font></code> directory to <code><font size="2">/mnt/search-head-pooling/etc/system</font></code>.
</p>
<code><font size="2"><br>cp -r $SPLUNK_HOME/etc/system /mnt/search-head-pooling/etc/<br></font></code>
<p><b>5.</b> Review the <code><font size="2">/mnt/search-head-pooling/etc/system/local/server.conf</font></code> file for a <code><font size="2">[pooling]</font></code> stanza. If it exists, remove any entries.
</p><p><b>6.</b> <b>On each search head,</b> edit the <code><font size="2">distsearch.conf</font></code> file to set <code><font size="2">shareBundles = false</font></code>:
</p>
<code><font size="2"><br>[distributedSearch]<br>...<br>shareBundles = false<br></font></code> 
<p><b>7.</b> <b>On each search head,</b> start <code><font size="2">splunkd</font></code>:
</p>
<code><font size="2"><br>splunk start splunkd<br></font></code>
<p>Your search head pool should now be up and running.
</p>
<h4><font size="3"><b><i> <a name="mountedbundlesandsearchheadpooling_part_2:_mount_bundles_on_the_search_peers"><span class="mw-headline" id="Part_2:_Mount_bundles_on_the_search_peers">Part 2: Mount bundles on the search peers</span></a></i></b></font></h4>
<p>Now, mount the bundles on the search peers.
</p><p><b>On each search peer, perform these steps:</b>
</p><p><b>1.</b> Mount the shared storage location (the same location that was earlier set to <code><font size="2">/mnt/search-head-pooling</font></code> on the search heads) so that it appears as <code><font size="2">/mnt/bundles</font></code> on the peer.
</p><p><b>2.</b> Create a directory that consists of symbolic links to the bundle subdirectories:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/bundles/system -&gt; /mnt/bundles/etc/system<br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/bundles/users -&gt; /mnt/bundles/etc/users<br>&nbsp;&nbsp;&nbsp;/opt/shared_bundles/bundles/apps -&gt; /mnt/bundles/etc/apps<br></font></code>
<p><b>3.</b> Create a <code><font size="2">distsearch.conf</font></code> file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code> on the search peer, with stanzas for each of the two search heads:
</p>
<code><font size="2"><br>[searchhead:searcher01]<br>mounted_bundles = true<br>bundles_location = /opt/shared_bundles/bundles<br><br>[searchhead:searcher02]<br>mounted_bundles = true<br>bundles_location = /opt/shared_bundles/bundles<br></font></code>
<p><b>4.</b> Restart the search peer:
</p>
<code><font size="2"><br>splunk restart splunkd<br></font></code>
<p>Repeat the process for each search peer.
</p>
<h1>Distributed search in action</h1><a name="howauthorizationworksindistributedsearches"></a><h2> <a name="howauthorizationworksindistributedsearches_how_authorization_works_in_distributed_searches"><span class="mw-headline" id="How_authorization_works_in_distributed_searches"> How authorization works in distributed searches</span></a></h2>
<p>The authorization settings that a search peer uses when processing distributed searches are different from those that it uses for its local activities, such as administration and local search requests:
</p>
<ul><li> When processing a distributed search, the search peer uses the settings contained in the <b>knowledge bundle</b> that the search head distributes to all the search peers when it sends them a search request. These settings are created and managed on the search head. 
</li><li> When performing local activities, the search peer uses the authorization settings created and stored locally on the search peer itself.
</li></ul><p>When managing distributed searches, it is therefore important that you distinguish between these two types of authorization. You need to be particularly aware of how authorization settings get distributed through the knowledge bundle when you're managing a system with  <b>search head pooling</b> or <b>mounted bundles</b>.
</p><p>For background information, read about these key concepts:
</p>
<ul><li> <b>Splunk Enterprise authorization:</b> The topic "About role-based user access" in the Securing Splunk Enterprise manual
</li><li> <b>Mounted bundles:</b> The chapter <a href="#mounttheknowledgebundle" class="external text">"Mount the knowledge bundle"</a> in this manual
</li><li> <b>Search head pooling:</b> The chapter <a href="#configuresearchheadpooling" class="external text">"Search head pooling"</a> in this manual
</li></ul><h3> <a name="howauthorizationworksindistributedsearches_manage_authorization_for_distributed_searches"><span class="mw-headline" id="Manage_authorization_for_distributed_searches"> Manage authorization for distributed searches</span></a></h3>
<p>All authorization settings are stored in one or more <code><font size="2">authorize.conf</font></code> files. This includes settings configured through Splunk Web or the CLI. It is these <code><font size="2">authorize.conf</font></code> files that get distributed from the search head to the search peers. On the knowledge bundle, the files are usually located in either <code><font size="2">/etc/system/{local,default}</font></code> and/or <code><font size="2">/etc/apps/&lt;app-name&gt;/{local,default}</font></code>.
</p><p>Since search peers automatically use the settings in the knowledge bundle, things normally work fine. You configure roles for your users on the search head, and the search head automatically distributes those configurations to the search peers when it distributes the search itself. 
</p><p>With search head pooling, however, you must take care to ensure that the search heads and the search peers all use the same set of <code><font size="2">authorize.conf</font></code> file(s). For this to happen, you must make sure:
</p>
<ul><li> All search heads in the pool use the same set of <code><font size="2">authorize.conf</font></code> files
</li></ul><ul><li> The set of <code><font size="2">authorize.conf</font></code> files that the search heads use goes into the knowledge bundle so that they get distributed to the search peers.
</li></ul><p>This topic describes the four main scenarios, based on whether or not you're using search head pooling or mounted bundles. It describes the scenarios in order from simple to complex.
</p>
<h3> <a name="howauthorizationworksindistributedsearches_four_scenarios"><span class="mw-headline" id="Four_scenarios">Four scenarios</span></a></h3>
<p>What you need to do with the distributed search <code><font size="2">authorize.conf</font></code> files depends on whether your deployment implements search head pooling or mounted bundles.
The four scenarios are:
</p>
<ul><li> No search head pooling, no mounted bundles
</li><li> No search head pooling, mounted bundles
</li><li> Search head pooling, no mounted bundles
</li><li> Search head pooling, mounted bundles
</li></ul><p>The first two scenarios "just work" but the last two scenarios require careful planning. For the sake of completeness, this section describes all four scenarios.
</p><p><b>Note:</b> These scenarios address authorization settings for distributed search only. Local authorization settings function the same independent of your distributed search deployment.
</p>
<h4><font size="3"><b><i> <a name="howauthorizationworksindistributedsearches_no_search_head_pooling.2c_no_mounted_bundles"><span class="mw-headline" id="No_search_head_pooling.2C_no_mounted_bundles">No search head pooling, no mounted bundles</span></a></i></b></font></h4>
<p>Whatever authorization settings you have on the search head get automatically distributed to its search peers as part of the replicated knowledge bundle that they receive with distributed search requests.
</p>
<h4><font size="3"><b><i> <a name="howauthorizationworksindistributedsearches_no_search_head_pooling.2c_mounted_bundles"><span class="mw-headline" id="No_search_head_pooling.2C_mounted_bundles">No search head pooling, mounted bundles</span></a></i></b></font></h4>
<p>Whatever authorization settings you have on the search head get automatically placed in the mounted bundle and used by the search peers during distributed search processing.
</p>
<h4><font size="3"><b><i> <a name="howauthorizationworksindistributedsearches_search_head_pooling.2c_no_mounted_bundles"><span class="mw-headline" id="Search_head_pooling.2C_no_mounted_bundles">Search head pooling, no mounted bundles</span></a></i></b></font></h4>
<p>The search heads in the pool share their <code><font size="2">/apps</font></code> and <code><font size="2">/users</font></code> directories but not their <code><font size="2">/etc/system/local</font></code> directories. Any <code><font size="2">authorize.conf</font></code> file in an <code><font size="2">/apps</font></code> subdirectory will be automatically shared by all search heads and included in the knowledge bundle when any of the search heads distributes a search request to the search peers.
</p><p>The problem arises because authorization changes can also get saved to an <code><font size="2">authorize.conf</font></code> file in a search head's <code><font size="2">/etc/system/local</font></code> directory (for example, if you update the search head's authorization settings via Splunk Web). This directory does not get shared among the search heads in the pool, but it still gets distributed to the search peers as part of the knowledge bundle. Because of how the configuration system works, any copy of <code><font size="2">authorize.conf</font></code> file in <code><font size="2">/etc/system/local</font></code> will have precedence over a copy in an <code><font size="2">/apps</font></code> subdirectory. (See "Configuration file precedence" in the Admin manual for details.)
</p><p>Therefore, a copy of <code><font size="2">authorize.conf</font></code> that gets distributed to the search peers from a single search head's <code><font size="2">/etc/system/local</font></code> directory has precedence over any copies distributed from the search head pool's shared directory. Unless you account for this situation, the search peers can end up using different authorization settings for different searches, depending on which search head distributed the search to them. For most situations, this is not what you want to occur.
</p><p>To avoid this problem, you need to make sure that any changes made to a search head's <code><font size="2">/etc/system/local/authorize.conf</font></code> file get propagated to all search heads in the pool. One way to handle this is to move any changed <code><font size="2">/etc/system/local/authorize.conf</font></code> file into an app subdirectory, since all search heads in the pool share the <code><font size="2">/apps</font></code> directory.
</p>
<h4><font size="3"><b><i> <a name="howauthorizationworksindistributedsearches_search_head_pooling.2c_mounted_bundles"><span class="mw-headline" id="Search_head_pooling.2C_mounted_bundles">Search head pooling, mounted bundles</span></a></i></b></font></h4>
<p>This is similar to the previous scenario. The search heads in the pool share their <code><font size="2">/apps</font></code> and <code><font size="2">/users</font></code> directories but not their <code><font size="2">/etc/system/local</font></code> directories. Any <code><font size="2">authorize.conf</font></code> file in an <code><font size="2">/apps</font></code> subdirectory will be automatically shared by all search heads. It will also be included in the mounted bundle that the search peers use when processing a search request from any of the search heads.
</p><p>However, authorization changes can also wind up in an <code><font size="2">authorize.conf</font></code> file in a search head's <code><font size="2">/etc/system/local</font></code> directory (for example, if you update the search head's authorization settings via Splunk Web). This directory does not get automatically shared among the search heads in the pool. It also does not get automatically distributed to the mounted bundle that the search peers use. Therefore, you must provide some mechanism that ensures that all the search heads and all the search peers have access to that version of <code><font size="2">authorize.conf</font></code>.
</p><p>The simplest way to handle this is to move any changed <code><font size="2">/etc/system/local/authorize.conf</font></code> file into an app subdirectory, since both the pooled search heads and all the search peers share the <code><font size="2">/apps</font></code> directory.
</p>
<a name="howuserscancontroldistributedsearches"></a><h2> <a name="howuserscancontroldistributedsearches_how_users_can_control_distributed_searches"><span class="mw-headline" id="How_users_can_control_distributed_searches"> How users can control distributed searches</span></a></h2>
<p>From the user standpoint, specifying and running a <b>distributed search</b> is essentially the same as running any other search. Behind the scenes, the <b> search head</b> distributes the query to its <b> search peers</b> and consolidates the results when presenting them to the user. 
</p><p>Users can limit the search peers that participate in a search. They also need to be aware of the distributed search configuration to troubleshoot.
</p>
<h3> <a name="howuserscancontroldistributedsearches_perform_distributed_searches"><span class="mw-headline" id="Perform_distributed_searches"> Perform distributed searches </span></a></h3>
<p>In general, you specify a distributed search through the same set of commands as for a local search. However, several additional commands and options are available specifically to assist with controlling and limiting a distributed search.
</p><p>A search head by default runs its searches across its full set of search peers. You can limit a search to one or more search peers by specifying the <code><font size="2">splunk_server</font></code> field in your query. See "Retrieve events from indexes and distributed search peers" in the Search manual. 
</p><p>The search command <code><font size="2">localop</font></code> is also of use in defining distributed searches. It enables you to limit the execution of subsequent commands to the search head. See the description of localop in the Search Reference for details and an example.
</p><p>In addition, the <code><font size="2">lookup</font></code> command provides a <code><font size="2">local</font></code> argument for use with distributed searches. If set to <code><font size="2">true</font></code>, the lookup occurs only on the search head; if <code><font size="2">false</font></code>, the lookup occurs on the search peers as well. This is particularly useful for scripted lookups, which replicate lookup tables. See the description of lookup in the Search Reference for details and an example.
</p>
<h1>Troubleshoot distributed search</h1><a name="troubleshootdistributedsearch"></a><h2> <a name="troubleshootdistributedsearch_general_troubleshooting_issues"><span class="mw-headline" id="General_troubleshooting_issues">General troubleshooting issues </span></a></h2>
<h3> <a name="troubleshootdistributedsearch_clock_skew_between_search_heads_and_search_peers_can_affect_search_behavior"><span class="mw-headline" id="Clock_skew_between_search_heads_and_search_peers_can_affect_search_behavior"> Clock skew between search heads and search peers can affect search behavior </span></a></h3>
<p>It's important to keep the clocks on your search heads and search peers in sync, via NTP (network time protocol) or some similar means. If the clocks are out-of-sync by more than a few seconds, you can end up with search failures or premature expiration of search artifacts.
</p>
<h3> <a name="troubleshootdistributedsearch_searches_can_fail_if_configurations_in_a_knowledge_bundle_have_not_yet_been_replicated_to_search_peers"><span class="mw-headline" id="Searches_can_fail_if_configurations_in_a_knowledge_bundle_have_not_yet_been_replicated_to_search_peers"> Searches can fail if configurations in a knowledge bundle have not yet been replicated to search peers</span></a></h3>
<p>Configuration changes can take a short time to propagate from search heads to search peers. As a result, during the time between when configuration changes are made on the search head and when they're replicated to the search peers (typically, not more than a few minutes), distributed searches can either fail or provide results based on the previous configuration. 
</p><p>Types of configuration changes that can cause search failures are those that involve new apps or changes to <code><font size="2">authentication.conf</font></code> or <code><font size="2">authorize.conf</font></code>. Examples include: 
</p>
<ul><li> changing the allowed indexes for a role and then running a search as a user within that role 
</li><li> creating a new app and then running a search from within that app
</li></ul><p>Any failures will be noted in messages on the search head.
</p><p>Types of changes that can provide results based on the previous configuration include changing a field extraction or a lookup table file.
</p><p>To remediate, run the search again.
</p>
<h3> <a name="troubleshootdistributedsearch_network_problems_can_reduce_search_performance"><span class="mw-headline" id="Network_problems_can_reduce_search_performance">Network problems can reduce search performance </span></a></h3>
<p>A 6.x search head by default asks its search peers to generate a remote timeline. This can result in slow searches if the connection between the search head and the search peers is unstable.
</p><p>The workaround is to add the following setting to <code><font size="2">limits.conf</font></code> on the search head&nbsp;:
</p>
<code><font size="2"><br>[search]<br>remote_timeline_fetchall = false<br></font></code>
<p>After making this change, you must restart the search head.
</p>
<a name="searchheadpoolingconfigurationissues"></a><h2> <a name="searchheadpoolingconfigurationissues_search_head_pooling_configuration_issues"><span class="mw-headline" id="Search_head_pooling_configuration_issues"> Search head pooling configuration issues</span></a></h2>
<p>When implementing search head pooling, there are a few potential issues you should be aware of, mainly having to do with coordination among search heads.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_authentication_and_authorization_changes_made_in_splunk_web_apply_only_to_a_single_search_head"><span class="mw-headline" id="Authentication_and_authorization_changes_made_in_Splunk_Web_apply_only_to_a_single_search_head">Authentication and authorization changes made in Splunk Web apply only to a single search head</span></a></h3>
<p>Authentication and authorization changes made through a search head's Splunk Web apply only to that search head and not to other search heads in that pool. Each member of the pool maintains its local 
configurations in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>. To share configurations across the pool, set them up in shared storage, as described in <a href="#configuresearchheadpooling" class="external text">"Configure search head pooling"</a>.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_clock_skew_between_search_heads_and_shared_storage_can_affect_search_behavior"><span class="mw-headline" id="Clock_skew_between_search_heads_and_shared_storage_can_affect_search_behavior"> Clock skew between search heads and shared storage can affect search behavior </span></a></h3>
<p>It's important to keep the clocks on your search heads and shared storage server in sync, via NTP (network time protocol) or some similar means. If the clocks are out-of-sync by more than a few seconds, you can end up with search failures or premature expiration of search artifacts.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_permission_problems_on_the_shared_storage_server_can_cause_pooling_failure"><span class="mw-headline" id="Permission_problems_on_the_shared_storage_server_can_cause_pooling_failure"> Permission problems on the shared storage server can cause pooling failure </span></a></h3>
<p>On each search head, the user account Splunk runs as must have read/write permissions to the files on the shared storage server.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_performance_analysis"><span class="mw-headline" id="Performance_analysis"> Performance analysis </span></a></h3>
<p>A large percentage of search head pooling issues boil down to insufficient performance.
</p><p>When deploying or investigating a search head pooling environment, it's important to consider these factors:
</p>
<ul><li> <b>Storage:</b> The storage backing the pool must be able to handle a very high number of IOPS.  IOPS under 1000 will probably never work well.
</li><li> <b>Network:</b>  The communication path between the backing store and the search heads must be high bandwidth and extremely low latency.  This probably means your storage system should be on the same switch as your search heads.  WAN links are not going to work.
</li><li> <b>Server Parallelism:</b>  Because searching results in a large number of processes requesting a large number of files, the parallelism in the system must be high.  This can require tuning the NFS server to handle a larger number of requests in parallel.
</li><li> <b>Client Parallelism:</b> The client operating system must be able to handle a significant number of requests at the same time.
</li></ul><p>To validate an environment, a typical approach would be:
</p>
<ul><li> Use a storage benchmarking tool, such as Bonnie++, while the file store is not in use to validate that the IOPS provided are robust.
</li><li> Use network testing methods to determine that the roundtrip time between search heads and the storage system is on the order of 10ms.  
</li><li> Perform known simple tasks such as creating a million files and then deleting them.  
</li><li> Assuming the above tests have not shown any weaknesses, perform some IO load generation or run the actual Splunk Enterprise load while gathering NFS stat data to see what's happening with the NFS requests.
</li></ul><h3> <a name="searchheadpoolingconfigurationissues_nfs_client_concurrency_limits_can_cause_search_timeouts_or_slow_search_behavior"><span class="mw-headline" id="NFS_client_concurrency_limits_can_cause_search_timeouts_or_slow_search_behavior">NFS client concurrency limits can cause search timeouts or slow search behavior </span></a></h3>
<p>The search performance in a search head pool is a function of the throughput of the shared storage and the search workload.  The combined effect of concurrent search users and concurrent scheduled searches running will yield a total IOPs that the shared volume needs to support. IOP requirements will also vary by the kind of searches run.  To adequately provision a device to be shared between search heads, you need to know the number of concurrent users submitting searches and the number of jobs/apps that will be executed simultaneously.
</p><p>If searches are timing out or running slowly, you might be exhausting the maximum number of concurrent requests supported by the NFS client. To solve this problem, increase your client concurrency limit. For example, on a Linux NFS client, adjust the <code><font size="2">tcp_slot_table_entries</font></code> setting.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_nfs_latency_for_large_user_count_can_incur_configuration_access_latency_or_slow_dispatch_reaping"><span class="mw-headline" id="NFS_latency_for_large_user_count_can_incur_configuration_access_latency_or_slow_dispatch_reaping"> NFS latency for large user count can incur configuration access latency or slow dispatch reaping </span></a></h3>
<p>Splunk Enterprise synchronizes the search head pool storage configuration state with the in-memory state when it detects changes.  Essentially, it reads the configuration into memory when it detects updates.  When dealing either with overloaded search pool storage or with large numbers of users, apps, and configuration files, this synchronization process can reduce performance. To mitigate this, the minimum frequency of reading can be increased, as discussed in   <a href="#configuresearchheadpooling_select_timing_for_configuration_refresh" class="external text">"Select timing for configuration refresh"</a>.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_warning_about_unique_servername_attribute"><span class="mw-headline" id="Warning_about_unique_serverName_attribute"> Warning about unique serverName attribute</span></a></h3>
<p>Each search head in the pool must have a unique <code><font size="2">serverName</font></code> attribute. Splunk Enterprise validates this condition when each search head starts. If it finds a problem, it generates this error message:
</p>
<code><font size="2"><br>serverName "&lt;xxx&gt;" has already been claimed by a member of this search head pool <br>in &lt;full path to pooling.ini on shared storage&gt;<br>There was an error validating your search head pooling configuration. For more <br>information, run 'splunk pooling validate'<br></font></code>
<p>The most common cause of this error is that another search head in the pool is already using the current search head's <code><font size="2">serverName</font></code>. To fix the problem, change the current search head's <code><font size="2">serverName</font></code> attribute in .<code><font size="2">system/local/server.conf</font></code>.
</p><p>There are a few other conditions that also can generate this error:
</p>
<ul><li> The current search head's <code><font size="2">serverName</font></code> has been changed.
</li><li> The current search head's GUID has been changed. This is usually due to <code><font size="2">/etc/instance.cfg</font></code> being deleted.
</li></ul><p>To fix these problems, run
</p>
<code><font size="2"><br>splunk pooling replace-member<br></font></code>
<p>This updates the <code><font size="2">pooling.ini</font></code> file with the current search head's <code><font size="2">serverName</font></code>-&gt;GUID mapping, overwriting any previous mapping.
</p>
<h3> <a name="searchheadpoolingconfigurationissues_artifacts_and_incorrectly_displayed_items_in_splunk_web_after_upgrade"><span class="mw-headline" id="Artifacts_and_incorrectly_displayed_items_in_Splunk_Web_after_upgrade">Artifacts and incorrectly displayed items in Splunk Web after upgrade</span></a></h3>
<p>When upgrading pooled search heads, you must copy <b>all</b> updated apps - even those that ship with Splunk Enterprise (such as the Search app and the data preview feature, which is implemented as an app) - to the search head pool's shared storage after the upgrade is complete. If you do not, you might see artifacts or other incorrectly-displayed items in Splunk Web. 
</p><p>To fix the problem, copy all updated apps from an upgraded search head to the shared storage for the search head pool, taking care to exclude the <code><font size="2">local</font></code> sub-directory of each app.
</p><p><b>Important:</b> Excluding the <code><font size="2">local</font></code> sub-directory of each app from the copy process prevents the overwriting of configuration files on the shared storage with local copies of configuration files.
</p><p>Once the apps have been copied, restart Splunk Enterprise on all search heads in the pool.
</p>
<a name="distributedsearcherrormessages"></a><h2> <a name="distributedsearcherrormessages_distributed_search_error_messages"><span class="mw-headline" id="Distributed_search_error_messages"> Distributed search error messages</span></a></h2>
<p>This table lists some of the more common search-time error messages associated with distributed search:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0"> Error message
</th><th bgcolor="#C0C0C0"> Meaning
</th></tr><tr><td valign="center" align="left"> <code><font size="2">status=down</font></code>
</td><td valign="center" align="left"> The specified remote peer is not available.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">status=not a splunk server</font></code>
</td><td valign="center" align="left"> The specified remote peer is not a Splunk Enterprise server.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">duplicate license</font></code>
</td><td valign="center" align="left"> The specified remote peer is using a duplicate license.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">certificate mismatch</font></code>
</td><td valign="center" align="left"> Authentication with the specified remote peer failed.
</td></tr></table><code><font size="2"><br><br></font></code>

</body><script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

        <script src="js/index.js"></script></html>
