<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="http://ogp.me/ns/fb#" charset="utf-8"><head><meta charset="UTF-8"><title></title>
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" type="text/css" href="css/main.css">
<link rel="stylesheet" href="css/style.css">
<style>
html,body {
margin: 0px;
padding: 10px;
width: 210mm;
max-width: 210mm;
overflow-x: hidden;
}
pre {
	width: 100%;
	overflow-x: hidden;
}
</style></head><body><h1>Indexing overview</h1><div class="all-questions">
<a name="aboutindexesandindexers"></a><h2> <a name="aboutindexesandindexers_indexes.2c_indexers.2c_and_indexer_clusters"><span class="mw-headline" id="Indexes.2C_indexers.2C_and_indexer_clusters"> Indexes, indexers, and indexer clusters</span></a></h2>
<p>This manual discusses Splunk Enterprise data repositories and the Splunk Enterprise components that create and manage them.
</p><p>The <b>index</b> is the repository for Splunk Enterprise data. Splunk Enterprise transforms incoming data into <b>events</b>, which it stores in indexes.
</p><p>An  <b>indexer</b> is a Splunk Enterprise instance that indexes data. For small deployments, a single instance might perform other Splunk Enterprise functions as well, such as data input and search management. In a larger, distributed <b>deployment</b>, however, the functions of data input and search management are allocated to other Splunk Enterprise components. This manual focuses exclusively on the indexing function, in the context of either a single-instance or a distributed deployment.
</p><p>An <b>indexer cluster</b> is a group of indexers configured to replicate each others' data, so that the system keeps multiple copies of all data. This process is known as  <b>index replication</b>, or indexer clustering. By maintaining multiple, identical copies of data, clusters prevent data loss while promoting data availability for searching. 
</p>
<h3> <a name="aboutindexesandindexers_indexes"><span class="mw-headline" id="Indexes"> Indexes </span></a></h3>
<p>As Splunk Enterprise indexes your data, it creates a number of files. These files fall into two main categories:
</p>
<ul><li> The raw data in compressed form (<b>rawdata</b>)
</li><li> Indexes that point to the raw data (<b>index files</b>, also referred to as <b>tsidx files</b>), plus some metadata files 
</li></ul><p>Together, these files constitute the Splunk Enterprise index. The files reside in sets of directories organized by age. These directories are called  <b>buckets</b>. For more information, see <a href="#howsplunkstoresindexes" class="external text">"How Splunk Enterprise stores indexes"</a> in this manual.
</p><p>Splunk Enterprise manages its indexes to facilitate flexible searching and fast data retrieval, eventually archiving them according to a user-configurable schedule. Splunk Enterprise handles everything with flat files; it doesn't require any third-party database software running in the background.
</p><p>During indexing, Splunk Enterprise processes incoming data to enable fast search and analysis, storing the results in the index as events. While indexing, Splunk  Enterprise enhances the data in various ways, including by:
</p>
<ul><li> Separating the datastream into individual, searchable events.
</li><li> Creating or identifying timestamps.
</li><li> Extracting fields such as host, source, and sourcetype.
</li><li> Performing user-defined actions on the incoming data, such as identifying custom fields, masking sensitive data, writing new or modified keys, applying breaking rules for multi-line events, filtering unwanted events, and routing events to specified indexes or servers.
</li></ul><p>This indexing process is also known as <b>event processing</b>.
</p><p>To start indexing, you simply specify the data inputs that you want Splunk Enterprise to index. You can add more inputs at any time, and Splunk Enterprise will begin indexing them as well. See "What Splunk Enterprise can index" in the Getting Data In Manual to learn how to add data inputs. The Getting Data In Manual also describes how you can configure index-time event processing to meet the needs of your data. See "Overview of event processing".
</p><p>Splunk Enterprise, by default, puts all user data into a single, preconfigured index. It also employs several other indexes for internal purposes. You can add new indexes and manage existing ones to meet your data requirements. See <a href="#aboutmanagingindexes" class="external text">"Manage indexes"</a> in this manual.
</p>
<h3> <a name="aboutindexesandindexers_indexers"><span class="mw-headline" id="Indexers"> Indexers </span></a></h3>
<p>The indexer is the Splunk Enterprise component that creates and manages indexes. The primary functions of an indexer are:
</p>
<ul><li> Indexing incoming data.
</li><li> Searching the indexed data.
</li></ul><p>In single-machine deployments consisting of just one Splunk Enterprise instance, the indexer also handles the <b>data input</b> and <b>search management</b> functions. This type of small deployment might handle the needs of a single department in an organization. 
</p><p>For larger-scale needs, indexing is split out from the data input function and sometimes from the search management function as well. In these larger, distributed deployments, the Splunk Enterprise indexer might reside on its own machine and handle only indexing, along with searching of its indexed data. In those cases, other Splunk Enterprise components take over the non-indexing roles. <b>Forwarders</b> consume the data, indexers index and search the data, and <b>search heads</b> coordinate searches across the set of indexers. Here's an example of a scaled-out deployment:
</p><p><br><img alt="Horizontal scaling new2 60.png" src="images/e/e0/Horizontal_scaling_new2_60.png" width="700" height="552"></p><p>For more information on using indexers in a distributed deployment, see <a href="#advancedindexingstrategy" class="external text">"Indexers in a distributed deployment"</a> in this manual.
</p><p>A Splunk indexer is simply a Splunk Enterprise instance. To learn how to install a Splunk Enterprise instance, read the Installation Manual.
</p>
<h3> <a name="aboutindexesandindexers_indexer_clusters"><span class="mw-headline" id="Indexer_clusters"> Indexer clusters </span></a></h3>
<p>An indexer cluster is a group of Splunk Enterprise nodes that, working in concert, provide a redundant indexing and searching capability.  There are three types of nodes in a cluster:
</p>
<ul><li> A single <b>master node</b> to manage the cluster. The master node is a specialized type of indexer.
</li><li> Several <b>peer nodes</b> that handle the indexing function for the cluster, indexing and maintaining multiple copies of the data and running searches across the data.
</li><li> One or more search heads to coordinate searches across all the peer nodes.
</li></ul><p>Indexer clusters feature automatic failover from one peer node to the next. This means that, if one or more peers fail, incoming data continues to get indexed and indexed data continues to be searchable.
</p><p>The first part of this manual contains configuration and management information relevant for all indexers, independent of whether they are part of a cluster. The second part of this manual, starting with the topic <a href="#aboutclusters" class="external text">"About indexer clusters and index replication"</a>, is relevant only for clusters.
</p>
<a name="howindexingworks"></a><h2> <a name="howindexingworks_how_indexing_works"><span class="mw-headline" id="How_indexing_works"> How indexing works</span></a></h2>
<p>Splunk Enterprise can <b>index</b> any type of time-series data (data with <b>timestamps</b>). When Splunk Enterprise indexes data, it breaks it into <b>events</b>, based on the timestamps.
</p>
<h3> <a name="howindexingworks_event_processing"><span class="mw-headline" id="Event_processing"> Event processing </span></a></h3>
<p><b>Event processing</b> occurs in two stages, parsing and indexing. All data that comes into Splunk Enterprise enters through the <b>parsing pipeline</b> as large (10,000 bytes) chunks. During parsing, Splunk Enterprise breaks these chunks into events which it hands off to the <b>indexing pipeline</b>, where final processing occurs.
</p><p>While parsing, Splunk Enterprise performs a number of actions, including:
</p>
<ul><li> Extracting a set of default fields for each event, including <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, and <code><font size="2">sourcetype</font></code>.
</li><li> Configuring character set encoding.
</li><li> Identifying line termination using linebreaking rules. While many events are short and only take up a line or two, others can be long. 
</li><li> Identifying timestamps or creating them if they don't exist. At the same time that it processes timestamps, Splunk identifies event boundaries.
</li><li> Splunk can be set up to mask sensitive event data (such as credit card or social security numbers) at this stage. It can also be configured to apply custom metadata to incoming events.
</li></ul><p>In the indexing pipeline, Splunk Enterprise performs additional processing, including:
</p>
<ul><li> Breaking all events into segments that can then be searched upon. You can determine the level of segmentation, which affects indexing and searching speed, search capability, and efficiency of disk compression.
</li><li> Building the index data structures.
</li><li> Writing the raw data and index files to disk, where post-indexing compression occurs.
</li></ul><p>The breakdown between parsing and indexing pipelines is of relevance mainly when deploying <b>forwarders</b>. <b>Heavy forwarders</b> can parse data and then forward the parsed data on to indexers for final indexing. Some source types - those that reference structured data - require configuration on the forwarder prior to indexing. See "Extract data from files with headers".
</p><p>For more information about events and what happens to them during the indexing process, see the chapter "Configure event processing" in the Getting Data In Manual.
</p><p><b>Note:</b> Indexing is an I/O-intensive process.
</p><p>This diagram shows the main processes inherent in indexing:
</p><p><img alt="Datapipeline 60.png" src="images/b/b7/Datapipeline_60.png" width="700" height="915"></p><p><b>Note:</b> This diagram represents a simplified view of the indexing architecture.  It provides a functional view of the architecture and does not fully describe Splunk Enterprise internals. In particular, the parsing pipeline actually consists of three pipelines: <b>parsing</b>, <b>merging</b>, and <b>typing</b>, which together handle the parsing function. The distinction can matter during troubleshooting, but does not generally affect how you configure or deploy Splunk Enterprise.
</p>
<h3> <a name="howindexingworks_what.27s_in_an_index.3f"><span class="mw-headline" id="What.27s_in_an_index.3F"> What's in an index? </span></a></h3>
<p>Splunk Enterprise stores all of the data it processes in indexes.  An index is a collection of databases, which are subdirectories located in <code><font size="2">$SPLUNK_HOME/var/lib/splunk</font></code>.  Indexes consist of two types  of files: <b>rawdata files</b> and <b>index files</b>. For detailed information, see <a href="#howsplunkstoresindexes" class="external text">"How Splunk Enterprise stores indexes"</a> in this manual.
</p>
<h3> <a name="howindexingworks_default_set_of_indexes"><span class="mw-headline" id="Default_set_of_indexes">Default set of indexes</span></a></h3>
<p>Splunk Enterprise comes with a number of preconfigured indexes, including:
</p>
<ul><li> <b>main</b>: This is the default Splunk Enterprise index. All processed data is stored here unless otherwise specified.
</li><li> <b>_internal</b>: Stores Splunk Enterprise internal logs and processing metrics.
</li><li> <b>_audit</b>: Contains events related to the file system change monitor, auditing, and all user search history.
</li></ul><p>A Splunk Enterprise administrator can create new indexes, edit index properties, remove unwanted indexes, and relocate existing indexes. Splunk Enterprise administrators manage indexes through Splunk Web, the CLI, and configuration files such as <code><font size="2">indexes.conf</font></code>. For more information, See <a href="#aboutmanagingindexes" class="external text">"Managing indexes"</a> in this manual.
</p>
<h3> <a name="howindexingworks_answers"><span class="mw-headline" id="Answers">Answers</span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has around indexing.
</p>
<a name="indextimeversussearchtime"></a><h2> <a name="indextimeversussearchtime_index_time_versus_search_time"><span class="mw-headline" id="Index_time_versus_search_time"> Index time versus search time</span></a></h2>
<p>Splunk Enterprise documentation includes many references to the terms <b>"index time"</b> and <b>"search time"</b>. These terms  distinguish between the types of processing that occur during indexing, and the types that occur when a search is run. 
</p><p>It is important to consider this distinction when administering Splunk Enterprise. For example, if you haven't yet started indexing data and you think you're going to have a lot of custom <b>source types</b> and <b>hosts</b>, you might want to get those in place before you start indexing. You can do this by defining custom source types and hosts (through rule-based source type assignment, source type overriding, input-based host assignment, and host overrides), so that these things are handled during the indexing process. 
</p><p>On the other hand, if you have already begun to index your data, you might want to handle the issue at search time. Otherwise, you will need to re-index your data, in order to apply the custom source types and hosts to your existing data as well as new data. After indexing, you can't change the host or source type assignments, but you can tag them with alternate values and manage the issue that way. 
</p><p>As a general rule, it is better to perform most knowledge-building activities, such as field extraction, at search time. Additional, custom field extraction, performed at index time, can degrade performance at both index time and search time. When you add to the number of fields extracted during indexing, the indexing process slows. Later, searches on the index are also slower, because the index has been enlarged by the additional fields, and a search on a larger index takes longer. You can avoid such performance issues by instead relying on search-time field extraction. For details on search-time field extraction, see "About fields" and "When Splunk Enterprise extracts fields" in the <i>Knowledge Manager Manual</i>.
</p>
<h3> <a name="indextimeversussearchtime_at_index_time"><span class="mw-headline" id="At_index_time">At index time</span></a></h3>
<p><b>Index-time</b> processes take place just before event data is actually indexed. 
</p><p>The following processes occur during (or before) index time:
</p>
<ul><li> Default field extraction (such as <code><font size="2">host</font></code>, <code><font size="2">source</font></code>, <code><font size="2">sourcetype</font></code>, and <code><font size="2">timestamp</font></code>)
</li><li> Static or dynamic host assignment for specific inputs
</li><li> Default host assignment overrides
</li><li> Source type customization
</li><li> Index-time field extraction
</li><li> Event timestamping
</li><li> Event linebreaking
</li><li> Event segmentation (also happens at search time)
</li></ul><h3> <a name="indextimeversussearchtime_at_search_time"><span class="mw-headline" id="At_search_time">At search time</span></a></h3>
<p><b>Search-time</b> processes take place while a search is run, as events are collected by the search. The following processes occur at search time:
</p>
<ul><li> Event segmentation (also happens at index time)
</li><li> Event type matching
</li><li> Search-time field extraction (automatic and custom field extractions, including multivalue fields and calculated fields)
</li><li> Field aliasing
</li><li> Addition of fields from lookups
</li><li> Source type renaming
</li><li> Tagging
</li></ul><a name="installanindexer"></a><h2> <a name="installanindexer_install_an_indexer"><span class="mw-headline" id="Install_an_indexer"> Install an indexer</span></a></h2>
<p>All full Splunk Enterprise instances serve as indexers by default. To learn how to install a Splunk Enterprise instance, read the Installation Manual. Then return to this manual to learn how to configure the indexer.
</p><p>If you plan to deploy an indexer in a distributed deployment, next read the topic, <a href="#advancedindexingstrategy" class="external text">Indexers in a distributed environment</a>.
</p>
<a name="advancedindexingstrategy"></a><h2> <a name="advancedindexingstrategy_indexers_in_a_distributed_deployment"><span class="mw-headline" id="Indexers_in_a_distributed_deployment">Indexers in a distributed deployment </span></a></h2>
<p><b>Important:</b> To better understand this topic, you should be familiar with Splunk Enterprise distributed environments, covered in the Distributed Deployment Manual.
</p><p>The indexer is the Splunk Enterprise component that creates and manages indexes. The primary functions of an indexer are:
</p>
<ul><li> Indexing incoming data.
</li><li> Searching the indexed data.
</li></ul><p>In single-machine deployments consisting of just one Splunk Enterprise instance, the indexer also handles the <b>data input</b> and <b>search management</b> functions. 
</p><p>For larger-scale needs, indexing is split out from the data input function and sometimes from the search management function as well. In these larger, distributed deployments, the indexer might reside on its own machine and handle only indexing, along with searching of its indexed data. In those cases, other Splunk Enterprise components take over the non-indexing roles. 
</p><p>For instance, you might have a set of Windows and Linux machines generating events, which need to go to a central indexer for consolidation. Usually the best way to do this is to install a lightweight instance of Splunk Enterprise, known as a <b>forwarder</b>, on each of the event-generating machines. These forwarders handle data input and send the data across the network to the indexer residing on its own machine.
</p><p>Similarly, in cases where you have a large amount of indexed data and numerous concurrent users searching on it, it can make sense to split off the search management function from indexing. In this type of scenario, known as <b>distributed search</b>, one or more <b>search heads</b> distribute search requests across multiple indexers. The indexers still perform the actual searching of their own indexes, but the search heads manage the overall search process across all the indexers and present the consolidated search results to the user.
</p><p>Here's an example of a scaled-out deployment:
</p><p><img alt="Horizontal scaling new2 60.png" src="images/e/e0/Horizontal_scaling_new2_60.png" width="700" height="552"></p><p><br>
While the fundamental issues of indexing and event processing remain the same for distributed deployments, it is important to take into account deployment needs when planning your indexing strategy.
</p>
<h3> <a name="advancedindexingstrategy_forward_data_to_an_indexer"><span class="mw-headline" id="Forward_data_to_an_indexer"> Forward data to an indexer </span></a></h3>
<p>To forward remote data to an indexer, you use forwarders, which are Splunk Enterprise instances that receive data inputs and then consolidate and send the data to a Splunk Enterprise indexer. Forwarders come in two flavors:
</p>
<ul><li><b>Universal forwarders</b>. These maintain a small footprint on their host machine. They perform minimal processing on the incoming data streams before forwarding them on to an indexer, also known as the <b>receiver</b>.
</li><li><b>Heavy forwarders</b>. These retain most of the functionality of a full Splunk Enterprise instance. They can parse data before forwarding it to the receiving indexer. (See <a href="#howindexingworks" class="external text">"How indexing works"</a> for the distinction between parsing and indexing.) They can store indexed data locally and also forward the parsed data to a receiver for final indexing on that machine as well. 
</li></ul><p>Both types of forwarders tag data with metadata such as host, source, and source type, before forwarding it on to the indexer.
</p><p>Forwarders allow you to use resources efficiently when processing large quantities or disparate types of data coming from remote sources. They also enable a number of interesting deployment topologies, by offering capabilities for <b>load balancing</b>, data <b>filtering</b>, and <b>routing</b>.
</p><p>For an extended discussion of forwarders, including configuration and detailed use cases, read the Forwarding Data Manual.
</p>
<h3> <a name="advancedindexingstrategy_search_across_multiple_indexers"><span class="mw-headline" id="Search_across_multiple_indexers">Search across multiple indexers</span></a></h3>
<p>In distributed search, search heads send search requests to indexers and then merge the results back to the user. This is useful for a number of purposes, including horizontal scaling, access control, and managing geo-dispersed data.
</p><p>For an extended discussion of distributed search and search heads, including configuration and detailed use cases, see the Distributed Search manual.
</p><p><b>Indexer clusters</b> also use search heads to coordinate searches across the cluster's peer nodes. See <a href="#aboutclusters" class="external text">"About indexer clusters and index replication"</a>.
</p>
<h3> <a name="advancedindexingstrategy_deploy_indexers_in_a_distributed_environment"><span class="mw-headline" id="Deploy_indexers_in_a_distributed_environment">Deploy indexers in a distributed environment</span></a></h3>
<p>To implement a distributed environment similar to the diagram earlier in this topic, you need to install and configure three types of components:
</p>
<ul><li> Indexers
</li><li> Forwarders (typically, universal forwarders)
</li><li> Search head(s)
</li></ul><h4><font size="3"><b><i> <a name="advancedindexingstrategy_install_and_configure_the_indexers"><span class="mw-headline" id="Install_and_configure_the_indexers">Install and configure the indexers</span></a></i></b></font></h4>
<p>By default, all full Splunk Enterprise instances serve as indexers. For horizontal scaling, you can install multiple indexers on separate machines. 
</p><p>To learn how to install a Splunk Enterprise instance, read the Installation Manual.
</p><p>Then return to this manual for information on configuring each individual indexer to meet the needs of your specific deployment.
</p>
<h4><font size="3"><b><i> <a name="advancedindexingstrategy_install_and_configure_the_forwarders"><span class="mw-headline" id="Install_and_configure_the_forwarders">Install and configure the forwarders</span></a></i></b></font></h4>
<p>A typical distributed deployment has a large number of forwarders feeding data to a few indexers. For most forwarding purposes, the universal forwarder is the best choice. The universal forwarder is a separate downloadable from the full Splunk Enterprise instance. 
</p><p>To learn how to install and configure forwarders, read the Forwarding Data Manual.
</p>
<h4><font size="3"><b><i> <a name="advancedindexingstrategy_install_and_configure_the_search_head.28s.29"><span class="mw-headline" id="Install_and_configure_the_search_head.28s.29"> Install and configure the search head(s)</span></a></i></b></font></h4>
<p>You can install one or more search heads to handle your distributed search needs. Search heads are just full Splunk Enterprise instances that have been specially configured. 
</p><p>To learn how to configure a search head, read the Distributed Search manual.
</p>
<h4><font size="3"><b><i> <a name="advancedindexingstrategy_other_deployment_tasks"><span class="mw-headline" id="Other_deployment_tasks">Other deployment tasks</span></a></i></b></font></h4>
<p>You need to configure Splunk Enterprise licensing by designating a <b>license master</b>. See the chapter <b>Configure Splunk Enterprise licenses</b> in the Admin Manual for more information.
</p><p>You can use the Splunk Enterprise <b>deployment server</b> to simplify the job of updating the deployment components. For details on how to configure a deployment server, see the manual <b>Updating Splunk Enterprise Instances</b>.
</p>
<h4><font size="3"><b><i> <a name="advancedindexingstrategy_install_a_cluster_of_indexers"><span class="mw-headline" id="Install_a_cluster_of_indexers">Install a cluster of indexers</span></a></i></b></font></h4>
<p>If data availability, data fidelity, and data recovery are key issues for your deployment, then you should consider deploying an indexer cluster, rather than a series of individual indexers. For further information, see <a href="#aboutclusters" class="external text">"About indexer clusters and index replication"</a> in this manual.
</p>
<h1>Manage indexes</h1><a name="aboutmanagingindexes"></a><h2> <a name="aboutmanagingindexes_about_managing_indexes"><span class="mw-headline" id="About_managing_indexes"> About managing indexes</span></a></h2>
<p>When you add data, the indexer processes it and stores it in an <b>index</b>. By default, data you feed to an indexer is stored in the <b>main</b> index, but you can create and specify other indexes for different data inputs.
</p><p>An index is a collection of directories and files. These are located under <code><font size="2">$SPLUNK_HOME/var/lib/splunk</font></code>. Index directories are also called <b>buckets</b> and are organized by age. For detailed information on index storage, see <a href="#howsplunkstoresindexes" class="external text">"How Splunk Enterprise stores indexes"</a>.
</p><p>In addition to the <b>main</b> index, Splunk Enterprise comes preconfigured with a number of internal indexes. Internal indexes are named starting with an underscore (_). To see a full list of indexes in Splunk Web, click the <b>Settings</b> link in the upper portion of Splunk Web and then select <b>Indexes</b>. The list includes:
</p>
<ul><li> <b>main</b>: The default Splunk Enterprise index.  All processed external data is stored here unless otherwise specified.
</li><li> <b>_internal</b>: This index includes Splunk Enterprise internal logs and metrics.
</li><li> <b>_audit</b>: Events from the file system change monitor, auditing, and all user search history.
</li></ul><p>A number of topics in this manual describe ways you can manage your indexes. In particular the following topics are helpful in index management:
</p>
<ul><li>  <a href="#setupmultipleindexes" class="external text">Set up multiple indexes</a>
</li><li>  <a href="#removedatafromsplunk" class="external text">Remove indexes and data from Splunk</a>
</li><li>  <a href="#configureindexstorage" class="external text">Configure index storage</a>
</li><li>  <a href="#moveanindex" class="external text">Move the index database</a>
</li><li>  <a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a>
</li><li>  <a href="#configureindexstoragesize" class="external text">Configure index size</a>
</li><li>  <a href="#setlimitsondiskusage" class="external text">Set limits on disk usage</a>
</li><li>  <a href="#backupindexeddata" class="external text">Back up indexed data</a>
</li><li>  <a href="#setaretirementandarchivingpolicy" class="external text">Set a retirement and archiving policy</a>
</li></ul><h3> <a name="aboutmanagingindexes_for_more_information_on_the_indexing_process"><span class="mw-headline" id="For_more_information_on_the_indexing_process">For more information on the indexing process</span></a></h3>
<p>To learn more about indexes, see:
</p>
<ul><li> The topic "<a href="#howindexingworks" class="external text">How indexing works</a>" in this manual. 
</li><li> The topic <a href="#howsplunkstoresindexes" class="external text">"How Splunk stores indexes"</a> in this manual.
</li><li> The chapter <a href="#aboutclusters" class="external text">"About clusters and index replication"</a> in this manual.
</li><li> The chapter "Configure event processing" in the Getting Data In Manual.
</li><li> The chapter "Set up and use summary indexes" in the Knowledge Manager Manual, for information on working with extremely large datasets.
</li></ul><a name="setupmultipleindexes"></a><h2> <a name="setupmultipleindexes_set_up_multiple_indexes"><span class="mw-headline" id="Set_up_multiple_indexes"> Set up multiple indexes</span></a></h2>
<p>The <code><font size="2">main</font></code> index, by default, holds all your events. The indexer also has a number of other indexes for use by its internal systems, as well as for additional features such as summary indexing and event auditing.
</p><p>With a Splunk Enterprise license, you can add an unlimited number of additional indexes.  The <code><font size="2">main</font></code> index serves as the default index for any input or search command that doesn't specify an index, although you can change the default. You can add indexes using Splunk Web, the CLI, or indexes.conf.
</p><p>This topic covers:
</p>
<ul><li> The reasons why you might want multiple indexes.
</li><li> How to create new indexes.
</li><li> How to send events to specific indexes.
</li><li> How to search specific indexes.
</li></ul><h3> <a name="setupmultipleindexes_why_have_multiple_indexes.3f"><span class="mw-headline" id="Why_have_multiple_indexes.3F">Why have multiple indexes?</span></a></h3>
<p>There are several key reasons for having multiple indexes:
</p>
<ul><li> To control user access.
</li><li> To accommodate varying retention policies.
</li><li> To speed searches in certain situations.
</li></ul><p>The main reason you'd set up multiple indexes is to control user access to the data that's in them. When you assign users to <b>roles</b>, you can limit user searches to specific indexes based on the role they're in. 
</p><p>In addition, if you have different policies for retention for different sets of data, you might want to send the data to different indexes and then <a href="#setaretirementandarchivingpolicy" class="external text">set a different archive or retention policy</a> for each index. 
</p><p>Another reason to set up multiple indexes has to do with the way search works. If you have both a high-volume/high-noise data source and a low-volume data source feeding into the same index, and you search mostly for events from the low-volume data source, the search speed will be slower than necessary, because the indexer also has to search through all the data from the high-volume source. To mitigate this, you can create dedicated indexes for each data source and <a href="#setupmultipleindexes_send_all_events_from_a_data_input_to_a_specific_index" class="external text">send data from each source to its dedicated index</a>. Then, you can specify which index to search on. You'll probably notice an increase in search speed.
</p>
<h3> <a name="setupmultipleindexes_create_and_edit_indexes"><span class="mw-headline" id="Create_and_edit_indexes"> Create and edit indexes </span></a></h3>
<p>You can create or edit indexes with Splunk Web, the CLI, or by editing <code><font size="2">indexes.conf</font></code> directly.
</p><p><b>Note:</b> To add a new index to an indexer cluster, you must directly edit <code><font size="2">indexes.conf</font></code>. You cannot add an index via Splunk Web or the CLI. For information on how to configure <code><font size="2">indexes.conf</font></code> for clusters, see <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>. That topic includes an example of creating a new cluster index.
</p>
<h4><font size="3"><b><i> <a name="setupmultipleindexes_use_splunk_web"><span class="mw-headline" id="Use_Splunk_Web"> Use Splunk Web </span></a></i></b></font></h4>
<p><b>1.</b> In Splunk Web, navigate to <b>Settings &gt; Indexes</b> and click <b>New</b>. 
</p><p><b>2.</b> To create a new index, enter:
</p>
<ul><li> A name for the index. User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen, or contain the word "kvstore". 
</li><li> The path locations for index data storage: 
<ul><li> Home path; leave blank for default <code><font size="2">$SPLUNK_DB/&lt;index_name&gt;/db</font></code>
</li><li> Cold db path; leave blank for default <code><font size="2">$SPLUNK_DB/&lt;index_name&gt;/colddb</font></code>
</li><li> Thawed/resurrected db path, leave blank for default <code><font size="2">$SPLUNK_DB/&lt;index_name&gt;/thaweddb</font></code>
</li></ul></li><li> The maximum size of the entire index. Defaults to 500000MB.
</li><li> The maximum size of the hot (currently written to) portion of this index.  When setting the maximum size, you should use <code><font size="2">auto_high_volume</font></code> for high volume indexes (such as the main index); otherwise, use <code><font size="2">auto</font></code>.
</li><li> The frozen archive path. Set this field if you want to archive frozen buckets. For information on bucket archiving, see <a href="#automatearchiving" class="external text">"Archive indexed data"</a>.
</li></ul><p><b>Note:</b> For detailed information on each of these settings, see <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</p><p><b>3.</b> Click <b>Save</b>.
</p><p>You can edit an index by clicking on the index name in the <b>Indexes</b> section of the <b>Settings</b> menu in Splunk Web. Properties that you cannot change in Splunk Web are grayed out.  To change those properties, edit <code><font size="2">indexes.conf</font></code>, then restart the indexer.  
</p><p><b>Note:</b> Some index properties are configurable only by editing the <code><font size="2">indexes.conf</font></code> file. Check the <code><font size="2">indexes.conf</font></code> topic for a complete list of properties.
</p>
<h4><font size="3"><b><i> <a name="setupmultipleindexes_use_the_cli"><span class="mw-headline" id="Use_the_CLI"> Use the CLI </span></a></i></b></font></h4>
<p>Navigate to the <code><font size="2">$SPLUNK_HOME/bin/</font></code> directory and use the <code><font size="2">add index</font></code> command. You do not need to stop the indexer first. 
</p><p>To add a new index called "fflanda", enter the following command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add index fflanda<br></font></code>
</div>
<p><b>Note:</b>  User-defined index names must consist of only numbers, lowercase letters, underscores, and hyphens. They cannot begin with an underscore or hyphen. 
</p><p>If you do not want to use the default path for your new index, you can use parameters to specify a new location:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add index foo -homePath /your/path/foo/db -coldPath /your/pat/foo/colddb<br>&nbsp;&nbsp;&nbsp;&nbsp;-thawedPath /your/path/foo/thawedDb<br></font></code>
</div>
<p>You can also edit an index's properties from the CLI. For example, to edit an index called "fflanda" using the CLI, type:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit index fflanda -&lt;parameter&gt; &lt;value&gt;<br></font></code>
</div>
<p>For detailed information on index settings, see <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</p>
<h4><font size="3"><b><i> <a name="setupmultipleindexes_edit_indexes.conf"><span class="mw-headline" id="Edit_indexes.conf"> Edit indexes.conf </span></a></i></b></font></h4>
<p>To add a new index, add a stanza to <code><font size="2">indexes.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>, identified by the name of the new index.  For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[newindex]<br>homePath=&lt;path for hot and warm buckets&gt;<br>coldPath=&lt;path for cold buckets&gt;<br>thawedPath=&lt;path for thawed buckets&gt;<br>...<br></font></code>
</div>
<p>For information on index settings, see <a href="#configureindexstorage" class="external text">"Configure index storage"</a> and the indexes.conf spec file.
</p><p><b>Note:</b> The index name, specified in the stanza, must consist of only numbers, lowercase letters, underscores, and hyphens. User-defined index names cannot begin with an underscore or hyphen. 
</p><p>You must restart the indexer after editing <code><font size="2">indexes.conf</font></code>.
</p><p><b>Important:</b> For information on adding or editing index configurations on cluster nodes, see <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>.
</p>
<h3> <a name="setupmultipleindexes_send_events_to_specific_indexes"><span class="mw-headline" id="Send_events_to_specific_indexes"> Send events to specific indexes</span></a></h3>
<p>By default, all external events go to the index called <b>main</b>. However, you might want to send some events to other indexes.  For example, you might want to route all data from a particular input to its own index.  Or you might want to segment data or send event data from a noisy source to an index that is dedicated to receiving it. 
</p><p><b>Important:</b> To send events to a specific index, the index must already exist on the indexer. If you route any events to an index that doesn't exist, the indexer will drop those events.
</p>
<h4><font size="3"><b><i> <a name="setupmultipleindexes_send_all_events_from_a_data_input_to_a_specific_index"><span class="mw-headline" id="Send_all_events_from_a_data_input_to_a_specific_index">Send all events from a data input to a specific index</span></a></i></b></font></h4>
<p>To send all events from a particular data input to a specific index, add the following line to the input's stanza in inputs.conf on the Splunk Enterprise component where the data is entering the system: either the indexer itself or a forwarder sending data to the indexer:
</p><p><code><font size="2">index = &lt;index_name&gt;</font></code>
</p><p>The following example <code><font size="2">inputs.conf</font></code> stanza sends all data from <code><font size="2">/var/log</font></code> to an index named <code><font size="2">fflanda</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[monitor:///var/log]<br>disabled = false<br>index = fflanda<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="setupmultipleindexes_route_specific_events_to_a_different_index"><span class="mw-headline" id="Route_specific_events_to_a_different_index"> Route specific events to a different index </span></a></i></b></font></h4>
<p>Just as you can route events to specific queues, you can also route specific events to specific indexes. You configure this on the indexer itself, <i>not</i> on the forwarder sending data to the indexer, if any.
</p><p>To route certain events to a specific index, edit props.conf and transforms.conf on the indexer:
</p><p><b>1.</b> Identify a common attribute for the events that can be used to differentiate them.
</p><p><b>2.</b> In <code><font size="2">props.conf</font></code>, create a stanza for the source, source type, or host.  This stanza specifies a <code><font size="2">transforms_name</font></code> that corresponds to a regex-containing stanza you will create in <code><font size="2">transforms.conf</font></code>. 
</p><p><b>3.</b> In <code><font size="2">transforms.conf</font></code>, create an stanza named with the <code><font size="2">transforms_name</font></code> you specified in step 2. This stanza:
</p>
<ul><li> Specifies a regular expression that matches the identified attribute from step 1.
</li><li> Specifies the alternate index that events matching the attribute should be routed to.
</li></ul><p>The sections below fill out the details for steps 2 and 3.
</p>
<h5> <a name="setupmultipleindexes_edit_props.conf"><span class="mw-headline" id="Edit_props.conf"> Edit props.conf </span></a></h5>
<p>Add the following stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/props.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;spec&gt;]<br>TRANSFORMS-&lt;class_name&gt; = &lt;transforms_name&gt;<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">&lt;spec&gt;</font></code> is one of the following:
<ul><li> <code><font size="2">&lt;sourcetype&gt;</font></code>, the sourcetype of an event
</li><li> <code><font size="2">host::&lt;host&gt;</font></code>, where <code><font size="2">&lt;host&gt;</font></code> is the host for an event
</li><li> <code><font size="2">source::&lt;source&gt;</font></code>, where <code><font size="2">&lt;source&gt;</font></code> is the source for an event
</li></ul></li></ul><ul><li> <code><font size="2">&lt;class_name&gt;</font></code> is any unique identifier.
</li></ul><ul><li> <code><font size="2">&lt;transforms_name&gt;</font></code> is whatever unique identifier you want to give to your transform in <code><font size="2">transforms.conf</font></code>.
</li></ul><h5> <a name="setupmultipleindexes_edit_transforms.conf"><span class="mw-headline" id="Edit_transforms.conf"> Edit transforms.conf </span></a></h5>
<p>Add the following stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/transforms.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;transforms_name&gt;]<br>REGEX = &lt;your_custom_regex&gt;<br>DEST_KEY = _MetaData:Index<br>FORMAT = &lt;alternate_index_name&gt;<br></font></code>
</div>
<p>Note the following: 
</p>
<ul><li> <code><font size="2">&lt;transforms_name&gt;</font></code> must match the <code><font size="2">&lt;transforms_name&gt;</font></code> identifier you specified in <code><font size="2">props.conf</font></code>. 
</li></ul><ul><li> <code><font size="2">&lt;your_custom_regex&gt;</font></code> must provide a match for the attribute you identified earlier, in step 1.
</li></ul><ul><li> <code><font size="2">DEST_KEY</font></code> must be set to the index attribute <code><font size="2">_MetaData:Index</font></code>. 
</li></ul><ul><li> <code><font size="2">&lt;alternate_index_name&gt;</font></code> specifies the alternate index that the events will route to.
</li></ul><h4><font size="3"><b><i> <a name="setupmultipleindexes_example"><span class="mw-headline" id="Example"> Example </span></a></i></b></font></h4>
<p>This examples routes events of <code><font size="2">windows_snare_log</font></code> source type to the appropriate index based on their log types. "Application" logs will go to an alternate index, while all other log types, such as "Security", will go to the default index. 
</p><p>To make this determination, it uses <code><font size="2">props.conf</font></code> to direct events of <code><font size="2">windows_snare_log</font></code> source type through the <code><font size="2">transforms.conf</font></code> stanza named "AppRedirect", where a regex then looks for the log type, "Application". Any event with a match on "Application" in the appropriate location is routed to the alternate index, "applogindex". All other events go to the default index.
</p>
<h5> <a name="setupmultipleindexes_1._identify_an_attribute"><span class="mw-headline" id="1._Identify_an_attribute"> 1. Identify an attribute </span></a></h5>
<p>The events in this example look like this:
</p>
<div class="samplecode">
<code><font size="2"><br>web1.example.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MSWinEventLog&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Application&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;721&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wed Sep 06 17:05:31 2006<br>4156&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MSDTC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unknown User&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Information&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WEB1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Printers&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;String<br>message: Session idle timeout over, tearing down the session.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;179<br><br>web1.example.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MSWinEventLog&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Security&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;722&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wed Sep 06 17:59:08 2006<br>576&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Security&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SYSTEM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;User&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Success Audit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WEB1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Privilege Use<br>Special privileges assigned to new logon: &nbsp;&nbsp;&nbsp;&nbsp;User Name: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Domain: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logon<br>ID: (0x0,0x4F3C5880) &nbsp;&nbsp;&nbsp;&nbsp;Assigned: SeBackupPrivilege &nbsp;&nbsp;SeRestorePrivilege<br>SeDebugPrivilege &nbsp;&nbsp;SeChangeNotifyPrivilege &nbsp;&nbsp;SeAssignPrimaryTokenPrivilege 525<br></font></code>
</div>
<p>Some events contain the value "Application", while others contain the value "Security" in the same location. 
</p>
<h5> <a name="setupmultipleindexes_2._edit_props.conf"><span class="mw-headline" id="2._Edit_props.conf"> 2. Edit props.conf </span></a></h5>
<p>Add this stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/props.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[windows_snare_syslog]<br>TRANSFORMS-index = AppRedirect<br></font></code>
</div>
<p>This directs events of <code><font size="2">windows_snare_syslog</font></code> sourcetype to the <code><font size="2">AppRedirect</font></code> stanza in <code><font size="2">transforms.conf</font></code>.
</p>
<h5> <a name="setupmultipleindexes_3._edit_transforms.conf"><span class="mw-headline" id="3._Edit_transforms.conf"> 3. Edit transforms.conf </span></a></h5>
<p>Add this stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/transforms.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[AppRedirect]<br>REGEX = MSWinEventLog\s+\d+\s+Application<br>DEST_KEY = _MetaData:Index<br>FORMAT = applogindex<br></font></code>
</div>
<p>This stanza processes the events directed here by <code><font size="2">props.conf</font></code>. Events that match the regex (because they contain the string "Application" in the specified location) get routed to the alternate index, "applogindex". All other events route as usual to the default index.
</p>
<h3> <a name="setupmultipleindexes_search_a_specific_index"><span class="mw-headline" id="Search_a_specific_index">Search a specific index</span></a></h3>
<p>When the indexer searches, it targets the default index (by default, <b>main</b>), unless the search explicitly specifies an index. For example, this search command searches in the <code><font size="2">hatch</font></code> index:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=hatch userid=henry.gale</font></code><br></div>
<p>You can also specify an alternate default index for a given role to search when you create or edit that role.
</p>
<a name="removedatafromsplunk"></a><h2> <a name="removedatafromsplunk_remove_indexes_and_indexed_data"><span class="mw-headline" id="Remove_indexes_and_indexed_data"> Remove indexes and indexed data </span></a></h2>
<p>You can remove indexed data or even entire indexes from the indexer. These are the main options:
</p>
<ul><li> Delete events from subsequent searches.
</li><li> Remove all data from one or more indexes. 
</li><li> Remove or disable an entire index.
</li><li> Delete older data, based on a retirement policy.
</li></ul><p><b>Caution:</b> Removing data is irreversible. If you want to get your data back once you've removed data using any of the techniques described in this topic, you must re-index the applicable data sources.
</p>
<h3> <a name="removedatafromsplunk_delete_events_from_subsequent_searches"><span class="mw-headline" id="Delete_events_from_subsequent_searches"> Delete events from subsequent searches </span></a></h3>
<p>The Splunk search language provides the special operator <code><font size="2">delete</font></code> to delete event data from subsequent searches. Before using <code><font size="2">delete</font></code> , read this section carefully. 
</p><p><b>Note:</b> You cannot run the <code><font size="2">delete</font></code> operator during a real-time search; you cannot delete events as they come in. If you try to use <code><font size="2">delete</font></code> during a real-time search, Splunk Enterprise will display an error. 
</p>
<h4><font size="3"><b><i> <a name="removedatafromsplunk_who_can_delete.3f"><span class="mw-headline" id="Who_can_delete.3F">Who can delete?</span></a></i></b></font></h4>
<p>The <code><font size="2">delete</font></code> operator can only be run by a user with the "delete_by_keyword" <b>capability</b>. By default, Splunk Enterprise ships with a special <b>role</b>, "can_delete" that has this capability (and no others). The admin role does not have this capability by default. It's recommended that you create a special user that you log into when you intend to delete index data. 
</p><p>For more information, refer to "Add and edit roles" in the Security Manual.
</p>
<h4><font size="3"><b><i> <a name="removedatafromsplunk_how_to_delete"><span class="mw-headline" id="How_to_delete">How to delete</span></a></i></b></font></h4>
<p>First run a search that returns the events you want deleted. Make sure that this search returns <i>only</i> the events you want to delete, and no other events. Once you're certain of that, you can pipe the results of the search to the <code><font size="2">delete</font></code> operator.
</p><p>For example, if you want to remove the events you've indexed from a source called <code><font size="2">/fflanda/incoming/cheese.log</font></code> so that they no longer appear in searches, do the following:
</p><p><b>1.</b> Disable or remove that source so that it no longer gets indexed.
</p><p><b>2.</b> Search for events from that source in your index:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source="/fflanda/incoming/cheese.log"</font></code><br></div>
<p><b>3.</b> Look at the results to confirm that this is the data you want to delete.
</p><p><b>4.</b> Once you've confirmed that this is the data you want to delete, pipe the search to <code><font size="2">delete</font></code>:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source="/fflanda/incoming/cheese.log" | delete </font></code><br></div>
<p>See the page about the delete operator in the Search Reference Manual for more examples. 
</p><p><b>Note:</b> When running Splunk on Windows, substitute the forward slashes (/) in the examples  with backslashes (\).
</p><p>Piping a search to the <code><font size="2">delete</font></code> operator marks all the events returned by that search so that subsequent searches do not return them. No user (even with admin permissions) will be able to see this data when searching.
</p><p><b>Note:</b> Piping to <code><font size="2">delete</font></code> does not reclaim disk space. The data is not actually removed from the index; it is just invisible to searches. 
</p><p>The <code><font size="2">delete</font></code> operator does not update the metadata of the events, so any metadata searches will still include the events although they are not searchable. The main <b>All indexed data</b> dashboard will still show event counts for the deleted sources, hosts, or sourcetypes.
</p>
<h4><font size="3"><b><i> <a name="removedatafromsplunk_the_delete_operation_and_indexer_clusters"><span class="mw-headline" id="The_delete_operation_and_indexer_clusters">The delete operation and indexer clusters</span></a></i></b></font></h4>
<p>In the normal course of index replication, the effects of a <code><font size="2">delete</font></code> operation get quickly propagated across all bucket copies in the cluster, typically within a few seconds or minutes, depending on the cluster load and amount of data and buckets affected by the <code><font size="2">delete</font></code> operation. During this propagation interval, a search can return results that have already been deleted. 
</p><p>Also, if a peer that had primary bucket copies at the time of the <code><font size="2">delete</font></code> operation goes down before all the results have been propagated, some of the deletes will be lost. In that case, you must rerun the operation after the primary copies from the downed peer have been reassigned.
</p>
<h3> <a name="removedatafromsplunk_remove_data_from_one_or_all_indexes"><span class="mw-headline" id="Remove_data_from_one_or_all_indexes">Remove data from one or all indexes</span></a></h3>
<p>To delete indexed data permanently from your disk, use the CLI <code><font size="2">clean</font></code> command. This command completely deletes the data in one or all indexes, depending on whether you provide an <code><font size="2">&lt;index_name&gt;</font></code> argument. Typically, you run <code><font size="2">clean</font></code> before re-indexing all your data.
</p><p><b>Note:</b> The <code><font size="2">clean</font></code> command does not work on clustered indexes.
</p>
<h4><font size="3"><b><i> <a name="removedatafromsplunk_how_to_use_the_clean_command"><span class="mw-headline" id="How_to_use_the_clean_command">How to use the clean command</span></a></i></b></font></h4>
<p>Here are the main ways to use the <code><font size="2">clean</font></code> command:
</p>
<ul><li> To access the help page for <code><font size="2">clean</font></code>, type:
</li></ul><div class="samplecode">
<code><font size="2"><br>splunk help clean<br></font></code>
</div>
<ul><li> To permanently remove event data from <b>all indexes</b>, type:
</li></ul><div class="samplecode">
<code><font size="2"><br>splunk clean eventdata<br></font></code>
</div> 
<ul><li> To permanently remove event data from <b>a single index</b>, type:
</li></ul><div class="samplecode">
<code><font size="2"><br>splunk clean eventdata -index &lt;index_name&gt;<br></font></code>
</div>
<p>where <code><font size="2">&lt;index_name&gt;</font></code> is the name of the targeted index. 
</p>
<ul><li> Add the <code><font size="2">-f</font></code> parameter to force <code><font size="2">clean</font></code> to skip its confirmation prompts.
</li></ul><p><b>Important:</b> You must stop the indexer before you run the <code><font size="2">clean</font></code> command.
</p><p><b>Note:</b> In pre-5.0 versions of Splunk Enterprise, running the <code><font size="2">clean</font></code> command caused the indexer to reset the next bucket ID value for the index to 0. Starting with version 5.0, this is no longer the case. So, if the latest bucket ID was 3, after you run <code><font size="2">clean</font></code>, the next bucket ID will be 4, not 0. For more information on bucket naming conventions and the bucket ID, refer to "<a href="#howsplunkstoresindexes_what_the_index_directories_look_like" class="external text">What the index directories look like</a>" in this manual.
</p>
<h4><font size="3"><b><i> <a name="removedatafromsplunk_examples"><span class="mw-headline" id="Examples"> Examples </span></a></i></b></font></h4>
<p>This example removes event data from all indexes:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br>splunk clean eventdata <br></font></code>
</div>
<p>This example removes event data from the <code><font size="2">_internal</font></code> index and forces Splunk to skip the confirmation prompt:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br>splunk clean eventdata -index _internal -f<br></font></code>
</div>
<h3> <a name="removedatafromsplunk_remove_an_index_entirely"><span class="mw-headline" id="Remove_an_index_entirely"> Remove an index entirely </span></a></h3>
<p>To remove an index entirely (and not just the data contained in it), use the CLI command <code><font size="2">remove index</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk remove index &lt;index_name&gt;<br></font></code>
</div>
<p>This command deletes the index's data directories and removes the index's stanza from <code><font size="2">indexes.conf</font></code>.
</p><p>Before running the command, look through all <code><font size="2">inputs.conf</font></code> files (on your indexer and on any forwarders sending data to the indexer) and make sure that none of the stanzas are directing data to the index you plan to delete. In other words, if you want to delete an index called "nogood", make sure the following attribute/value pair does not appear in any of your input stanzas: <code><font size="2">index=nogood</font></code>. Once the index has been deleted, the indexer will discard any data still being sent to that index.
</p><p>When you run <code><font size="2">remove index</font></code>, it first warns you if any of the inputs on the indexer (but not on any forwarders) are still configured to send data to the specified index. You'll see a message like this:
</p>
<div class="samplecode">
<code><font size="2"><br>03-28-2012 23:59:22.973 -0700 WARN &nbsp;IndexAdminHandler - Events from the following 3 inputs will now be discarded, since they had targeted index=zzz:<br>03-28-2012 23:59:22.973 -0700 WARN &nbsp;IndexAdminHandler - type: monitor, id: /home/v/syslog-avg-1000-lines<br>03-28-2012 23:59:22.973 -0700 WARN &nbsp;IndexAdminHandler - type: monitor, id: /mnt/kickstart/internal/fermi<br>03-28-2012 23:59:22.973 -0700 WARN &nbsp;IndexAdminHandler - type: monitor, id: /mnt/kickstart/internal/flights<br></font></code>
</div>
<p>You can run <code><font size="2">remove index</font></code> while <code><font size="2">splunkd</font></code> is running. You do not need to restart <code><font size="2">splunkd</font></code> after the command completes.
</p><p>The index deletion process is ordinarily fast, but the duration depends on several factors:
</p>
<ul><li> The amount of data being deleted. 
</li><li> Whether you are currently performing heavy writes to other indexes on the same disk. 
</li><li> Whether you have a large number of small <code><font size="2">.tsidx</font></code> files in the index you're deleting.
</li></ul><h3> <a name="removedatafromsplunk_disable_an_index_without_deleting_it"><span class="mw-headline" id="Disable_an_index_without_deleting_it"> Disable an index without deleting it</span></a></h3>
<p>Use the <code><font size="2">disable index</font></code> CLI command to disable an index without deleting it:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk disable index &lt;index_name&gt;<br></font></code>
</div>
<p>Unlike the <code><font size="2">remove index</font></code> command, <code><font size="2">disable index</font></code> does not delete index data, and it is reversible (with the <code><font size="2">enable index</font></code> command). However, once an index is disabled, <code><font size="2">splunkd</font></code> will no longer accept data targeted at it.
</p><p>You can also disable an index in Splunk Web.  To do this, navigate to <b>Settings &gt; Indexes</b> and click <b>Disable</b> to the right of the index you want to disable.
</p>
<h3> <a name="removedatafromsplunk_delete_older_data_based_on_retirement_policy"><span class="mw-headline" id="Delete_older_data_based_on_retirement_policy"> Delete older data based on retirement policy</span></a></h3>
<p>When data in an index reaches a specified age or when the index grows to a specified size, it rolls to the "frozen" state, at which point the indexer deletes it from the index. Just before deleting the data, the indexer can move it to an archive, depending on how you configure your retirement policy.
</p><p>For more information, refer to "<a href="#setaretirementandarchivingpolicy" class="external text">Set a retirement and archiving policy</a>" in this manual.
</p>
<a name="optimizeindexes"></a><h2> <a name="optimizeindexes_optimize_indexes"><span class="mw-headline" id="Optimize_indexes"> Optimize indexes</span></a></h2>
<p>While the indexer is indexing data, one or more instances of the <code><font size="2">splunk-optimize</font></code> process will run intermittently, merging index files together to optimize performance when searching the data. The <code><font size="2">splunk-optimize</font></code> process can use a significant amount of cpu but only briefly. You can reduce the number of concurrent instances of <code><font size="2">splunk-optimize</font></code> by changing the value of <code><font size="2">maxConcurrentOptimizes</font></code> in <code><font size="2">indexes.conf</font></code>, but this is not typically necessary. 
</p><p>If <code><font size="2">splunk-optimize</font></code> does not run frequently enough, searching will be less efficient.
</p><p><code><font size="2">splunk-optimize</font></code> runs only on hot  <b>buckets</b>. You can run it on warm buckets manually, if you find one with a larger number of index (<code><font size="2">.tsidx</font></code>) files; typically, more than 25. To run <code><font size="2">splunk-optimize</font></code>, go to <code><font size="2">$SPLUNKHOME/bin</font></code> and type:
</p>
<div class="samplecode">
<code><font size="2"><br>&nbsp;splunk-optimize -d|--directory &lt;bucket_directory&gt;<br></font></code>
</div>
<p><code><font size="2">splunk-optimize</font></code> accepts a number of optional parameters. To see a list of available parameters, type:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk-optimize<br></font></code>
</div>
<p>For more information on buckets, see <a href="#howsplunkstoresindexes" class="external text">"How Splunk stores indexes"</a>.
</p>
<h1>Manage index storage</h1><a name="howsplunkstoresindexes"></a><h2> <a name="howsplunkstoresindexes_how_the_indexer_stores_indexes"><span class="mw-headline" id="How_the_indexer_stores_indexes"> How the indexer stores indexes </span></a></h2>
<p>As the indexer indexes your data, it creates a bunch of files. These files contain two types of data:
</p>
<ul><li> The raw data in compressed form (<b>rawdata</b>)
</li><li> Indexes that point to the raw data, plus some metadata files (<b>index files</b>)
</li></ul><p>Together, these files constitute the Splunk Enterprise <b>index</b>. The files reside in sets of directories organized by age. Some directories contain newly indexed data; others contain previously indexed data. The number of such directories can grow quite large, depending on how much data you're indexing.
</p>
<h3> <a name="howsplunkstoresindexes_why_you_might_care"><span class="mw-headline" id="Why_you_might_care"> Why you might care </span></a></h3>
<p>You might not care, actually. The indexer handles indexed data by default in a way that gracefully ages the data through several stages. After a long period of time, typically several years, the indexer removes old data from your system. You might well be fine with the default scheme it uses.
</p><p>However, if you're indexing large amounts of data, have specific data retention requirements, or otherwise need to carefully plan your aging policy, you've got to read this topic. Also, to back up your data, it helps to know where to find it. So, read on....
</p>
<h3> <a name="howsplunkstoresindexes_how_data_ages"><span class="mw-headline" id="How_data_ages"> How data ages </span></a></h3>
<p>Each of the index directories is known as a <b>bucket</b>. To summarize so far:
</p>
<ul><li> An "index" contains compressed raw data and associated index files.
</li><li> An index resides across many age-designated index directories.
</li><li> An index directory is called a bucket.
</li></ul><p>A bucket moves through several stages as it ages:
</p>
<ul><li> hot
</li><li> warm
</li><li> cold
</li><li> frozen 
</li><li> thawed
</li></ul><p>As buckets age, they "roll" from one stage to the next. As data is indexed, it goes into a hot bucket. Hot buckets are both searchable and actively being written to. An index can have several hot buckets open at a time. 
</p><p>When certain conditions occur (for example, the hot bucket reaches a certain size or <code><font size="2">splunkd</font></code> gets restarted), the hot bucket becomes a warm bucket ("rolls to warm"), and a new hot bucket is created in its place. Warm buckets are searchable, but are not actively written to. There are many warm buckets. 
</p><p>Once further conditions are met (for example, the index reaches some maximum number of warm buckets), the indexer begins to roll the warm buckets to cold, based on their age. It always selects the oldest warm bucket to roll to  cold. Buckets continue to roll to cold as they age in this manner. After a set period of time, cold buckets roll to frozen, at which point they are either archived or deleted. By editing attributes in indexes.conf, you can specify the <a href="#setaretirementandarchivingpolicy" class="external text">bucket aging policy</a>, which determines when a bucket moves from one stage to the next.
</p><p>If the frozen data has been archived, it can later be thawed.  Thawed data is available for searches.
</p><p>Here are the stages that buckets age through:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Bucket stage
</th><th bgcolor="#C0C0C0">Description
</th><th bgcolor="#C0C0C0">Searchable?
</th></tr><tr><td width="10%" valign="center" align="left"><b>Hot</b>
</td><td width="45%" valign="center" align="left">Contains newly indexed data. Open for writing. One or more hot buckets for each index.
</td><td width="45%" valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Warm</b>
</td><td valign="center" align="left">Data rolled from hot. There are many warm buckets. Data is not actively written to warm buckets.
</td><td valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Cold</b>
</td><td valign="center" align="left">Data rolled from warm. There are many cold buckets.
</td><td valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Frozen</b>
</td><td valign="center" align="left">Data rolled from cold. The indexer deletes frozen data by default, but you can choose to archive it instead. Archived data can later be thawed.
</td><td valign="center" align="left">No
</td></tr><tr><td valign="center" align="left"><b>Thawed</b>
</td><td valign="center" align="left">Data restored from an archive. If you archive frozen data, you can later return it to the index by thawing it.
</td><td valign="center" align="left">Yes
</td></tr></table><p>The collection of buckets in a particular stage is sometimes referred to as a database or "db": the "hot db", the "warm db", the "cold db", etc.
</p>
<h3> <a name="howsplunkstoresindexes_what_the_index_directories_look_like"><span class="mw-headline" id="What_the_index_directories_look_like"> What the index directories look like </span></a></h3>
<p>Each index occupies its own directory under <code><font size="2">$SPLUNK_HOME/var/lib/splunk</font></code>. The name of the directory is the same as the index name. Under the index directory are a series of subdirectories that categorize the buckets by stage (hot/warm, cold, or thawed). 
</p><p>The buckets themselves are subdirectories within those directories. The bucket directory names are based on the age of the data.
</p><p>Here is the directory structure for the default index (<code><font size="2">defaultdb</font></code>):
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Bucket stage
</th><th bgcolor="#C0C0C0"> Default location
</th><th bgcolor="#C0C0C0"> Notes
</th></tr><tr><td valign="center" align="left"> <b>Hot</b>
</td><td valign="center" align="left"> <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</font></code>
</td><td valign="center" align="left"> There can be multiple hot subdirectories, one for each hot bucket. See <a href="#howsplunkstoresindexes_bucket_naming_conventions" class="external text">"Bucket naming conventions"</a>.
</td></tr><tr><td valign="center" align="left"><b>Warm</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</font></code>
</td><td valign="center" align="left">There are separate subdirectories for each warm bucket. See <a href="#howsplunkstoresindexes_bucket_naming_conventions" class="external text">"Bucket naming conventions"</a>.
</td></tr><tr><td valign="center" align="left"><b>Cold</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/colddb/*</font></code>
</td><td valign="center" align="left">There are multiple cold subdirectories. When warm buckets roll to cold, they  get moved into this directory, but are not renamed.
</td></tr><tr><td valign="center" align="left"><b>Frozen</b>
</td><td valign="center" align="left">N/A: Frozen data gets deleted or archived into a directory location you specify.
</td><td valign="center" align="left">Deletion is the default; see <a href="#automatearchiving" class="external text">"Archive indexed data"</a> for information on how to archive the data instead.
</td></tr><tr><td valign="center" align="left"><b>Thawed</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/*</font></code>
</td><td valign="center" align="left">Location for data that has been archived and later thawed. See <a href="#restorearchiveddata" class="external text">"Restore archived data"</a> for information on restoring archived data to a thawed state.
</td></tr></table><p>The paths for the hot/warm, cold, and thawed directories are configurable, so, for example, you can store cold buckets in a separate location from hot/warm buckets. See <a href="#configureindexstorage" class="external text">"Configure index storage"</a> and <a href="#usemultiplepartitionsforindexdata" class="external text">"Use multiple partitions for index data"</a>.
</p><p><b>Important:</b> All index locations must be writable.
</p><p><b>Note:</b> In pre-6.0 versions of Splunk Enterprise, replicated copies of cluster buckets always resided in the <code><font size="2">colddb</font></code> directory, even if they were hot or warm buckets.  Starting with 6.0, hot and warm replicated copies reside in the <code><font size="2">db</font></code> directory, the same as for non-replicated copies.
</p>
<h3> <a name="howsplunkstoresindexes_bucket_naming_conventions"><span class="mw-headline" id="Bucket_naming_conventions">Bucket naming conventions</span></a></h3>
<p>Bucket names depend on:
</p>
<ul><li> The stage of the bucket: hot or warm/cold/thawed
</li><li> The type of bucket directory: non-clustered, clustered originating, or clustered replicated
</li></ul><p><b>Important:</b> Bucket naming conventions are subject to change.
</p>
<h4><font size="3"><b><i> <a name="howsplunkstoresindexes_non-clustered_buckets"><span class="mw-headline" id="Non-clustered_buckets">Non-clustered buckets</span></a></i></b></font></h4>
<p>A standalone indexer creates non-clustered buckets.  These use one type of naming convention.
</p>
<h4><font size="3"><b><i> <a name="howsplunkstoresindexes_clustered_buckets"><span class="mw-headline" id="Clustered_buckets">Clustered buckets</span></a></i></b></font></h4>
<p>An indexer that is part of an <b>indexer cluster</b> creates clustered buckets. A clustered bucket has multiple exact copies. The naming convention for clustered buckets distinguishes the types of copies, originating or replicated.
</p><p>Briefly, a bucket in an indexer cluster has multiple copies, according to its <b>replication factor</b>. When the data enters the cluster, the receiving indexer writes the data to a hot bucket. This receiving indexer is known as the source cluster <b>peer</b>, and the bucket where the data gets written is called the originating copy of the bucket. 
</p><p>As data is written to the hot copy, the source peer streams copies of the hot data, in blocks, to other indexers in the cluster. These indexers are referred to as the target peers for the bucket. The copies of the streamed data on the target peers are known as replicated copies of the bucket. 
</p><p>When the source peer rolls its originating hot bucket to warm, the target peers roll their replicated copies of that bucket. The warm copies are exact replicas of each other.
</p><p>For an introduction to indexer cluster architecture and replicated data streaming, read <a href="#basicclusterarchitecture" class="external text">"Basic indexer cluster architecture"</a>.
</p>
<h4><font size="3"><b><i> <a name="howsplunkstoresindexes_bucket_names"><span class="mw-headline" id="Bucket_names">Bucket names</span></a></i></b></font></h4>
<p>These are the naming conventions:
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Bucket type
</th><th bgcolor="#C0C0C0"> Hot bucket
</th><th bgcolor="#C0C0C0"> Warm/cold/thawed bucket
</th></tr><tr><td valign="center" align="left"> <b>Non-clustered</b>
</td><td valign="center" align="left"> <code><font size="2">hot_v1_&lt;localid&gt;</font></code>
</td><td valign="center" align="left"> <code><font size="2">db_&lt;newest_time&gt;_&lt;oldest_time&gt;_&lt;localid&gt;</font></code>
</td></tr><tr><td valign="center" align="left"><b>Clustered originating</b>
</td><td valign="center" align="left"><code><font size="2">hot_v1_&lt;localid&gt;</font></code>
</td><td valign="center" align="left"><code><font size="2">db_&lt;newest_time&gt;_&lt;oldest_time&gt;_&lt;localid&gt;_&lt;guid&gt;</font></code>
</td></tr><tr><td valign="center" align="left"><b>Clustered replicated</b>
</td><td valign="center" align="left"><code><font size="2">&lt;localid&gt;_&lt;guid&gt;</font></code>
</td><td valign="center" align="left"><code><font size="2">rb_&lt;newest_time&gt;_&lt;oldest_time&gt;_&lt;localid&gt;_&lt;guid&gt;</font></code>
</td></tr></table><p>Note:
</p>
<ul><li> <code><font size="2">&lt;newest_time&gt;</font></code> and <code><font size="2">&lt;oldest_time&gt;</font></code> are timestamps indicating the age of the data in the bucket. The timestamps are expressed in UTC epoch time (in seconds). For example: <code><font size="2">db_1223658000_1223654401_2835</font></code> is a warm, non-clustered bucket containing data from October 10, 2008, covering the exact period of 9am-10am.
</li><li> <code><font size="2">&lt;localid&gt;</font></code> is an ID for the bucket. For a clustered bucket, the originating and replicated copies of the bucket have the same <code><font size="2">&lt;localid&gt;</font></code>.
</li><li> <code><font size="2">&lt;guid&gt;</font></code> is the <code><font size="2">guid</font></code> of the source peer node. The <code><font size="2">guid</font></code> is located in the peer's <code><font size="2">$SPLUNK_HOME/etc/instance.cfg</font></code> file.
</li></ul><p>In an indexer cluster, the originating warm bucket and its replicated copies have identical names, except for the prefix (<code><font size="2">db</font></code> for the originating bucket; <code><font size="2">rb</font></code> for the replicated copies).
</p><p><b>Note:</b> In an indexer cluster, when data is streamed from the source peer to a target peer, the data first goes into a temporary directory on the target peer, identified by the hot bucket convention of <code><font size="2">&lt;localid&gt;_&lt;guid&gt;</font></code>. This is true for any replicated bucket copy, whether or not the streaming bucket is a hot bucket. For example, during bucket fix-up activities, a peer might stream a warm bucket to other peers. When the replication of that bucket has completed, the <code><font size="2">&lt;localid&gt;_&lt;guid&gt;</font></code> directory is rolled into a warm bucket directory, identified by the <code><font size="2">rb_</font></code> prefix.
</p>
<h3> <a name="howsplunkstoresindexes_buckets_and_splunk_enterprise_administration"><span class="mw-headline" id="Buckets_and_Splunk_Enterprise_administration"> Buckets and Splunk Enterprise administration </span></a></h3>
<p>When you are administering Splunk Enterprise, it helps to understand how the indexer stores indexes across buckets. In particular, several admin activities require a good understanding of buckets:
</p>
<ul><li> For information on setting a retirement and archiving policy, see <a href="#setaretirementandarchivingpolicy" class="external text">"Set a retirement and archiving policy."</a> You can base the retirement policy on either the size or the age of data.
</li></ul><ul><li> For information on how to archive your indexed data, see <a href="#automatearchiving" class="external text">"Archive indexed data"</a>. To learn how to restore data from archive, read <a href="#restorearchiveddata" class="external text">"Restore archived data."</a>
</li></ul><ul><li> To learn how to back up your data, read <a href="#backupindexeddata" class="external text">"Back up indexed data."</a> That topic also discusses how to manually roll hot buckets to warm, so that you can then back them up. Also, see "Best practices for backing up" on the Community Wiki.
</li></ul><ul><li> For information on setting limits on disk usage, see <a href="#setlimitsondiskusage" class="external text">"Set limits on disk usage."</a>
</li></ul><ul><li> For a list of configurable bucket settings, see <a href="#configureindexstorage" class="external text">"Configure index storage."</a>
</li></ul><ul><li> For information on configuring index size, see <a href="#configureindexstoragesize" class="external text">"Configure index size."</a>
</li></ul><ul><li> For information on partitioning index data, see <a href="#usemultiplepartitionsforindexdata" class="external text">"Use multiple partitions for index data."</a>
</li></ul><ul><li> For information on how buckets function in indexer clusters, see <a href="#bucketsandclusters" class="external text">"Buckets and indexer clusters."</a>
</li></ul><p>In addition, see "indexes.conf" in the <i>Admin Manual</i> and "Understanding buckets" on the Community Wiki.
</p>
<a name="configureindexstorage"></a><h2> <a name="configureindexstorage_configure_index_storage"><span class="mw-headline" id="Configure_index_storage"> Configure index storage</span></a></h2>
<p>You configure indexes in <code><font size="2">indexes.conf</font></code>. How you edit <code><font size="2">indexes.conf</font></code> depends on whether you're using <b>index replication</b>, also known as indexer clustering:
</p>
<ul><li> <b>For non-clustered indexes,</b> edit a copy of <code><font size="2">indexes.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code> or in a custom app directory in <code><font size="2">$SPLUNK_HOME/etc/apps/</font></code>.  Do not edit the copy in <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>. For information on configuration files and directory locations, see "About configuration files".  
</li></ul><ul><li> <b>For clustered indexes,</b> edit a copy of <code><font size="2">indexes.conf</font></code> on the cluster master node and then distribute it to all the peer nodes, as described in <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>.
</li></ul><p>This table lists the key <code><font size="2">indexes.conf</font></code> attributes affecting <b>buckets</b> and what they configure. It also provides links to other topics that show how to use these attributes. For the most detailed information on these attributes, as well as others, always refer to the indexes.conf spec file.
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Attribute
</th><th bgcolor="#C0C0C0">What it configures
</th><th bgcolor="#C0C0C0">Default
</th><th bgcolor="#C0C0C0">For more information, see ...
</th></tr><tr><td width="15%" valign="center" align="left"><b>homePath</b>
</td><td width="35%" valign="center" align="left">The path that contains the hot and warm buckets. (<b>Required.</b>)
<p>This location must be writable.
</p>
</td><td width="30%" valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/ defaultdb/db/</font></code> (for the default index only)
</td><td width="20%" valign="center" align="left"><a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a>
</td></tr><tr><td valign="center" align="left"><b>coldPath</b>
</td><td valign="center" align="left">The path that contains the cold buckets. (<b>Required.</b>)
<p>This location must be writable.
</p>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/ defaultdb/colddb/</font></code> (for the default index only)
</td><td valign="center" align="left"><a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a>
</td></tr><tr><td valign="center" align="left"><b>thawedPath</b>
</td><td valign="center" align="left">The path that contains any thawed buckets. (<b>Required.</b>)
<p>This location must be writable.
</p>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/ defaultdb/thaweddb/</font></code> (for the default index only)
</td><td valign="center" align="left"><a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a>
</td></tr><tr><td valign="center" align="left"><b>repFactor</b>
</td><td valign="center" align="left">Determines whether the index gets replicated to other cluster peers. (<b>Required for indexes on cluster peer nodes.</b>)
</td><td valign="center" align="left">0 (which means that the index will not get replicated to other peers; the correct behavior for non-clustered indexes). For clustered indexes, you <i>must</i> set <code><font size="2">repFactor</font></code> to <code><font size="2">auto</font></code>, which causes the index to get replicated.
</td><td valign="center" align="left"><a href="#configurethepeerindexes" class="external text">Configure the peer indexes in an indexer cluster</a>
</td></tr><tr><td valign="center" align="left"><b>maxHotBuckets</b>
</td><td valign="center" align="left">The maximum number of hot buckets. This value should be at least 2, to deal with any archival data. The <b>main</b> default index, for example, has this value set to 10.
</td><td valign="center" align="left">3, for new, custom indexes.
</td><td valign="center" align="left"><a href="#howsplunkstoresindexes_how_data_ages" class="external text">How data ages</a><br></td></tr><tr><td valign="center" align="left"><b>maxDataSize</b>
</td><td valign="center" align="left"><b>Determines rolling behavior, hot to warm.</b> The maximum size for a hot bucket. When a hot bucket reaches this size, it rolls to warm. This attribute also determines the approximate size for all buckets.
</td><td valign="center" align="left">Depends; see indexes.conf.
</td><td valign="center" align="left"><a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a><br><p><a href="#setaretirementandarchivingpolicy" class="external text">Set a retirement and archiving policy</a> 
</p>
</td></tr><tr><td valign="center" align="left"><b>maxWarmDBCount</b>
</td><td valign="center" align="left"><b>Determines rolling behavior, warm to cold.</b> The maximum number of warm buckets. When the maximum is reached, warm buckets begin rolling to cold.
</td><td valign="center" align="left">300
</td><td valign="center" align="left"><a href="#usemultiplepartitionsforindexdata" class="external text">Use multiple partitions for index data</a>
</td></tr><tr><td valign="center" align="left"><b>maxTotalDataSizeMB</b>
</td><td valign="center" align="left"><b>Determines rolling behavior, cold to frozen.</b> The maximum size of an index. When this limit is reached, cold buckets begin rolling to frozen.
</td><td valign="center" align="left">500000 (MB)
</td><td valign="center" align="left"><a href="#setaretirementandarchivingpolicy" class="external text">Set a retirement and archiving policy</a>
</td></tr><tr><td valign="center" align="left"><b>frozenTimePeriodInSecs</b>
</td><td valign="center" align="left"><b>Determines rolling behavior, cold to frozen.</b> Maximum age for a bucket, after which it rolls to frozen.
</td><td valign="center" align="left">188697600 (in seconds; approx. 6 years)
</td><td valign="center" align="left"><a href="#setaretirementandarchivingpolicy" class="external text">Set a retirement and archiving policy</a>
</td></tr><tr><td valign="center" align="left"><b>coldToFrozenDir</b>
</td><td valign="center" align="left">Location for archived data. Determines behavior when a bucket rolls from cold to frozen. If set, the indexer will archive frozen buckets into this directory just before deleting them from the index.
</td><td valign="center" align="left">If you don't set either this attribute or <code><font size="2">coldToFrozenScript</font></code>, the indexer will just log the bucket's directory name and then delete it once it rolls to frozen.
</td><td valign="center" align="left"><a href="#automatearchiving" class="external text">Archive indexed data</a>
</td></tr><tr><td valign="center" align="left"><b>coldToFrozenScript</b>
</td><td valign="center" align="left">Script to run just before a cold bucket rolls to frozen. If you set both this attribute and <code><font size="2">coldToFrozenDir</font></code>, the indexer will use <code><font size="2">coldToFrozenDir</font></code> and ignore this attribute.
</td><td valign="center" align="left">If you don't set either this attribute or <code><font size="2">coldToFrozenDir</font></code>, the indexer will just log the bucket's directory name and then delete it once it rolls to frozen.
</td><td valign="center" align="left"><a href="#automatearchiving" class="external text">Archive indexed data</a>
</td></tr><tr><td valign="center" align="left"><b>homePath.maxDataSizeMB</b>
<p><b>coldPath.maxDataSizeMB</b>
</p>
</td><td valign="center" align="left">Maximum size for <code><font size="2">homePath</font></code> (hot/warm bucket storage) or <code><font size="2">coldPath</font></code> (cold bucket storage). If either attribute is missing or set to 0, its path is not individually constrained in size.
</td><td valign="center" align="left">None
</td><td valign="center" align="left"><a href="#configureindexstoragesize_configure_index_size_according_to_bucket_type" class="external text">Configure index size according to bucket type</a>
</td></tr><tr><td valign="center" align="left"><b>maxVolumeDataSizeMB</b>
</td><td valign="center" align="left">Maximum size for a volume. If the attribute is missing, the individual volume is not constrained in size.
</td><td valign="center" align="left">None
</td><td valign="center" align="left"><a href="#configureindexstoragesize_configure_index_size_with_volumes" class="external text">Configure index size with volumes</a>
</td></tr></table><p><b>Note:</b> <i>For non-clustered indexes only,</i> you can use Splunk Web to configure the path to your indexes. Go to <b>Settings &gt; System settings &gt; General settings</b>. Under the section <b>Index settings</b>, set the field <b>Path to indexes</b>. After doing this, you must restart the indexer from the CLI, not from within Splunk Web.
</p>
<a name="moveanindex"></a><h2> <a name="moveanindex_move_the_index_database"><span class="mw-headline" id="Move_the_index_database"> Move the index database</span></a></h2>
<p>You can move the entire index database from one location to another. The sections in this topic provide procedures for doing so. The procedures assume that the index database is in its default location, created during the original installation.
</p><p>You can also move individual indexes or parts of an index to separate locations. Once you do so, the procedures in this topic are no longer valid. For detailed information on the structure of Splunk Enterprise indexes, read <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes".</a> For information on how to change the location(s) for a single index, read <a href="#configureindexstorage" class="external text">"Configure index storage".</a> 
</p>
<h3> <a name="moveanindex_for_.2anix_users"><span class="mw-headline" id="For_.2Anix_users"> For *nix users </span></a></h3>
<p><b>1.</b> Make sure the target file system has enough space - at least 1.2 times the size of the total amount of raw data you plan to index.
</p><p><b>2.</b> Create the target directory and make sure it has write permissions for the user Splunk Enterprise runs as. For example, if Splunk Enterprise runs as user "splunk", give it ownership of the directory:
</p>
<div class="samplecode">
<code><font size="2"><br>mkdir /foo/bar<br>chown splunk /foo/bar/<br></font></code>
</div>
<p>For information on setting the user that Splunk Enterprise runs as, read this topic.
</p><p><b>3.</b> When the new index home is ready, stop the indexer. Navigate to the <code><font size="2">$SPLUNK_HOME/bin/</font></code> directory and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br></font></code>
</div>
<p><b>4.</b> Copy the existing index file system to its new home:
</p>
<div class="samplecode">
<code><font size="2"><br>cp -rp $SPLUNK_DB/* /foo/bar/<br></font></code>
</div>
<p><b>5.</b> Unset the <code><font size="2">SPLUNK_DB</font></code> environment variable:
</p>
<div class="samplecode">
<code><font size="2"><br>unset SPLUNK_DB<br></font></code>
</div>
<p><b>6.</b> Edit <code><font size="2">$SPLUNK_HOME/etc/splunk-launch.conf</font></code> to reflect the new index directory. Change the <code><font size="2">SPLUNK_DB</font></code> attribute in that file to point to your new index directory:
</p>
<div class="samplecode">
<code><font size="2"><br>SPLUNK_DB=/foo/bar<br></font></code>
</div>
<p><b>7.</b> Start the indexer.  Navigate to <code><font size="2">$SPLUNK_HOME/bin/</font></code> and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk start<br></font></code>
</div>
<p>The indexer picks up where it left off, reading from, and writing to, the new copy of the index.
</p><p><b>8.</b> You can delete the old index database after verifying that the indexer can read and write to the new location.
</p>
<h3> <a name="moveanindex_for_windows_users"><span class="mw-headline" id="For_Windows_users"> For Windows users</span></a></h3>
<p><b>1.</b> Make sure the target drive or directory has enough space available.  
</p><p><b>Caution:</b> Using mapped network drives for index stores is strongly discouraged and not supported.
</p><p><b>2.</b> From a command prompt, go to your target drive and make sure the target directory has the correct permissions, so that the <code><font size="2">splunkd</font></code> process can write to files there:
</p>
<div class="samplecode">
<code><font size="2"><br>C:\Program Files\Splunk&gt; D:<br>D:\&gt; mkdir \new\path\for\index<br>D:\&gt; cacls D:\new\path\for\index /T /E /G &lt;the user Splunk Enterprise runs as&gt;:F<br></font></code>
</div>
<p>For more information about determining the user Splunk Enterprise runs as, review this topic on installing Splunk on Windows.
</p><p><b>Note:</b> Windows Vista, 7, Server 2003 and Server 2008 users can also use <code><font size="2">icacls</font></code> to ensure directory permissions are correct; this Microsoft TechNet article gives information on specific command-line arguments.
</p><p><b>3.</b> Stop the indexer. Navigate to the <code><font size="2">%SPLUNK_HOME%\bin</font></code> directory and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk stop<br></font></code>
</div>
<p><b>Note:</b> You can also use the Services control panel to stop the <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services.
</p><p><b>4.</b> Copy the existing index file system to its new home:
</p>
<div class="samplecode">
<code><font size="2"><br>xcopy "C:\Program Files\Splunk\var\lib\splunk\*.*" D:\new\path\for\index /s /e /v /o /k<br></font></code>
</div>
<p><b>5.</b> Unset the <code><font size="2">SPLUNK_DB</font></code> environment variable:
</p>
<div class="samplecode">
<code><font size="2"><br>set SPLUNK_DB=<br></font></code>
</div>
<p><b>6.</b> Edit <code><font size="2">%SPLUNK_HOME%\etc\splunk-launch.conf</font></code> to reflect the new index directory. Change the <code><font size="2">SPLUNK_DB</font></code> attribute in that file to point to your new index directory:
</p>
<div class="samplecode">
<code><font size="2"><br>SPLUNK_DB=D:\new\path\for\index<br></font></code>
</div>
<p><b>Note:</b> If the line in the configuration file that contains the <code><font size="2">SPLUNK_DB</font></code> attribute has a pound sign (#) as its first character, the line is commented out, and the # needs to be removed.
</p><p><b>7.</b> Start the indexer. Navigate to the <code><font size="2">%SPLUNK_HOME%\bin</font></code> directory and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk start<br></font></code>
</div>
<p>The indexer picks up where it left off, reading from, and writing to, the new copy of the index.
</p><p><b>8.</b> You can delete the old index database after verifying that the indexer can read and write to the new location.
</p>
<h3> <a name="moveanindex_use_splunk_web_to_change_the_path_to_indexes"><span class="mw-headline" id="Use_Splunk_Web_to_change_the_path_to_indexes"> Use Splunk Web to change the path to indexes </span></a></h3>
<p>You can use Splunk Web to change the path to your indexes. Unlike the earlier procedures that actually move the indexes, when you change the path in Splunk Web, it only affects new data coming into your indexes. For that reason, it's recommended that you use Splunk Web for this purpose only for a new indexer - before you start adding data to it.
</p><p>To change the path:
</p><p><b>1.</b> Go to <b>Settings&gt;System settings&gt;General settings</b>.
</p><p><b>2.</b> Under the <b>Index settings</b> section on that page, go to the field <b>Path to indexes</b>. 
</p><p><b>3.</b> Enter a new path in that field. This is where you want newly indexed data to reside.
</p><p><b>4.</b> Unset the <code><font size="2">SPLUNK_DB</font></code> environment variable, if it's currently set in your environment:
</p>
<ul><li> For *nix, on the command line, type:
</li></ul><div class="samplecode">
<code><font size="2"><br>unset SPLUNK_DB<br></font></code>
</div>
<ul><li> For Windows, on the command line, type:
</li></ul><div class="samplecode">
<code><font size="2"><br>set SPLUNK_DB=<br></font></code>
</div>
<p><b>5.</b> Use the CLI to restart the indexer. Navigate to <code><font size="2">$SPLUNK_HOME/bin/</font></code> (*nix) or <code><font size="2">%SPLUNK_HOME%\bin</font></code> (Windows) and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk restart<br></font></code>
</div>
<p><b>Important:</b> Do not use the restart function inside Splunk Web. This will not have the intended effect of causing the index directory to change. You <i>must</i> restart from the CLI.
</p>
<a name="usemultiplepartitionsforindexdata"></a><h2> <a name="usemultiplepartitionsforindexdata_use_multiple_partitions_for_index_data"><span class="mw-headline" id="Use_multiple_partitions_for_index_data"> Use multiple partitions for index data</span></a></h2>
<p>The indexer can use multiple disks and partitions for its index data.  It's possible to configure the indexer to use many disks/partitions/filesystems on the basis of multiple indexes and <b>bucket</b> types, so long as you mount them correctly and point to them properly from <code><font size="2">indexes.conf</font></code>.  However, we recommend that you use a single high performance file system to hold your index data for the best experience.
</p><p>If you do use multiple partitions, the most common way to arrange the index data is to keep the hot/warm buckets on the local machine, and to put the cold bucket on a separate array of disks (for longer term storage).  You'll want to run your hot/warm buckets on a machine with with fast read/write partitions, since most searching will happen there.  Cold buckets should be located on a reliable array of disks. 
</p>
<h3> <a name="usemultiplepartitionsforindexdata_configure_multiple_partitions"><span class="mw-headline" id="Configure_multiple_partitions">Configure multiple partitions</span></a></h3>
<p>To configure multiple partitions:
</p><p><b>1.</b> Set up partitions just as you'd normally set them up in any operating system.  
</p><p><b>2.</b> Mount the disks/partitions.
</p><p><b>3.</b> Edit indexes.conf to point to the correct paths for the partitions. You set paths on a per-index basis, so you can also set separate partitions for different indexes. Each index has its own <code><font size="2">[&lt;index&gt;]</font></code> stanza, where <code><font size="2">&lt;index&gt;</font></code> is the name of the index. These are the settable path attributes:
</p>
<ul><li> <code><font size="2">homePath = &lt;path on server&gt;</font></code>
<ul><li> This is the path that contains the hot and warm databases for the index. 
</li><li> <b>Caution:</b> The path must be writable.
</li></ul></li></ul><ul><li> <code><font size="2">coldPath = &lt;path on server&gt;</font></code>
<ul><li> This is the path that contains the cold databases for the index. 
</li><li> <b>Caution:</b> The path must be writable.
</li></ul></li></ul><ul><li><code><font size="2">thawedPath = &lt;path on server&gt;</font></code>
<ul><li> This is the path that contains any thawed databases for the index.
</li></ul></li></ul><a name="configureindexstoragesize"></a><h2> <a name="configureindexstoragesize_configure_index_size"><span class="mw-headline" id="Configure_index_size"> Configure index size</span></a></h2>
<p>You can configure index storage size in a number of ways:
</p>
<ul><li> On a per-index basis
</li><li> For hot/warm and cold <b>buckets</b> separately
</li><li> Across indexes, using volumes
</li></ul><p>To configure index storage size, you set attributes in indexes.conf. For more information on the attributes mentioned in this topic, read <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</p><p><b>Caution:</b> While processing indexes, the indexer might occasionally exceed the configured maximums for short periods of time. When setting limits, be sure to factor in some buffer space. Also, note that certain systems, such as most Unix systems, maintain a configurable reserve space on their partitions. You must take that reserve space, if any, into account when determining how large your indexes can grow.
</p>
<h3> <a name="configureindexstoragesize_configure_index_size_for_each_index"><span class="mw-headline" id="Configure_index_size_for_each_index">Configure index size for each index</span></a></h3>
<p>To set the maximum index size on a per-index basis, use the <code><font size="2">maxTotalDataSizeMB</font></code> attribute. When this limit is reached, buckets begin rolling to frozen.
</p>
<h3> <a name="configureindexstoragesize_configure_index_size_according_to_bucket_type"><span class="mw-headline" id="Configure_index_size_according_to_bucket_type"> Configure index size according to bucket type </span></a></h3>
<p>To set the maximum size for <code><font size="2">homePath</font></code> (hot/warm bucket storage) or <code><font size="2">coldPath</font></code> (cold bucket storage), use the <code><font size="2">maxDataSizeMB</font></code> attributes:
</p>
<div class="samplecode">
<code><font size="2"><br># set hot/warm storage to 10,000MB<br>homePath.maxDataSizeMB = 10000 <br># set cold storage to 5,000MB<br>coldPath.maxDataSizeMB = 5000<br></font></code>
</div>
<p>The <code><font size="2">maxDataSizeMB</font></code> attributes can be set globally or for each index. An index-level setting will override a global setting. To control bucket storage across groups of indexes, use the <code><font size="2">maxVolumeDataSizeMB</font></code> attribute, described below.
</p>
<h3> <a name="configureindexstoragesize_configure_index_size_with_volumes"><span class="mw-headline" id="Configure_index_size_with_volumes">Configure index size with volumes</span></a></h3>
<p>You can manage disk usage across multiple indexes by creating volumes and specifying maximum data size for them. A volume represents a directory on the file system where indexed data resides. 
</p><p>Volumes can store data from multiple indexes. You would typically use separate volumes for hot/warm and cold buckets. For instance, you can set up one volume to contain the hot/warm buckets for all your indexes, and another volume to contain the cold buckets.
</p><p>You can use volumes  to define <code><font size="2">homePath</font></code> and <code><font size="2">coldPath</font></code>. You cannot use them to define <code><font size="2">thawedPath</font></code>.  
</p><p>In addition, you must use volumes if you explicitly define <code><font size="2">bloomHomePath</font></code>. For information on <code><font size="2">bloomHomePath</font></code>, see the topic <a href="#bloomfilters" class="external text">"Configure bloom filters"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="configureindexstoragesize_configure_a_volume"><span class="mw-headline" id="Configure_a_volume">Configure a volume </span></a></i></b></font></h4>
<p>To set up a volume, use this syntax:
</p>
<div class="samplecode">
<code><font size="2"><br>[volume:&lt;volume_name&gt;]<br>path = &lt;pathname_for_volume&gt;<br></font></code>
</div>
<p>You can also optionally include a <code><font size="2">maxVolumeDataSizeMB</font></code> attribute, which specfies the maximum size for the volume. 
</p><p>For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[volume:hot1]<br>path = /mnt/fast_disk<br>maxVolumeDataSizeMB = 100000<br></font></code>
</div>
<p>The example defines a volume called "hot1", located at <code><font size="2">/mnt/fast_disk</font></code>, with a maximum size of 100,000MB.
</p><p>Similarly, this stanza defines a volume called "cold1" that uses a maximum of 150,000MB:
</p>
<div class="samplecode">
<code><font size="2"><br>[volume:cold1]<br>path = /mnt/big_disk<br>maxVolumeDataSizeMB = 150000<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="configureindexstoragesize_use_a_volume"><span class="mw-headline" id="Use_a_volume">Use a volume </span></a></i></b></font></h4>
<p>You can now define an index's <code><font size="2">homePath</font></code> and <code><font size="2">coldPath</font></code> in terms of volumes. For example, using the  volumes defined above, you can define two indexes:
</p>
<div class="samplecode">
<code><font size="2"><br>[idx1]<br>homePath = volume:hot1/idx1<br>coldPath = volume:cold1/idx1<br><br>[idx2]<br>homePath = volume:hot1/idx2<br>coldPath = volume:cold1/idx2<br></font></code>
</div>
<p>You can use volumes to manage index storage space in any way that makes sense to you. Usually, however, volumes correlate to hot/warm and cold buckets, because of the different storage requirements typical when dealing with different bucket types. So, you will probably use some volumes exclusively for designating <code><font size="2">homePath</font></code> (hot/warm buckets) and others for <code><font size="2">coldPath</font></code> (cold buckets).
</p><p>When a volume containing warm buckets reaches its <code><font size="2">maxVolumeDataSizeMB</font></code>, it starts rolling buckets to cold. When a volume containing cold buckets reaches its <code><font size="2">maxVolumeDataSizeMB</font></code>, it starts rolling buckets to frozen. If a volume contains both warm and cold buckets (which will happen if an index's <code><font size="2">homePath</font></code> and <code><font size="2">coldPath</font></code> are both set to the same volume), the oldest bucket will be rolled to frozen.
</p>
<h3> <a name="configureindexstoragesize_put_it_all_together"><span class="mw-headline" id="Put_it_all_together"> Put it all together</span></a></h3>
<p>This example shows how to use the per-index <code><font size="2">homePath.maxDataSizeMB</font></code> and <code><font size="2">coldPath.maxDataSizeMB</font></code> attributes in combination with volumes to maintain fine-grained control over index storage. In particular, it shows how to prevent bursts of data into one index from triggering massive bucket moves from other indexes. The per-index settings can be used to assure that no index will ever occupy more than a specified size, thereby alleviating the concern:
</p>
<div class="samplecode">
<code><font size="2"><br># global settings<br><br># Inheritable by all indexes: no hot/warm bucket can exceed 1 TB.<br># Individual indexes can override this setting.<br>homePath.maxDataSizeMB = 1000000<br><br># volumes<br><br>[volume:caliente]<br>path = /mnt/fast_disk<br>maxVolumeDataSizeMB = 100000<br><br>[volume:frio]<br>path = /mnt/big_disk<br>maxVolumeDataSizeMB = 1000000<br><br># indexes<br><br>[i1]<br>homePath = volume:caliente/i1<br># homePath.maxDataSizeMB is inherited from the global setting<br>coldPath = volume:frio/i1<br># coldPath.maxDataSizeMB not specified anywhere: <br># This results in no size limit - old-style behavior<br><br>[i2]<br>homePath = volume:caliente/i2<br>homePath.maxDataSizeMB = 1000 &nbsp;# overrides the global default<br>coldPath = volume:frio/i2<br>coldPath.maxDataSizeMB = 10000 &nbsp;# limits the size of cold buckets<br><br>[i3]<br>homePath = /old/style/path<br>homePath.maxDataSizeMB = 1000<br>coldPath = volume:frio/i3<br>coldPath.maxDataSizeMB = 10000<br></font></code>
</div>

<a name="setlimitsondiskusage"></a><h2> <a name="setlimitsondiskusage_set_limits_on_disk_usage"><span class="mw-headline" id="Set_limits_on_disk_usage"> Set limits on disk usage</span></a></h2>
<p>Splunk Enterprise uses several methods to control disk space. Indexes consume most of the disk space. If you run out of disk space, the indexer stops indexing. You can set a minimum free space limit to control how low free disk space falls before indexing stops. Indexing resumes once space exceeds the minimum. 
</p><p><b>Note:</b> To determine how much space you need for your indexes, see "Estimate your storage requirements" in the <i>Capacity Planning Manual</i>. 
</p>
<h3> <a name="setlimitsondiskusage_set_minimum_free_disk_space"><span class="mw-headline" id="Set_minimum_free_disk_space"> Set minimum free disk space </span></a></h3>
<p>You can set a minimum amount of free disk space for the disk where indexed data is stored. If the limit is reached, the indexer stops operating. Both indexing and searching are affected:
</p>
<ul><li> Periodically, the indexer checks space on all partitions that contain indexes. If the free disk space limit has been reached on any of those partitions, the indexer stops indexing data until more space is available. A UI banner and <code><font size="2">splunkd</font></code> warning are posted to indicate the need to clear more disk space.
</li><li> Before attempting to launch a search, the indexer requires that the specified amount of free space be available on the file system where the dispatch directory is stored, <code><font size="2">$SPLUNK_HOME/var/run/splunk/dispatch</font></code>
</li></ul><p>The default minimum free disk space is 5000MB.
</p><p><b>Note:</b>
</p>
<ul><li> The indexer does not clear any of its disk space with this method.  It simply pauses until more space becomes available.
</li><li> Incoming data can be lost while indexing is suspended.
</li></ul><p>You can set minimium free disk space through Splunk Web, the CLI, or the <code><font size="2">server.conf</font></code> configuration file.
</p>
<h4><font size="3"><b><i> <a name="setlimitsondiskusage_in_splunk_web"><span class="mw-headline" id="In_Splunk_Web"> In Splunk Web </span></a></i></b></font></h4>
<p>To specify minimum disk usage in Splunk Web:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right portion of Splunk Web.
</p><p><b>2.</b> Click <b>System settings</b>.
</p><p><b>3.</b> Click <b>General settings</b>.
</p><p><b>4.</b>  Under the <b>Index settings</b> section, set the field called <b>Pause indexing if free disk space (in MB) falls below</b>:
</p><p><img alt="Disk settings.jpg" src="images/3/33/Disk_settings.jpg" width="682" height="264"></p><p><b>5.</b> Enter the desired minimum free disk space in megabytes.
</p><p><b>6.</b>  Click <b>Save</b>.
</p><p><b>7.</b> Restart the indexer for your changes to take effect.
</p>
<h4><font size="3"><b><i> <a name="setlimitsondiskusage_from_the_command_line_interface_.28cli.29"><span class="mw-headline" id="From_the_command_line_interface_.28CLI.29"> From the command line interface (CLI) </span></a></i></b></font></h4>
<p>You can set the minimum free disk space with the CLI.  This example sets the minimum free disk space to 20,000MB (20GB):
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set minfreemb 20000<br>splunk restart<br></font></code>
</div>
<p>For information on using the CLI, see "About the CLI" in the Admin manual.
</p>
<h4><font size="3"><b><i> <a name="setlimitsondiskusage_in_server.conf"><span class="mw-headline" id="In_server.conf"> In server.conf </span></a></i></b></font></h4>
<p>You can also set the minimum free disk space in the server.conf file. The relevant stanza/attribute is this:
</p>
<div class="samplecode">
<code><font size="2"><br>[diskUsage]<br>minFreeSpace = &lt;num&gt;<br></font></code>
</div>
<p>Note that <code><font size="2">&lt;num&gt;</font></code> represents megabytes. The default is 5000.
</p>
<h3> <a name="setlimitsondiskusage_control_index_storage"><span class="mw-headline" id="Control_index_storage"> Control index storage </span></a></h3>
<p>The indexes.conf file contains index configuration settings. You can control disk storage usage by specifying maximum index size or maximum age of data. When one of these limits is reached, the oldest indexed data will be deleted (the default) or archived. You can archive the data by using a predefined archive script or creating your own. 
</p><p>For detailed instructions on how to use <code><font size="2">indexes.conf</font></code> to set maximum index size or age, see <a href="#setaretirementandarchivingpolicy" class="external text">"Set a retirement and archiving policy"</a>.
</p><p>For detailed information on index storage, see <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>.
</p>
<a name="bloomfilters"></a><h2> <a name="bloomfilters_configure_bloom_filters"><span class="mw-headline" id="Configure_bloom_filters"> Configure bloom filters</span></a></h2>
<p>This topic talks about bloom filters and how Splunk Enterprise uses them to improve search performance, particularly for rare term searches.  
</p><p>Before you continue reading this topic, you should be familiar with how the indexer stores data and how the data ages after it has been indexed. Basically, indexed data resides in database directories consisting of subdirectories called <b>buckets</b>. Each index has its own set of databases. As data ages, it moves through several types of buckets (hot, warm, cold, and frozen). Read more about <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores data"</a> and <a href="#backupindexeddata" class="external text">"How data ages"</a>.
</p>
<h3> <a name="bloomfilters_why_bloom_filters.3f"><span class="mw-headline" id="Why_bloom_filters.3F"> Why bloom filters? </span></a></h3>
<p>A Bloom filter is a data structure that is used to test whether an element is a member of a set. Our implementation stores the bloom filter as a file on disk in each bucket. When you run a search, especially when you are searching for rare terms, using bloom filters significantly decreases the time it takes to retrieve events from an index. 
</p><p>As the indexer indexes your time-series data, it creates a compressed file that contains the raw data broken into events based on timestamps and a set of <b>time-series index (tsidx) files</b>. The tsidx files are lexicon files that act as a dictionary of all the keywords in your data (error codes, response times, etc.) and contain references to the location of events in the raw data. When you run a search, the indexer searches the tsidx files for the keywords and retrieves the events from the referenced raw data file.
</p><p>Bloom filters work at the bucket level and use a separate file, <code><font size="2">bloomfilter</font></code>, which is basically a hash table that can tell you that a keyword definitely does not exist in a bucket. Then, when you run a search, the indexer only need search the tsidx files in the buckets that the bloom filters do not rule out. The execution cost of retrieving events from disk grows with the size and number of tsidx files. Because they decrease the number of tsidx files that the indexer need search, bloom filters decrease the time it takes to search each bucket.
</p><p>Instead of storing all of the unique keywords found in a bucket's tsidx files, the bloom filter computes a hash for each keyword. Multiple keywords can result in the same hash, which means that you can have false positives but never false negatives. Because of this, bloom filters can quickly rule out terms that definitely do not exist in a particular bucket and the indexer moves on to searching the next bucket. If the bloom filter cannot rule out a bucket (the keyword may or may not actually exist in the bucket), the indexer searches the bucket normally.
</p>
<h3> <a name="bloomfilters_configure_bloom_filters_2"><span class="mw-headline" id="Configure_bloom_filters_2"> Configure bloom filters </span></a></h3>
<p>Bloom filters are created when buckets roll from hot to warm. By default, they are deleted when the buckets roll to frozen, unless you have configured a different retention behavior. This section talks about the configuration file parameters you can use to configure and manage your bloomfilter files.
</p><p>To specify whether or not you want to use bloom filters, use the <code><font size="2">use_bloomfilter</font></code> parameter in <code><font size="2">limits.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[search]<br>use_bloomfilter = true|false<br>* Control whether to use bloom filters to rule out buckets.<br>* Defaults to True.<br></font></code>
</div>
<p>To create a bloom filter for a specific index, edit the following <b>Per Index</b> options in <code><font size="2">indexes.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>bloomHomePath = &lt;path on indexer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The location where the bloom filter files for the index are stored.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If specified, it must be defined in terms of a volume definition.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If not specified, bloom filter files for the index will be stored inside the bucket directories.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The path must be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;* You must restart splunkd after changing this parameter.<br><br>createBloomfilter = true|false<br>* Determines whether to create bloom filter files for the index.<br>* Defaults to "true".<br></font></code>
</div>
<p>In addition, you must use volumes if you explicitly define <code><font size="2">bloomHomePath</font></code>. A volume represents a directory on the file system where indexed data resides. For more information, read <a href="#configureindexstoragesize" class="external text">"Configure index size"</a>.
</p><p><b>Note:</b> A marker file, <code><font size="2">corrupt.bloomOnly.maker</font></code>, appears temporarily when the bucket rolls from hot to warm. The name of this file is misleading; it does <i>not</i> indicate a corrupt bloomfilter. No remedial action is needed on your part; just ignore the file.
</p>
<a name="determinerestart"></a><h2> <a name="determinerestart_determine_which_indexes.conf_changes_require_restart"><span class="mw-headline" id="Determine_which_indexes.conf_changes_require_restart"> Determine which indexes.conf changes require restart</span></a></h2>
<p>Some changes to <code><font size="2">indexes.conf</font></code> require that you restart the indexer for the changes to take effect:
</p>
<ul><li> Changing any of these attributes: <code><font size="2">homePath, coldPath, thawedPath, bloomHomePath, summaryHomePath, tstatsHomePath, repFactor, rawChunkSizeBytes, minRawFileSyncSecs, syncMeta, maxConcurrentOptimizes, coldToFrozenDir</font></code>
</li><li> Adding or removing a volume
</li><li> Enabling or disabling an index with data
</li><li> Removing an index 
</li></ul><p>You do not need to restart the indexer if you only make these  changes:
</p>
<ul><li> Adding new index stanzas 
</li><li> Changing any attributes not listed as requiring restart
</li><li> Enabling or disabling an index with no data
</li></ul><h1>Back up and archive your indexes</h1><a name="backupindexeddata"></a><h2> <a name="backupindexeddata_back_up_indexed_data"><span class="mw-headline" id="Back_up_indexed_data"> Back up indexed data</span></a></h2>
<p>To decide how to back up indexed data, it helps to understand first how the indexer stores data and how the data ages once it has been indexed. Then you can decide on a backup strategy. 
</p><p>Before you read this topic, you should look at <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a> to get familiar with the structure of indexes and the options for configuring them. But if you want to jump right in, the next section below attempts to summarize the key points from that topic.
</p>
<h3> <a name="backupindexeddata_how_data_ages"><span class="mw-headline" id="How_data_ages">How data ages</span></a></h3>
<p>Indexed data resides in database directories consisting of subdirectories called <b>buckets</b>. Each index has its own set of databases. 
</p><p>As data ages, it moves through several types of buckets. You determine how the data ages by by configuring attributes in indexes.conf. Read <a href="#configureindexstorage" class="external text">"Configure index storage"</a> for a description of the settings in <code><font size="2">indexes.conf</font></code> that control how data ages.
</p><p>Briefly, here is a somewhat simplified version of how data ages in an index: 
</p><p><b>1.</b> When the indexer first indexes data, it goes into a "hot" bucket. Depending on your configuration, there can be several hot buckets open at one time. Hot buckets cannot be backed up because the indexer is actively writing to them, but you can take a snapshot of them.
</p><p><b>2.</b> The data remains in the hot bucket until the policy conditions are met for it to be reclassified as "warm" data. This is called "rolling" the data into the warm bucket. This happens when a hot bucket reaches a specified size or age, or whenever <code><font size="2">splunkd</font></code> gets restarted. When a hot bucket is rolled, its directory is renamed, and it becomes a warm bucket. (You can also manually roll a bucket from hot to warm, as described <a href="#backupindexeddata_rolling_buckets_manually_from_hot_to_warm" class="external text">below</a>.) It is safe to back up the warm buckets. 
</p><p><b>3.</b> When the index reaches one of several possible configurable limits, usually a specified number of warm buckets, the oldest bucket becomes a "cold" bucket. The indexer moves the bucket to the <code><font size="2">colddb</font></code> directory. The default number of warm buckets is 300. 
</p><p><b>4.</b> Finally, at a time based on your defined policy requirements, the bucket rolls from cold to "frozen". The indexer deletes frozen buckets. However, if you need to preserve the data, you can tell the indexer to archive the data before deleting the bucket. See <a href="#automatearchiving" class="external text">"Archive indexed data"</a> for more information. 
</p><p>You can set <a href="#setaretirementandarchivingpolicy" class="external text">retirement and archiving policy</a> by controlling several different parameters, such as the size of indexes or buckets or the age of the data. 
</p><p>To summarize:
</p>
<ul><li> <b>hot buckets</b> - Currently being written to; do not back these up.
</li><li> <b>warm buckets</b> - Rolled from hot; can be safely backed up.
</li><li> <b>cold buckets</b> - Rolled from warm; buckets are moved to another location.
</li><li> <b>frozen buckets</b> - The indexer deletes these, but you can archive their contents first.
</li></ul><p>You set the locations of index databases in <code><font size="2">indexes.conf</font></code>. (See below for detailed information on the database locations for the default index.) You also specify numerous other attributes there, such as the maximum size and age of hot buckets. 
</p>
<h3> <a name="backupindexeddata_locations_of_the_index_database_directories"><span class="mw-headline" id="Locations_of_the_index_database_directories">Locations of the index database directories</span></a></h3>
<p>Here's the directory structure for the default index (<code><font size="2">defaultdb</font></code>):
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Bucket type
</th><th bgcolor="#C0C0C0"> Default location
</th><th bgcolor="#C0C0C0"> Notes
</th></tr><tr><td valign="center" align="left"> <b>Hot</b>
</td><td valign="center" align="left"> <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</font></code>
</td><td valign="center" align="left"> There can be multiple hot subdirectories. Each hot bucket occupies its own subdirectory, which uses this naming convention:
<p><code><font size="2">hot_v1_&lt;ID&gt;</font></code>
</p>
</td></tr><tr><td valign="center" align="left"><b>Warm</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</font></code>
</td><td valign="center" align="left">There are separate subdirectories for each warm bucket. These are named as described below in <a href="#howsplunkstoresindexes_warm.2fcold_bucket_naming_convention" class="external text">"Warm/cold bucket naming convention"</a>.
</td></tr><tr><td valign="center" align="left"><b>Cold</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/colddb/*</font></code>
</td><td valign="center" align="left">There are multiple cold subdirectories. When warm buckets roll to cold, they  get moved into this directory, but are not renamed.
</td></tr><tr><td valign="center" align="left"><b>Frozen</b>
</td><td valign="center" align="left">N/A: Frozen data gets deleted or archived into a directory location you specify.
</td><td valign="center" align="left">Deletion is the default; see <a href="#automatearchiving" class="external text">"Archive indexed data"</a> for information on how to archive the data instead.
</td></tr><tr><td valign="center" align="left"><b>Thawed</b>
</td><td valign="center" align="left"><code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/*</font></code>
</td><td valign="center" align="left">Location for data that has been archived and later thawed. See <a href="#restorearchiveddata" class="external text">"Restore archived data"</a> for information on restoring archived data to a thawed state.
</td></tr></table><p>The paths for hot/warm and cold directories are configurable, so you can store cold buckets in a separate location from hot/warm buckets. See <a href="#configureindexstorage" class="external text">"Configure index storage"</a> and <a href="#usemultiplepartitionsforindexdata" class="external text">"Use multiple partitions for index data"</a>.
</p><p><b>Important:</b> All index locations must be writable.
</p>
<h3> <a name="backupindexeddata_choose_your_backup_strategy"><span class="mw-headline" id="Choose_your_backup_strategy">Choose your backup strategy</span></a></h3>
<p>There are two basic backup scenarios to consider:
</p>
<ul><li> Ongoing, incremental backups of warm data
</li><li> Backup of all data - for example, before upgrading the indexer
</li></ul><p>How you actually perform the backup will, of course, depend entirely on the tools and procedures in place at your organzation, but this section should help provide you the guidelines you need to proceed.
</p>
<h4><font size="3"><b><i> <a name="backupindexeddata_incremental_backups"><span class="mw-headline" id="Incremental_backups">Incremental backups</span></a></i></b></font></h4>
<p>The general recommendation is to schedule backups of any new warm buckets regularly, using the incremental backup utility of your choice. If you're rolling buckets frequently, you should also include the cold database directory in your backups, to ensure that you don't miss any buckets that have rolled to cold before they've been backed up. Since bucket directory names don't change when they roll from warm to cold, you can just filter by name. 
</p><p>To back up hot buckets as well, you need to take a snapshot of the files, using a tool like VSS (on Windows/NTFS), ZFS snapshots (on ZFS), or a snapshot facility provided by the storage subsystem. If you do not have a snapshot tool available, you can manually roll a hot bucket to warm and then back it up, as described <a href="#backupindexeddata_rolling_buckets_manually_from_hot_to_warm" class="external text">below</a>. However, this is not generally recommended, for reasons also discussed below. 
</p>
<h4><font size="3"><b><i> <a name="backupindexeddata_back_up_all_data"><span class="mw-headline" id="Back_up_all_data">Back up all data</span></a></i></b></font></h4>
<p>It is recommended that you back up all your data before upgrading the indexer. This means the hot, warm, and cold buckets. 
</p><p>There are obviously a number of ways to do this, depending on the size of your data and how much downtime you can afford. Here are some basic guidelines:
</p>
<ul><li> For smaller amounts of data, shut down the indexer and just make a copy of your database directories before performing the upgrade. 
</li><li> For larger amounts of data, you will probably instead want to snapshot your hot buckets prior to upgrade.
</li></ul><p>In any case, if you have been doing incremental backups of your warm buckets as they've rolled from hot, you should really need to backup only your hot buckets at this time.
</p>
<h3> <a name="backupindexeddata_rolling_buckets_manually_from_hot_to_warm"><span class="mw-headline" id="Rolling_buckets_manually_from_hot_to_warm">Rolling buckets manually from hot to warm</span></a></h3>
<p>To roll the buckets of an index manually from hot to warm, use the following CLI command, replacing <code><font size="2">&lt;index_name&gt;</font></code> with the name of the index you want to roll:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk _internal call /data/indexes/&lt;index_name&gt;/roll-hot-buckets &acirc;&#128;&#147;auth &lt;admin_username&gt;:&lt;admin_password&gt;<br></font></code>
</div>
<p><b>Important:</b> It is ordinarily not advisable to roll hot buckets manually, as each forced roll permanently decreases search performance over the data. As a general rule, larger buckets are more efficient to search. By prematurely rolling buckets, you're producing smaller, less efficient buckets. In cases where hot data needs to be backed up, a snapshot backup is the preferred method.
</p><p><b>Note:</b> It is not recommended to roll hot buckets manually in an environment that leverages accelerated data-model summaries and index-replication. If a hot bucket is rolled while other index-management jobs are in progress, the integrity of the data may be compromised.
</p>
<h3> <a name="backupindexeddata_recommendations_for_recovery"><span class="mw-headline" id="Recommendations_for_recovery">Recommendations for recovery</span></a></h3>
<p>If you experience a non-catastrophic disk failure (for example you still have some of your data, but the indexer won't run), Splunk recommends that you move the index directory aside and restore from a backup rather than restoring on top of a partially corrupted datastore. The indexer will automatically create hot directories on startup as necessary and resume indexing. Monitored files and directories will pick up where they were at the time of the backup.
</p>
<h3> <a name="backupindexeddata_clustered_data_backups"><span class="mw-headline" id="Clustered_data_backups"> Clustered data backups </span></a></h3>
<p>Even though an <b>indexer cluster</b> already contains redundant copies of data, you might also want to back up the cluster data to another location; for example, to keep a copy of the data offsite as part of an overall disaster recovery plan.
</p><p>The simplest way to do this is to back up the data on each individual peer node on your cluster, in the same way that you back up data on individual, non-clustered indexers, as described earlier in this topic. However, this approach will result in backups of duplicate data. For example, if you have a cluster with a <b>replication factor</b> of 3, the cluster is storing three copies of all the data across its set of peer nodes. If you then back up the data residing on each individual node, you end up with backups containing, in total, three copies of the data. You cannot solve this problem by backing up just the data on a single node, since there's no certainty that a single node contains all the data in the cluster.
</p><p>The solution to this would be to identify exactly one copy of each bucket on the cluster and then back up just those copies. However, in practice, it is quite a complex matter to do that. One approach is to create a script that goes through each peer's index storage and uses the bucket ID value contained in the bucket name to identify exactly one copy of each bucket. The bucket ID is the same for all copies of a bucket. For information on the bucket ID, read <a href="#howsplunkstoresindexes_warm.2fcold_bucket_naming_convention" class="external text">"Warm/cold bucket naming convention"</a>. Another thing to consider when designing a cluster backup script is whether you want to back up just the bucket's rawdata or both its rawdata and index files. If the latter, the script must also identify a searchable copy of each bucket.
</p><p>Because of the complications of cluster backup, it is recommended that you contact Splunk Professional Services for guidance in backing up single copies of clustered data. They can help design a solution customized to the needs of your environment.
</p>
<a name="setaretirementandarchivingpolicy"></a><h2> <a name="setaretirementandarchivingpolicy_set_a_retirement_and_archiving_policy"><span class="mw-headline" id="Set_a_retirement_and_archiving_policy"> Set a retirement and archiving policy</span></a></h2>
<p>Configure data retirement and <b>archiving</b> policy by controlling the size of indexes or the age of data in indexes. 
</p><p>The indexer stores indexed data in directories called <b>buckets</b>. Buckets go through four stages of retirement. When indexed data reaches the final, frozen state, the indexer removes it from the index. You can configure the indexer to archive the data when it freezes, instead of deleting it entirely. See <a href="#automatearchiving" class="external text">"Archive indexed data"</a> for details. 
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Bucket stage
</th><th bgcolor="#C0C0C0">Description
</th><th bgcolor="#C0C0C0">Searchable?
</th></tr><tr><td width="10%" valign="center" align="left"><b>Hot</b>
</td><td width="75%" valign="center" align="left">Contains newly indexed data. Open for writing. One or more hot buckets for each index.
</td><td width="15%" valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Warm</b>
</td><td valign="center" align="left">Data rolled from hot. There are many warm buckets.
</td><td valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Cold</b>
</td><td valign="center" align="left">Data rolled from warm. There are many cold buckets.
</td><td valign="center" align="left">Yes
</td></tr><tr><td valign="center" align="left"><b>Frozen</b>
</td><td valign="center" align="left">Data rolled from cold. The indexer deletes frozen data by default, but you can also archive it. Archived data can later be thawed.
</td><td valign="center" align="left">No
</td></tr></table><p>You configure the sizes, locations, and ages of indexes and their buckets by editing indexes.conf, as described in <a href="#configureindexstorage" class="external text">"Configure index storage"</a>. 
</p><p><b>Caution:</b> When you change your data retirement and archiving policy settings, the indexer can delete old data without prompting you. 
</p>
<h3> <a name="setaretirementandarchivingpolicy_set_attributes_for_cold_to_frozen_rolling_behavior"><span class="mw-headline" id="Set_attributes_for_cold_to_frozen_rolling_behavior"> Set attributes for cold to frozen rolling behavior</span></a></h3>
<p>The <code><font size="2">maxTotalDataSizeMB</font></code> and <code><font size="2">frozenTimePeriodInSecs</font></code> attributes in <code><font size="2">indexes.conf</font></code> help determine when buckets roll from cold to frozen. These attributes are described in detail below.
</p>
<h4><font size="3"><b><i> <a name="setaretirementandarchivingpolicy_freeze_data_when_an_index_grows_too_large:_set_maxtotaldatasizemb"><span class="mw-headline" id="Freeze_data_when_an_index_grows_too_large:_Set_maxTotalDataSizeMB"> Freeze data when an index grows too large: Set maxTotalDataSizeMB </span></a></i></b></font></h4>
<p>You can use the size of an index to determine when data gets frozen and removed from the index. If an index grows larger than its maximum specified size, the oldest data is rolled to the frozen state.
</p><p>The default maximum size for an index is 500,000MB. To change the maximum size, edit the <code><font size="2">maxTotalDataSizeMB</font></code> attribute in <code><font size="2">indexes.conf</font></code>. For example, to specify the maximum size as 250,000MB:
</p>
<div class="samplecode">
<code><font size="2"><br>[main]<br>maxTotalDataSizeMB = 250000<br></font></code>
</div>
<p><b>Important:</b> Specify the size in megabytes. 
</p><p>Restart the indexer for the new setting to take effect. Depending on how much data there is to process, it can take some time for the indexer to begin to move buckets out of the index to conform to the new policy. You might see high CPU usage during this time. 
</p>
<h4><font size="3"><b><i> <a name="setaretirementandarchivingpolicy_freeze_data_when_it_grows_too_old:_set_frozentimeperiodinsecs"><span class="mw-headline" id="Freeze_data_when_it_grows_too_old:_Set_frozenTimePeriodInSecs"> Freeze data when it grows too old: Set frozenTimePeriodInSecs </span></a></i></b></font></h4>
<p>You can use the age of data to determine when a bucket gets rolled to frozen. When the most recent data in a particular bucket reaches the configured age, the entire bucket is rolled. 
</p><p>To specify the age at which data should freeze, edit the <code><font size="2">frozenTimePeriodInSecs</font></code> attribute in <code><font size="2">indexes.conf</font></code>. This attribute specifies the number of seconds to elapse before data gets frozen. The default value is 188697600 seconds, or approximately 6 years. This example configures the indexer to cull old events from its index when they become more than 180 days (15552000 seconds) old: 
</p>
<div class="samplecode">
<code><font size="2"><br>[main]<br>frozenTimePeriodInSecs = 15552000<br></font></code>
</div>
<p><b>Important:</b> Specify the time in seconds.
</p><p>Restart the indexer for the new setting to take effect. Depending on how much data there is to process, it can take some time for the indexer to begin to move buckets out of the index to conform to the new policy. You might see high CPU usage during this time.
</p>
<h3> <a name="setaretirementandarchivingpolicy_archive_data"><span class="mw-headline" id="Archive_data"> Archive data </span></a></h3>
<p>If you want to archive frozen data instead of deleting it entirely, you must tell the indexer to do so, as described in <a href="#automatearchiving" class="external text">"Archive indexed data"</a>. You can create your own archiving script or you can just let the indexer handle the archiving for you. You can later restore ("thaw") the archived data, as described in <a href="#restorearchiveddata" class="external text">"Restore archived data"</a>.
</p>
<h3> <a name="setaretirementandarchivingpolicy_other_ways_that_buckets_age"><span class="mw-headline" id="Other_ways_that_buckets_age"> Other ways that buckets age</span></a></h3>
<p>There are a number of other conditions that can cause buckets to roll from one stage to another, some of which can also trigger deletion or archiving. These are all configurable, as described in <a href="#configureindexstorage" class="external text">"Configure index storage"</a>. For a full understanding of all your options for controlling retirement policy, read that topic and look at the indexes.conf spec file.
</p><p>For example, the indexer rolls buckets when they reach their maximum size. You can reduce bucket size by setting a smaller <code><font size="2">maxDataSize</font></code> in <code><font size="2">indexes.conf</font></code> so they roll faster. But note that it takes longer to search more small buckets than fewer large buckets. To get the results you are after, you will have to experiment a bit to determine the right size for your buckets.
</p>
<h3> <a name="setaretirementandarchivingpolicy_troubleshoot_the_archive_policy"><span class="mw-headline" id="Troubleshoot_the_archive_policy">Troubleshoot the archive policy </span></a></h3>
<h4><font size="3"><b><i> <a name="setaretirementandarchivingpolicy_i_ran_out_of_disk_space_so_i_changed_the_archive_policy.2c_but_it.27s_still_not_working"><span class="mw-headline" id="I_ran_out_of_disk_space_so_I_changed_the_archive_policy.2C_but_it.27s_still_not_working">I ran out of disk space so I changed the archive policy, but it's still not working</span></a></i></b></font></h4>
<p>If you changed your archive policy to be more restrictive because you've run out of disk space, you may notice that events haven't started being archived according to your new policy. This is most likely because you must first free up some space so the process has room to run. Stop the indexer, clear out ~5GB of disk space, and then start the indexer again. After a while (exactly how long depends on how much data there is to process) you should see INFO entries about <code><font size="2">BucketMover</font></code> in <code><font size="2">splunkd.log</font></code> showing that buckets are being archived.
</p>
<a name="automatearchiving"></a><h2> <a name="automatearchiving_archive_indexed_data"><span class="mw-headline" id="Archive_indexed_data"> Archive indexed data</span></a></h2>
<p>You can configure the indexer to archive your data automatically as it ages; specifically, at the point when it rolls to "frozen".  To do this, you configure indexes.conf. 
</p><p><b>Caution:</b> By default, the indexer deletes all frozen data. It removes the data from the index at the moment it becomes frozen. If you need to keep the data around, you <b>must</b> configure the indexer to archive the data before removing it. You do this  by either setting the <code><font size="2">coldToFrozenDir</font></code> attribute or specifying a valid <code><font size="2">coldToFrozenScript</font></code> in <code><font size="2">indexes.conf</font></code>.
</p><p>For detailed information on data storage, see <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>. For information on editing <code><font size="2">indexes.conf</font></code>, see <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</p>
<h3> <a name="automatearchiving_how_the_indexer_archives_data"><span class="mw-headline" id="How_the_indexer_archives_data">How the indexer archives data</span></a></h3>
<p>The indexer rotates old data out of the index based on your data retirement policy, as described in <a href="#setaretirementandarchivingpolicy" class="external text">"Set a retirement and archiving policy"</a>. Data moves through several stages, which correspond to file directory locations. Data starts out in the <b>hot</b> database, located as subdirectories ("<b>buckets</b>") under <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db/</font></code>.  It then moves to the <b>warm</b> database, also located as subdirectories under <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/db</font></code>.  Eventually, data is aged into the <b>cold</b> database <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/colddb</font></code>.  
</p><p>Finally, data reaches the <b>frozen</b> state. This can happen for a number of reasons, as described in  <a href="#setaretirementandarchivingpolicy" class="external text">"Set a retirement and archiving policy"</a>. At this point, the indexer erases the data from the index. If you want the indexer to archive the frozen data before erasing it from the index, you must specify that behavior. You can choose two ways of handling the archiving:
</p>
<ul><li> <a href="#automatearchiving_let_splunk_archive_the_data_for_you" class="external text">Let the indexer perform the archiving automatically.</a>
</li><li> <a href="#automatearchiving_specify_an_archiving_script" class="external text">Specify an archiving script for the indexer to run.</a>
</li></ul><p>The archiving behavior depends on which of these <code><font size="2">indexes.conf</font></code> attributes you set:
</p>
<ul><li> <code><font size="2">coldToFrozenDir</font></code>. This attribute specifes a location where the indexer will automatically archive frozen data.
</li><li> <code><font size="2">coldToFrozenScript</font></code>. This attribute specifes a user-supplied script that the indexer will run when the data is frozen. Typically, this will be a script that archives the frozen data. The script can also serve some other purpose altogether. While the indexer ships with one example archiving script that you can edit and use (<code><font size="2">$SPLUNK_HOME/bin/coldToFrozenExample.py</font></code>), you can actually specify any script you want the indexer to run. 
</li></ul><p><b>Note:</b> You can only set one or the other of these attributes. The <code><font size="2">coldToFrozenDir</font></code> attribute takes precedence over <code><font size="2">coldToFrozenScript</font></code>, if both are set.
</p><p>If you don't specify either of these attributes, the indexer runs a default script that simply writes the name of the bucket being erased to the log file <code><font size="2">$SPLUNK_HOME/var/log/splunk/splunkd_stdout.log</font></code>. It then erases the bucket.
</p>
<h4><font size="3"><b><i> <a name="automatearchiving_let_the_indexer_archive_the_data_for_you"><span class="mw-headline" id="Let_the_indexer_archive_the_data_for_you"> Let the indexer archive the data for you </span></a></i></b></font></h4>
<p>If you set the <code><font size="2">coldToFrozenDir</font></code> attribute in <code><font size="2">indexes.conf</font></code>, the indexer will automatically copy frozen buckets to the specified location before erasing the data from the index. 
</p><p>Add this stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/indexes.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;index&gt;]<br>coldToFrozenDir = "&lt;path to frozen archive&gt;"<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">&lt;index&gt;</font></code> specifies which index contains the data to archive.
</li><li> <code><font size="2">&lt;path to frozen archive&gt;</font></code> specifies the directory where the indexer will put the archived buckets.
</li></ul><p><b>Note:</b> When you use Splunk Web to create a new index, you can also specify a frozen archive path for that index. See <a href="#setupmultipleindexes_use_splunk_web" class="external text">"Set up multiple indexes"</a> for details.
</p><p>How the indexer archives the frozen data depends on whether the data was originally indexed in a pre-4.2 release:
</p>
<ul><li> For buckets created from version 4.2 and on, the indexer will remove all files except for the <b>rawdata file</b>. 
</li></ul><ul><li> For pre-4.2 buckets, the script simply gzip's all the <code><font size="2">.tsidx</font></code> and <code><font size="2">.data</font></code> files in the bucket.
</li></ul><p>This difference is due to a change in the format of rawdata. Starting with 4.2, the rawdata file contains all the information the indexer needs to reconstitute an index bucket.
</p><p>For information on thawing these buckets, see <a href="#restorearchiveddata" class="external text">"Restore archived indexed data"</a>.
</p>
<h4><font size="3"><b><i> <a name="automatearchiving_specify_an_archiving_script"><span class="mw-headline" id="Specify_an_archiving_script"> Specify an archiving script </span></a></i></b></font></h4>
<p>If you set the <code><font size="2">coldToFrozenScript</font></code> attribute in <code><font size="2">indexes.conf</font></code>, the script you specify will run just before the indexer erases the frozen data from the index. 
</p><p>You'll need to supply the actual script. Typically, the script will archive the data, but you can provide a script that performs any action you want.
</p><p>Add this stanza to <code><font size="2">$SPLUNK_HOME/etc/system/local/indexes.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;index&gt;]<br>coldToFrozenScript = ["&lt;path to program that runs script&gt;"] "&lt;path to script&gt;"<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> <code><font size="2">&lt;index&gt;</font></code> specifies which index contains the data to archive.
</li><li> <code><font size="2">&lt;path to script&gt;</font></code> specifies the path to the archiving script. The script must be in <code><font size="2">$SPLUNK_HOME/bin</font></code> or one of its subdirectories. 
</li><li> <code><font size="2">&lt;path to program that runs script&gt;</font></code> is optional. You must set it if your script requires a program, such as python, to run it.
</li><li> If your script is located in <code><font size="2">$SPLUNK_HOME/bin</font></code> and is named <code><font size="2">myColdToFrozen.py</font></code>, set the attribute like this: 
</li></ul><div class="samplecode">
<code><font size="2"><br>coldToFrozenScript = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/bin/myColdToFrozen.py"<br></font></code>
</div>
<ul><li> For detailed information on the archiving script, see the indexes.conf spec file. 
</li></ul><p>The indexer ships with an example archiving script that you can edit, <code><font size="2">$SPLUNK_HOME/bin/coldToFrozenExample.py</font></code>. 
</p><p><b>Note:</b> If using the example script, edit it to specify the archive location for your installation. Also, rename the script or move it to another location to avoid having changes overwritten when you upgrade the indexer. <b>This is an example script</b> and should not be applied to a production instance without editing to suit your environment and testing extensively.
</p><p>The example script archives the frozen data differently, depending on whether the data was originally indexed in a pre-4.2 release:
</p>
<ul><li> For buckets created from version 4.2 and on, it will remove all files except for the rawdata file. 
</li></ul><ul><li> For pre-4.2 buckets, the script simply gzip's all the <code><font size="2">.tsidx</font></code> and <code><font size="2">.data</font></code> files. 
</li></ul><p>This difference is due to a change in the format of rawdata. Starting with 4.2, the rawdata file contains all the information the indexer needs to reconstitute an index bucket. 
</p><p>For information on thawing these buckets, see <a href="#restorearchiveddata" class="external text">"Restore archived indexed data"</a>.
</p><p>As a best practice, make sure the script you create completes as quickly as possible, so that the indexer doesn't end up waiting for the return indicator. For example, if you want to archive to a slow volume, set the script to copy the buckets to a temporary location on the same (fast) volume as the index. Then use a separate script, outside the indexer, to move the buckets from the temporary location to their destination on the slow volume.
</p>
<h3> <a name="automatearchiving_clustered_data_archiving"><span class="mw-headline" id="Clustered_data_archiving"> Clustered data archiving </span></a></h3>
<p><b>Indexer clusters</b> contain redundant copies of indexed data. If you archive that data using the techniques described above, you archive multiple copies of the data.
</p><p>For example, if you have a cluster with a <b>replication factor</b> of 3, the cluster stores three copies of all its data across its set of peer nodes. If you set up each peer node to archive its own data when it rolls to frozen, you end up with three archived copies of the data. You cannot solve this problem by archiving just the data on a single node, since there's no certainty that a single node contains all the data in the cluster.
</p><p>The solution to this would be to archive just one copy of each bucket on the cluster and discard the rest. However, in practice, it is quite a complex matter to do that. If you want guidance in archiving single copies of clustered data, contact Splunk Professional Services. They can help design a solution customized to the needs of your environment.
</p>
<h4><font size="3"><b><i> <a name="automatearchiving_specifying_the_archive_destination"><span class="mw-headline" id="Specifying_the_archive_destination">Specifying the archive destination</span></a></i></b></font></h4>
<p>If you choose to take the easy approach and archive multiple copies of the clustered data, you must guard against name collisions. You cannot route the data from all peer nodes into a single archive directory, because multiple, identically named copies of the bucket will exist across the cluster (for deployments where replication factor &gt; 2), and the contents of a directory must be named uniquely. Instead, you need to ensure that the buckets from each of your peer nodes go to a separate archive directory. This, of course, will be somewhat difficult to manage if you specify a destination directory in shared storage by means of the <code><font size="2">coldToFrozenDir</font></code> attribute  in <code><font size="2">indexes.conf</font></code>, because the <code><font size="2">indexes.conf</font></code> file must be the same across all peer nodes, as discussed in <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>.  One alternative approach would be to create a script that directs each peer's archived buckets to a separate location on the shared storage, and then use the <code><font size="2">coldToFrozenScript</font></code> attribute to specify that script.
</p>
<a name="restorearchiveddata"></a><h2> <a name="restorearchiveddata_restore_archived_indexed_data"><span class="mw-headline" id="Restore_archived_indexed_data"> Restore archived indexed data</span></a></h2>
<p>You restore archived data by moving the archived <b>bucket</b> into your thawed directory (for example, <code><font size="2">$SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb</font></code>) and then processing it, as described later in this topic.  Data in <code><font size="2">thaweddb</font></code> is not subject to the server's index aging scheme (hot &gt; warm&gt; cold &gt; frozen).  You can put archived data in the thawed directory for as long as you need it. When the data is no longer needed, simply delete it or move it out of thawed.
</p><p><b>Important:</b> You restore archived data differently depending on whether it was originally indexed in Splunk Enterprise version 4.2 or later. This is because Splunk Enterprise changed its rawdata format in 4.2.
</p><p>See <a href="#automatearchiving" class="external text">"Archive indexed data"</a> for information on how to archive data in the first place. You can also use that page as guidance if you want to re-archive data after you've thawed it.
</p>
<h3> <a name="restorearchiveddata_restrictions_when_restoring_an_archive_to_a_different_instance_of_the_indexer"><span class="mw-headline" id="Restrictions_when_restoring_an_archive_to_a_different_instance_of_the_indexer"> Restrictions when restoring an archive to a different instance of the indexer </span></a></h3>
<p>For the most part, you can restore an archive to any instance of the indexer, not just the one that originally indexed it. This, however, depends on a couple of factors:
</p>
<ul><li> <b>Splunk Enterprise version.</b> You cannot restore a bucket created by Splunk  Enterprise 4.2 or later to a pre-4.2 indexer. The bucket data format changed between 4.1 and 4.2, and pre-4.2 indexers do not understand the new format. This means: 
<ul><li><b>4.2+ buckets:</b> You can restore a 4.2+ bucket to any 4.2+ instance. 
</li><li><b>Pre-4.2 buckets:</b> You can restore a pre-4.2 bucket to any indexer, pre-4.2 or post-4.2, aside from a few OS-related issues, described in the next bullet. 
</li></ul></li></ul><ul><li> <b>OS version.</b> You can usually restore buckets to an indexer running on a different OS. Specifically:
<ul><li><b>4.2+ buckets:</b> You can restore a 4.2+ bucket to an indexer running any operating system.  
</li><li><b>Pre-4.2 buckets:</b> You can restore a pre-4.2 bucket to an indexer running any operating system, with the restriction that you cannot restore pre-4.2 data to a system of different endian-ness. For example, data generated on 64-bit systems is not likely to work well on 32-bit systems, and data cannot be moved from PowerPC or Sparc systems to x86 or x86-64 systems, and vice versa. 
</li></ul></li></ul><p>In addition, make sure that you do not introduce bucket ID conflicts to your index when restoring the archived bucket. This issue is discussed later.
</p>
<h3> <a name="restorearchiveddata_how_to_tell_whether_your_archive_bucket_contains_4.2.2b_data"><span class="mw-headline" id="How_to_tell_whether_your_archive_bucket_contains_4.2.2B_data"> How to tell whether your archive bucket contains 4.2+ data </span></a></h3>
<p>Before thawing the archive bucket, you need to identify whether the archive bucket is pre- or post-4.2. Here's how to tell the difference, assuming you archived the buckets using <code><font size="2">coldToFrozenDir</font></code> or the provided example script:
</p>
<ul><li><b>4.2+ bucket:</b> The bucket directory contains only the rawdata directory, which contains <code><font size="2">journal.gz</font></code>.
</li><li><b>Pre-4.2 bucket:</b> The bucket directory contains gzipped versions of <code><font size="2">.tsidx</font></code> and <code><font size="2">.data</font></code> files, along with a rawdata directory containing files named <code><font size="2">&lt;int&gt;.gz</font></code>. 
</li></ul><p><b>Important:</b> If you archived the data through some script of your own, the resulting bucket could contain just about anything.
</p><p>If you archived the buckets using <code><font size="2">coldToFrozenDir</font></code> or the provided example script, you can use the following procedures to thaw them.
</p>
<h3> <a name="restorearchiveddata_thaw_a_4.2.2b_archive"><span class="mw-headline" id="Thaw_a_4.2.2B_archive"> Thaw a 4.2+ archive </span></a></h3>
<h4><font size="3"><b><i> <a name="restorearchiveddata_.2anix_users"><span class="mw-headline" id=".2Anix_users"> *nix users </span></a></i></b></font></h4>
<p>Here is an example of safely restoring a 4.2+ archive bucket to thawed:
</p><p><b>1.</b> Copy your archive bucket into the thawed directory:
</p>
<div class="samplecode">
<code><font size="2"><br>cp -r db_1181756465_1162600547_1001 $SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb<br></font></code>
</div>
<p><b>Note: </b> The bucket id cannot conflict with any other bucket in the index. This example assumes that the bucket id '1001' is unique for the index. If it isn't, choose some other, non-conflicting bucket ID.
</p><p><b>2.</b> Execute the <code><font size="2">splunk rebuild</font></code> command on the archive bucket to rebuild the indexes and associated files:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rebuild $SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/db_1181756465_1162600547_1001<br></font></code>
</div>
<p><b>3.</b> Restart the indexer:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk restart <br></font></code>
</div>
<h4><font size="3"><b><i> <a name="restorearchiveddata_windows_users"><span class="mw-headline" id="Windows_users"> Windows users </span></a></i></b></font></h4>
<p>Here is an example of safely restoring a 4.2+ archive bucket to thawed:
</p><p><b>1.</b> Copy your archive bucket into the thawed directory:
</p>
<div class="samplecode">
<code><font size="2"><br>xcopy D:\MyArchive\db_1181756465_1162600547_1001&nbsp;%SPLUNK_HOME%\var\lib\splunk\defaultdb\thaweddb\db_1181756465_1162600547_1001 /s /e /v</font></code>
</div>
<p><b>Note: </b> The bucket id cannot conflict with any other bucket in the index. This example assumes that the bucket id '1001' is unique for the index. If it isn't, choose some other, non-conflicting bucket ID.
</p><p><b>2.</b> Execute the <code><font size="2">splunk rebuild</font></code> command on the archive bucket to rebuild the indexes and associated files:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rebuild&nbsp;%SPLUNK_HOME%\var\lib\splunk\defaultdb\thaweddb\db_1181756465_1162600547_1001<br></font></code>
</div>
<p><b>3.</b> Restart the indexer:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk restart <br></font></code>
</div>
<h3> <a name="restorearchiveddata_thaw_a_pre-4.2_archive"><span class="mw-headline" id="Thaw_a_pre-4.2_archive"> Thaw a pre-4.2 archive </span></a></h3>
<h4><font size="3"><b><i> <a name="restorearchiveddata_.2anix_users_2"><span class="mw-headline" id=".2Anix_users_2"> *nix users </span></a></i></b></font></h4>
<p>Here is an example of safely restoring a pre-4.2 archive bucket to thawed:
</p><p><b>1.</b> Copy your archive bucket to a temporary location in the thawed directory:
</p>
<div class="samplecode">
<code><font size="2"><br># cp -r db_1181756465_1162600547_0 &nbsp;$SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/temp_db_1181756465_1162600547_0<br></font></code>
</div>
<p><b>2.</b> If the bucket was compressed when originally archived,  uncompress the contents in the thawed directory.
</p><p><b>3.</b> Rename the temporary bucket to something that the indexer will recognize:
</p>
<div class="samplecode">
<code><font size="2"><br># cd $SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/<br># mv temp_db_1181756465_1162600547_0 db_1181756465_1162600547_1001<br></font></code>
</div>
<p><b>Note: </b> You must choose a bucket id that does not conflict with any other bucket in the index. This example assumes that the bucket id '1001' is unique for the index. If it isn't, choose some other, non-conflicting bucket ID.
</p><p><b>4.</b> Refresh the manifests:
</p>
<div class="samplecode">
<code><font size="2"><br># cd $SPLUNK_HOME/bin<br># ./splunk login<br># ./splunk _internal call /data/indexes/main/rebuild-metadata-and-manifests <br></font></code>
</div>
<p>After a few moments, the contents of your newly thawed bucket should be searchable again.
</p>
<h4><font size="3"><b><i> <a name="restorearchiveddata_windows_users_2"><span class="mw-headline" id="Windows_users_2"> Windows users </span></a></i></b></font></h4>
<p>Here is an example of safely restoring a pre-4.2 archive bucket to thawed:
</p><p><b>1.</b> Copy your archive bucket to the thawed directory:
</p>
<div class="samplecode">
<code><font size="2"><br>&gt; xcopy D:\MyArchive\db_1181756465_1162600547_0&nbsp;%SPLUNK_HOME%\var\lib\splunk\defaultdb\thaweddb\temp_db_1181756465_1162600547_0 /s /e /v<br></font></code>
</div>
<p><b>2.</b> If the bucket was compressed when originally archived,  uncompress the contents in the thawed directory.
</p><p><b>3.</b> Rename the temporary bucket to something that the indexer will recognize:
</p>
<div class="samplecode">
<code><font size="2"><br>&gt; cd&nbsp;%SPLUNK_HOME%\var\lib\splunk\defaultdb\thaweddb<br>&gt; move temp_db_1181756465_1162600547_0 db_1181756465_1162600547_1001<br></font></code>
</div>
<p><b>Note: </b> You must choose a bucket id that does not conflict with any other bucket in the index. This example assumes that the bucket id '1001' is unique for the index. If it isn't, choose some other, non-conflicting bucket ID.
</p><p><b>4.</b> Refresh the manifests:
</p>
<div class="samplecode">
<code><font size="2"><br>&gt; cd&nbsp;%SPLUNK_HOME%\bin<br>&gt; splunk login<br>&gt; splunk _internal call /data/indexes/main/rebuild-metadata-and-manifests<br></font></code>
</div>
<p>After a few moments, the contents of your newly thawed bucket should be searchable again.
</p>
<h3> <a name="restorearchiveddata_clustered_data_thawing"><span class="mw-headline" id="Clustered_data_thawing"> Clustered data thawing </span></a></h3>
<p>You can thaw archived clustered data onto individual peer nodes the same way that you thaw data onto any individual indexer. However, as described in <a href="#automatearchiving" class="external text">"Archive indexed data"</a>, it is difficult to archive just a single copy of clustered data in the first place. If, instead, you archive data across all peer nodes in a cluster, you can later thaw the data, placing the data into the thawed directories of the peer nodes from which it was originally archived. You will end up with replication factor copies of the thawed data on your cluster, since you are thawing all of the original data, including the copies.
</p><p><b>Note:</b> Data does not get replicated from the thawed directory. So, if you thaw just a single copy of some bucket, instead of all the copies, only that single copy will reside in the cluster, in the thawed directory of the peer node where you placed it.
</p>
<h1>Overview of indexer clusters and index replication</h1><a name="aboutclusters"></a><h2> <a name="aboutclusters_about_indexer_clusters_and_index_replication"><span class="mw-headline" id="About_indexer_clusters_and_index_replication"> About indexer clusters and index replication</span></a></h2>
<p><b>Indexer clusters</b> are groups of Splunk Enterprise indexers configured to replicate each others' data, so that the system keeps multiple copies of all data. This process is known as 
<b>index replication</b>. By maintaining multiple, identical copies of Splunk Enterprise data, clusters prevent data loss while promoting data availability for searching. 
</p><p>Indexer clusters feature automatic failover from one indexer to the next. This means that, if one or more indexers fail, incoming data continues to get indexed and indexed data continues to be searchable.
</p><p>The key benefits of index replication are:
</p>
<ul><li> <b>Data availability.</b> An indexer is always available to handle incoming data, and the indexed data is available for searching.
</li><li> <b>Data fidelity.</b> You never lose any data. You have assurance that the data sent to the cluster is exactly the same data that gets stored in the cluster and that a search can later access.
</li><li> <b>Data recovery.</b> Your system can tolerate downed indexers without losing data or losing access to data.
</li><li> <b>Disaster recovery.</b> With multisite clustering, your system can tolerate the failure of an entire data center.
</li><li> <b>Search affinity.</b> With multisite clustering, search heads can access the entire set of data through their local sites, greatly reducing long-distance network traffic.
</li></ul><p>The key trade-off in index replication is between the benefits of data availability/recovery and the costs of storage (and, to a minor degree, increased processing load). The degree of data recovery that the cluster possesses is directly proportional to the number of copies of data it maintains. But maintaining more copies of data means higher storage requirements. To manage this trade-off to match the needs of your enterprise, you can configure the number of copies of data that you want the cluster to maintain. This is known as the  <b>replication factor</b>.
</p><p>You can also use clusters to scale indexing capacity, even in situations where index replication is not a requirement. See <a href="#clustersinscaledoutdeployments" class="external text">"Use indexer clusters to scale indexing"</a>.
</p><p><b>Note:</b> <b>Search head clusters</b> provide high availability and scalabilty for groups of search heads. They are a separate feature from indexer clusters, but you can combine them with indexer clusters to build a high availability, scalable solution across your entire Splunk Enterprise deployment. See "About search head clustering" in the <i>Distributed Search</i> manual.
</p>
<h3> <a name="aboutclusters_parts_of_an_indexer_cluster"><span class="mw-headline" id="Parts_of_an_indexer_cluster"> Parts of an indexer cluster </span></a></h3>
<p>An indexer cluster is a group of Splunk Enterprise instances, or <b>nodes</b>, that, working in concert, provide a redundant indexing and searching capability. Each cluster has three types of nodes:
</p>
<ul><li> A single <b>master node</b> to manage the cluster.
</li><li> Several to many <b>peer nodes</b> to index and maintain multiple copies of the data and to search the data.
</li><li> One or more  <b>search heads</b> to coordinate searches across the set of peer nodes.
</li></ul><p>The <b>master node</b> manages the cluster. It coordinates the replicating activities of the peer nodes and tells the search head where to find data. It also helps manage the configuration of peer nodes and orchestrates remedial activities if a peer goes down.
</p><p>The <b>peer nodes</b> receive and index incoming data, just like non-clustered, stand-alone indexers. Unlike stand-alone indexers, however, peer nodes also replicate data from other nodes in the cluster. A peer node can index its own incoming data while simultaneously storing copies of data from other nodes. You must have at least as many peer nodes as the replication factor. That is, to support a replication factor of 3, you need a minimum of three peer nodes.
</p><p>The <b>search head</b> runs searches across the set of peer nodes. You must use a search head to manage searches across indexer clusters.  
</p><p>For most purposes, it is recommended that you use <b>forwarders</b> to get data into the cluster.
</p><p>Here is a diagram of a basic, single-site indexer cluster, containing three peer nodes and supporting a replication factor of 3:
</p><p><img alt="Simplified basic cluster 60.png" src="images/5/58/Simplified_basic_cluster_60.png" width="700" height="554"></p><p>This diagram shows a simple deployment, similar to a small-scale non-clustered deployment, with some forwarders sending load-balanced data to a group of indexers (peer nodes), and the indexers sending search results to a search head. There are two additions that you don't find in a non-clustered deployment:
</p>
<ul><li> The indexers are streaming copies of their data to other indexers.
</li><li> The master node, while it doesn't participate in any data streaming, coordinates a range of activities involving the search peers and the search head.
</li></ul><h3> <a name="aboutclusters_multisite_indexer_clusters"><span class="mw-headline" id="Multisite_indexer_clusters"> Multisite indexer clusters</span></a></h3>
<p><b>Multisite indexer clusters</b> allow you to maintain complete copies of your indexed data in multiple locations. This offers the advantages of enhanced disaster recovery and <b>search affinity</b>. You can specify the number of copies of data on each site. Multisite clusters are similar in most respects to basic, single-site clusters, with some differences in configuration and behavior. See <a href="#multisiteclusters" class="external text">"Multisite indexer clusters"</a>.
</p>
<h3> <a name="aboutclusters_how_to_set_up_a_cluster"><span class="mw-headline" id="How_to_set_up_a_cluster"> How to set up a cluster </span></a></h3>
<p>Clusters are easy to set up. The process is similar to setting up a group of stand-alone indexers. Basically, you install the indexers and perform a bit of configuration.
</p><p>The main difference is that you also need to identify and enable the cluster nodes. You designate one Splunk Enterprise instance as the master node and other instances as peer nodes. You need at least as many peer nodes as the size of your replication factor. To increase indexing capacity for horizontal scaling, you just add more peer nodes. 
</p><p>You also need to set up one or more search heads to manage searches across the peers and to consolidate the results for the user. 
</p><p>You enable cluster nodes in the same way that you configure any settings in Splunk Enterprise: through Splunk Web or the CLI, or directly, by editing configuration files. 
</p><p>See the chapter <a href="#clusterdeploymentoverview" class="external text">"Deploy the indexer cluster"</a>.
</p>
<h3> <a name="aboutclusters_how_to_search_a_cluster"><span class="mw-headline" id="How_to_search_a_cluster"> How to search a cluster </span></a></h3>
<p>You search a cluster the same way you search any non-clustered group of indexers. You submit your searches through a search head. 
</p><p>What happens behind the scenes is a bit different, though. Once you have submitted your search, the search head consults the master node to determine the current set of peer nodes. The search head then distributes the search tasks directly to those peers. The peers do their part and send their results back to the search head, which then consolidates the results and returns them to Splunk Web. From the user's standpoint, it is no different than searching any standalone indexer or non-clustered group of indexers. See <a href="#howclusteredsearchworks" class="external text">"How search works in an indexer cluster."</a>
</p><p>With a multisite cluster, you can also implement search affinity. In search affinity, a search head gets search results only from indexers local to its site, when possible. At the same time, the search still has access to the full set of data. See <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster."</a>
</p>
<h3> <a name="aboutclusters_before_you_go_any_further"><span class="mw-headline" id="Before_you_go_any_further"> Before you go any further </span></a></h3>
<p>Clusters are easy to set up and use, but you need to have a good grounding in the basics of Splunk Enterprise indexing and deployment first. Before you continue, make sure you know this stuff:
</p>
<ul><li> <b>How to configure indexers.</b> See <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>, along with the other topics in the current manual that describe managing indexes.
</li><li> <b>What a search head does.</b> For an introduction to distributed search and search heads, see "About distributed search" in the <i>Distributed Search</i> manual.
</li><li> <b>How to use a forwarder to get data into an indexer.</b> See "Use forwarders" in the <i>Getting Data In</i> manual.
</li></ul><h3> <a name="aboutclusters_migrating_from_a_non-clustered_splunk_enterprise_deployment.3f"><span class="mw-headline" id="Migrating_from_a_non-clustered_Splunk_Enterprise_deployment.3F">Migrating from a non-clustered Splunk Enterprise deployment?</span></a></h3>
<p>Clustered indexers have several different requirements from non-clustered indexers. It is important that  you be aware of these issues before you migrate your indexers. For details, see <a href="#keydifferences" class="external text">"Key differences between clustered and non-clustered Splunk Enterprise deployments of indexers"</a>.  After you read that material, go to <a href="#migratenon-clusteredindexerstoaclusteredenvironment" class="external text">"Migrate non-clustered indexers to a clustered environment"</a> for details on the actual migration process.
</p>
<a name="multisiteclusters"></a><h2> <a name="multisiteclusters_multisite_indexer_clusters"><span class="mw-headline" id="Multisite_indexer_clusters"> Multisite indexer clusters</span></a></h2>
<p>In Splunk Enterprise 6.1, indexer clusters have built-in site-awareness, meaning that you can explicitly configure a <b>multisite indexer cluster</b> on a site-by-site basis. This simplifies and extends the ability to implement a cluster that spans multiple physical sites, such as data centers.
</p>
<h3> <a name="multisiteclusters_use_cases"><span class="mw-headline" id="Use_cases">Use cases</span></a></h3>
<p>Multisite clusters offer two key benefits over single-site clusters:
</p>
<ul><li> <b>Improved disaster recovery.</b> By storing copies of your data at multiple locations, you maintain access to the data if a disaster strikes at one location. Multisite clusters provide site failover capability. If a site goes down, indexing and searching can continue on the remaining sites, without interruption or loss of data.
</li></ul><ul><li> <b>Search affinity.</b> If you configure each site so that it has both a search head and a full set of searchable data, the search head on each site will limit its searches to local peer nodes. This eliminates any need, under normal conditions, for search heads to access data on other sites, greatly reducing network traffic between sites.
</li></ul><h3> <a name="multisiteclusters_multisite_configuration"><span class="mw-headline" id="Multisite_configuration">Multisite configuration</span></a></h3>
<p>You configure multisite clusters somewhat differently from basic, single-site clusters.  These are the key differences for multisite clusters:
</p>
<ul><li> You assign a site to each node.
</li><li> You can specify the replication and search factors on a site-by-site basis.  That is, you can specify the number of copies and searchable copies that you want to maintain on each site, along with the number that you want to maintain on the cluster overall.  
</li></ul><p>There are a few other configuration differences as well. See <a href="#multisitedeploymentoverview" class="external text">"Multisite deployment overview"</a>.
</p>
<h3> <a name="multisiteclusters_multisite_architecture"><span class="mw-headline" id="Multisite_architecture">Multisite architecture</span></a></h3>
<p>The architecture of single-site and multisite clusters is similar. These are the main differences for multisite clusters:
</p>
<ul><li> Each node belongs to an assigned site.  
</li><li> Replication of bucket copies occurs in a site-aware manner.
</li><li> Search heads distribute their searches across local peers only, whenever possible. 
</li></ul><p>For more information on multisite cluster architecture, read <a href="#multisitearchitecture" class="external text">"Multisite indexer cluster architecture"</a>.
</p>
<h3> <a name="multisiteclusters_for_more_information"><span class="mw-headline" id="For_more_information">For more information</span></a></h3>
<p>These chapters and topics describe multisite clusters in detail:
</p>
<ul><li> <a href="#multisitedeploymentoverview" class="external text">"Deploy and configure a multisite indexer cluster"</a>. This chapter contains topics that describe multisite configuration, including search affinity configuration and multisite replication and search factors.
</li><li> <a href="#moveapeertoanewsite" class="external text">"Manage a multisite indexer cluster"</a>. This chapter covers issues such as handling master site failure and converting a multisite cluster to single-site.
</li><li> <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>. This topic describes how to convert a single-site cluster to multisite.
</li></ul><p>Other topics in this manual differentiate multisite and single-site clusters as needed.
</p>
<a name="basicclusterarchitecture"></a><h2> <a name="basicclusterarchitecture_the_basics_of_indexer_cluster_architecture"><span class="mw-headline" id="The_basics_of_indexer_cluster_architecture"> The basics of indexer cluster architecture</span></a></h2>
<p>This topic introduces <b>indexer cluster</b> architecture. It describes the nodes of a single-site cluster and how they work together. It also covers some essential concepts and describes briefly how clusters handle indexing and searching. 
</p><p>Multisite cluster architecture is similar to single-site cluster architecture. There are, however, a few areas of significant difference. For information on multisite cluster architecture and how it differs from single-site cluster architecture, read the topic <a href="#multisitearchitecture" class="external text">"Multisite indexer cluster architecture"</a>.
</p><p>For a deeper dive into cluster architecture, read the chapter <a href="#basicconcepts" class="external text">"How indexer clusters work"</a>.
</p>
<h3> <a name="basicclusterarchitecture_cluster_nodes"><span class="mw-headline" id="Cluster_nodes">Cluster nodes</span></a></h3>
<p>A cluster includes three types of nodes:
</p>
<ul><li> A single <b>master node</b> to manage the cluster.
</li><li> Multiple <b>peer nodes</b> to index and replicate data and to search the data.
</li><li> One or more  <b>search heads</b> to coordinate searches across all the peer nodes.
</li></ul><p>In addition, a cluster deployment usually employs  <b>forwarders</b> to ingest and forward data to the peers.
</p><p>Master nodes, peer nodes, and search heads are all specialized Splunk Enterprise instances.  All nodes must reside on separate instances and separate machines. For example, the master node cannot reside on the same instance or machine as a peer node or a search head.
</p><p>Here is a diagram of a simple single-site cluster, with a few peers and some forwarders sending data to them:
</p><p><img alt="Basic cluster 60.png" src="images/e/e2/Basic_cluster_60.png" width="700" height="553"></p><p>Some of what is happening in this diagram might not make sense yet; read on.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_master_node"><span class="mw-headline" id="Master_node">Master node</span></a></i></b></font></h4>
<p>The <b>master node</b> manages the cluster. It coordinates the replicating activities of the peer nodes and tells the search head where to find data. It also helps manage the configuration of peer nodes and orchestrates remedial activities if a peer goes offline. 
</p><p>Unlike the peer nodes, the master does not index <b>external data</b>. A cluster has exactly one master node. 
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_peer_node"><span class="mw-headline" id="Peer_node">Peer node</span></a></i></b></font></h4>
<p><b>Peer nodes</b> perform the indexing function for the cluster. They receive and index incoming data. They also send replicated data to other peer nodes in the cluster and receive replicated data from other peers. A peer node can index its own external data while simultaneously receiving and sending replicated data. Like all indexers, peers also search across their indexed data in response to search requests from the search head.
</p><p>The number of peer nodes you deploy is dependent on two factors: the cluster <b>replication factor</b> and the indexing load. For example, if you have a replication factor of 3 (which means you intend to store three copies of your data), you need at least three peers. If you have more indexing load than three indexers can handle, you can add more peers to increase capacity.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_search_head"><span class="mw-headline" id="Search_head">Search head</span></a></i></b></font></h4>
<p>The <b>search head</b> manages searches across the set of peer nodes. It distributes search queries to the peers and consolidates the results. You initiate all searches from the search head. A cluster must have at least one search head.  
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_forwarder"><span class="mw-headline" id="Forwarder">Forwarder</span></a></i></b></font></h4>
<p><b>Forwarders</b> function the same as in any Splunk Enterprise deployment. They consume data from external sources and then forward that data to indexers, which, in clusters, are the peer nodes. You are not required to use forwarders to get data into a cluster, but, for most purposes, you will want to. This is because only with forwarders can you enable  <b>indexer acknowledgment</b>, which ensures that incoming data gets reliably indexed. In addition, to deal with potential peer node failures, it is advisable to use  <b>load-balancing</b> forwarders. That way, if one peer goes down, the forwarder can switch its forwarding to other peers in the load-balanced group. For more information on forwarders in a clustered environment, read <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a> in this manual.
</p>
<h3> <a name="basicclusterarchitecture_important_concepts"><span class="mw-headline" id="Important_concepts"> Important concepts </span></a></h3>
<p>To understand how a cluster functions, you need to be familiar with a few concepts:
</p>
<ul><li> <b>Replication factor</b>. This determines the number of copies of data the cluster maintains and therefore, the cluster's fundamental level of failure tolerance.
</li><li> <b>Search factor</b>. This determines the number of searchable copies of data the cluster maintains, and therefore how quickly the cluster can recover its searching capability after a peer node goes down.
</li><li> <b>Buckets</b>. Buckets are the basic units of index storage. A cluster maintains replication factor number of copies of each bucket. 
</li></ul><p>This section provides a brief introduction to these concepts. 
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_replication_factor"><span class="mw-headline" id="Replication_factor">Replication factor </span></a></i></b></font></h4>
<p>As part of configuring the master, you specify the number of copies of data that you want the cluster to maintain. The number of copies is called the cluster's <b>replication factor</b>. The replication factor is a key concept in index replication, because it determines the cluster's failure tolerance: a cluster can tolerate a failure of (replication factor - 1) peer nodes. For example, if you want to ensure that your system can handle the failure of two peer nodes, you  must configure a replication factor of 3, which means that the cluster stores three identical copies of your data on separate nodes. If two peers go down, the data is still available on a third peer. The default value for the replication factor is 3.
</p><p>Here is a high-level representation of a cluster with three peers and a replication factor of 3:
</p><p><img alt="Replication factor 3 60.png" src="images/5/5f/Replication_factor_3_60.png" width="700" height="554"></p><p>In this diagram, one peer is receiving data from a forwarder, which it processes and then streams to two other peers. The cluster will contain three complete copies of the peer's data. This diagram represents a very simplified version of peer replication, where all data is coming into the system through a single peer. In most three-peer clusters, all three peers would be receiving external data from a forwarder, as well as replicated data from other peers.
</p><p>For a detailed discussion of the replication factor and the trade-offs involved in adjusting its value, see  the topic <a href="#thereplicationfactor" class="external text">"Replication factor"</a>.
</p><p><b>Important:</b> Multisite clusters use a significantly different version of the replication factor.  See <a href="#multisitearchitecture_multisite_replication_and_search_factors" class="external text">"Multisite replication and search factors"</a>.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_search_factor"><span class="mw-headline" id="Search_factor"> Search factor </span></a></i></b></font></h4>
<p>When you configure the master, you also designate a <b>search factor</b>. The search factor determines the number of immediately <b>searchable</b> copies of data the cluster maintains. 
</p><p>Searchable copies of data require more storage space than <b>non-searchable</b> copies, so it is best to limit the size of your search factor to fit your exact needs. For most purposes, use the default value of 2. This allows the cluster to continue searches with little interruption if a single peer node goes down.
</p><p>The difference between a searchable and a non-searchable copy of some data is this: The searchable copy contains both the data itself and some extensive index files that the cluster uses to search the data. The non-searchable copy contains just the data. Even the data stored in the non-searchable copy, however, has undergone initial processing and is stored in a form that makes it possible to recreate the index files later, if necessary. 
</p><p>For a detailed discussion of the search factor and the trade-offs involved in adjusting its value, see the topic <a href="#thesearchfactor" class="external text">"Search factor"</a>.
</p><p><b>Important:</b> Multisite clusters use a significantly different version of the search factor.  See <a href="#multisitearchitecture_multisite_replication_and_search_factors" class="external text">"Multisite replication and search factors"</a>.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_buckets"><span class="mw-headline" id="Buckets"> Buckets </span></a></i></b></font></h4>
<p>Splunk Enterprise stores indexed data in <b>buckets</b>, which are directories containing files of data. An index typically consists of many buckets. 
</p><p>A <b>complete</b> cluster maintains replication factor number of copies of each bucket, with each copy residing on a separate peer node. The bucket copies are either searchable or non-searchable. A complete cluster also has search factor number of searchable copies of each bucket.
</p><p>Buckets contain two types of files: a <b>rawdata file</b>, which contains the data along with some metadata, and - for searchable copies of buckets - <b>index files</b> into the data.
</p><p>The cluster replicates data on a bucket-by-bucket basis. The original bucket and its copies on other peer nodes have identical sets of rawdata. Searchable copies also contain the index files.
</p><p>Each time a peer creates a new bucket, it communicates with the master to get a list of peers to stream the bucket's data to. If you have a cluster in which the number of peer nodes exceeds the replication factor, a peer might stream data to a different set of peers each time it creates a new bucket. Eventually, the copies of the peer's original buckets are likely to be spread across a large number of peers, even if the replication factor is only 3.
</p><p>You need a good grasp of buckets to understand cluster architecture. For an overview of buckets in general, read  <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>. Then read the topic <a href="#bucketsandclusters" class="external text">"Buckets and indexer clusters"</a>. It provides detailed information on bucket concepts of particular importance for a clustered deployment.
</p>
<h3> <a name="basicclusterarchitecture_how_indexing_works"><span class="mw-headline" id="How_indexing_works"> How indexing works</span></a></h3>
<p>Clustered indexing functions like non-clustered indexing, except that the cluster stores multiple copies of the data. Each peer node receives, processes, and indexes external data - the same as any non-clustered indexer. The key difference is that the peer node also streams, or "replicates", copies of the processed data to other peers in the cluster, which then store those copies in their own buckets. Some of the peers receiving the processed data might also index it. The replication factor determines the number of peers that receive the copies of data. The search factor determines the number of peers that index the data.
</p><p>A peer node can be indexing external data while simultaneously storing, and potentially indexing, copies of replicated data sent to it from other peers. For example, if you have a three-peer cluster configured with a replication factor of 3, each peer can be ingesting and indexing external data while also storing copies of replicated data streamed to it by the other peers. If the cluster's search factor is 2, one of the peers receiving a copy of streamed data will also index it. (In addition, the peer that originally ingests the data always indexes its own copy.) This diagram shows the movement of data into peers, both from forwarders and from other peers:
</p><p><img alt="Indexing cluster 60.png" src="images/9/97/Indexing_cluster_60.png" width="700" height="554"></p><p>You can set up your cluster so that all the peer nodes ingest external data. This is the most common scenario. You do this simply by configuring inputs on each peer node. However, you can also set up the cluster so that only a subset of the peer nodes ingest data. No matter how you disperse your inputs across the cluster, all the peer nodes can, and likely will, also store replicated data. The master determines, on a bucket-by-bucket basis, which peer nodes will get replicated data. You cannot configure this, except in the case of multisite clustering, where you can specify the number of copies of data that each site's set of peers receives.
</p><p>In addition to replicating indexes of external data, the peers also replicate their internal indexes, such as <code><font size="2">_audit</font></code>, <code><font size="2">_internal</font></code>, etc.
</p><p>The master manages the peer-to-peer interactions. Most importantly, it tells each peer what peers to stream its data to.  Once the master has communicated this, the peers then exchange data with each other, without the master's involvement, unless a peer node goes down. The master also keeps track of which peers have searchable data and ensures that there are always search factor number of copies of searchable data available. When a peer goes down, the master coordinates remedial activities.
</p><p>For detailed information, read the topic <a href="#howclusteredindexingworks" class="external text">"How clustered indexing works"</a>.
</p><p>For information on how indexing works in a multisite cluster, read <a href="#multisitearchitecture_multisite_indexing" class="external text">"Multisite indexing"</a>.
</p>
<h3> <a name="basicclusterarchitecture_how_search_works"><span class="mw-headline" id="How_search_works"> How search works</span></a></h3>
<p>In an indexer cluster, a search head coordinates all searches. The process is similar to how <b>distributed searches</b>  work in a non-clustered environment. The main difference is that the search head relies on the master to tell it who its search peers are. Also, there are various processes in place to ensure that a search occurs over one and only one copy of each bucket. 
</p><p>To ensure that exactly one copy of each bucket participates in a search, one searchable copy of each bucket in the cluster is designated as <b>primary</b>.  Searches occur only across the set of primary copies. 
</p><p>The set of primary copies can change over time, for example, in response to a peer node going down. If some of the bucket copies on the downed node were primary, other searchable copies of those buckets will be made primary to replace them. If there are no other searchable copies (because the cluster has a search factor of 1), non-searchable copies will first have to be made searchable before they can be designated as primary. 
</p><p>The master rebalances primaries across the set of peers whenever a peer joins or rejoins the cluster, in an attempt to improve distribution of the search load. See <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a>. 
</p><p>The master keeps track of all bucket copies on all peer nodes, and the peer nodes themselves know the status of their bucket copies. That way, in response to a search request, a peer knows which of its bucket copies to search. 
</p><p>Periodically, the search head gets a list of active search peers from the master. To handle searches, it then communicates directly with those peers, as it would for any distributed search, sending search requests and replication bundles to the peers and consolidating search results returned from the peers.
</p><p>For example, assume a cluster of three peers is maintaining 20 buckets that need to be searched to fulfill a particular search request coming from the search head. Primary copies of those 20 buckets could be spread across all three peers, with 10 primaries on the first peer, six on the second, and four on the third. Each peer gets the search request and then determines for itself whether its particular copy of a bucket is primary and therefore needs to participate in the search. 
</p><p>For detailed information, read the topic <a href="#howclusteredsearchworks" class="external text">"How search works in an indexer cluster"</a>.
</p><p><b>Important:</b> There are key differences in how searching works in a multisite cluster. For example, each site in the cluster typically has a complete set of primary buckets, so that a search head can perform its searches entirely on data local to its site. For more information, read  <a href="#multisitearchitecture_multisite_searching" class="external text">"Multisite searching"</a>.
</p>
<h3> <a name="basicclusterarchitecture_how_clusters_deal_with_peer_node_failure"><span class="mw-headline" id="How_clusters_deal_with_peer_node_failure">How clusters deal with peer node failure </span></a></h3>
<p>If a peer node goes down, the master coordinates attempts to reproduce the peer's buckets on other peers. For example, if a downed node was storing 20 copies of buckets, of which 10 were searchable (including three primary bucket copies), the master will direct efforts to create copies of those 20 buckets on other nodes. It will likewise attempt to replace the 10 searchable copies with searchable copies of the same buckets on other nodes. And it will replace the primary copies by changing the status of corresponding searchable copies on other peers from non-primary to primary.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_replication_factor_and_node_failure"><span class="mw-headline" id="Replication_factor_and_node_failure">Replication factor and node failure</span></a></i></b></font></h4>
<p>If there are less peer nodes remaining than the number specified by the replication factor, the cluster will not be able to replace the 20 missing copies. For example, if you have a three-node cluster with a replication factor of 3, the cluster cannot replace the missing copies when a node goes down, because there is no other node where replacement copies can go.
</p><p>Except in extreme cases, however, the cluster should be able to replace the missing primary bucket copies by designating searchable copies of those buckets on other peers as primary, so that all the data continues to be accessible to the search head. 
</p><p>The only case in which the cluster cannot maintain a full set of primary copies is if a replication factor number of nodes goes down. For example, if you have a cluster of five peer nodes, with a replication factor of 3, the cluster will still be able to maintain a full set of primary copies if one or two peers go down but not if a third peer goes down.
</p>
<h4><font size="3"><b><i> <a name="basicclusterarchitecture_search_factor_and_node_failure"><span class="mw-headline" id="Search_factor_and_node_failure">Search factor and node failure</span></a></i></b></font></h4>
<p>The search factor determines whether full search capability can quickly resume after a node goes down. To ensure rapid recovery from one downed node, the search factor must be set to at least 2. That allows the master to immediately replace primaries on the downed node with existing searchable copies on other nodes.
</p><p>If instead the search factor is set to 1, that means the cluster is maintaining just a single set of searchable bucket copies. If a peer with some primary copies goes down, the cluster must first convert a corresponding set of non-searchable copies on the remaining peers to searchable before it can designate them as primary to replace the missing primaries. While this time-intensive process is occurring, the cluster has an incomplete set of primary buckets. Searches can continue, but only across the available primary buckets. Eventually, the cluster will replace all the missing primary copies. Searches can then occur across the full set of data.
</p><p>If, on the other hand, the search factor is at least 2, the cluster can immediately
assign primary status to searchable copies on the remaining nodes. The activity to replace the searchable copies from the downed node will still occur, but in the meantime searches can continue uninterrupted across all the cluster's data.
</p><p>For detailed information on peer failure, read the topic <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down"</a>.
</p><p>For information on how a multisite cluster handles peer node failure, read <a href="#multisitearchitecture_how_multisite_clusters_deal_with_peer_node_failure" class="external text">"How multisite indexer clusters deal with peer node failure"</a>.
</p>
<h3> <a name="basicclusterarchitecture_how_clusters_deal_with_master_node_failure"><span class="mw-headline" id="How_clusters_deal_with_master_node_failure">How clusters deal with master node failure </span></a></h3>
<p>If a master node goes down, peer nodes can continue to index and replicate data, and the search head can continue to search across the data, for some period of time. Problems eventually will arise, however, particularly if one of the peers goes down. There is no way to recover from peer loss without the master, and the search head will then be searching across an incomplete set of data. Generally speaking, the cluster continues as best it can without the master, but the system is in an inconsistent state and results cannot be guaranteed.
</p><p>For detailed information on master failure, read the topic <a href="#whathappenswhenamasternodegoesdown" class="external text">"What happens when a master node goes down"</a>.
</p>
<a name="multisitearchitecture"></a><h2> <a name="multisitearchitecture_multisite_indexer_cluster_architecture"><span class="mw-headline" id="Multisite_indexer_cluster_architecture"> Multisite indexer cluster architecture</span></a></h2>
<p>This topic describes the architecture of <b>multisite indexer clusters</b>. It focuses primarily on how multisite clusters differ from single-site clusters. For an overview of cluster architecture, focusing on single-site clusters, read <a href="#basicclusterarchitecture" class="external text">"The basics of indexer cluster architecture"</a>. 
</p>
<h3> <a name="multisitearchitecture_how_multisite_and_single-site_architecture_differ"><span class="mw-headline" id="How_multisite_and_single-site_architecture_differ">How multisite and single-site architecture differ</span></a></h3>
<p>Multisite clusters differ from single-site clusters in these key respects:
</p>
<ul><li> Each node (master/peer/search head) has an assigned site.  
</li><li> Replication of bucket copies occurs with site-awareness.
</li><li> Search heads distribute their searches across local peers only, when possible. 
</li><li> Bucket-fixing activities respect site boundaries when applicable.
</li></ul><h3> <a name="multisitearchitecture_multisite_cluster_nodes"><span class="mw-headline" id="Multisite_cluster_nodes">Multisite cluster nodes</span></a></h3>
<p>Multisite and single-site nodes share these characteristics:
</p>
<ul><li> Clusters have three types of nodes: master, peers, and search heads.
</li><li> Each cluster has exactly one master node. 
</li><li> The cluster can have any number of peer nodes and search heads.  
</li></ul><p>Multisite nodes differ in these ways:
</p>
<ul><li> Every node belongs to a specific site. Physical location typically determines a site. That is, if you want your cluster to span servers in Boston and Philadelphia, you assign all nodes in Boston to site1 and all nodes in Philadelphia to site2.  
</li><li> A typical multisite cluster has search heads on each site. This is necessary for <b>search affinity</b>, which increases search efficiency by allowing a search head to access all data locally.
</li></ul><p>Here is an example of a two-site cluster.
</p><p><img alt="Basic multisite cluster.png" src="images/9/97/Basic_multisite_cluster.png" width="700" height="559"></p><p>Note the following: 
</p>
<ul><li> The master node controls the entire cluster. Although the master resides physically on a site, the master is not actually a member of any site. However, each master has a built-in search head, and that search head requires that you specify a site for the master as a whole. Note that the master's search head is for testing purposes only. Do not use it in a production environment.
</li><li> Each site has its own search head, which searches the set of peer nodes on its site. This is an example of a cluster that has been configured for search affinity.
</li><li> The peers replicate data across site boundaries. This behavior is fundamental for both disaster recovery and search affinity.
</li></ul><h3> <a name="multisitearchitecture_multisite_replication_and_search_factors"><span class="mw-headline" id="Multisite_replication_and_search_factors"> Multisite replication and search factors</span></a></h3>
<p>As with their single-site counterparts, multisite replication and search factors determine the number of bucket copies and searchable bucket copies, respectively, in the cluster. The difference is that the multisite replication and search factors also determine the number of copies on each site.  A multisite replication factor for a three-site cluster might look like this:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:2, site1:1, site2:1, site3:1, total:4<br></font></code>
</div>
<p>This replication factor specifies that each site will get one copy of each bucket, except when the site is the originating site for the data, in which case it will get two copies. It also specifies that the total number of copies across the cluster is four.  
</p><p>In this particular example, the replication factor explicitly specifies all sites, but that is not a requirement. An <b>explicit site</b> is a site that the replication factor explicitly specifies. A <b>non-explicit site</b> is a site that the replication factor does not explicitly specify. 
</p><p>Here is another example of a multisite replication factor for a three-site cluster. This replication factor specifies only two of the sites explicitly:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:2, site1:1, site2:2, total:5<br></font></code>
</div>
<p>In this example, the number of copies that the non-explicit site3 gets varies. If site1 is the origin, site1 gets two copies, site2 gets two copies, and site3 gets the remainder of one copy.  If site2 is the origin, site1 gets one copy, site2 gets two copies, and site3 gets two copies. If site3 is the origin, site1 gets one copy, site2 gets two copies, and site3 gets two copies.
</p><p><b>Note:</b> In this example, the <code><font size="2">total</font></code> value cannot be 4. It must be at least 5. This is because, when the replication factor has non-explicit sites, the total must be at least the sum of all explicit site and origin values. 
</p><p>For details on replication factor syntax and behavior, read <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</p><p>The site search factor works the same way. For details, read <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>.
</p>
<h3> <a name="multisitearchitecture_multisite_indexing"><span class="mw-headline" id="Multisite_indexing"> Multisite indexing </span></a></h3>
<p>Multisite indexing is similar to single-site indexing, described in <a href="#basicclusterarchitecture" class="external text">"The basics of indexer cluster architecture"</a>. A single master coordinates replication across all the peers on all the sites.
</p><p>This section briefly describes the main multisite differences, using the example of a three-site cluster with this replication factor:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:2, site1:1, site2:1, site3:1, total:4<br></font></code>
</div>
<p>These are the main multisite issues to be aware of:
</p>
<ul><li> Data replication occurs across site boundaries, based on the replication factor. If, in the example, a peer in site1 ingests the data, it will stream one copy of the data to another peer in site1 (to fulfill the <code><font size="2">origin</font></code> setting of 2), one copy to a peer in site2, and one copy to a peer in site3.
</li></ul><ul><li> Multisite replication has the concept of the origin site, which allows the cluster to handle data differently for the site that it originates on. The  example illustrates this. If site1 originates the data, it gets two copies. If another site originates the data, site1 gets only one copy.
</li></ul><ul><li> As with single-site replication, you cannot specify the exact peers that will receive the replicated data. However, you can specify the sites whose peers will receive the data.
</li></ul><p>For information on how the cluster handles migrated single-site buckets, see <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</p>
<h3> <a name="multisitearchitecture_multisite_searching:_search_affinity"><span class="mw-headline" id="Multisite_searching:_search_affinity"> Multisite searching: search affinity </span></a></h3>
<p>Multisite searching is similar in most ways to single-site searching, described in <a href="#basicclusterarchitecture" class="external text">"The basics of indexer cluster architecture"</a>. Each search occurs across a set of primary bucket copies. There is one key difference, however.
</p><p>Multisite clusters provide <b>search affinity</b>, which allows searches to occur on site-local data. Search affinity is always enabled in a multisite cluster, but you must perform a few steps to take advantage of it. Specifically, you must ensure that both the searchable data and the search heads are available locally. 
</p><p>To accomplish this, you configure the search factor so that each site has at least one full set of searchable data. The master then ensures that each site has a complete set of primary bucket copies, as long as the sites are functioning properly. This is known as the <b>valid</b> state.
</p><p>With search affinity, the search heads still distribute their search requests to all peers in the cluster, but only the peers on the same site as the search head respond to the request by searching their primary bucket copies and returning results to the search head.  
</p><p>If the loss of some of a site's peers means that the site no longer has a full set of primaries (and thus is no longer in the valid state), bucket fix-up activities will attempt to return the site to the valid state. During the fix-up period, peers on remote sites will participate in searches, as necessary, to ensure that the search head still gets a full set of results. After the site regains its valid state, the search head once again uses only local peers to fulfill its searches.
</p><p>For more information on search affinity and how to configure the search factor to support it, see <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster"</a>. For more information on the internals of search processing, including search affinity, see <a href="#howclusteredsearchworks" class="external text">"How search works in an indexer cluster"</a>.
</p>
<h3> <a name="multisitearchitecture_multisite_clusters_and_node_failure"><span class="mw-headline" id="Multisite_clusters_and_node_failure">Multisite clusters and node failure</span></a></h3>
<p>The way that multisite clusters deal with node failure has some significant differences from single-site clusters.
</p><p><b>Important:</b> Before reading this section, you must understand the concept of a "reserve" bucket copy.  A reserve copy is a copy awaiting assignment to a peer. As a consequence of multisite peer node failure, some bucket copies might not be immediately assigned to a peer. For example, in a cluster with a total replication factor of 5, the master might tell the originating peer to stream buckets to just three other peers.  This results in four copies (the original plus the three streamed copies), with the fifth copy awaiting assignment to a peer once certain conditions are met. The fifth, unassigned copy is known as a reserve copy. This section describes how the cluster must sometimes reserve copies when peer nodes fail.
</p>
<h4><font size="3"><b><i> <a name="multisitearchitecture_how_multisite_clusters_deal_with_peer_node_failure"><span class="mw-headline" id="How_multisite_clusters_deal_with_peer_node_failure">How multisite clusters deal with peer node failure</span></a></i></b></font></h4>
<p>When a peer goes down, bucket fix-up happens within the same site, if possible. The cluster tries to replace any missing bucket copies by adding copies to peers remaining on that site. (In all cases, each peer can have at most one copy of any particular bucket.) If it is not possible to fix up all buckets by adding copies to peers within the site, then, depending on the replication and search factors, the cluster might make copies on peers on other sites. 
</p><p>The fix-up behavior under these circumstances depends partially on whether the failed peer was on an explicit or non-explicit site.
</p><p>If enough peers go down on an explicit site such that the site can no longer meet its site-specific replication factor, the cluster does not attempt to compensate by making copies on peers on other sites.  Rather, it assumes that the requisite number of peers will eventually return to the site. For new buckets also, it holds copies in reserve for the return of peers to the site. In other words, it does not assign those copies to peers on a different site, but instead waits until the first site has peers available and then assigns the copies to those peers.   
</p><p>For example, given a three-site cluster (site1, site2, site3) with this replication factor:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:2, site1:1, site2:2, total:5<br></font></code>
</div>
<p>the cluster ordinarily maintains two copies on site2. But if enough peers go down on site2, so that only one remains and the site can no longer meet its replication factor of 2, that remaining peer gets one copy of all buckets in the cluster, and the cluster reserves another set of copies for the site. When a second peer rejoins site2, the cluster streams the reserved copies to that peer.
</p><p>When a non-explicit site loses enough peers such that it can no longer maintain the number of bucket copies that it already has, the cluster compensates by adding copies to other sites to make up the difference. For example, assume that the non-explicit site3 in the example above has two copies of some bucket, and that it then loses all but one of its peers, so that it can only hold one copy of each bucket.  The cluster compensates by streaming a copy of that bucket to a peer on one of the other sites, assuming there is at least one peer that does not yet have a copy of that bucket.
</p><p>For information on how a cluster handles the case where all the peer nodes on a site fail, see See <a href="#multisitearchitecture_how_the_cluster_handles_site_failure" class="external text">"How the cluster handles site failure"</a>.
</p>
<h4><font size="3"><b><i> <a name="multisitearchitecture_how_the_cluster_handles_site_failure"><span class="mw-headline" id="How_the_cluster_handles_site_failure">How the cluster handles site failure</span></a></i></b></font></h4>
<p>Site failure is just a special case of peer node failure. Cluster fix-up occurs following the rules described earlier for peer node failure. Of particular note, the cluster might hold copies in reserve against the eventual return of the site.  
</p><p>For any copies of existing buckets that are held in reserve, the cluster does not add copies to other sites during its fix-up activities. Similarly, for any new buckets added after the site goes down, the cluster reserves some number of copies until the site returns to the cluster.
</p><p>Here is how the cluster determines the number of copies to reserve:
</p>
<ul><li> For explicit sites, the cluster reserves the number of copies and searchable copies specified in the site's search and replication factors.  
</li><li> For non-explicit sites, the cluster reserves one searchable copy if the <code><font size="2">total</font></code> components of the site's search and replication factors are sufficiently large, after handling any explicit sites, to accommodate the copy. (If the search factor isn't sufficiently large but the replication factor is, the cluster reserves one non-searchable copy.)
</li></ul><p>For example, say you have a three-site cluster with two explicit sites (site1 and site2) and one non-explicit site (site3), with this configuration:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:2, site1:1, site2:2, total:5<br>site_search_factor = origin:1, site1:1, site2:1, total:2<br></font></code>
</div>
<p>In the case of a site going down, the cluster reserves bucket copies like this:
</p>
<ul><li> If site1 goes down, the cluster reserves one searchable copy. 
</li><li> If site2 goes down, the cluster reserves two copies, including one that is searchable.
</li><li> If site3 goes down, the cluster reserves one non-searchable copy.
</li></ul><p>Once the reserved copies are accounted for, the cluster replicates any remaining copies to other available sites, both during fix-up of existing buckets and when adding new buckets.
</p><p>When the site returns to the cluster, bucket fix-up occurs for that site to the degree necessary to ensure that the site has, at a minimum, its allocation of reserved bucket copies, both for new buckets and  for buckets that were on the site when it went down.
</p><p>If the site that fails is the site on which the master resides, you can bring up a stand-by master on one of the remaining sites. See <a href="#mastersitefailure" class="external text">"Handle indexer cluster master site failure"</a>.
</p>
<h4><font size="3"><b><i> <a name="multisitearchitecture_how_multisite_clusters_deal_with_master_node_failure"><span class="mw-headline" id="How_multisite_clusters_deal_with_master_node_failure">How multisite clusters deal with master node failure</span></a></i></b></font></h4>
<p>A multisite cluster handles master node failure the same as a single-site cluster. The cluster continues to function as best it can under the circumstances.  See <a href="#whathappenswhenamasternodegoesdown" class="external text">"What happens when a master node goes down"</a>.
</p>
<h1>Deploy the indexer cluster</h1><a name="clusterdeploymentoverview"></a><h2> <a name="clusterdeploymentoverview_indexer_cluster_deployment_overview"><span class="mw-headline" id="Indexer_cluster_deployment_overview"> Indexer cluster deployment overview</span></a></h2>
<p>This topic describes the main steps to deploying <b>indexer clusters</b>. Subsequent topics describe these steps in detail.
</p><p>Before you attempt to deploy a cluster, you must be familiar with several areas of Splunk Enterprise administration:
</p>
<ul><li> <b>How to configure indexers.</b> In particular, see <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>, along with the other topics in this manual that describe managing indexes.
</li><li> <b>What a search head does.</b> For an introduction to distributed search, see "About distributed search" in the <i>Distributed Search</i> manual. Note, however, that you configure search heads for indexer clusters somewhat differently from other search heads. For information on the differences see <a href="#configurethesearchhead" class="external text">"Search head configuration overview"</a> in this manual.
</li><li> <b>How to use a forwarder to get data into an indexer.</b> See "Use forwarders" in the <i>Getting Data In</i> manual.
</li></ul><p><b>Important:</b> This chapter assumes that you are deploying independent search heads in the indexer cluster. For information on how to incorporate search heads that are members of a <b>search head cluster</b>, see "Integrate the search head cluster with an indexer cluster"  in the <i>Distributed Search</i> manual.
</p>
<h3> <a name="clusterdeploymentoverview_migrating_from_a_non-clustered_splunk_enterprise_deployment.3f"><span class="mw-headline" id="Migrating_from_a_non-clustered_Splunk_Enterprise_deployment.3F">Migrating from a non-clustered Splunk Enterprise deployment?</span></a></h3>
<p>Clustered indexers (peers) have several different requirements from non-clustered indexers. It is important that  you be aware of these issues before you migrate your indexers. See <a href="#keydifferences" class="external text">"Key differences between clustered and non-clustered deployments of indexers"</a>.  After you read that material, go to <a href="#migratenon-clusteredindexerstoaclusteredenvironment" class="external text">"Migrate non-clustered indexers to a clustered environment"</a> for the actual migration process.
</p><p><b>Important:</b> Before migrating an indexer from non-clustered to clustered, be certain of your needs. The process goes in one direction only.  There is no supported procedure for converting an indexer from clustered to non-clustered.
</p>
<h3> <a name="clusterdeploymentoverview_deploying_a_multisite_cluster.3f"><span class="mw-headline" id="Deploying_a_multisite_cluster.3F">Deploying a multisite cluster?</span></a></h3>
<p><b>Multisite indexer clusters</b> are considerably more complex than single-site clusters. Deploying them requires that you consider some additional issues and perform an entirely different set of configurations.  If you are deploying a multisite cluster, read this topic first and then read <a href="#multisitedeploymentoverview" class="external text">"Multisite indexer cluster deployment overview"</a>.
</p>
<h3> <a name="clusterdeploymentoverview_deploy_a_cluster"><span class="mw-headline" id="Deploy_a_cluster"> Deploy a cluster </span></a></h3>
<p>When you deploy a cluster, you enable and configure the cluster master and the peer nodes that perform the indexing. You also enable a search head to search data in the cluster. In addition, you usually set up forwarders to send data to the cluster. Here is a diagram of a small cluster, showing the various nodes that you deploy:
</p><p><img alt="Simplified basic cluster 60.png" src="images/5/58/Simplified_basic_cluster_60.png" width="700" height="554"></p><p>These are the key steps in deploying clusters:
</p><p><b>1.</b> <b>Identify your requirements:</b>
</p><p><b>a.</b> Understand your data availability and failover needs. See <a href="#aboutclusters" class="external text">"About indexer clusters"</a>.
</p><p><b>b.</b> Determine whether you will be deploying a basic, single-site cluster or a multisite cluster. Multisite clusters offer strong disaster recovery capabilities because they allow you to distribute copies of your data across multiple locations. They also enable search affinity, which reduces network traffic by limiting searches to local data. For more information, read <a href="#multisiteclusters" class="external text">"Multisite indexer clusters"</a>.
</p><p><b>c.</b> Decide what <b>replication factor</b> you want to implement. The replication factor is the number of copies of raw data that the cluster maintains. Your optimal replication factor depends on factors specific to your environment, but essentially involves a trade-off between failure tolerance and storage capacity. A higher replication factor means that more copies of the data will reside on more peer nodes, so your cluster can tolerate more node failures without loss of data availability. But it also means that you will need more nodes and more storage to handle the additional data. For multisite clusters, you also need to decide how many copies to put on each site. For more information, see <a href="#thereplicationfactor" class="external text">"Replication factor"</a>.
</p><p><b>Warning:</b> Make sure you start by choosing the right replication factor for your needs. It is inadvisable to increase the replication factor after the cluster contains a significant amount of data. The cluster would need to perform a large amount of bucket copying to match the increased replication factor, slowing significantly the overall performance of your cluster while the copying is occurring.
</p><p><b>d.</b> Decide what <b>search factor</b> you want to implement. The search factor tells the cluster how many searchable copies of indexed data to maintain. This helps determine the speed with which a cluster can recover from a downed node. A higher search factor allows the cluster to recover more quickly, but it also requires more storage space and processing power. For most single-site deployments, the default search factor value of 2 represents the right trade-off, allowing searches usually to continue with little interruption when a node goes down. For multisite clusters, you also need to decide how many searchable copies to put on each site. For more information, see <a href="#thesearchfactor" class="external text">"Search factor"</a>.
</p><p><b>Warning:</b> Make sure you start by choosing the right search factor for your needs. It is inadvisable to increase the search factor after the cluster contains a significant amount of data. The cluster would need to perform a large amount of processing (transforming non-searchable bucket copies into searchable copies) to match the increased search factor, and this will have an adverse effect on the overall performance of your cluster while the processing is occurring. 
</p><p><b>e.</b> Identify other factors that also determine the size of your cluster; for example, the quantity of data you will be indexing. It usually makes sense to keep all your indexers in a single cluster, so for horizontal scaling, you will need to add peer nodes beyond those required by the replication factor. Similarly, depending on the anticipated search load, you might need to add more than one search head.
</p><p><b>f.</b> Study the topic <a href="#systemrequirements" class="external text">"System requirements and other deployment considerations for indexer clusters"</a> for information on other key issues.
</p><p><b>2.</b> <b>Install the Splunk Enterprise cluster instances on your network.</b> At a minimum, you will need (replication factor + 2) instances:
</p>
<ul><li> You need at least the replication factor number of <b>peer nodes</b>, but you might want to add more peers to boost indexing capacity, as mentioned in step 1e. 
</li><li> You also need two more instances, one for the <b>master node</b> and the other for the <b>search head</b>.
</li></ul><p>For multisite clusters, you must also take into account the search head and peer node requirements of each site, as determined by your search affinity and disaster recovery needs. See <a href="#multisitedeploymentoverview" class="external text">"Multisite indexer cluster deployment overview"</a>. 
</p><p>For information on how to install Splunk Enterprise, read the <i>Installation Manual</i>. 
</p><p><b>3.</b> <b>Enable clustering on the instances:</b>
</p><p><b>a.</b> Enable the master node. See <a href="#enablethemasternode" class="external text">"Enable the master node"</a>.
</p><p><b>b.</b> Enable the peer nodes. See <a href="#enablethepeernodes" class="external text">"Enable the peer nodes"</a>.
</p><p><b>c.</b> Enable the search head. See <a href="#enablethesearchhead" class="external text">"Enable the search head"</a>.
</p><p><b>Important:</b> For multisite clusters, the process of enabling cluster nodes is different. See <a href="#multisitedeploymentoverview" class="external text">"Multisite indexer cluster deployment overview"</a>.
</p><p><b>4.</b> <b>Complete the peer node configuration:</b>
</p><p><b>a.</b> Configure the peers' index settings. This step is necessary only if you need to augment the set of default indexes and apps. In general, all the peers must use the same set of indexes, so if you add indexes (or apps that define indexes) to one peer, you must add them to all peers, using a cluster-specific distribution method. There might also be other configurations that you need to coordinate across the set of peers. See <a href="#preparethepeers" class="external text">"Prepare the peers for index replication"</a> for information on how to do this.
</p><p><b>b.</b> Configure the peers' data inputs. For most purposes, it is best to use forwarders to send data to the peers, as discussed in  <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a>. As that topic states, you will usually want to deploy load-balancing forwarders with indexer acknowledgment enabled.
</p><p>After you enable the nodes and set up data inputs for the peers, the cluster automatically begins indexing and replicating the data.
</p><p><b>5.</b> <b>Configure the master node to forward its data to the peer nodes.</b> This best practice provides several advantages. See <a href="#forwardmasterdata" class="external text">"Best practice: Forward master node data to the indexer layer"</a>.
</p>
<h3> <a name="clusterdeploymentoverview_other_deployment_scenarios"><span class="mw-headline" id="Other_deployment_scenarios">Other deployment scenarios</span></a></h3>
<p>This chapter also provides guidance on a few other cluster deployment scenarios:
</p>
<ul><li> Add indexers with existing data to a cluster. See <a href="#migratenon-clusteredindexerstoaclusteredenvironment" class="external text">"Migrate non-clustered indexers to a clustered environment"</a>.
</li><li> Migrate a single-site cluster to multisite. See <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</li><li> Employ clusters purely for index scalability, where index replication is not a requirement. See <a href="#clustersinscaledoutdeployments" class="external text">"Use indexer clusters to scale indexing"</a>.
</li></ul><a name="keydifferences"></a><h2> <a name="keydifferences_key_differences_between_clustered_and_non-clustered_deployments_of_indexers"><span class="mw-headline" id="Key_differences_between_clustered_and_non-clustered_deployments_of_indexers"> Key differences between clustered and non-clustered deployments  of indexers</span></a></h2>
<p>This topic describes key differences between clustered and non-clustered indexers.  In particular, it discusses issues regarding system requirements and deployment.
</p><p><b>Read this topic carefully if you plan to migrate your current set of indexers to a cluster.</b>
</p>
<h3> <a name="keydifferences_do_not_use_deployment_server_or_third-party_deployment_tools_with_cluster_peers"><span class="mw-headline" id="Do_not_use_deployment_server_or_third-party_deployment_tools_with_cluster_peers">Do not use deployment server or third-party deployment tools with cluster peers</span></a></h3>
<p>Neither the deployment server nor any third party deployment tool (such as Puppet or CFEngine, among others) is supported as a means to distribute configurations or apps to cluster peers (indexers). 
</p><p>To distribute configurations across the set of cluster peers, instead use the <b>configuration bundle</b> method outlined in the topic <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a>. As that topic explains, the configuration bundle method involves first placing peer apps on the master node, which then distributes those apps to the peer nodes in a coordinated fashion. 
</p><p>For information on how to migrate app distribution from the deployment server to the configuration bundle method, see <a href="#migratenon-clusteredindexerstoaclusteredenvironment_migrate_apps_to_a_cluster" class="external text">"Migrate apps to a cluster"</a>.
</p><p><b>Note:</b> You can use deployment server to distribute updates to search heads in indexer clusters, as long as they are standalone search heads. You cannot use the deployment server to distribute updates to members of a <b>search head cluster</b>.
</p>
<h3> <a name="keydifferences_differences_in_system_requirements"><span class="mw-headline" id="Differences_in_system_requirements"> Differences in system requirements </span></a></h3>
<p>Peer nodes have some different system requirements compared to non-clustered indexers. Before migrating your indexer, read the topic <a href="#systemrequirements" class="external text">"System requirements and other deployment considerations for indexer clusters"</a>. In particular, be aware of the following differences:
</p>
<ul><li> When you convert an indexer to a cluster peer, disk usage will go up significantly. Make sure that you have sufficient disk space available, relative to daily indexing volume, search factor, and replication factor. For detailed information on peer disk usage, read <a href="#systemrequirements_storage_considerations" class="external text">"Storage considerations"</a>.
</li><li> Cluster nodes cannot share Splunk Enterprise instances. The master node, peer nodes, and search head must each run on its own instance.
</li></ul><h3> <a name="keydifferences_other_considerations_and_differences_from_a_non-cluster_deployment"><span class="mw-headline" id="Other_considerations_and_differences_from_a_non-cluster_deployment"> Other considerations and differences from a non-cluster deployment </span></a></h3>
<p>In addition, note the following:
</p>
<ul><li> For most types of cluster deployments, you should enable <b>indexer acknowledgment</b> for the forwarders sending data to the peer. See <a href="#useforwarderstogetyourdata_how_indexer_acknowledgment_works" class="external text">"How indexer acknowledgment works"</a>. 
</li><li> There will be some overall reduction in performance due to a few factors. The primary one is indexer acknowledgment. In addition, the  need to store, and potentially index, replicated data coming from other peer nodes has some affect on performance.
</li><li> When restarting cluster peers, you should use Splunk Web or one of the cluster-aware CLI commands, such as <code><font size="2">splunk offline</font></code> or <code><font size="2">splunk rolling-restart</font></code>. Do not use <code><font size="2">splunk restart</font></code>. For details, see <a href="#restartthecluster" class="external text">"Restart the entire indexer cluster or a single cluster node"</a>.
</li></ul><h3> <a name="keydifferences_migrate_a_non-clustered_indexer"><span class="mw-headline" id="Migrate_a_non-clustered_indexer"> Migrate a non-clustered indexer </span></a></h3>
<p>To learn how to migrate an existing indexer to a cluster and the ramifications of doing so, read the topic  <a href="#migratenon-clusteredindexerstoaclusteredenvironment" class="external text">"Migrate non-clustered indexers to a clustered environment"</a>.
</p>
<a name="systemrequirements"></a><h2> <a name="systemrequirements_system_requirements_and_other_deployment_considerations_for_indexer_clusters"><span class="mw-headline" id="System_requirements_and_other_deployment_considerations_for_indexer_clusters"> System requirements and other deployment considerations for indexer clusters</span></a></h2>
<p>Indexer clusters are groups of Splunk Enterprise indexers, so, for the most part, you just need to adhere to the system requirements for indexers. For detailed software and hardware requirements for indexers, read "System requirements" in the <i>Installation Manual</i>. The current topic notes additional requirements for clusters. 
</p>
<h3> <a name="systemrequirements_summary_of_key_requirements"><span class="mw-headline" id="Summary_of_key_requirements">Summary of key requirements</span></a></h3>
<p>These are the main issues to note:
</p>
<ul><li> Each cluster node (master, peer, or search head) must reside on a separate Splunk Enterprise instance.
</li><li> Each node instance must run on a separate machine or virtual machine, and each machine must be running the same operating system.
</li><li> All nodes must be connected over a network.
</li></ul><p>For example, to deploy a cluster consisting of three peers, one master, and one search head, you need five Splunk Enterprise instances running on five machines connected over a network. And all machines must be running the same operating system.
</p><p>These are some additional issues to be aware of:
</p>
<ul><li> Compared to a non-clustered deployment, clusters require more storage, to accommodate the multiple copies of data.
</li><li> Index replication, in and of itself, does not increase your licensing needs.
</li><li> You cannot use a deployment server to distribute updates to peers.
</li></ul><p>See the remainder of this topic for details.
</p>
<h3> <a name="systemrequirements_required_splunk_enterprise_instances"><span class="mw-headline" id="Required_Splunk_Enterprise_instances"> Required Splunk Enterprise instances </span></a></h3>
<p>Each cluster node must reside on its own Splunk Enterprise instance. Therefore, the cluster must consist of at least (replication factor + 2) instances: a minimum of <b>replication factor</b> number of peer nodes, plus one master node and one or more search heads. For example, if you want to deploy a cluster with a replication factor of 3, you must set up at least five instances: three peers, one master, and one search head. To learn more about the replication factor, see <a href="#basicclusterarchitecture_replication_factor" class="external text">"Replication factor"</a> in this manual. 
</p><p>The size of your cluster depends on other factors besides the replication factor, such as the amount of data you need to index. See <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>.
</p><p><b>Important:</b> While the master has search capabilities, you should only use those capabilities for debugging purposes. The resources of the master must be dedicated to fulfilling its critical role of coordinating cluster activities. Under no circumstances should the master be employed as a production search head. See <a href="#systemrequirements_additional_roles_for_the_master_node" class="external text">"Additional roles for the master node"</a>.
</p>
<h3> <a name="systemrequirements_splunk_enterprise_version_compatibility"><span class="mw-headline" id="Splunk_Enterprise_version_compatibility">Splunk Enterprise version compatibility</span></a></h3>
<p>You can implement clustering on any group of indexers, version 5.0 or above. 
</p>
<h4><font size="3"><b><i> <a name="systemrequirements_for_master_nodes_running_splunk_enterprise_version_6.1_and_below"><span class="mw-headline" id="For_master_nodes_running_Splunk_Enterprise_version_6.1_and_below"> For master nodes running Splunk Enterprise version 6.1 and below</span></a></i></b></font></h4>
<p>Peer nodes and search heads must run the same version as the master. 
</p>
<h4><font size="3"><b><i> <a name="systemrequirements_for_master_nodes_running_splunk_enterprise_version_6.2_and_above"><span class="mw-headline" id="For_master_nodes_running_Splunk_Enterprise_version_6.2_and_above">For master nodes running Splunk Enterprise version 6.2 and above</span></a></i></b></font></h4>
<p>Peer nodes and search heads can run different versions from the master, subject to these restrictions:
</p>
<ul><li> The peer nodes and search heads must run version 6.1 or above. 
</li><li> The peer nodes and search heads must run the same or a lower version than the master node.  
</li><li> The peer nodes and search heads within a site must all run the same version.
</li></ul><p>This means, for example, regarding compatibility between peers and the master:
</p>
<ul><li> A 6.1 peer node is compatible with a 6.2 master.
</li><li> A 6.2 peer node is compatible with a 6.2 master.
</li><li> A 6.2 peer node is not compatible with a 6.1 master.
</li><li> A 6.0 peer node is not compatible with a 6.2 master.
</li></ul><p>The examples are also true if you substitute "search head" for "peer node".
</p><p>In addition, regarding compatibility between peers and search heads on multisite clusters:
</p>
<ul><li> A 6.1 peer node is not compatible with a 6.2 search head or peer node at the same site.
</li><li> A 6.1 search head is not compatible with a 6.2 peer node or search head at the same site.
</li><li> A mixed environment consisting of all 6.1 peers and search heads at one site and all 6.2 peers and search heads at a second site is compatible with a 6.2 master.
</li></ul><h4><font size="3"><b><i> <a name="systemrequirements_run_a_6.2.2b_master_against_6.1_peer_nodes"><span class="mw-headline" id="Run_a_6.2.2B_master_against_6.1_peer_nodes">Run a 6.2+ master against 6.1 peer nodes </span></a></i></b></font></h4>
<p>To run a 6.2+ master against 6.1&Acirc;&nbsp;peer nodes, you must set the attribute <code><font size="2">use_batch_mask_changes</font></code> in the master's <code><font size="2">server.conf</font></code> file to <code><font size="2">false</font></code>.
</p><p>To avoid restarting the master, you can set this attribute through the CLI:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -use_batch_mask_changes false<br></font></code>
</div>
<p>You can also set this attribute when configuring the master instance for the first time:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode master -use_batch_mask_changes false<br></font></code>
</div>
<p><b>Caution:</b> After upgrading all peer nodes to 6.2, you must revert <code><font size="2">use_batch_mask_changes</font></code> to <code><font size="2">true</font></code>.
</p>
<h3> <a name="systemrequirements_machine_requirements"><span class="mw-headline" id="Machine_requirements">Machine requirements</span></a></h3>
<p>Each node of the cluster (master node, peer nodes, and search head) must run on its own, separate machine or virtual machine. Other than that, the hardware requirements, aside from storage, are basically the same as for any Splunk Enterprise instance. See "Reference hardware" in the <i>Capacity Planning Manual</i>. 
</p><p>The main difference is in the storage requirements for peer nodes, discussed below.
</p><p><b>Note:</b> The storage needs of the master node are significantly lower than those specified in the "Reference hardware" topic, since the master does not index external data.
</p><p>In addition, all cluster instances must be running on the same operating system.
</p>
<h3> <a name="systemrequirements_synchronization_of_system_clocks_across_the_cluster"><span class="mw-headline" id="Synchronization_of_system_clocks_across_the_cluster"> Synchronization of system clocks across the cluster </span></a></h3>
<p>It is important that you synchronize the system clocks on all machines, virtual or physical, that are running Splunk Enterprise instances participating in the cluster. Specifically, this means your master node, peer nodes, and search heads. Otherwise, various issues can arise, such as timing problems between the master and peer nodes, search failures, or premature expiration of search artifacts.
</p><p>The synchronization method you use depends on your specific set of machines. Consult the system documentation for the particular machines and operating systems on which you are running Splunk Enterprise. For most environments, Network Time Protocol (NTP) is the best approach.
</p>
<h3> <a name="systemrequirements_storage_considerations"><span class="mw-headline" id="Storage_considerations"> Storage considerations</span></a></h3>
<p>When determining storage requirements for your clustered indexes, you need to consider the increased capacity, across the set of peer nodes, necessary to handle the multiple copies of data.
</p><p>Clusters use the usual settings for managing index storage, as described in <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</p>
<h4><font size="3"><b><i> <a name="systemrequirements_determine_your_storage_requirements"><span class="mw-headline" id="Determine_your_storage_requirements">Determine your storage requirements</span></a></i></b></font></h4>
<p>It is important to ensure you have enough disk space to accommodate the volume of data your peer nodes will be processing. For a general discussion of Splunk Enterprise data volume and how to estimate your storage needs, refer to "Estimating your storage requirements" in the <i>Installation Manual</i>. That topic provides information on how to estimate storage for non-clustered indexers, so you need to supplement its guidelines to  account for the extra copies of data that a cluster stores.
</p><p>With a cluster, in addition to considering the volume of incoming data, you must consider the replication factor and <b>search factor</b> to arrive at your total storage requirements across the set of peer nodes. With a replication factor of 3, you are storing three copies of your data. You will need extra storage space to accommodate these copies, but you will not need three times as much storage. Replicated copies of non-searchable data are smaller than copies of searchable data, because they include only the data and not the associated index files. So, for example, if your replication factor is 3 and your search factor is 2, you will need more than two, but less than three, times the storage capacity compared to storing the same data on non-clustered indexers.
</p><p>Exactly how much less storage your non-searchable copies require takes some investigation on your part. The index files excluded by non-searchable copies can vary greatly in size, depending on factors described in "Estimating your storage requirements" in the <i>Installation Manual</i>. 
</p><p><b>Important:</b> A master is not aware of the amount of storage on individual peer nodes, and therefore it does not take available storage into account when it makes decisions about which peer node should receive a particular set of replicated data. It also makes arbitrary decisions about which peer should make some set of replicated data searchable (in cases where the search factor is 2 or greater). Therefore, you must ensure that each peer node has sufficient storage not only for the data originating on that peer, but also for any replicated copies of data that might get streamed to it from other peers. You should continue to monitor storage usage throughout the life of the cluster.
</p>
<h4><font size="3"><b><i> <a name="systemrequirements_storage_requirement_examples"><span class="mw-headline" id="Storage_requirement_examples">Storage requirement examples</span></a></i></b></font></h4>
<p>As a ballpark figure, incoming syslog data, after it has been compressed and indexed, occupies approximately 50% of its original size:
</p>
<ul><li> 15% for the rawdata file.
</li><li> 35% for associated index files.
</li></ul><p>In practice, this estimate can vary substantially, based on the factors described in "Estimating your storage requirements" in the <i>Installation Manual</i>. 
</p><p>Assume you have 100GB of syslog data coming into Splunk Enterprise. In the case of a non-clustered indexer, that data would occupy approximately 50GB (50% of 100GB) of storage on the indexer. However, in the case of clusters, storage calculations must factor in the replication factor and search factor to arrive at total storage requirements across all the cluster peers. (As mentioned earlier, you cannot easily predict exactly how much storage will be required on any specific peer.) 
</p><p>Here are two examples of estimating cluster storage requirements, both assuming 100GB of incoming syslog data, resulting in 15GB for each set of rawdata and 35GB for each set of index files:
</p>
<ul><li> <b>3 peer nodes, with replication factor = 3; search factor = 2:</b> This requires a total of 115GB across all peer nodes (averaging 38GB/peer), calculated as follows:
<ul><li> Total rawdata = (15GB * 3) = 45GB.
</li><li> Total index files = (35GB * 2) = 70 GB.
</li></ul></li><li> <b>5 peer nodes, with replication factor = 5; search factor = 3:</b> This requires a total of 180GB across all peer nodes (averaging 36GB/peer), calculated as follows:
<ul><li> Total rawdata = (15GB * 5) = 75GB.
</li><li> Total index files = (35GB * 3) = 105 GB.
</li></ul></li></ul><h4><font size="3"><b><i> <a name="systemrequirements_storage_hardware"><span class="mw-headline" id="Storage_hardware"> Storage hardware </span></a></i></b></font></h4>
<p>In pre-6.0 versions of Splunk Enterprise, replicated copies of cluster buckets always resided in the <code><font size="2">colddb</font></code> directory, even if they were hot or warm buckets.  Starting with 6.0, hot and warm replicated copies reside in the <code><font size="2">db</font></code> directory, the same as for non-replicated copies.   This eliminates any need to consider faster storage for <code><font size="2">colddb</font></code> for clustered indexes, compared to non-clustered indexes.
</p>
<h3> <a name="systemrequirements_licensing_information"><span class="mw-headline" id="Licensing_information">Licensing information</span></a></h3>
<p>As with any Splunk Enterprise deployment, your licensing requirements are driven by the volume of data your indexers process. Contact your Splunk sales representative to purchase additional license volume. Refer to "How licensing works" in the <i>Admin Manual</i> for more information about Splunk Enterprise licensing.
</p><p>There are just a few license issues that are specific to index replication:
</p>
<ul><li> All cluster members, including masters, peers, and search heads, need to be in an Enterprise <b>license pool</b>, even if they're not expected to index any data. 
</li><li> Cluster members must share the same licensing configuration.
</li><li> Only incoming data counts against the license; replicated data does not.
</li><li> You cannot use index replication with a free license.
</li></ul><h3> <a name="systemrequirements_ports_that_the_cluster_nodes_use"><span class="mw-headline" id="Ports_that_the_cluster_nodes_use">Ports that the cluster nodes use</span></a></h3>
<p>These ports must be available to cluster nodes:
</p>
<ul><li> <b>On the master:</b>
<ul><li> The management port (by default, 8089) must be available to all other cluster nodes.
</li></ul></li><li> <b>On each peer:</b>
<ul><li> The management port must be available to all other cluster nodes.
</li><li> The replication port must be available to all other peer nodes.
</li><li> The receiving port must be available to all forwarders sending data to that peer.
</li></ul></li><li> <b>On each search head:</b>
<ul><li> The management port must be available to all other nodes.
</li><li> The http port (by default, 8000) must be available to any browsers accessing data from the search head.
</li></ul></li></ul><h3> <a name="systemrequirements_deployment_server_and_clusters"><span class="mw-headline" id="Deployment_server_and_clusters">Deployment server and clusters </span></a></h3>
<p>Do not use deployment server with cluster peers.
</p><p>The deployment server is not supported as a means to distribute configurations or apps to cluster peers. To distribute configurations across the set of cluster peers, instead use the <b>configuration bundle</b> method outlined in the topic <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a>.
</p><p>For information on how to migrate app distribution from deployment server to the configuration bundle method, see <a href="#migratenon-clusteredindexerstoaclusteredenvironment_migrate_apps_to_a_cluster" class="external text">"Migrate apps to a cluster"</a>.
</p>
<h3> <a name="systemrequirements_additional_roles_for_the_master_node"><span class="mw-headline" id="Additional_roles_for_the_master_node">Additional roles for the master node </span></a></h3>
<p>As a general rule, you should dedicate the Splunk Enterprise instance running the master node to that single purpose. Under limited circumstances, however, the master instance can also fulfill certain other lightweight functions:
</p>
<ul><li> You can use the master's built-in search head for debugging purposes.
</li><li> You might be able to run a search head cluster deployer on the master, depending on the master's load.
</li><li> You might be able to run a distributed management console on the master instance, depending on the master's load.
</li></ul><p>To run a deployer or a distributed management console on the master, the master's cluster should stay below the following limits: 
</p>
<ul><li> 30 indexers
</li><li> 100,000 buckets
</li><li> 10 indexes
</li><li> 10 search heads
</li></ul><a name="enablethemasternode"></a><h2> <a name="enablethemasternode_enable_the_indexer_cluster_master_node"><span class="mw-headline" id="Enable_the_indexer_cluster_master_node"> Enable the indexer cluster master node</span></a></h2>
<p><b>Before reading this topic, read <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>.</b>
</p><p>A cluster has one, and only one, <b>master node</b>. The master node coordinates the activities of the peer nodes. It does not itself store or replicate data (aside from its own internal data). 
</p><p><b>Important:</b> A master node cannot do double duty as a peer node or a search node. The Splunk Enterprise instance that you enable as master node must perform only that single indexer cluster role. In addition, the master cannot share a machine with a peer. Under certain limited circumstances, however, the master instance can handle a few other lightweight functions. See <a href="#systemrequirements_additional_roles_for_the_master_node" class="external text">"Additional roles for the master node"</a>.
</p><p>You must enable the master node as the first step in deploying a cluster, before setting up the peer nodes.
</p><p>The procedure in this topic explains how to use Splunk Web to enable a master node. You can also enable a master in two other ways:
</p>
<ul><li> Directly edit the master's <code><font size="2">server.conf</font></code> file. See <a href="#configuremasterwithserverconf" class="external text">"Configure the master with server.conf"</a> for details. Some advanced settings can only be configured by editing this file.
</li><li> Use the CLI <code><font size="2">edit cluster-config</font></code> command. See <a href="#configuremasterwithcli" class="external text">"Configure the master with the CLI"</a> for details. 
</li></ul><p><b>Important:</b> This topic explains how to enable a master for a single-site cluster only. If you plan to deploy a multisite cluster, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h3> <a name="enablethemasternode_enable_the_master"><span class="mw-headline" id="Enable_the_master"> Enable the master </span></a></h3>
<p>To enable an indexer as the master node:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right corner of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select <b>Enable indexer clustering</b>.
</p><p><b>4.</b> Select <b>Master node</b> and click <b>Next</b>.
</p><p><b>5.</b> There are a few fields to fill out:
</p>
<ul><li> <b>Replication Factor.</b>The <b>replication factor</b> determines how many copies of data the cluster maintains. The default is 3. For more information on the replication factor, see <a href="#basicclusterarchitecture_replication_factor" class="external text">"Replication factor"</a>. Be sure to choose the right replication factor now. It is inadvisable to increase the replication factor later, after the cluster contains significant amounts of data.
</li><li>  <b>Search Factor.</b> The <b>search factor</b> determines how many immediately searchable copies of data the cluster maintains. The default is 2. For more information on the search factor, see <a href="#basicclusterarchitecture_search_factor" class="external text">"Search factor"</a>. Be sure to choose the right search factor now. It is inadvisable to increase the search factor later, once the cluster has significant amounts of data.
</li><li> <b>Security Key</b>. This is the key that authenticates communication between the master and the peers and search heads. The key must be the same across all cluster instances. If you leave the field empty here, leave it empty on the peers and search heads as well.
</li></ul><p><b>6.</b> Click <b>Enable master node</b>.
</p><p>The message appears, "You must restart Splunk for the master node to become active. You can restart Splunk from Server Controls."  
</p><p><b>7.</b> Click <b>Go to Server Controls</b>. This takes you to the Settings page where you can initiate the restart. 
</p><p><b>Important:</b> When the master starts up for the first time, it will block indexing on the peers until you enable and restart the full replication factor number of peers. Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p>
<h3> <a name="enablethemasternode_view_the_master_dashboard"><span class="mw-headline" id="View_the_master_dashboard"> View the master dashboard </span></a></h3>
<p>After the restart, log back into the master and return to the Clustering page in Splunk Web. This time, you see the master clustering dashboard. For information on the dashboard, see <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p>
<h3> <a name="enablethemasternode_perform_additional_configuration"><span class="mw-headline" id="Perform_additional_configuration">Perform additional configuration</span></a></h3>
<p>For information on post-deployment master node configuration, see <a href="#configurethemaster" class="external text">"Master configuration overview"</a>.
</p>
<a name="enablethepeernodes"></a><h2> <a name="enablethepeernodes_enable_the_peer_nodes"><span class="mw-headline" id="Enable_the_peer_nodes"> Enable the peer nodes</span></a></h2>
<p><b>Before reading this topic, read <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>.</b>
</p><p>You ordinarily need to enable multiple <b>peer nodes</b> to deploy a cluster. At a minimum, you must enable at least <b>replication factor</b> number of peers - and potentially more to accommodate horizontal scaling.
</p><p>Before enabling the set of peers, you must enable and restart the <b>master node</b>, as described in <a href="#enablethemasternode" class="external text">"Enable the master node"</a>. When the master starts up for the first time, it will block indexing on the peers until you have enabled and restarted the replication factor number of peers.
</p><p>The procedure in this topic explains how to use Splunk Web to enable a peer node. You can also enable a peer in two other ways:
</p>
<ul><li> Directly edit the peer's <code><font size="2">server.conf</font></code> file. See <a href="#configurepeerswithserverconf" class="external text">"Configure peer nodes with server.conf"</a> for details. 
</li><li> Use the CLI <code><font size="2">edit cluster-config</font></code> command. See <a href="#configurepeerswithcli" class="external text">"Configure peer nodes with the CLI"</a> for details. 
</li></ul><p><b>Important:</b> This topic explains how to enable a peer for a single-site cluster only. If you plan to deploy a multisite cluster, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h3> <a name="enablethepeernodes_enable_the_peer"><span class="mw-headline" id="Enable_the_peer"> Enable the peer </span></a></h3>
<p>To enable an indexer as a peer node:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right corner of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select <b>Enable indexer clustering</b>.
</p><p><b>4.</b> Select <b>Peer node</b> and click <b>Next</b>.
</p><p><b>5.</b> There are a few fields to fill out:
</p>
<ul><li> <b>Master IP address or Hostname</b>. Enter the master's IP address or hostname. For example: <code><font size="2">https://10.152.31.202</font></code>.
</li><li> <b>Master port</b>. Enter the master's management port. For example: <code><font size="2">8089</font></code>. 
</li><li> <b>Peer replication port</b>. This is the port on which the peer receives replicated data streamed from the other peers. You can specify any available, unused port for this purpose. This port must be different from the management or receiving ports.
</li><li> <b>Security key</b>. This is the key that authenticates communication between the master and the peers and search heads. The key must be the same across all cluster instances. If the master has a security key, you must enter it here.
</li></ul><p><b>6.</b> Click <b>Enable peer node</b>.
</p><p>The message appears, "You must restart Splunk for the peer node to become active."  
</p><p><b>7.</b> Click <b>Go to Server Controls</b>. This takes you to the Settings page where you can initiate the restart. 
</p><p><b>8.</b> Repeat this process for all the cluster's peer nodes.
</p><p>When you have enabled the replication factor number of peers, the cluster can start indexing and replicating data, as described in <a href="#enablethemasternode" class="external text">"Enable the master node"</a>.
</p>
<h3> <a name="enablethepeernodes_view_the_peer_dashboard"><span class="mw-headline" id="View_the_peer_dashboard">View the peer dashboard</span></a></h3>
<p>After the restart, log back into the peer node and return to the Clustering page in Splunk Web. This time, you see the peer's clustering dashboard. For information on the dashboard, see <a href="#viewthepeerdashboard" class="external text">"View the peer dashboard"</a>.
</p>
<h3> <a name="enablethepeernodes_configure_the_peers"><span class="mw-headline" id="Configure_the_peers">Configure the peers</span></a></h3>
<p>After enabling the peers, you need to perform additional configuration before you start indexing data. For details, read these topics:
</p>
<ul><li> <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>
</li><li> <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a>
</li></ul><p>You might also need to configure some other settings on the peers. See 
<a href="#configurethepeers" class="external text">"Peer node configuration overview"</a>.
</p>
<a name="enablethesearchhead"></a><h2> <a name="enablethesearchhead_enable_the_search_head"><span class="mw-headline" id="Enable_the_search_head"> Enable the search head</span></a></h2>
<p><b>Before reading this topic, read <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>.</b>
</p><p>To search the cluster, you need to enable at least one <b>search head</b> in the indexer cluster.  
</p><p>Before enabling the search head, you must enable and restart the master node, as described in <a href="#enablethemasternode" class="external text">"Enable the master node"</a>.
</p><p>The procedure in this topic explains how to use Splunk Web to enable a search head. You can also enable a search head in two other ways:
</p>
<ul><li> Directly edit the search head's <code><font size="2">server.conf</font></code> file. See <a href="#configuresearchheadwithserverconf" class="external text">"Configure the search head with server.conf"</a> for details. Some advanced settings, including multi-cluster search, can only be configured by editing this file.
</li><li> Use the CLI <code><font size="2">edit cluster-config</font></code> command. See <a href="#configuresearchheadwithcli" class="external text">"Configure the search head with the CLI"</a> for details. 
</li></ul><p><b>Important:</b> This topic explains how to enable a search head for a single-site cluster only. If you plan to deploy a multisite cluster, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>. If you plan to incorporate search heads that are members of a <b>search head cluster</b>, see "Integrate the search head cluster with an indexer cluster"  in the <i>Distributed Search</i> manual.
</p><p><br></p>
<h3> <a name="enablethesearchhead_enable_the_search_head_2"><span class="mw-headline" id="Enable_the_search_head_2"> Enable the search head</span></a></h3>
<p>To enable a Splunk instance as a search head in an indexer cluster:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right corner of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select <b>Enable indexer clustering</b>.
</p><p><b>4.</b> Select <b>Search head node</b> and click <b>Next</b>.
</p><p><b>5.</b> There are a few fields to fill out:
</p>
<ul><li> <b>Master IP address or Hostname</b>. Enter the master's IP address or hostname. For example: <code><font size="2">https://10.152.31.202</font></code>.
</li><li> <b>Master port</b>. Enter the master's management port. For example: <code><font size="2">8089</font></code>. 
</li><li> <b>Security key</b>. This is the key that authenticates communication between the master and the peers and search heads. The key must be the same across all cluster instances. If the master has a security key, you must enter it here.
</li></ul><p><b>6.</b> Click <b>Enable search head node</b>.
</p><p>The message appears, "You must restart Splunk for the search node to become active. You can restart Splunk from Server Controls."  
</p><p><b>7.</b> Click <b>Go to Server Controls</b>. This takes you to the Settings page where you can initiate the restart.
</p>
<h3> <a name="enablethesearchhead_next_steps"><span class="mw-headline" id="Next_steps">Next steps</span></a></h3>
<p>Now that you have enabled the search head, you can:
</p>
<ul><li> View the search head dashboard
</li><li> Allow the search head to search other clusters
</li><li> Add search heads to the cluster
</li><li> Perform additional configuration on the search head
</li></ul><h4><font size="3"><b><i> <a name="enablethesearchhead_view_the_search_head_dashboard"><span class="mw-headline" id="View_the_search_head_dashboard">View the search head dashboard</span></a></i></b></font></h4>
<p>After the restart, log back into the search head and return to the Clustering page in Splunk Web. This time, you see the search head's clustering dashboard. See <a href="#viewthesearchheaddashboard" class="external text">"View the search head dashboard"</a> for more information.
</p>
<h4><font size="3"><b><i> <a name="enablethesearchhead_allow_the_search_head_to_search_multiple_clusters"><span class="mw-headline" id="Allow_the_search_head_to_search_multiple_clusters"> Allow the search head to search multiple clusters</span></a></i></b></font></h4>
<p>From the dashboard, you can add additional clusters for the search head to search. For details, see <a href="#configuremulti-clustersearch" class="external text">"Search across multiple indexer clusters"</a>.
</p>
<h4><font size="3"><b><i> <a name="enablethesearchhead_add_search_heads_to_an_indexer_cluster"><span class="mw-headline" id="Add_search_heads_to_an_indexer_cluster">Add search heads to an indexer cluster</span></a></i></b></font></h4>
<p>You can set up multiple search heads to accommodate more simultaneous searches.  For information on how to determine your search head needs, see "Hardware capacity planning" in the <i>Capacity Planning</i> manual.
</p><p>If you want to set up more search heads for an indexer cluster, just repeat the enablement procedure for additional instances. If you want to deploy a search head cluster, so that the search heads share configurations and jobs, see the additional configuration instructions in the topic "Integrate the search head cluster with an indexer cluster" in the <i>Distributed Search</i> manual.
</p>
<h4><font size="3"><b><i> <a name="enablethesearchhead_perform_additional_configuration"><span class="mw-headline" id="Perform_additional_configuration">Perform additional configuration</span></a></i></b></font></h4>
<p>For more information on configuration of search heads in an indexer cluster, see <a href="#configurethesearchhead" class="external text">"Search head configuration overview"</a>.
</p>
<a name="forwardmasterdata"></a><h2> <a name="forwardmasterdata_best_practice:_forward_master_node_data_to_the_indexer_layer"><span class="mw-headline" id="Best_practice:_Forward_master_node_data_to_the_indexer_layer"> Best practice: Forward master node data to the indexer layer</span></a></h2>
<p>It is considered a best practice to forward all master node internal data to the indexer (peer node) layer.  This has several advantages:
</p>
<ul><li> It accumulates all data in one place. This simplifies the process of managing your data: You only need to manage your indexes and data at one level, the indexer level.
</li><li> It enables diagnostics for the master node if it goes down. The data leading up to the failure is accumulated on the indexers, where a search head can later access it.
</li></ul><p>The preferred approach is to forward the data directly to the indexers, without indexing separately on the master. You do this by configuring the master as a forwarder.  These are the main steps:
</p><p><b>1.</b> <b>Make sure that all necessary indexes exist on the indexers.</b> This is normally the case, unless you have created custom indexes on the master node. Since <code><font size="2">_audit</font></code> and <code><font size="2">_internal</font></code> exist on indexers as well as the master, you do not need to create separate versions of those indexes to hold the corresponding master data.
</p><p><b>2.</b> <b>Configure the master as a forwarder.</b> Create an <code><font size="2">outputs.conf</font></code> file on the master node that configures it for load-balanced forwarding across the set of peer nodes. You must also turn off indexing on the master, so that the master does not both retain the data locally as well as forward it to the peers.
</p><p>Here is an example <code><font size="2">outputs.conf</font></code> file:
</p>
<div class="samplecode">
<code><font size="2"><br># Turn off indexing on the master<br>[indexAndForward]<br>index = false<br>&nbsp;<br>[tcpout]<br>defaultGroup = my_peers_nodes <br>forwardedindex.filter.disable = true &nbsp;<br>indexAndForward = false <br>&nbsp;<br>[tcpout:my_peers_nodes]<br>server=10.10.10.1:9997,10.10.10.2:9997,10.10.10.3:9997<br>autoLB = true<br></font></code>
</div>
<p>This example assumes that each peer node's receiving port is set to 9997.
</p><p>For details on configuring <code><font size="2">outputs.conf</font></code>, read "Configure forwarders with outputs.conf" in the Forwarding Data manual.
</p>
<a name="preparethepeers"></a><h2> <a name="preparethepeers_prepare_the_peers_for_index_replication"><span class="mw-headline" id="Prepare_the_peers_for_index_replication"> Prepare the peers for index replication</span></a></h2>
<p>After you enable the peers, you might need to perform further configuration to prepare them for index replication. 
</p><p>If you use only the default set of indexes and default configurations, you can start replicating data right away. However, if you need to install apps or change configurations on the peers, you usually must apply the set of changes to all peers. In particular, if you need to add indexes (including indexes defined by an app), you must do so in a way that ensures that the peers use a common set of indexes. 
</p><p>For information on how to configure indexes across cluster peers, read <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>. 
</p><p>For information on how to configure apps across peers, as well as other peer configuration issues, read <a href="#configurethepeers" class="external text">"Peer node configuration overview"</a>.
</p>
<a name="useforwarderstogetyourdata"></a><h2> <a name="useforwarderstogetyourdata_use_forwarders_to_get_your_data_into_the_indexer_cluster"><span class="mw-headline" id="Use_forwarders_to_get_your_data_into_the_indexer_cluster"> Use forwarders to get your data into the indexer cluster</span></a></h2>
<p>Cluster peer nodes can get their data directly from any of the same sources as a non-clustered indexer. However, if data fidelity matters to you, you will use <b>forwarders</b> to initially consume the data before forwarding it to the peer nodes, rather than ingesting the data directly into the nodes. The node that receives the data from the forwarder is called the <b>receiver</b> or the <b> </b><b>receiving</b> node<b>.</b>
</p><p>There are two key reasons for using forwarders to send data to your cluster:
</p>
<ul><li> <b>To ensure that all incoming data gets indexed.</b> By activating the forwarder's optional <b>indexer acknowledgment</b> feature, you can ensure that all incoming data gets indexed and stored on the cluster. It works like this: When a peer receives a block of data from a forwarder, it sends the forwarder an acknowledgment after it successfully indexes the data. If the forwarder does not receive an acknowledgment from the peer, it resends the data.  The forwarder continues to resend the data until it gets the acknowledgment.  Indexer acknowledgment is the only way to ensure end-to-end data fidelity. See <a href="#useforwarderstogetyourdata_how_indexer_acknowledgment_works" class="external text">"How indexer acknowledgment works"</a> later in this topic.
</li><li> <b>To handle potential node failure.</b> With forwarder <b>load balancing</b>, if one receiving node in the load-balanced group goes down, the forwarder continues to send its data to the remaining peers in the group. Without load balancing, the forwarder has no way to continue sending data if its receiving node goes down. See <a href="#useforwarderstogetyourdata_how_load_balancing_works" class="external text">"How load balancing works"</a> later in this topic.
</li></ul><p><b>Important:</b> Before continuing, you must be familiar with forwarders and how to use them to get data into Splunk.  For an introduction to forwarders, read "About forwarding and receiving" in the <i>Forwarding Data</i> manual. Subsequent topics in that manual describe all aspects of deploying and configuring forwarders.
</p><p>To use forwarders to get data into clusters, you must perform two types of configuration:
</p>
<ul><li> Configure the connection from forwarder to peer node.
</li><li> Configure the forwarder's data inputs. 
</li></ul><p><b>Note:</b> This topic assumes you are deploying <b>universal forwarders</b>, but the steps are basically the same for light or heavy forwarders.
</p>
<h3> <a name="useforwarderstogetyourdata_configure_the_connection_from_forwarder_to_peer_node"><span class="mw-headline" id="Configure_the_connection_from_forwarder_to_peer_node">Configure the connection from forwarder to peer node </span></a></h3>
<p>There are three steps to setting up connections between forwarders and peer nodes:
</p><p><b>1.</b> Configure the peer nodes to receive data from forwarders. 
</p><p><b>2.</b> Configure the forwarders to send data to the peer nodes.
</p><p><b>3.</b> Enable indexer acknowledgment for each forwarder. This step is required to ensure end-to-end data fidelity. If that is not a requirement for your deployment, you can skip this step.
</p><p>Once you are finished setting up the connection, you need to configure the data inputs that control the data that streams into the forwarders (and onwards to the cluster). How to do this is the subject of a later section in this topic, <a href="#useforwarderstogetyourdata_configure_the_forwarder.27s_data_inputs" class="external text">"Configure the forwarder's data inputs"</a>.
</p>
<h4><font size="3"><b><i> <a name="useforwarderstogetyourdata_1._configure_the_peer_nodes_to_receive_data_from_forwarders"><span class="mw-headline" id="1._Configure_the_peer_nodes_to_receive_data_from_forwarders">1. Configure the peer nodes to receive data from forwarders</span></a></i></b></font></h4>
<p>In order for a peer to receive data from forwarders, you must configure the peer's <b>receiving port</b>. For information on how to configure the receiving port, read "Enable a receiver" in the <i>Forwarding Data</i> manual.
</p><p><b>Important:</b> One of the ways you can specify the receiving port is by editing the peer's inputs.conf file in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>. For many clusters, you can simplify peer input configuration by deploying a single, identical <code><font size="2">inputs.conf</font></code> file across all the peers. In that case, the receiving port you specify in the common copy of <code><font size="2">inputs.conf</font></code> will supersede any ports you enable on each individual peer. For details on how to create and deploy a common <code><font size="2">inputs.conf</font></code> across all peers, read <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a>.
</p>
<h4><font size="3"><b><i> <a name="useforwarderstogetyourdata_2._configure_the_forwarders_to_send_data_to_the_peer_nodes"><span class="mw-headline" id="2._Configure_the_forwarders_to_send_data_to_the_peer_nodes">2. Configure the forwarders to send data to the peer nodes </span></a></i></b></font></h4>
<p>When you set up a forwarder, you specify its receiving peer by providing the peer's IP address and receving port number. For example: <code><font size="2">10.10.10.1:9997</font></code>. You do this in the forwarder's outputs.conf file, as described in "Configure forwarders with outputs.conf" in the <i>Forwarding Data</i> manual. To specify the receiving peer, set the <code><font size="2">server</font></code> attribute, like this:
</p>
<div class="samplecode">
<code><font size="2"><br>server=10.10.10.1:9997<br></font></code>
</div>
<p>The receiving port that you specify here is the port configured in step 1. 
</p><p>To set up the forwarder to use load-balancing, so that the data goes to multiple peer nodes in sequence, you configure a load-balanced group of receiving peers. For example, this attribute/value pair in <code><font size="2">outputs.conf</font></code> specifies a load-balanced group of three peers:
</p>
<div class="samplecode">
<code><font size="2"><br>server=10.10.10.1:9997,10.10.10.2:9997,10.10.10.3:9997<br></font></code>
</div>
<p>To learn more about configuring load balancing, read "Set up load balancing" in the <i>Forwarding Data</i> manual.
</p><p><b>Note:</b> There are several other ways that you can specify a forwarder's receiving peer(s). For example:
</p>
<ul><li> You can specify the receiving peer during forwarder deployment (for Windows forwarders only), as described in "Deploy a Windows forwarder manually" in the <i>Forwarding Data</i> manual. 
</li><li> You can specify the receiver with the CLI command <code><font size="2">add forward-server</font></code>, as described in "Deploy a *nix forwarder manually" in the <i>Forwarding Data</i> manual.
</li></ul><p>Both of these methods work by modifying the underlying <code><font size="2">outputs.conf</font></code> file. No matter what method you use to specify the receiving peers, you still need to directly edit the underlying <code><font size="2">outputs.conf</font></code> file if you want to turn on indexer acknowledgment, as described in the next step.
</p>
<h4><font size="3"><b><i> <a name="useforwarderstogetyourdata_3._enable_indexer_acknowledgment_for_each_forwarder"><span class="mw-headline" id="3._Enable_indexer_acknowledgment_for_each_forwarder">3. Enable indexer acknowledgment for each forwarder </span></a></i></b></font></h4>
<p><b>Note:</b> This step is required to ensure end-to-end data fidelity. <b>If that is not a requirement for your deployment, you can skip this step.</b>
</p><p>To ensure that the cluster receives and indexes all incoming data, you must turn on indexer acknowledgment for each forwarder. 
</p><p>To configure indexer acknowledgment, set the <code><font size="2">useACK</font></code> attribute in each forwarder's <code><font size="2">outputs.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[tcpout:&lt;peer_target_group&gt;]<br>useACK=true<br></font></code>
</div>
<p>For detailed information on configuring indexer acknowledgment, read "Protect against loss of in-flight data" in the <i>Forwarding Data</i> manual.
</p><p><b>Important:</b> For indexer acknowledgment to work properly, the forwarders' wait queues must be configured to the optimal size. For forwarders at version 5.0.4 or above, the system handles this automatically. For earlier version forwarders, follow  the instructions in the version of the "Protect against loss of in-flight data" topic <i>for that forwarder version.</i> Specifically, read the subtopic on adjusting the <code><font size="2">maxQueueSize</font></code> setting.
</p>
<h4><font size="3"><b><i> <a name="useforwarderstogetyourdata_example:_a_load-balancing_forwarder_with_indexer_acknowledgment"><span class="mw-headline" id="Example:_A_load-balancing_forwarder_with_indexer_acknowledgment">Example: A load-balancing forwarder with indexer acknowledgment</span></a></i></b></font></h4>
<p>Here's a sample <code><font size="2">outputs.conf</font></code> configuration for a forwarder that's using load balancing to send data in sequence to three peers in a cluster. It assumes that each of the peers has previously been configured to use 9997 for its receiving port:
</p>
<div class="samplecode">
<code><font size="2"><br>[tcpout]<br>defaultGroup=my_LB_peers<br><br>[tcpout:my_LB_peers]<br>autoLBFrequency=40<br>server=10.10.10.1:9997,10.10.10.2:9997,10.10.10.3:9997<br>useACK=true<br></font></code>
</div>
<p>The forwarder starts by sending data to one of the peers listed for the <code><font size="2">server</font></code> attribute.  After 40 seconds, it switches to another peer, and so on. If, at any time, it doesn't receive acknowledgment from the current receiving node, it resends the data, this time to the next available node.
</p>
<h3> <a name="useforwarderstogetyourdata_configure_the_forwarder.27s_data_inputs"><span class="mw-headline" id="Configure_the_forwarder.27s_data_inputs">Configure the forwarder's data inputs </span></a></h3>
<p>Once you've specified the connection between the forwarder and the receiving peer(s), you must specify the data inputs to the forwarder, so that the forwarder has data to send to the cluster. You usually do this by editing the forwarder's <code><font size="2">inputs.conf</font></code> file. Read the <i>Getting Data In</i> manual, starting with "What Splunk can index" for detailed information on configuring data inputs. The topic in that manual entitled "Use forwarders" provides an introduction to specifying data inputs on forwarders.
</p>
<h3> <a name="useforwarderstogetyourdata_how_indexer_acknowledgment_works"><span class="mw-headline" id="How_indexer_acknowledgment_works"> How indexer acknowledgment works </span></a></h3>
<p>In brief, indexer acknowledgment works like this: The forwarder sends data continuously to the receiving peer, in blocks of approximately 64kB. The forwarder maintains a copy of each block in memory until it gets an acknowledgment from the peer. While waiting, it continues to send more data blocks.
</p><p>If all goes well, the receiving peer:
</p><p><b>1.</b> receives the block of data, parses and indexes it, and writes the data (raw data and index data) to the file system. 
</p><p><b>2.</b> streams copies of the raw data to each of its target peers.
</p><p><b>3.</b> sends an acknowledgment back to the forwarder.
</p><p>The acknowledgment assures the forwarder that the data was successfully written to the cluster. Upon receiving the acknowledgment, the forwarder releases the block from memory. 
</p><p>If the forwarder does not receive the acknowledgment, that means there was a failure along the way. Either the receiving peer went down or that peer was unable to contact its set of target peers. The forwarder then automatically resends the block of data. If the forwarder is using load-balancing, it sends the block to another receiving node in the load-balanced group. If the forwarder is not set up for load-balancing, it attempts to resend data to the same node as before.
</p><p><b>Important:</b> To ensure end-to-end data fidelity, you must explicitly enable indexer acknowledgment for each forwarder that's sending data to the cluster, as described earlier in this topic. If end-to-end data fidelity is not a requirement for your deployment, you can skip this step. 
</p><p>For more information on how indexer acknowledgment works, read "Protect against loss of in-flight data" in the <i>Forwarding Data</i> manual.
</p>
<h3> <a name="useforwarderstogetyourdata_how_load_balancing_works"><span class="mw-headline" id="How_load_balancing_works"> How load balancing works </span></a></h3>
<p>In load balancing, the forwarder distributes incoming data across several receiving peer nodes. Each node gets a portion of the total data, and together the receiving nodes get all the data. 
</p><p>Splunk forwarders perform "automatic load balancing". The forwarder routes data to different nodes based on a specified time interval. For example, assume you have a load-balanced group consisting of three peer nodes: A, B, and C. At some specified interval, such as every 30 seconds, the forwarder switches the data stream to another node in the group, selected at random. So, the forwarder might switch from node B to node A to node C, and so on. If one node is down, the forwarder immediately switches to another.
</p><p><b>Note:</b> To expand on this a bit, each of the forwarder's inputs has its own data stream. At the specified interval, the forwarder switches the data stream to the newly selected node, if it's safe to do so. If it cannot safely switch the data stream to the new node, it keeps the connection to the previous node open and continues to send the data stream to that node until it has been safely sent.
</p><p>Load balancing, in conjunction with indexer acknowledgment, is of key importance in a clustered deployment because it helps ensure that you don't lose any data in case of node failure. If a forwarder does not receive indexer acknowledgment from the node it is sending data to, it resends the data to the next available node in the load-balanced group.
</p><p>For more information on forwarder load balancing, read "Set up load balancing" in the <i>Forwarding Data</i> manual. For information on how load balancing works with indexer acknowledgment, read "Protect against loss of in-flight data" in the <i>Forwarding Data</i> manual.
</p>
<h3> <a name="useforwarderstogetyourdata_configure_inputs_directly_on_the_peers"><span class="mw-headline" id="Configure_inputs_directly_on_the_peers"> Configure inputs directly on the peers</span></a></h3>
<p>If you decide not to use forwarders to handle your data inputs, you can set up inputs on each peer in the usual fashion; for example, by editing <code><font size="2">inputs.conf</font></code>. For information on configuring inputs, read "Configure your inputs" in the <i>Getting Data In</i> Manual.
</p>
<a name="clustersinscaledoutdeployments"></a><h2> <a name="clustersinscaledoutdeployments_use_indexer_clusters_to_scale_indexing"><span class="mw-headline" id="Use_indexer_clusters_to_scale_indexing"> Use indexer clusters to scale indexing </span></a></h2>
<p>The main purpose of indexer clusters is to enable <b>index replication</b>. However, clusters can also be generally useful in scale-out deployment topologies  as a way to manage multiple indexers, even when index replication isn't a requirement.
</p><p>For example, say you want to create a deployment of three indexers and one search head, so that you can index larger quantities of data than a single indexer is capable of.  The customary way of doing this, and the only way possible prior to Splunk Enterprise 5.0, is to set up each of the indexers independently, add in a search head, and then use a tool like <b>deployment server</b> to coordinate the indexer configurations.
</p><p>With clustering, you can instead configure this deployment scenario as a cluster, with three peer nodes replacing the three independent indexers. Even if you don't need index replication and its key advantages like data availability and disaster tolerance, there are several reasons why it might be beneficial to use a cluster to coordinate multiple indexer instances:
</p>
<ul><li> Simplified management and coordination of indexer configuration (in place of using deployment server or performing manual updates). See   <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a> for details.
</li><li> Simplified set up and control of distributed search. See   <a href="#enablethesearchhead" class="external text">"Enable the search head"</a>.
</li><li> Better insight into the state of your indexers through the clustering dashboards. See   <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</li><li> Ability to take advantage of additional cluster management capabilities as they're developed.
</li></ul><p>The main downsides of employing a cluster for scaling indexing capacity are these:
</p>
<ul><li> You must install an additional Splunk Enterprise instance to function as the cluster master node. 
</li><li> The cluster does not support heterogeneous indexers. All cluster nodes must be at the same version level. In addition, all peer nodes in a cluster must use the same <code><font size="2">indexes.conf</font></code> configuration. For further details, see the next section, <a href="#clustersinscaledoutdeployments_cluster_peer_management_compared_to_deployment_server" class="external text">"Cluster peer management compared to deployment server"</a>.
</li><li> You cannot use the deployment server to distribute configurations or apps across the cluster peers. For further details, see the next section, <a href="#clustersinscaledoutdeployments_cluster_peer_management_compared_to_deployment_server" class="external text">"Cluster peer management compared to deployment server"</a>.
</li></ul><h3> <a name="clustersinscaledoutdeployments_cluster_peer_management_compared_to_deployment_server"><span class="mw-headline" id="Cluster_peer_management_compared_to_deployment_server"> Cluster peer management compared to deployment server </span></a></h3>
<p>One useful cluster feature is the ability to manage and update the configuration for all indexers (peer nodes) from a central location, the master node. In that respect, it's similar in function to the <b>deployment server</b>. Unlike the deployment server, however, cluster peer management does not have any concept of server classes. Because of this, and because of the way clusters coordinate their activities, you cannot specify different app or <code><font size="2">indexes.conf</font></code> configurations for different groups of indexers. (All peer nodes in a cluster must use the same <code><font size="2">indexes.conf</font></code> configurations, as well as some other configurations, as described in <a href="#configurethepeers" class="external text">"Peer node configuration overview"</a>.) If you need to maintain a heterogeneous set of indexers, you cannot employ clusters for scaling purposes.
</p><p>On the other hand, the configuration bundle method used to download updates to peer nodes has certain advantages over the deployment server. Specifically, it not only distributes updates, it also validates them on the peers, and then (when necessary) initiates a rolling restart of the peers. See <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a> for details.
</p><p><b>Important:</b> Do not use deployment server or third-party distributed configuration management software, such as Puppet or Chef, to deploy updates directly to peer nodes. You can use such tools to deploy updates to the master, which then deploys those updates to the peers. See <a href="#updatepeerconfigurations_use_deployment_server_to_distribute_the_apps_to_the_master" class="external text">"Use deployment server to distribute the apps to the master"</a>.
</p>
<h3> <a name="clustersinscaledoutdeployments_configure_a_cluster_for_scale-out_deployment"><span class="mw-headline" id="Configure_a_cluster_for_scale-out_deployment"> Configure a cluster for scale-out deployment</span></a></h3>
<p>To set up a cluster for scale-out deployment, without index replication, just set both the <b>replication factor</b> and <b>search factor</b> to 1. This causes the cluster to function purely as a coordinated set of Splunk Enterprise instances, without data replication. The cluster will not make any duplicate copies of the data, so you can keep storage size and processing overhead to a minimum.
</p>
<a name="migratenon-clusteredindexerstoaclusteredenvironment"></a><h2> <a name="migratenon-clusteredindexerstoaclusteredenvironment_migrate_non-clustered_indexers_to_a_clustered_environment"><span class="mw-headline" id="Migrate_non-clustered_indexers_to_a_clustered_environment"> Migrate non-clustered indexers to a clustered environment</span></a></h2>
<p>Read the topic <a href="#keydifferences" class="external text">"Key differences between clustered and non-clustered deployments of indexers"</a> carefully if you plan to migrate your current set of indexers to an indexer cluster. That topic describes issues that you must understand before initiating the migration process.
</p><p>You can add a non-clustered indexer to a cluster (as a peer node) at any time. To do so, just enable the indexer as a peer, as described in <a href="#enablethepeernodes" class="external text">"Enable the peer nodes"</a>.
</p><p>Once the indexer has been made a peer, it participates in the cluster the same as any other peer. Any new data coming into the peer gets replicated according to the cluster's replication factor, and the peer is also a candidate for receiving replicated data from other peers. Data already on the indexer does not get automatically replicated, but it does participate in searches, as described below.
</p><p><b>Important:</b> Before migrating an indexer from non-clustered to clustered, be certain of your needs. The process goes in one direction only.  There is no supported procedure for converting an indexer from clustered to non-clustered.
</p>
<h3> <a name="migratenon-clusteredindexerstoaclusteredenvironment_manage_legacy_data"><span class="mw-headline" id="Manage_legacy_data"> Manage legacy data</span></a></h3>
<p>The term "legacy data" means data stored on an indexer prior to its conversion to a cluster peer.
</p>
<h4><font size="3"><b><i> <a name="migratenon-clusteredindexerstoaclusteredenvironment_how_the_cluster_handles_legacy_indexed_data"><span class="mw-headline" id="How_the_cluster_handles_legacy_indexed_data"> How the cluster handles legacy indexed data</span></a></i></b></font></h4>
<p>When you add an existing indexer to the cluster, the cluster does not replicate any buckets that are already on the indexer. 
</p><p>Buckets already on the indexer prior to its being added to the cluster are called "standalone" buckets. Searches will still occur across those buckets and will be combined with search results from the cluster's replicated buckets.
</p>
<h4><font size="3"><b><i> <a name="migratenon-clusteredindexerstoaclusteredenvironment_is_there_any_way_to_migrate_my_legacy_data.3f"><span class="mw-headline" id="Is_there_any_way_to_migrate_my_legacy_data.3F"> Is there any way to migrate my legacy data? </span></a></i></b></font></h4>
<p>Because of the high processing cost of converting standalone buckets to replicated buckets (due to the need to make multiple searchable and non-searchable copies of those buckets to fulfill the cluster's replication and search factors), it is generally a bad idea to attempt to do so, particularly in the case of indexers with large numbers of standalone buckets. There is no supported procedure for this conversion. If, however, you have a strong need for such a conversion, contact Splunk Professional Services to discuss the trade-offs and requirements for this operation.
</p>
<h3> <a name="migratenon-clusteredindexerstoaclusteredenvironment_migrate_apps_to_a_cluster"><span class="mw-headline" id="Migrate_apps_to_a_cluster">Migrate apps to a cluster </span></a></h3>
<p>In your current, non-clustered environment, you might be using <b>deployment server</b> to distribute apps to your set of indexers. You will no longer be able to do so once you convert the indexers to cluster peer nodes. See <a href="#keydifferences" class="external text">"Key differences between clustered and non-clustered deployments of indexers"</a> for details. 
</p><p>To distribute apps to peer nodes, you must instead use the <b>configuration bundle</b> method described in <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>. You place the apps in a special location on the master node, and the master pushes the apps to each indexer when you first enable it as a peer. If you need to add or change apps later on, you make the changes and additions on the master and then tell the master to push the updated configuration bundle to the entire set of peers.
</p><p>For information on how the configuration bundle method compares with the deployment server, see <a href="#clustersinscaledoutdeployments_cluster_peer_management_compared_to_deployment_server" class="external text">"Cluster peer management compared to deployment server"</a>. 
</p><p><b>Important:</b> You <i>must</i> use the configuration bundle method to distribute apps to peer nodes;  the deployment server is unsupported for that purpose.
</p>
<h4><font size="3"><b><i> <a name="migratenon-clusteredindexerstoaclusteredenvironment_how_to_migrate_apps"><span class="mw-headline" id="How_to_migrate_apps">How to migrate apps</span></a></i></b></font></h4>
<p>It's recommended that you migrate apps at the time that you enable the cluster. The procedure described below describes how to do that. 
</p><p><b>Important:</b> You must be familiar with the preceding topics in the current chapter before attempting this procedure. Start with <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a> and read onwards until you return to this topic.
</p><p>This procedure assumes that you are starting with a distributed search environment with a set of indexers configured as deployment clients of a deployment server. The deployment server is being used to push apps to the indexers. It is these indexers that you will be converting to cluster peer nodes. Once they've become peer nodes, you can no longer use the deployment server to push apps to them; instead, you must use the configuration bundle method.
</p><p>To migrate your apps at the time of cluster enablement, follow these steps:
</p><p><b>1.</b> Enable the master, as described in <a href="#enablethemasternode" class="external text">"Enable the master node"</a>. 
</p><p><b>2.</b> On the deployment server, locate the set of apps that it pushes to the indexers that you're planning to migrate. These should be under the deployment server's <code><font size="2">$SPLUNK_HOME/etc/deployment-apps</font></code> directory. 
</p><p><b>3.</b> Copy these apps from the deployment server to the cluster master's <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> directory.  See <a href="#updatepeerconfigurations_structure_of_the_configuration_bundle" class="external text">"Structure of the configuration bundle"</a> for information on the <code><font size="2">master-apps</font></code> directory.
</p><p><b>4.</b> Examine each app in <code><font size="2">master-apps</font></code>  for any <code><font size="2">indexes.conf</font></code> file(s). In those files, locate any stanzas that define new indexes.  In each of those stanzas, add this attribute/value pair:
</p>
<code><font size="2"><br>repFactor=auto<br></font></code>
<p>This enables replication for the index.  See <a href="#configurethepeerindexes_the_indexes.conf_repfactor_attribute" class="external text">"The indexes.conf repFactor attribute"</a> for more information.
</p><p><b>Note:</b> If you are the creator and maintainer of the app, you can also make this change in <code><font size="2">$SPLUNK_HOME/etc/master-apps/&lt;appname&gt;/default/indexes.conf</font></code>.
</p><p><b>5.</b> Convert each indexer to a cluster peer, one at a time:
</p>
<dl><dd><b>a.</b> Reconfigure the indexer so that it's no longer a deployment client.
</dd></dl><dl><dd><b>b.</b> Delete the apps in the indexer's <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> directory.
</dd></dl><dl><dd><b>c.</b> Enable the indexer as a cluster peer, as described in <a href="#enablethepeernodes" class="external text">"Enable the peer nodes"</a>.
</dd></dl><dl><dd><b>d.</b> Restart the peer to complete the enablement process.
</dd></dl><dl><dd><b>e.</b> Verify that the master has pushed the expected set of apps to the peer's <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code> directory.
</dd></dl><p><b>6.</b> Once all peers have been enabled, go to the master dashboard and verify that the expected set of indexes is being replicated.  This dashboard is described in <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p><p>For more information on configuring cluster peers, see these topics:
</p>
<ul><li> <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a> 
</li><li> <a href="#configurethepeers" class="external text">"Peer node configuration overview"</a>
</li><li> <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>
</li></ul><a name="upgradeacluster"></a><h2> <a name="upgradeacluster_upgrade_an_indexer_cluster"><span class="mw-headline" id="Upgrade_an_indexer_cluster">Upgrade an indexer cluster</span></a></h2>
<p><b>Important:</b> The instructions in this topic assume you are upgrading all nodes to the new version at the same time. This is not necessary in all cases. See <a href="#systemrequirements_splunk_enterprise_version_compatibility" class="external text">"Splunk Enterprise version compatibility"</a> for information on mixed version clusters.
</p><p>The upgrade process differs considerably depending on the nature of the upgrade.  This topic covers these scenarios:
</p>
<ul><li> Upgrading from 6.x 
</li><li> Upgrading from 5.x 
</li><li> Upgrading to a new maintenance release (for example, from 6.1.1 to 6.1.2)
</li><li> Upgrading from 5.0.1 or earlier
</li></ul><h3> <a name="upgradeacluster_migrating_from_single-site_to_multisite.3f"><span class="mw-headline" id="Migrating_from_single-site_to_multisite.3F">Migrating from single-site to multisite?</span></a></h3>
<p>If you want to convert a single-site indexer cluster to multisite, perform the 6.2 upgrade first and then read <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>. 
</p>
<h3> <a name="upgradeacluster_upgrade_from_6.x_to_6.2"><span class="mw-headline" id="Upgrade_from_6.x_to_6.2"> Upgrade from 6.x to 6.2 </span></a></h3>
<p>When you upgrade from a 6.0 or 6.1 indexer cluster to a 6.2 cluster, you must take all cluster nodes offline. You cannot perform a rolling, online upgrade.
</p><p>Perform the following steps:
</p><p><b>1.</b> Stop the master.
</p><p><b>2.</b> Stop all the peers and search heads. 
</p><p>When bringing down the peers, use the <code><font size="2">splunk stop</font></code> command, not <code><font size="2">splunk offline</font></code>.
</p><p><b>3.</b> Upgrade the master node, following the normal procedure for any Splunk Enterprise upgrade, as described in "How to upgrade Splunk Enterprise" in the <i>Installation Manual</i>.  <b>Do not upgrade the peers yet.</b>
</p><p><b>4.</b> Start the master, accepting all prompts, if it is not already running.
</p><p><b>5.</b> Run <code><font size="2">splunk enable maintenance-mode</font></code> on the master. To confirm that the master has entered maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. This step prevents unnecessary bucket fix-ups.  See <a href="#usemaintenancemode" class="external text">"Use maintenance mode"</a>.
</p><p><b>6.</b> Upgrade the peer nodes and search heads, following the normal procedure for any Splunk Enterprise upgrade, as described in "How to upgrade Splunk Enterprise" in the <i>Installation Manual</i>. 
</p><p><b>7.</b> Start the peer nodes and search heads, if they are not already running.
</p><p><b>8.</b> Run <code><font size="2">splunk disable maintenance-mode</font></code> on the master. To confirm that the master has left maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. 
</p><p>You can view the master dashboard to verify that all cluster nodes are up and running.
</p>
<h3> <a name="upgradeacluster_upgrade_from_5.x_to_6.x"><span class="mw-headline" id="Upgrade_from_5.x_to_6.x">Upgrade from 5.x to 6.x</span></a></h3>
<p>When you upgrade from a 5.x indexer cluster to a 6.x cluster, you must take all cluster nodes offline. You cannot perform a rolling, online upgrade.
</p><p>Perform the following steps:
</p><p><b>1.</b> On the master, run the <code><font size="2">safe_restart_cluster_master</font></code> script with the <code><font size="2">--get_list</font></code> option:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk cmd python safe_restart_cluster_master.py &lt;master_uri&gt; --auth &lt;username&gt;:&lt;password&gt; --get_list <br></font></code>
</div>
<p><b>Note:</b> For the <code><font size="2">master_uri</font></code> parameter, use the URI and port number of the master node. For example: <code><font size="2">https://10.152.31.202:8089</font></code>
</p><p>This command puts a list of all cluster bucket copies and their states into the file <code><font size="2">$SPLUNK_HOME/var/run/splunk/cluster/buckets.xml</font></code>.  This list gets fed back to the master after the master upgrade.  
</p><p><b>Important:</b> To obtain a copy of this script, copy and paste it from here: <a href="#upgradeacluster_the_safe_restart_cluster_master_script" class="external text">"The safe_restart_cluster_master script"</a>.
</p><p>For information on why this step is needed, see <a href="#upgradeacluster_why_the_safe_restart_cluster_master_script_is_necessary" class="external text">"Why the safe_restart_cluster_master script is necessary"</a>. 
</p><p><b>2.</b> Stop the master.
</p><p><b>3.</b> Stop all the peers and search heads. 
</p><p>When bringing down the peers, use the <code><font size="2">splunk stop</font></code> command, not <code><font size="2">splunk offline</font></code>.
</p><p><b>4.</b> Upgrade the master node, following the normal procedure for any Splunk Enterprise upgrade, as described in "How to upgrade Splunk Enterprise" in the <i>Installation Manual</i>.  <b>Do not upgrade the peers yet.</b>
</p><p><b>5.</b> Start the master, accepting all prompts, if it is not already running.
</p><p><b>6.</b> Run the <code><font size="2">splunk apply cluster-bundle</font></code> command, using the syntax described in <a href="#updatepeerconfigurations" class="external text">"Update cluster peer configurations and apps"</a>.  (This step is necessary to avoid extra peer restarts, due to a 6.0 change in how the configuration bundle checksum is calculated.)
</p><p><b>7.</b> Run <code><font size="2">splunk enable maintenance-mode</font></code> on the master. To confirm that the master has entered maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. This step prevents unnecessary bucket fix-ups. See <a href="#usemaintenancemode" class="external text">"Use maintenance mode"</a>.
</p><p><b>8.</b> Upgrade the peer nodes and search heads, following the normal procedure for any Splunk Enterprise upgrade, as described in "How to upgrade Splunk Enterprise" in the <i>Installation Manual</i>. 
</p><p><b>9.</b> Start the peer nodes and search heads, if they are not already running.
</p><p><b>10.</b> On the master, run the <code><font size="2">safe_restart_cluster_master</font></code> script again, this time with the <code><font size="2">freeze_from</font></code> option, specifying the location of the bucket list created in step 1:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk cmd python safe_restart_cluster_master.py &lt;master_uri&gt; --auth &lt;username&gt;:&lt;password&gt; --freeze_from &lt;path_to_buckets_xml&gt;<br></font></code>
</div>
<p>For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk cmd python safe_restart_cluster_master.py &lt;master_uri&gt; --auth admin:changeme --freeze_from $SPLUNK_HOME/var/run/splunk/cluster/buckets.xml <br></font></code>
</div>
<p>This feeds the master the list of frozen buckets obtained in step 1.
</p><p><b>11.</b> Run <code><font size="2">splunk disable maintenance-mode</font></code> on the master. To confirm that the master has left maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. 
</p><p>You can view the master dashboard to verify that all cluster nodes are up and running.
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_why_the_safe_restart_cluster_master_script_is_necessary"><span class="mw-headline" id="Why_the_safe_restart_cluster_master_script_is_necessary"> Why the safe_restart_cluster_master script is necessary</span></a></i></b></font></h4>
<p>The <code><font size="2">safe_restart_cluster_master</font></code> script takes care of a problem in the way the 5.x master node handles frozen bucket copies.  This problem is fixed starting with release 6.0; however, it is still an issue during master upgrades from 5.x.  This section provides detail on the issue.
</p><p>When a peer freezes a copy of a bucket, the master stops doing fix-ups on that bucket. It operates under the assumption that the other peers will eventually freeze their copies of that bucket as well.
</p><p>This works well as long as the master continues to run. However, because (in 5.x) the knowledge of frozen buckets is not persisted on either the master or the peers, if you subsequently restart the master, the master will treat frozen copies (in the case where unfrozen copies of that bucket still exist on other peers) as missing copies and will perform its usual fix-up activities to return the cluster to a complete state. If  the cluster has a lot of partially frozen buckets, this process can be lengthy. Until the process is complete, the master will not be able to commit the next generation.
</p><p>To prevent this situation from occurring when you restart the master after upgrading to 6.0, you must run the <code><font size="2">safe_restart_cluster_master</font></code> script on the master.  As described in the upgrade procedure, when you initially run this script on the 5.x master with the <code><font size="2">--get_list</font></code> option, it creates a list of all cluster bucket copies and their states, including whether they are frozen. When you then rerun it after upgrading the master to 6.x, using the <code><font size="2">freeze_from</font></code> option, it feeds the list to the upgraded master so that it doesn't attempt fix-up of the frozen buckets.
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_the_safe_restart_cluster_master_script"><span class="mw-headline" id="The_safe_restart_cluster_master_script"> The safe_restart_cluster_master script </span></a></i></b></font></h4>
<p>To perform steps 1 and 9 of the upgrade procedure, you must run the <code><font size="2">safe_restart_cluster_master</font></code> script.  <b>This script does not currently ship with the product.</b> To obtain the script,  copy the listing directly below and save it to <code><font size="2">$SPLUNK_HOME/bin/safe_restart_cluster_master.py</font></code>.
</p><p><b>Important:</b> You must also copy and save the <code><font size="2">parse_xml_v3</font></code> script, as described in the next section, <a href="#upgradeacluster_the_parse_xml_v3_script" class="external text">"The parse_xml_v3 script"</a>.
</p><p>Here are the contents of the script:
</p>
<div class="samplecode">
<code><font size="2"><br>import httplib2<br>from urllib import urlencode<br>import splunk, splunk.rest, splunk.rest.format<br>from parse_xml_v3 import *<br>import json<br>import re<br>import time<br>import os<br>import subprocess <br><br>#before restarting the master, store the buckets list in /var/run/splunk/cluster<br>BUCKET_LIST_PATH = os.path.join(os.environ['SPLUNK_HOME'] , 'var' , 'run' , 'splunk' , 'cluster' , 'buckets.xml')<br><br>def get_buckets_list(master_uri, auth):<br>&nbsp;&nbsp;&nbsp;&nbsp;f = open(BUCKET_LIST_PATH,'w')<br>&nbsp;&nbsp;&nbsp;&nbsp;atom_buckets = get_xml_feed(master_uri +'/services/cluster/master/buckets?count=-1',auth,'GET')<br>&nbsp;&nbsp;&nbsp;&nbsp;f.write(atom_buckets)<br>&nbsp;&nbsp;&nbsp;&nbsp;f.close()<br><br>def change_quiet_period(master_uri, auth):<br>&nbsp;&nbsp;&nbsp;&nbsp;args={'quite_period':'600'}<br>&nbsp;&nbsp;&nbsp;&nbsp;return get_response_feed(master_uri+'/services/cluster/config/system?quiet_period=600',auth, 'POST')<br><br>def num_peers_up(master_uri, auth):<br>&nbsp;&nbsp;&nbsp;&nbsp;count = 0 <br>&nbsp;&nbsp;&nbsp;&nbsp;f= open('peers.xml','w')<br>&nbsp;&nbsp;&nbsp;&nbsp;atom_peers = get_xml_feed(master_uri+'/services/cluster/master/peers?count=-1',auth,'GET')<br>&nbsp;&nbsp;&nbsp;&nbsp;f.write(atom_peers)<br>&nbsp;&nbsp;&nbsp;&nbsp;regex= re.compile('"status"&gt;Up')<br>&nbsp;&nbsp;&nbsp;&nbsp;f.close()<br>&nbsp;&nbsp;&nbsp;&nbsp;file = open('peers.xml','r')<br>&nbsp;&nbsp;&nbsp;&nbsp;for line in file:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;match = regex.findall(line)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for line in match:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count = count + 1 <br>&nbsp;&nbsp;&nbsp;&nbsp;file.close()<br>&nbsp;&nbsp;&nbsp;&nbsp;os.remove('peers.xml')<br>&nbsp;&nbsp;&nbsp;&nbsp;return count<br><br>def wait_for_peers(master_uri,auth,original_number):<br>&nbsp;&nbsp;&nbsp;&nbsp;while(num_peers_up(master_uri,auth)&nbsp;!= original_number):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_peers_not_up = original_number - num_peers_up(master_uri,auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Still waiting for " +str(num_peers_not_up) +" peers to join ..."<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(5)<br>&nbsp;&nbsp;&nbsp;&nbsp;print "All peers have joined"<br><br>def get_response_feed(url, auth, method='GET', body=None):<br>&nbsp;&nbsp;&nbsp;&nbsp;(user, password) = auth.split(':')<br>&nbsp;&nbsp;&nbsp;&nbsp;h = httplib2.Http(disable_ssl_certificate_validation=True)<br>&nbsp;&nbsp;&nbsp;&nbsp;h.add_credentials(user, password)<br>&nbsp;&nbsp;&nbsp;&nbsp;if body is None:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;body = {}<br>&nbsp;&nbsp;&nbsp;&nbsp;response, content = h.request(url, method, urlencode(body))<br>&nbsp;&nbsp;&nbsp;&nbsp;if response.status == 401:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise Exception("Authorization Failed", url, response)<br>&nbsp;&nbsp;&nbsp;&nbsp;elif response.status&nbsp;!= 200:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise Exception(url, response)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;return splunk.rest.format.parseFeedDocument(content)<br><br>def get_xml_feed(url, auth, method='GET', body=None):<br>&nbsp;&nbsp;&nbsp;&nbsp;(user, password) = auth.split(':')<br>&nbsp;&nbsp;&nbsp;&nbsp;h = httplib2.Http(disable_ssl_certificate_validation=True)<br>&nbsp;&nbsp;&nbsp;&nbsp;h.add_credentials(user, password)<br>&nbsp;&nbsp;&nbsp;&nbsp;if body is None:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;body = {}<br>&nbsp;&nbsp;&nbsp;&nbsp;response, content = h.request(url, method, urlencode(body))<br>&nbsp;&nbsp;&nbsp;&nbsp;if response.status == 401:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise Exception("Authorization Failed", url, response)<br>&nbsp;&nbsp;&nbsp;&nbsp;elif response.status&nbsp;!= 200:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise Exception(url, response)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;return content<br><br>def validate_rest(master_uri, auth):<br>&nbsp;&nbsp;&nbsp;&nbsp;return get_response_feed(master_uri + '/services/cluster/master/info', auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>def freeze_bucket(master_uri, auth, bid):<br>&nbsp;&nbsp;&nbsp;&nbsp;return get_response_feed(master_uri + '/services/cluster/master/buckets/' + bid + '/freeze', auth, 'POST')<br><br>def freeze_from_file(master_uri,auth,path=BUCKET_LIST_PATH): &nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;file = open(path) #read the buckets.xml from either path supplied or BUCKET_LIST_PATH<br><br>&nbsp;&nbsp;&nbsp;&nbsp;handler = BucketHandler()<br>&nbsp;&nbsp;&nbsp;&nbsp;parse(file, handler)<br>&nbsp;&nbsp;&nbsp;&nbsp;buckets = handler.getBuckets()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;fcount = 0<br>&nbsp;&nbsp;&nbsp;&nbsp;fdone = 0<br>&nbsp;&nbsp;&nbsp;&nbsp;for bid, bucket in buckets.iteritems():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if bucket.frozen:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fcount += 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;freeze_bucket(master_uri,auth, bid)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fdone += 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print e<br><br>&nbsp;&nbsp;&nbsp;&nbsp;print "Total bucket count:: ", len(buckets), "; number frozen: ", fcount, "; number re-frozen: ", fdone<br><br>def restart_master(master_uri,auth):<br>&nbsp;&nbsp;&nbsp;&nbsp;change_quiet_period(master_uri,auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;original_num_peers = num_peers_up(master_uri,auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;print "\n" + "Issuing restart at the master" +"\n"<br>&nbsp;&nbsp;&nbsp;&nbsp;subprocess.call([os.path.join(os.environ["SPLUNK_HOME"],"bin","splunk"), "restart"])<br><br>&nbsp;&nbsp;&nbsp;&nbsp;print "\n"+ "Master was restarted" + "\n" <br><br>&nbsp;&nbsp;&nbsp;&nbsp;print "\n" + "Waiting for all " +str(original_num_peers) + " peers to come back up" +"\n"<br><br>&nbsp;&nbsp;&nbsp;&nbsp;wait_for_peers(master_uri,auth,original_num_peers)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;print "\n" + "Making sure we have the correct number of frozen buckets" + "\n"<br><br>if __name__ == &nbsp;'__main__':<br>&nbsp;&nbsp;&nbsp;&nbsp;usage = "usage:&nbsp;%prog [options] &lt;master_uri&gt; --auth admin:changeme"<br>&nbsp;&nbsp;&nbsp;&nbsp;parser = OptionParser(usage)<br>&nbsp;&nbsp;&nbsp;&nbsp;parser.add_option("-a","--auth", dest="auth", metavar="user:password", default=':',<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;help="Splunk authentication parameters for the master instance");<br>&nbsp;&nbsp;&nbsp;&nbsp;parser.add_option("-g","--get_list", action="store_true",help="get a list of frozen buckets and strore them in buckets.xml");<br>&nbsp;&nbsp;&nbsp;&nbsp;parser.add_option("-f", "--freeze_from",dest="freeze_from", <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;help="path to the file that contains the list of buckets to be frozen. ie path to the buckets.xml generated by the get_list option above");<br><br>&nbsp;&nbsp;&nbsp;&nbsp;(options, args) = parser.parse_args()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;if len(args) == &nbsp;0:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parser.error("master_uri is required")<br>&nbsp;&nbsp;&nbsp;&nbsp;elif len(args) &gt; 1:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parser.error("incorrect number of arguments")<br><br>&nbsp;&nbsp;&nbsp;&nbsp;master_uri = args[0]<br>&nbsp;&nbsp;&nbsp;&nbsp;try:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validate_rest(master_uri, options.auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;except Exception as e:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Failed to access the master info endpoint make sure you've supplied the authentication credentials"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise<br>&nbsp;&nbsp;&nbsp;&nbsp;# Let's get a list of frozen buckets, stored in<br>&nbsp;&nbsp;&nbsp;&nbsp;if(options.get_list):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Only getting the list of buckets and storing it at " + BUCKET_LIST_PATH<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get_buckets_list(master_uri,options.auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;elif(options.freeze_from):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Reading the list of buckets from" + options.freeze_from + "and refreezing them"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;freeze_from_file(master_uri,options.auth,options.freeze_from)<br>&nbsp;&nbsp;&nbsp;&nbsp;else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Restarting the master safely to preserve knowledge of frozen buckets"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get_buckets_list(master_uri,options.auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;restart_master(master_uri,options.auth)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;freeze_from_file(master_uri,options.auth,BUCKET_LIST_PATH)<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="upgradeacluster_the_parse_xml_v3_script"><span class="mw-headline" id="The_parse_xml_v3_script">The parse_xml_v3 script</span></a></i></b></font></h4>
<p>The <code><font size="2">parse_xml_v3</font></code> script contains certain helper functions needed by the <code><font size="2">safe_restart_cluster_master</font></code> script. <b>This script does not currently ship with the product.</b> To obtain the script,  copy the listing directly below and save it to <code><font size="2">$SPLUNK_HOME/bin/parse_xml_v3.py</font></code>.
</p><p>Here are the contents of the script:
</p>
<div class="samplecode">
<code><font size="2"><br>import sys<br>from xml.sax import ContentHandler, parse<br>from optparse import OptionParser<br><br>class PeerBucketFlags:<br>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.primary = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.searchable = False<br><br>class Bucket:<br>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.peer_flags = {} &nbsp;# key is peer guid<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.frozen = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>class BucketHandler(ContentHandler):<br>&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ContentHandler.__init__(self)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.buckets = {}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_entry = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_peers = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_title = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_frozen = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.peer_nesting = 0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_flags = {}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_guid = None<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_frozen_flag = ''<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_field = None<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_field_value = ''<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_bucket = ''<br><br>&nbsp;&nbsp;&nbsp;&nbsp;def getBuckets(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.buckets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;def startDocument(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br><br>&nbsp;&nbsp;&nbsp;&nbsp;def endDocument(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;def startElement(self, name, attrs):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if name == 'entry':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_entry = True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.in_entry and name == 'title':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_title = True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.in_entry and name == 's:key' and attrs.get('name') == 'frozen':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_frozen = True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif name == 's:key' and attrs.get('name') == 'peers':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_peers = True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.in_peers and name == 's:key':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.peer_nesting += 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.peer_nesting == 1:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_flags = PeerBucketFlags()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_guid = attrs.get('name').encode('ascii')<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.peer_nesting == 2:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_field = attrs.get('name').encode('ascii')<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_field_value = ''<br><br>&nbsp;&nbsp;&nbsp;&nbsp;def endElement(self,name):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if name == 'entry':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_entry = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_bucket=''<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.save_title:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(idx, local_id, origin_guid) = self.current_bucket.split('~')<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except ValueError as e:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print "Invalid? ", self._locator.getLineNumber()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print self.current_bucket<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print e<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.buckets[self.current_bucket] = Bucket()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_title = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.save_frozen:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.current_frozen_flag in [1, '1', 'True', 'true']:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.buckets[self.current_bucket].frozen = True<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_frozen_flag = ''<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.save_frozen = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.peer_nesting == 2 and name == 's:key':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.current_peer_field == 'bucket_flags':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_flags.primary = (self.current_peer_field_value == '0xffffffffffffffff')<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.current_peer_field == 'search_state':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_flags.searchable = self.current_peer_field_value == 'Searchable'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Nesting level goes down in either case.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.peer_nesting -= 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.peer_nesting == 1 and name == 's:key':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.buckets[self.current_bucket].peer_flags[self.current_guid] = self.current_peer_flags<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.peer_nesting -= 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.in_peers and self.peer_nesting == 0 and name == 's:key':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.in_peers = False<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;def characters(self, content):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.save_title:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_bucket += content.encode('ascii').strip()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif self.save_frozen:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_frozen_flag += content.encode('ascii').strip()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if self.peer_nesting &gt; 0:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = content.encode('ascii').strip()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if s:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.current_peer_field_value += s<br></font></code>
</div>
<h3> <a name="upgradeacluster_upgrade_to_a_new_maintenance_release"><span class="mw-headline" id="Upgrade_to_a_new_maintenance_release">Upgrade to a new maintenance release</span></a></h3>
<p>To upgrade a cluster to a new maintenance release (for example, from 6.1 to 6.1.1), you do not need to bring down the entire cluster at once. Instead, you can perform a rolling, online upgrade, in which you upgrade the nodes one at a time. 
</p><p><b>Important:</b> Even with a rolling upgrade, you must upgrade all nodes expeditiously. Proper functioning of the cluster depends on all nodes running the same version of Splunk Enterprise, as stated in <a href="#systemrequirements" class="external text">"System requirements and other deployment considerations for indexer clusters"</a>. Therefore, do not begin the upgrade process until you are ready to upgrade all cluster nodes. If you upgrade the master but not the peers, the cluster might start to generate various errors and warnings. This is generally okay for a short duration, but you should complete the upgrade of all nodes as quickly as possible. 
</p><p>To upgrade a cluster node, follow the normal procedure for any Splunk Enterprise upgrade, with the few exceptions described below. For general information on upgrading Splunk Enterprise instances, see "How to upgrade Splunk Enterprise". 
</p><p>Perform the following steps: 
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_1._upgrade_the_master_node"><span class="mw-headline" id="1._Upgrade_the_master_node">1. Upgrade the master node</span></a></i></b></font></h4>
<p>Upgrade the master node first. 
</p><p>For information on what happens when the master goes down, as well as what happens when it comes back up, read <a href="#whathappenswhenamasternodegoesdown" class="external text">"What happens when the master node goes down"</a>.
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_2._put_the_master_into_maintenance_mode"><span class="mw-headline" id="2._Put_the_master_into_maintenance_mode">2. Put the master into maintenance mode</span></a></i></b></font></h4>
<p>Run <code><font size="2">splunk enable maintenance-mode</font></code> on the master. To confirm that the master has entered maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. This step prevents unnecessary bucket fix-ups. See <a href="#usemaintenancemode" class="external text">"Use maintenance mode"</a>.
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_3._upgrade_the_peer_nodes"><span class="mw-headline" id="3._Upgrade_the_peer_nodes">3. Upgrade the peer nodes</span></a></i></b></font></h4>
<p>When upgrading peer nodes, note the following:
</p>
<ul><li> Peer upgrades can disrupt ongoing searches.
</li></ul><ul><li> To minimize downtime and limit any disruption to indexing and searching, upgrade the peer nodes one at a time. 
</li></ul><ul><li> To bring a peer down prior to upgrade, use the <code><font size="2">offline</font></code> command, as described in <a href="#takeapeeroffline" class="external text">"Take a peer offline"</a>. To ensure sufficient time for the upgrade, you might need to extend the restart period by adjusting the <code><font size="2">restart_timeout</font></code> attribute, as explained in that topic.
</li></ul><ul><li> During the interim between when you upgrade the master and when you finish upgrading the peers, the cluster might generate various warnings and errors.
</li></ul><ul><li> For multisite clusters, the site order of peer upgrades doesn't matter. Maintenance mode has no notion of sites.
</li></ul><h4><font size="3"><b><i> <a name="upgradeacluster_4._upgrade_the_search_heads"><span class="mw-headline" id="4._Upgrade_the_search_heads">4. Upgrade the search heads</span></a></i></b></font></h4>
<p>The only impact to the cluster when you upgrade the search heads is the expected disruption to searches during that time.  
</p>
<h4><font size="3"><b><i> <a name="upgradeacluster_5._take_the_master_out_of_maintenance_mode"><span class="mw-headline" id="5._Take_the_master_out_of_maintenance_mode">5. Take the master out of maintenance mode</span></a></i></b></font></h4>
<p>Run <code><font size="2">splunk disable maintenance-mode</font></code> on the master. To confirm that the master has left maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>.
</p>
<h3> <a name="upgradeacluster_upgrade_from_5.0.1_or_earlier"><span class="mw-headline" id="Upgrade_from_5.0.1_or_earlier">Upgrade from 5.0.1 or earlier</span></a></h3>
<p>During an upgrade from 5.0.1 or earlier, the <code><font size="2">/cluster</font></code> directory under <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> (on the master) and <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code> (on the peers) gets renamed to <code><font size="2">/_cluster</font></code>. This happens automatically. For more details on this directory, see <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a>.
</p><p>When the master restarts after an upgrade from 5.0.1 or earlier, it performs a rolling restart on the set of peer nodes, to push the latest version of the configuration bundle (with the renamed <code><font size="2">/_cluster</font></code> directory).
</p>
<h1>Configure the indexer cluster</h1><a name="clusterconfigurationoverview"></a><h2> <a name="clusterconfigurationoverview_indexer_cluster_configuration_overview"><span class="mw-headline" id="Indexer_cluster_configuration_overview"> Indexer cluster configuration overview</span></a></h2>
<p>To configure the indexer cluster, you configure the individual nodes. You perform two types of configuration:
</p>
<ul><li> Configuration of the behavior of the cluster itself.
</li><li> Configuration of the cluster's indexing and search behavior.
</li></ul><p>The current chapter provides an overview of the ways to configure cluster behavior specifically.
</p>
<h3> <a name="clusterconfigurationoverview_configuration_of_each_node_type"><span class="mw-headline" id="Configuration_of_each_node_type">Configuration of each node type</span></a></h3>
<p>The settings for each node type handle different aspects of the cluster:
</p>
<ul><li> <b>Master node.</b> Configuration of overall cluster behavior.
</li><li> <b>Peer node.</b> Configuration of individual peer node and cluster indexing behavior.
</li><li> <b>Search head.</b> Configuration of individual search head and search behavior in an indexer cluster.
</li></ul><p>See the chapters on specific node types for information on configuring each node type. The chapter <a href="#configurethepeers" class="external text">"Configure  the peers,"</a> for example, includes some topics on configuring the peer cluster node settings and other topics that describe how to configure the indexes that a peer uses.
</p>
<h3> <a name="clusterconfigurationoverview_methods_for_configuring_cluster_behavior"><span class="mw-headline" id="Methods_for_configuring_cluster_behavior">Methods for configuring cluster behavior</span></a></h3>
<p>Initial configuration of each node occurs during deployment. If you need to change the configuration of a cluster node post-deployment, you have these choices:
</p>
<ul><li> You can edit the configuration from the node's dashboard in Splunk Web, as described in <a href="#configurewithdashboard" class="external text">"Configure  the indexer cluster with the dashboards"</a>.
</li></ul><ul><li> You can directly edit the <code><font size="2">[clustering]</font></code> stanza in the node's <code><font size="2">server.conf</font></code> file. See <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a> for details. To configure some advanced settings, you must edit this file.
</li></ul><ul><li> You can use the CLI. See <a href="#usethecli" class="external text">"Configure and manage the indexer cluster with the CLI"</a> for details.
</li></ul><a name="configurewithdashboard"></a><h2> <a name="configurewithdashboard_configure_the_indexer_cluster_with_the_dashboards"><span class="mw-headline" id="Configure_the_indexer_cluster_with_the_dashboards"> Configure the indexer cluster with the dashboards</span></a></h2>
<p>To configure an indexer cluster node through its dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web on the node.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select the <b>Edit</b> button on the upper right side of the dashboard.
</p><p>For information on the settings for each node type, see:
</p>
<ul><li> <a href="#configuremasterwithdashboard" class="external text">"Configure the master with the dashboard"</a>
</li><li> <a href="#configurepeerswithdashboard" class="external text">"Configure peer nodes with the dashboard"</a>
</li><li> <a href="#configuresearchheadwithdashboard" class="external text">"Configure the search head with the dashboard"</a>
</li></ul><a name="enableclustersindetail"></a><h2> <a name="enableclustersindetail_configure_the_indexer_cluster_with_server.conf"><span class="mw-headline" id="Configure_the_indexer_cluster_with_server.conf">Configure the indexer cluster with server.conf </span></a></h2>
<p>Before reading this topic, see "About configuration files" and the topics that follow it in the <i>Admin Manual.</i> Those topics explain how Splunk Enterprise uses configuration files.
</p><p>Indexer cluster settings reside in the server.conf file, located in <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>.  When you deploy a cluster node through Splunk Web or the CLI, the node saves the settings to that file. You can also edit <code><font size="2">server.conf</font></code> file directly, either to deploy initially or to change settings later. 
</p><p>The main <code><font size="2">server.conf</font></code> stanza that controls indexer clustering is <code><font size="2">[clustering]</font></code>.  Besides the basic attributes that correspond to settings in Splunk Web, <code><font size="2">server.conf</font></code> provides a number of advanced settings that control communication between cluster nodes. Unless advised by Splunk Support, do not change those settings.
</p><p>This topic discusses some issues that are common to all node types. For specific instructions for each node type, see: 
</p>
<ul><li> <a href="#configuremasterwithserverconf" class="external text">"Configure the master with server.conf"</a>.
</li><li> <a href="#configurepeerswithserverconf" class="external text">"Configure peer nodes with server.conf"</a>.
</li><li> <a href="#configuresearchheadwithserverconf" class="external text">"Configure the search head with server.conf"</a>.
</li></ul><p>For details on all the clustering attributes, including the advanced ones, read the server.conf specification.
</p><p>For multisite cluster configurations, also read <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h3> <a name="enableclustersindetail_example_configuration"><span class="mw-headline" id="Example_configuration">Example configuration</span></a></h3>
<p>Here is an example master node configuration:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = master<br>replication_factor = 4<br>search_factor = 3<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is a cluster master node.
</li><li> the cluster's replication factor is 4.
</li><li> the cluster's search factor is 3.
</li><li> the secret key is "whatever".
</li></ul><p>Peer node and search head configuration are similar.
</p>
<h3> <a name="enableclustersindetail_configure_the_secret_key"><span class="mw-headline" id="Configure_the_secret_key"> Configure the secret key </span></a></h3>
<p>You can optionally set the <code><font size="2">pass4SymmKey</font></code> attribute to configure a secret key that authenticates communication between the master, peers and, search heads. If you set it for one cluster node, you must also give it the same value for all other cluster nodes.  
</p><p>You must set the key inside the <code><font size="2">[clustering]</font></code> stanza for indexer clustering.  You can also set <code><font size="2">pass4SymmKey</font></code> in the <code><font size="2">[general]</font></code> stanza for licensing. 
</p><p><b>Important:</b> You should save a copy of the key in a safe place. Once an instance starts running, the secret key changes from clear text to encrypted form, and it is no longer recoverable from <code><font size="2">server.conf</font></code>. If you later want to add a new node, you will need to use the clear text version to set the key.
</p>
<h3> <a name="enableclustersindetail_restart_after_modifying_server.conf.3f"><span class="mw-headline" id="Restart_after_modifying_server.conf.3F"> Restart after modifying server.conf? </span></a></h3>
<p>After you configure an instance as a cluster node for the first time, you need to restart it for the change to take effect. 
</p><p>If you make a configuration change later on, you might not need to restart the instance, depending on the type of change. Avoid restarting peers when possible. Restarting the set of peers can result in prolonged amounts of bucket-fixing.
</p>
<h4><font size="3"><b><i> <a name="enableclustersindetail_initial_configuration"><span class="mw-headline" id="Initial_configuration">Initial configuration</span></a></i></b></font></h4>
<p>After initially configuring instances as cluster nodes, you need to restart all of them (master, peers, and search head) for the changes to take effect. You can do this by invoking the CLI <code><font size="2">restart</font></code> command on each node:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk restart<br></font></code>
</div>
<p>When the master starts up for the first time, it blocks indexing on the peers until you enable and restart the replication factor number of peers. Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p><p><b>Important:</b>  Although you can use the CLI <code><font size="2">restart</font></code> command when you initially enable an instance as a cluster peer node, do not use it for subsequent restarts. The <code><font size="2">restart</font></code> command is not compatible with index replication once replication has begun. For more information, including a discussion of safe restart methods, read <a href="#restartthecluster_restart_a_single_peer" class="external text">"Restart a single peer"</a>.
</p>
<h4><font size="3"><b><i> <a name="enableclustersindetail_subsequent_configuration_changes"><span class="mw-headline" id="Subsequent_configuration_changes">Subsequent configuration changes</span></a></i></b></font></h4>
<p>If you change any of the following attributes in the <code><font size="2">server.conf</font></code> file, you do not need to restart the node.
</p><p>On a peer node or search head:
</p>
<ul><li> <code><font size="2">master_uri</font></code>
</li><li> <code><font size="2">pass4SymmKey</font></code> 
</li></ul><p>On a master node:
</p>
<ul><li> <code><font size="2">quiet_period</font></code>
</li><li> <code><font size="2">heartbeat_timeout</font></code>
</li><li> <code><font size="2">restart_timeout</font></code>
</li><li> <code><font size="2">max_peer_build_load</font></code>
</li><li> <code><font size="2">max_peer_rep_load</font></code>
</li><li> <code><font size="2">pass4SymmKey</font></code> 
</li></ul><p>All other cluster-related configuration changes require a restart.
</p>
<a name="usethecli"></a><h2> <a name="usethecli_configure_and_manage_the_indexer_cluster_with_the_cli"><span class="mw-headline" id="Configure_and_manage_the_indexer_cluster_with_the_CLI"> Configure and manage the indexer cluster with the CLI</span></a></h2>
<p>You can use the CLI to perform a wide set of indexer cluster activities, including:
</p>
<ul><li> <a href="#usethecli_enable_cluster_nodes" class="external text">Enabling cluster nodes</a>
</li><li> <a href="#usethecli_edit_cluster_configurations" class="external text">Editing cluster configurations</a>
</li><li> <a href="#usethecli_view_cluster_information" class="external text">Viewing cluster information</a>
</li><li> <a href="#usethecli_manage_the_cluster" class="external text">Managing the cluster</a>
</li></ul><p>Some clustering commands are available only for a specific node type,&Acirc;&nbsp;such as the master node.
</p>
<h3> <a name="usethecli_enable_cluster_nodes"><span class="mw-headline" id="Enable_cluster_nodes"> Enable cluster nodes </span></a></h3>
<p>You can enable instances as cluster nodes with the <code><font size="2">splunk edit cluster-config</font></code> command. After enabling an instance, you must restart it.
</p><p>This topic discusses some issues that are common to all node types. For specific instructions for each node type, see: 
</p>
<ul><li> <a href="#configuremasterwithcli" class="external text">"Configure the master with the CLI"</a>
</li><li> <a href="#configurepeerswithcli" class="external text">"Configure peer nodes with the CLI"</a>
</li><li> <a href="#configuresearchheadwithcli" class="external text">"Configure the search head with the CLI"</a>
</li></ul><p>For details on the specific command-line options, read  <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a>.
</p><p>For multisite cluster configurations, also read <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>.
</p>
<h4><font size="3"><b><i> <a name="usethecli_example_configuration"><span class="mw-headline" id="Example_configuration">Example configuration</span></a></i></b></font></h4>
<p>To enable an instance as a master node, set <code><font size="2">mode</font></code> to "master" and configure other cluster options as needed:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode master -replication_factor 4 -search_factor 3 <br><br>splunk restart<br></font></code>
</div>
<p>Peer node and search head configuration are similar.
</p>
<h4><font size="3"><b><i> <a name="usethecli_specify_a_security_key"><span class="mw-headline" id="Specify_a_security_key"> Specify a security key </span></a></i></b></font></h4>
<p>You can optionally specify a security key for the cluster by appending the <code><font size="2">-secret</font></code> flag when you enable each cluster node. For example: 
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode slave -master_uri https://10.160.31.200:8089 -replication_port 9887 -secret your_key<br></font></code>
</div>
<p>The security key authenticates communication between the master and the peers and search heads. The key, if specified, must be the same across all cluster instances. If, for example, you specify it for the master, you must also specify it for all peers and search heads.
</p>
<h3> <a name="usethecli_edit_cluster_configurations"><span class="mw-headline" id="Edit_cluster_configurations"> Edit cluster configurations</span></a></h3>
<p>You can also use the CLI to edit the cluster node configuration after the node has been enabled. How you do this depends on the type of cluster node.  See the topics covering CLI configuration of the different node types.
</p>
<h3> <a name="usethecli_view_cluster_information"><span class="mw-headline" id="View_cluster_information"> View cluster information</span></a></h3>
<p>There are a number of <code><font size="2">splunk list</font></code> commands that return different types of cluster information. For example, to get detailed information on each peer in the cluster, run this command on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list cluster-peers<br></font></code>
</div>
<p>To get information on the cluster configuration, run this command from any node:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list cluster-config<br></font></code>
</div>
<p>See the CLI clustering help for the full set of <code><font size="2">splunk list</font></code> commands.
</p>
<h3> <a name="usethecli_manage_the_cluster"><span class="mw-headline" id="Manage_the_cluster"> Manage the cluster </span></a></h3>
<p>You can also use the CLI to perform a number of different actions on the cluster. Those actions are described in their own topics:
</p>
<ul><li> Use the <code><font size="2">splunk offline</font></code> command to <a href="#takeapeeroffline" class="external text">take a peer offline</a>.
</li><li> Use the <code><font size="2">splunk apply cluster-bundle</font></code> command to  <a href="#updatepeerconfigurations" class="external text">update common peer configurations</a>.
</li><li> Use the <code><font size="2">splunk rolling-restart cluster-peers</font></code> command to <a href="#restartthecluster" class="external text">restart all the cluster peers</a>.
</li><li> Use the <code><font size="2">splunk enable maintenance-mode</font></code> command to <a href="#usemaintenancemode" class="external text">enable maintenance mode</a>.
</li><li> Use the <code><font size="2">splunk remove excess-buckets</font></code> command to <a href="#removeextrabucketcopies" class="external text">remove excess bucket copies</a>.
</li><li> Configure <a href="#configuremulti-clustersearch" class="external text">multi-cluster search</a>.
</li></ul><h3> <a name="usethecli_get_help_on_the_cli_commands"><span class="mw-headline" id="Get_help_on_the_CLI_commands"> Get help on the CLI commands</span></a></h3>
<p>The CLI provides online help for its commands.  For general help on the full set of clustering commands, go to <code><font size="2">$SPLUNKHOME/bin</font></code> and type:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk help cluster<br></font></code>
</div>
<p>For help on specific commands, specify the command name. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk help list cluster-config<br></font></code>
</div>
<p>For general information on the CLI, read the "Administer Splunk Enterprise with the command line interface (CLI)" chapter in the <i>Admin Manual,</i> or type:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk help <br></font></code>
</div>

<h1>Configure the master</h1><a name="configurethemaster"></a><h2> <a name="configurethemaster_master_configuration_overview"><span class="mw-headline" id="Master_configuration_overview"> Master configuration overview</span></a></h2>
<p>You initially configure the master when you enable it, as described in <a href="#enablethemasternode" class="external text">"Enable the master node"</a>. This is usually all the configuration the master needs.
</p>
<h3> <a name="configurethemaster_change_the_configuration"><span class="mw-headline" id="Change_the_configuration">Change the configuration</span></a></h3>
<p>If you need to edit the configuration, you have these choices:
</p>
<ul><li> You can edit the configuration from the master node dashboard in Splunk Web. See <a href="#configuremasterwithdashboard" class="external text">"Configure the master with the dashboard"</a>.
</li></ul><ul><li> You can directly edit the <code><font size="2">[clustering]</font></code> stanza in the master's <code><font size="2">server.conf</font></code> file. To configure some advanced settings, you must edit this file. See <a href="#configuremasterwithserverconf" class="external text">"Configure the master with server.conf"</a>. 
</li></ul><ul><li> You can use the CLI. See <a href="#configuremasterwithcli" class="external text">"Configure the master with the CLI"</a>. 
</li></ul><p>After you change the master configuration, you must restart the master for the changes to take effect.
</p><p><b>Important:</b> The master has the sole function of managing the other cluster nodes. Do not use it to index external data or to search the cluster.
</p>
<h3> <a name="configurethemaster_changes_that_require_caution"><span class="mw-headline" id="Changes_that_require_caution">Changes that require caution</span></a></h3>
<p>Be careful when making changes to these settings:
</p>
<ul><li> <b>Replication factor and search factor.</b>  It is inadvisable to increase either of these settings after your indexer cluster contains significant amounts of data. This will kick off a great deal of bucket activity, affecting the cluster's performance adversely while bucket copies are being created or made searchable.
</li></ul><ul><li> <b>Heartbeat timeout.</b> Do not change the <code><font size="2">heartbeat_timeout</font></code> attribute from its default value of 60 (seconds) unless instructed to do so by Splunk Support. In particular, do not decrease it.  This can overload the peers.
</li></ul><h3> <a name="configurethemaster_configure_a_stand-by_master"><span class="mw-headline" id="Configure_a_stand-by_master"> Configure a stand-by master </span></a></h3>
<p>To prepare for master node failure, configure a stand-by master that can take over if the current master goes down. See <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.
</p>
<h3> <a name="configurethemaster_configure_a_multisite_master"><span class="mw-headline" id="Configure_a_multisite_master">Configure a multisite master</span></a></h3>
<p>A multisite master has a number of configuration differences and additions, compared to a basic, single-site cluster. See <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<a name="configuremasterwithdashboard"></a><h2> <a name="configuremasterwithdashboard_configure_the_master_with_the_dashboard"><span class="mw-headline" id="Configure_the_master_with_the_dashboard"> Configure the master with the dashboard</span></a></h2>
<p>You can edit the master configuration through its dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select the <b>Edit</b> button on the upper right side of the dashboard.
</p><p><b>Note:</b> The <b>Edit</b> button is disabled for multisite clusters.
</p><p>The <b>Edit</b> button presents several options:
</p>
<ul><li> <b>Node Type</b>. Change the instance's node type. <b>Caution:</b> It is extremely unlikely that you will want to change the node type for nodes in an active cluster. Consider the consequences carefully before doing so.
</li><li> <b>Master Node Configuration</b>. Change these master node settings:
<ul><li> <b>Replication Factor</b>. Change the cluster's replication factor. <b>Warning:</b> It is inadvisable to increase the replication factor after your cluster contains significant amounts of data. Doing so will kick off a great deal of bucket activity, which will have an adverse effect on the cluster's performance while bucket copies are being created.
</li><li> <b>Search Factor</b>. Change the cluster's search factor. <b>Warning:</b> It is inadvisable to increase the replication factor after your cluster contains significant amounts of data. Doing so will kick off a great deal of bucket activity, which will have an adverse effect on the cluster's performance while bucket copies are being made searchable.
</li><li> <b>Security Key</b>. Change the secret key. Only change the secret key if you are also changing it for all other nodes in the cluster. The key must be the same across all instances in a cluster.
</li></ul></li><li> <b>Distribute Configuration Bundle.</b> Distribute updated configurations and apps to the set of peer nodes. For details of this process, see the topic <a href="#updatepeerconfigurations" class="external text">"Update cluster peer configurations and apps"</a>.
</li><li> <b>Disable Indexer Clustering</b>. Remove this node from the cluster. <b>Warning:</b> If you remove the master node from the cluster, the entire cluster will eventually fail.
</li></ul><p>For information on using this dashboard to view cluster status, see <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p>
<a name="configuremasterwithserverconf"></a><h2> <a name="configuremasterwithserverconf_configure_the_master_with_server.conf"><span class="mw-headline" id="Configure_the_master_with_server.conf"> Configure the master with server.conf</span></a></h2>
<h3> <a name="configuremasterwithserverconf_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a>. This topic explains the basics of cluster configuration.  It provides details on issues that are common to all cluster node types.
</li></ul><h3> <a name="configuremasterwithserverconf_enable_the_master_node"><span class="mw-headline" id="Enable_the_master_node">Enable the master node</span></a></h3>
<p>The following example shows the basic settings that you must configure when enabling a master node. The configuration attributes correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = master<br>replication_factor = 4<br>search_factor = 3<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is a cluster master node.
</li><li> the cluster's replication factor is 4.
</li><li> the cluster's search factor is 3.
</li><li> the secret key is "whatever".
</li></ul><p><b>Important:</b> When the master starts up for the first time, it will block indexing on the peers until you enable and restart the full replication factor number of peers. Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p><p><b>Note:</b> When you enable the master node in Splunk Web, the resulting <code><font size="2">server.conf</font></code> stanza includes attributes only for non-default values. For example, if you accept the default replication factor of 3 and do not enter a new value for it, the resulting stanza does not include the <code><font size="2">replication_factor</font></code> attribute.
</p>
<h3> <a name="configuremasterwithserverconf_edit_the_master_settings"><span class="mw-headline" id="Edit_the_master_settings">Edit the master settings</span></a></h3>
<p>You can change these settings later, if necessary. For example, to change the cluster's secret key, you edit the <code><font size="2">pass4SymmKey</font></code> value on each node. 
</p><p>For details on all cluster attributes, including some advanced ones that rarely require editing, read the server.conf specification.
</p>
<a name="configuremasterwithcli"></a><h2> <a name="configuremasterwithcli_configure_the_master_with_the_cli"><span class="mw-headline" id="Configure_the_master_with_the_CLI"> Configure the master with the CLI</span></a></h2>
<h3> <a name="configuremasterwithcli_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#usethecli" class="external text">"Configure and manage the indexer cluster with the CLI"</a>. This topic explains the basics of indexer cluster configuration with the CLI.  It provides details on issues that are common to all cluster node types.
</li></ul><h3> <a name="configuremasterwithcli_enable_the_master_node"><span class="mw-headline" id="Enable_the_master_node">Enable the master node</span></a></h3>
<p>The following example shows the basic settings that you must configure when enabling a master node. The configuration attributes correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode master -replication_factor 4 -search_factor 3 <br><br>splunk restart<br></font></code>
</div>
<p><b>Important:</b> When the master starts up for the first time, it will block indexing on the peers until you enable and restart the full replication factor number of peers. Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p>
<h3> <a name="configuremasterwithcli_edit_the_master_settings"><span class="mw-headline" id="Edit_the_master_settings">Edit the master settings</span></a></h3>
<p>You can also use the CLI to edit the configuration later. Use the <code><font size="2">splunk edit cluster-config</font></code> command, the same command used to enable the master initially.
</p><p>Refer to the CLI clustering help, along with the  server.conf specification file, for the list of configurable settings.
</p>
<h3> <a name="configuremasterwithcli_warning:_do_not_increase_the_replication_factor_or_search_factor_on_the_master"><span class="mw-headline" id="Warning:_Do_not_increase_the_replication_factor_or_search_factor_on_the_master"> Warning: Do not increase the replication factor or search factor on the master </span></a></h3>
<p>Although it is possible to change the settings for the replication factor and search factor, it is inadvisable to increase either of them after your cluster contains significant amounts of data. Doing so will kick off a great deal of bucket activity, which will have an adverse effect on the cluster's performance while bucket copies are being created or made searchable.
</p>
<a name="handlemasternodefailure"></a><h2> <a name="handlemasternodefailure_replace_the_master_node_on_the_indexer_cluster"><span class="mw-headline" id="Replace_the_master_node_on_the_indexer_cluster"> Replace the master node on the indexer cluster</span></a></h2>
<p>You might need to replace the master node for either of these reasons:
</p>
<ul><li> The node fails.
</li><li> You need to move the master to a different machine or site.
</li></ul><p>Although there is currently no master failover capability, you can prepare the indexer cluster for master failure by configuring a stand-by master that you can immediately bring up if the primary master goes down.  You can use the same method to replace the master intentionally.
</p><p>This topic describes the key steps in replacing the master:
</p><p><b>1.</b> Back up the files that the stand-by/replacement master needs. 
</p><p><b>Important:</b>  This is a preparatory step. You must do this before the master fails or otherwise gets removed from the system.
</p><p><b>2.</b> Prepare the peers and search head to connect to the new master.
</p><p><b>3.</b> Replace the old master with the new master.
</p><p><b>Important:</b> In the case of a multisite cluster, you need to prepare for the possible failure of the site that houses the master. See <a href="#mastersitefailure" class="external text">"Handle master site failure"</a>. 
</p>
<h3> <a name="handlemasternodefailure_back_up_the_files_that_the_replacement_master_needs"><span class="mw-headline" id="Back_up_the_files_that_the_replacement_master_needs">Back up the files that the replacement master needs</span></a></h3>
<p>In preparing a replacement master, you need to copy over only the master's static state. 
</p><p><b>Note:</b> You do not copy or otherwise deal with the dynamic state of the cluster. The cluster peers as a group hold all information about the dynamic state of a cluster, such as the status of all cluster copies. They will communicate this information to the master node as necessary; for example, when a downed master returns to the cluster or when a stand-by master replaces a downed master. The master will then use that information to rebuild its map of the cluster's dynamic state.
</p><p>There are two static configurations on the master that you need to back up so that you can later copy them to the replacement master: 
</p>
<ul><li> The master's <code><font size="2">server.conf</font></code> file, which is where the master cluster settings are stored. You need to back up this file whenever you change the master's cluster configuration. 
</li></ul><ul><li> The master's <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> directory, which is where common peer configurations are stored, as described in <a href="#updatepeerconfigurations" class="external text">"Update cluster peer configurations"</a>. You need to back up this directory whenever you update the set of content that you are pushing to the peer nodes.
</li></ul><h3> <a name="handlemasternodefailure_ensure_that_the_peer_and_search_head_nodes_can_find_the_new_master"><span class="mw-headline" id="Ensure_that_the_peer_and_search_head_nodes_can_find_the_new_master">Ensure that the peer and search head nodes can find the new master</span></a></h3>
<p>You can choose between two approaches for ensuring that the peer nodes and search head can locate the replacement instance and recognize it as the master:
</p>
<ul><li>  <b>The replacement uses the same IP address and management port as the primary master. </b> To ensure that the replacement uses the same IP address, you need to employ DNS-based failover, a load balancer, or some other technique. The management port is set during installation but you can change it later by editing <code><font size="2">web.conf</font></code>. 
</li></ul><ul><li> <b>The replacement does not use the same IP address or management port as the primary master.</b> In this case, after you bring up the new master, you must update the <code><font size="2">master_uri</font></code> setting on all the peers and search heads to point to the new master's IP address and management port.
</li></ul><p><b>Important:</b> Neither approach requires a restart of the peer or search head nodes.
</p>
<h3> <a name="handlemasternodefailure_replace_the_master"><span class="mw-headline" id="Replace_the_master">Replace the master </span></a></h3>
<p>Assuming that you are keeping back-ups of the two sets of static configuration files, it is a simple process to replace a master:  
</p><p><b>1.</b> Stop the old master, if this is a planned replacement.  If the replacement is due to a failed master, then this step has already been accomplished for you.
</p><p><b>2.</b> Install, start, and stop a new Splunk Enterprise instance. This will be the replacement master.
</p><p><b>3.</b> Copy the <code><font size="2">sslKeysfilePassword</font></code> setting from the replacement master's <code><font size="2">server.conf</font></code> file to a temporary location.
</p><p><b>4.</b> Copy the back-up of the old master's <code><font size="2">server.conf</font></code> and <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> files to the replacement master. 
</p><p><b>5.</b> Delete the <code><font size="2">sslKeysfilePassword</font></code> setting in the copied <code><font size="2">server.conf</font></code> and replace it with the version of the setting saved in step 3.
</p><p><b>6.</b> Start the replacement master. 
</p><p><b>7.</b> Make sure that the peer and search head nodes are pointing to the new master through one of the methods described above in <a href="#handlemasternodefailure_ensure_that_the_peer_and_search_head_nodes_can_find_the_new_master" class="external text">"Ensure that the peer and search head nodes can find the new master"</a>.
</p><p><b>Note:</b> If you want to skip steps 3 and 5, you can simply replace the <code><font size="2">[general]</font></code> and <code><font size="2">[clustering]</font></code> stanzas on the replacement master instead of copying the entire <code><font size="2">server.conf</font></code> file.
</p><p>For information on the consequences of a master failing, see <a href="#whathappenswhenamasternodegoesdown" class="external text">"What happens when the master node goes down"</a>.
</p>
<h1>Configure the peers</h1><a name="configurethepeers"></a><h2> <a name="configurethepeers_peer_node_configuration_overview"><span class="mw-headline" id="Peer_node_configuration_overview"> Peer node configuration overview </span></a></h2>
<p>Configuration of the peer nodes falls under two categories:
</p>
<ul><li> Configuration of the basic indexer cluster settings, such as the master URI and the replication port.
</li><li> Configuration of input, indexing, and related settings. This includes the deployment of apps to the peer nodes.
</li></ul><h3> <a name="configurethepeers_initial_configuration"><span class="mw-headline" id="Initial_configuration"> Initial configuration</span></a></h3>
<p>Most peer cluster configuration happens during initial deployment:
</p><p><b>1.</b> When you enable the peer, you specify its cluster settings, such as its master node and the port on which it receives replicated data. See <a href="#enablethepeernodes" class="external text">"Enable the peer nodes"</a>.  
</p><p><b>2.</b> After you enable the set of peers, you configure their indexes, if necessary. See <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>.
</p><p><b>3.</b> Finally, you configure their inputs, usually by means of forwarders. See <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a>.
</p><p>These are the key steps in configuring a peer. You might also need to update the configurations later, as with any indexer. 
</p>
<h3> <a name="configurethepeers_change_the_cluster_configuration"><span class="mw-headline" id="Change_the_cluster_configuration">Change the cluster configuration</span></a></h3>
<p>There are two main reasons to change the cluster node configuration:
</p>
<ul><li> <b>Redirect the peer to another master.</b> This can be useful in the case where the master fails but you have a stand-by master ready to take over. For information on stand-by masters, see <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.
</li></ul><ul><li> <b>Change the peer's security key for the cluster.</b> Only change the key if you are also changing it for all other nodes in the cluster. The key must be the same across all instances in a cluster.
</li></ul><p>To edit the cluster configuration, change each peer node individually, using one of these methods:
</p>
<ul><li> Edit the configuration from the peer node dashboard in Splunk Web.  See <a href="#configurepeerswithdashboard" class="external text">"Configure peer nodes with the dashboard"</a>.
</li></ul><ul><li> Edit the peer's <code><font size="2">server.conf</font></code> file. See <a href="#configurepeerswithserverconf" class="external text">"Configure peer nodes with server.conf"</a> for details. 
</li></ul><ul><li> Use the CLI. See <a href="#configurepeerswithcli" class="external text">"Configure peer nodes with the CLI"</a> for details. 
</li></ul><p>For additions and differences when configuring multisite peer nodes, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h3> <a name="configurethepeers_configure_indexing_and_related_behavior"><span class="mw-headline" id="Configure_indexing_and_related_behavior">Configure indexing and related behavior</span></a></h3>
<p>The set of index stanzas in <code><font size="2">indexes.conf</font></code> must be identical across all peers, aside from very limited exceptions described in <a href="#managesinglepeerconfigurations" class="external text">"Manage configurations on a peer-by-peer basis"</a>. It is also important that index-time processing be the same across the peers. For the cluster to properly replicate data and handle node failover, peers must share the same indexing functionality, and they cannot do this if certain key files vary from peer to peer. 
</p><p>As a best practice, you should treat your peers as interchangeable, and therefore you should maintain identical versions of configuration files and apps across all peers. At the least, the following files should be identical:
</p>
<ul><li> <code><font size="2">indexes.conf</font></code> 
</li><li> <code><font size="2">props.conf</font></code> 
</li><li> <code><font size="2">transforms.conf</font></code> 
</li></ul><p>To ensure that the peers share a common set of configuration files and apps, place the files and apps on the master and then use the <b>configuration bundle</b> method to distribute them, in a single operation, to the set of peers.
</p><p>These topics describe how to maintain identical configurations across the set of peers:
</p>
<ul><li> <a href="#managecommonconfigurations" class="external text">"Manage common configurations across all peers"</a>
</li><li> <a href="#manageappdeployment" class="external text">"Manage app deployment across all peers"</a>
</li><li> <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>
</li><li> <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>
</li></ul><h3> <a name="configurethepeers_manage_single-peer_configurations"><span class="mw-headline" id="Manage_single-peer_configurations">Manage single-peer configurations</span></a></h3>
<p>You might occasionally need to handle some configurations on a peer-by-peer basis, for testing or other purposes. As a general rule, however, try to use the same configurations across all peers, so that the peers are interchangeable.
</p><p>For information on single-peer configuration, see <a href="#managesinglepeerconfigurations" class="external text">"Manage configurations on a peer-by-peer basis"</a>.
</p>
<a name="configurepeerswithdashboard"></a><h2> <a name="configurepeerswithdashboard_configure_peer_nodes_with_the_dashboard"><span class="mw-headline" id="Configure_peer_nodes_with_the_dashboard"> Configure peer nodes with the dashboard</span></a></h2>
<p>You can edit the peer cluster configuration through its dashboard. To access the dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Select the <b>Edit</b> button on the upper right side of the dashboard.
</p><p>The <b>Edit</b> button provides a number of options that affect the configuration.
</p><p><b>Note:</b> The <b>Edit</b> button is disabled for multisite clusters.
</p><p>For information on using this dashboard to view cluster status, see <a href="#viewthepeerdashboard" class="external text">"View the peer dashboard"</a>.
</p>
<h3> <a name="configurepeerswithdashboard_change_cluster_configuration"><span class="mw-headline" id="Change_cluster_configuration"> Change cluster configuration </span></a></h3>
<p>To change the configuration for this peer node, select the <b>Configuration</b> option:
</p>
<ul><li> To change the master, edit the <b>Master IP address or hostname</b> and <b>Master port</b> fields.
</li></ul><ul><li> To change the replication port, edit the <b>Peer replication port</b> field.
</li></ul><ul><li> To change the secret key, edit the <b>Security key</b> field.
</li></ul><h3> <a name="configurepeerswithdashboard_remove_the_search_head_from_a_cluster"><span class="mw-headline" id="Remove_the_search_head_from_a_cluster"> Remove the search head from a cluster </span></a></h3>
<p>To remove the peer from the cluster, select the <b>Disable Indexer Clustering</b> option.
</p>
<h3> <a name="configurepeerswithdashboard_other_edits"><span class="mw-headline" id="Other_edits"> Other edits </span></a></h3>
<p>The <b>Edit</b> button presents one other option: <b>Node Type</b>. 
</p><p><b>Caution:</b> It is extremely unlikely that you will want to change the node type for nodes in an active cluster. Consider the consequences carefully before doing so.
</p>
<a name="configurepeerswithserverconf"></a><h2> <a name="configurepeerswithserverconf_configure_peer_nodes_with_server.conf"><span class="mw-headline" id="Configure_peer_nodes_with_server.conf"> Configure peer nodes with server.conf</span></a></h2>
<h3> <a name="configurepeerswithserverconf_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a>. This topic explains the basics of cluster configuration.  It provides details on issues that are common to all indexer cluster node types.
</li></ul><h3> <a name="configurepeerswithserverconf_enable_a_peer_node"><span class="mw-headline" id="Enable_a_peer_node">Enable a peer node</span></a></h3>
<p>The following example shows the basic settings that you must configure when enabling a peer node. The configuration attributes shown in these examples correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p>
<div class="samplecode">
<code><font size="2"><br>[replication_port://9887]<br><br>[clustering]<br>master_uri = https://10.152.31.202:8089<br>mode = slave<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the peer will use port 9887 to listen for replicated data streamed from the other peers. You can specify any available, unused port as the replication port. Do not re-use the management or receiving ports. 
</li><li> the peer's cluster master resides at <code><font size="2">10.152.31.202:8089</font></code>.
</li><li> the instance is a cluster peer ("slave") node.
</li><li> the security key is "whatever".
</li></ul><h3> <a name="configurepeerswithserverconf_edit_the_peer_settings"><span class="mw-headline" id="Edit_the_peer_settings">Edit the peer settings</span></a></h3>
<p>You can change these settings later, if necessary. For example, to change the cluster's security key, you change the <code><font size="2">pass4SymmKey</font></code> value on each node.
</p>
<a name="configurepeerswithcli"></a><h2> <a name="configurepeerswithcli_configure_peer_nodes_with_the_cli"><span class="mw-headline" id="Configure_peer_nodes_with_the_CLI"> Configure peer nodes with the CLI</span></a></h2>
<h3> <a name="configurepeerswithcli_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#usethecli" class="external text">"Configure and manage the indexer cluster with the CLI"</a>. This topic explains the basics of cluster configuration with the CLI.  It provides details on issues that are common to all indexer cluster node types.
</li></ul><h3> <a name="configurepeerswithcli_enable_a_peer_node"><span class="mw-headline" id="Enable_a_peer_node">Enable a peer node</span></a></h3>
<p>The following example shows the basic settings that you typically configure when enabling a peer node. The configuration attributes correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p><p>To enable an instance as a peer node, set <code><font size="2">mode</font></code> to "slave". You also need to specify <code><font size="2">master_uri</font></code> and <code><font size="2">replication_port</font></code>. Optionally, you can specify the security key (<code><font size="2">secret</font></code>): 
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode slave -master_uri https://10.160.31.200:8089 -replication_port 9887 -secret your_key<br><br>splunk restart<br></font></code>
</div>
<h3> <a name="configurepeerswithcli_edit_the_peer_settings"><span class="mw-headline" id="Edit_the_peer_settings">Edit the peer settings</span></a></h3>
<p>You can also use the CLI to edit the configuration later. Use the <code><font size="2">splunk edit cluster-config</font></code> command, the same command that you use to enable the peer initially.
</p><p>Refer to the CLI clustering help, along with the  server.conf specification file, for the list of configurable settings.
</p>
<a name="managecommonconfigurations"></a><h2> <a name="managecommonconfigurations_manage_common_configurations_across_all_peers"><span class="mw-headline" id="Manage_common_configurations_across_all_peers"> Manage common configurations across all peers</span></a></h2>
<p>You should attempt to maintain a common set of configuration files, including apps, across all peers in an indexer cluster. This enhances high availability by making the peers essentially interchangeable. In addition, certain configurations must be identical so that all the peers index the data in the same way. 
</p><p>The master node distributes files and apps to all peers as a single action, through the <b>configuration bundle</b> method. You must use this method to manage common configurations. See <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>.
</p>
<h3> <a name="managecommonconfigurations_files_that_need_to_be_identical_across_all_peers"><span class="mw-headline" id="Files_that_need_to_be_identical_across_all_peers"> Files that need to be identical across all peers </span></a></h3>
<p>It is highly recommended that you distribute the same versions of these files across all peers:
</p>
<ul><li> <code><font size="2">indexes.conf</font></code>. It is critical that all peers share the same set of clustered indexes. 
</li></ul><ul><li> <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code>. All peers must use the same set of rules when indexing data. 
</li></ul><p>Beyond these three key files, you can greatly simplify cluster management by maintaining identical versions of other configuration files across all peers. For example, if your peers are able to share a single set of inputs, you can maintain a single <code><font size="2">inputs.conf</font></code> file across all peers. 
</p><p>Because apps often contain versions of those configuration files, you should ordinarily distribute the same set of apps to all peers, rather than installing them individually on single peers. See <a href="#manageappdeployment" class="external text">"Manage app deployment across all peers"</a>.
</p><p><b>Note:</b> Under limited circumstances (for example, to perform local testing or monitoring), you might want to add an index to one peer but not the others. You can do this by creating a single-peer <code><font size="2">indexes.conf</font></code>, as long as you are careful about how you configure the index and are clear about the ramifications. The data in such an index will not get replicated. The single-peer <code><font size="2">indexes.conf</font></code> supplements, but does not replace, the common version of the file that all peers get. You can similarly maintain single-peer apps, if necessary. See <a href="#managesinglepeerconfigurations_add_an_index_to_a_single_peer" class="external text">"Add an index to a single peer"</a>.
</p>
<h3> <a name="managecommonconfigurations_distribute_configuration_files_to_all_peers"><span class="mw-headline" id="Distribute_configuration_files_to_all_peers"> Distribute configuration files to all peers </span></a></h3>
<p>To distribute configurations across the peer nodes: 
</p><p><b>1.</b> If distributing any <code><font size="2">indexes.conf</font></code> files, configure them so that they support index replication. See <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a>.
</p><p><b>2.</b> Place the files in the <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> directory on the master. The set of subdirectories in this location constitute the configuration bundle.
</p><p><b>3.</b> Use Splunk Web or the CLI to distribute the configuration bundle to the peer nodes.
</p><p>For details on these steps, see <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>.
</p>
<h3> <a name="managecommonconfigurations_configuration_management_for_peers_compared_to_standalone_indexers"><span class="mw-headline" id="Configuration_management_for_peers_compared_to_standalone_indexers"> Configuration management for peers compared to standalone indexers </span></a></h3>
<p>The configuration bundle method is the only supported method for managing common configurations and app deployment across the set of peers. It ensures that all peers use the same versions of these files.
</p><p>Note these critical differences in how you manage peer configuration files compared to configurations for standalone indexers:
</p>
<ul><li> Do not make configuration changes on individual peers that will modify configurations you need to maintain on a cluster-wide basis. For example, do not use Splunk Web or the CLI to configure index settings. 
</li></ul><ul><li> Do not edit cluster-wide configuration files, like <code><font size="2">indexes.conf</font></code>, directly on the peers. Instead, edit the files on the master and distribute them through the configuration bundle method. 
</li></ul><ul><li> Do not use deployment server or any third party deployment tool, such as Puppet or CFEngine, to manage common configuration files across peer nodes. Instead, use the configuration bundle method. 
</li></ul><p>When you distribute updates through the configuration bundle, the master orchestrates the distribution to ensure that all peers use the same set of configurations, including the same set of clustered indexes. 
</p><p>If, despite all recommendations, you choose to use another distribution method instead of the configuration bundle method, you must make sure, at a minimum, that settings for any new clustered indexes are successfully distributed to all peers, and that all the peers have been reloaded, before you start sending data to the new indexes.
</p><p><b>Note:</b> Although you cannot use deployment server to directly distribute apps to the peers, you can use it to distribute apps to the master node's configuration bundle location. Once the apps are in that location, the master node can then distribute them to the peer nodes via the configuration bundle method. See <a href="#updatepeerconfigurations_use_deployment_server_to_distribute_the_apps_to_the_master" class="external text">"Use deployment server to distribute the apps to the master"</a>.
</p>
<a name="manageappdeployment"></a><h2> <a name="manageappdeployment_manage_app_deployment_across_all_peers"><span class="mw-headline" id="Manage_app_deployment_across_all_peers"> Manage app deployment across all peers</span></a></h2>
<p>Before reading this topic, see <a href="#managecommonconfigurations" class="external text">"Manage common configurations across all peers"</a>. App deployment is just a special case of the configuration file deployment described in that topic.
</p><p><b>Important:</b> You must use the master node to deploy apps to the peer nodes. Do not use deployment server or any third party deployment tool, such as Puppet or CFEngine. 
</p><p>To distribute an app across the peer nodes: 
</p><p><b>1.</b> Inspect the app for <code><font size="2">indexes.conf</font></code> files. For each index defined in an app-specific <code><font size="2">indexes.conf</font></code> file, set <code><font size="2">repFactor=auto</font></code>, so that the index gets replicated across all peers. See <a href="#configurethepeerindexes_the_indexes.conf_repfactor_attribute" class="external text">"The indexes.conf repFactor attribute"</a>.
</p><p><b>2.</b> Place the app in the <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> directory on the master. The set of subdirectories in this location constitute the <b>configuration bundle</b>.
</p><p><b>3.</b> Use Splunk Web or the CLI to distribute the configuration bundle to the peer nodes.
</p><p>For detailed information on each of these steps, read the topic <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>.
</p><p>Once an app has been distributed to the set of peers, you launch it on each peer in the usual manner, with Splunk Web. See the chapter "Meet Splunk apps" in the <i>Admin Manual</i>.
</p><p>When it comes time to access an app, you do so from the search head, not from an individual peer. Therefore, you must also install the app on the search head. On the search head, put the app in the conventional location for apps, that is, under the <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> directory.
</p>
<a name="configurethepeerindexes"></a><h2> <a name="configurethepeerindexes_configure_the_peer_indexes_in_an_indexer_cluster"><span class="mw-headline" id="Configure_the_peer_indexes_in_an_indexer_cluster"> Configure the peer indexes in an indexer cluster</span></a></h2>
<p>You configure indexes by editing the indexes.conf file. This file determines an indexer's set of indexes, as well as the size and attributes of its <b>buckets</b>. Since all peers in a cluster must use the same set of indexes (except for limited purposes, described later), the <code><font size="2">indexes.conf</font></code> file should ordinarily be the same across all peers.
</p><p>The cluster peers deploy with a peer-specific default <code><font size="2">indexes.conf</font></code> file that handles basic cluster needs. If you want to add indexes or change bucket behavior, you edit a new <code><font size="2">indexes.conf</font></code> file in a special location on the master and then distribute the file simultaneously to all the peers.
</p><p><b>Important:</b>  You cannot use Splunk Web or the CLI to configure index settings on peer nodes. You must edit <code><font size="2">indexes.conf</font></code> directly.
</p>
<h3> <a name="configurethepeerindexes_all_peers_must_use_the_same_set_of_indexes.conf_files"><span class="mw-headline" id="All_peers_must_use_the_same_set_of_indexes.conf_files">All peers must use the same set of indexes.conf files</span></a></h3>
<p>The set of <code><font size="2">indexes.conf</font></code> files should ordinarily be identical across all peers in a cluster. In particular, all peers must use the same set of clustered indexes. This is essential for index replication to work properly. (The master node, on the other hand, has its own, separate <code><font size="2">indexes.conf</font></code> file, because it indexes only its own internal data.) There is a limited exception to this stricture, which is described a bit later.
</p><p>When you first create the cluster, the master distributes a special default <code><font size="2">indexes.conf</font></code> file to each of the peers. This version supplements the standard default <code><font size="2">indexes.conf</font></code> that all indexers get. The peer-specific default <code><font size="2">indexes.conf</font></code> turns on replication for the <code><font size="2">main</font></code> index, as well as the internal indexes, such as <code><font size="2">_audit</font></code> and <code><font size="2">_internal</font></code>.
</p><p>Depending on your system, you might also need to edit and distribute a modified <code><font size="2">indexes.conf</font></code> to the peers, to accommodate additional indexes or changes to bucket attributes. To ensure that all peers use the same <code><font size="2">indexes.conf</font></code>, you must use the master node to distribute the file to all the peers as a single process.This process, known as the <b>configuration bundle</b> method, is described in <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>. 
</p><p>You must also use the configuration bundle method to distribute apps across all the peers. These apps might contain their own <code><font size="2">indexes.conf</font></code> files, which will layer appropriately with any non-app version of the file that you might also distribute to the peers. For information on app distribution, read <a href="#manageappdeployment" class="external text">"Manage app deployment across all peers"</a>.
</p><p><b>Note:</b> Under limited circumstances (for example, to perform local testing or monitoring), you can create an <code><font size="2">indexes.conf</font></code> for a single peer only. Such an index will not get replicated. The single-peer <code><font size="2">indexes.conf</font></code> supplements, but does not replace, the common version of the file that all peers get. See <a href="#managesinglepeerconfigurations_add_an_index_to_a_single_peer" class="external text">"Add an index to a single peer"</a> for details.
</p>
<h3> <a name="configurethepeerindexes_configure_a_set_of_indexes_for_the_peers"><span class="mw-headline" id="Configure_a_set_of_indexes_for_the_peers"> Configure a set of indexes for the peers </span></a></h3>
<p>There are two steps to configuring indexes across the set of peers:
</p><p><b>1.</b> Edit a common <code><font size="2">indexes.conf</font></code> file on the master. 
</p><p><b>2.</b> Use the master to distribute the file across the set of peers. 
</p><p>These two steps are described below.
</p>
<h3> <a name="configurethepeerindexes_1._edit_indexes.conf"><span class="mw-headline" id="1._Edit_indexes.conf"> 1. Edit indexes.conf</span></a></h3>
<p>For details on configuring <code><font size="2">indexes.conf</font></code>, read the topics in the chapters <a href="#aboutmanagingindexes" class="external text">"Manage indexes"</a> and <a href="#howsplunkstoresindexes" class="external text">"Manage index storage"</a> in this manual. For a list of all <code><font size="2">indexes.conf</font></code> attributes, see the indexes.conf specification file in the <i>Admin Manual</i>.
</p><p>For the most part, you edit the cluster peer <code><font size="2">indexes.conf</font></code> in the same way as for any indexer. However, there are a few differences to be aware of.
</p>
<h4><font size="3"><b><i> <a name="configurethepeerindexes_the_indexes.conf_repfactor_attribute"><span class="mw-headline" id="The_indexes.conf_repFactor_attribute">The indexes.conf repFactor attribute</span></a></i></b></font></h4>
<p>When you add a new index stanza, you must set the <code><font size="2">repFactor</font></code> attribute to "auto".  This causes the index's data to be replicated to other peers in the cluster.  For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[&lt;newindex&gt;]<br>repFactor=auto<br>homePath=&lt;path for hot and warm buckets&gt;<br>coldPath=&lt;path for cold buckets&gt;<br>thawedPath=&lt;path for thawed buckets&gt;<br>...<br></font></code>
</div>
<p><b>Note:</b> By default, <code><font size="2">repFactor</font></code> is set to 0, which means that the index will not be replicated. For clustered indexes, you must set it to  "auto". 
</p><p>Resetting <code><font size="2">repFactor</font></code> from "auto" to 0 will stop further replication, but it will not automatically remove copies of already replicated buckets.  In addition, searches across buckets with multiple copies will return duplicate events. To free up associated disk space and eliminate the possibility of duplicate events, you must remove the excess copies manually.
</p>
<h4><font size="3"><b><i> <a name="configurethepeerindexes_specify_homepath_and_coldpath_with_forward-slash_directory_separators"><span class="mw-headline" id="Specify_homePath_and_coldPath_with_forward-slash_directory_separators"> Specify homePath and coldPath with forward-slash directory separators</span></a></i></b></font></h4>
<p>In heterogeneous environments,  it is possible that the master node's operating system could use a different convention for specifying directory paths from the peer nodes' operating system. This presents a problem because you edit the <code><font size="2">indexes.conf</font></code> file on the master but then you distribute it to the peers.
</p><p>For example, if you have a Windows master and a set of Linux peers, the normal way to specify the <code><font size="2">homePath</font></code> on the Windows  master, where the file gets edited, would be to use the Windows backward-slash convention as a directory separator, while the Linux peers, where the file gets distributed, require forward slashes. 
</p><p>To deal with this possibility, the best practice is to always use forward slashes when specifying directory paths in <code><font size="2">homePath</font></code> and <code><font size="2">coldPath</font></code>, no matter which operating systems your master and peers use. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>homePath = $SPLUNK_HOME/var/lib/splunk/defaultdb/db/<br></font></code>
</div>
<p>Splunk Enterprise always accepts the forward slash as a directory separator.
</p>
<h3> <a name="configurethepeerindexes_2._distribute_the_new_indexes.conf_file_to_the_peers"><span class="mw-headline" id="2._Distribute_the_new_indexes.conf_file_to_the_peers"> 2. Distribute the new indexes.conf file to the peers</span></a></h3>
<p>After you edit <code><font size="2">indexes.conf</font></code>, you need to distribute it to the cluster's set of peer nodes. To learn how to distribute configuration files, including <code><font size="2">indexes.conf</font></code>, across all the peers, read <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>.
</p><p>For information about other types of peer configuration, including app distribution, read <a href="#configurethepeers" class="external text">"Peer node configuration overview"</a>.
</p>
<h3> <a name="configurethepeerindexes_view_the_indexes"><span class="mw-headline" id="View_the_indexes"> View the indexes </span></a></h3>
<p>To see the set of indexes on your peer nodes, click the Indexes tab on the master dashboard. See <a href="#howtomonitoracluster_indexes_tab" class="external text">"View the master dashboard."</a>
</p><p><b>Note:</b> A new index appears under the tab only after it contains some data. In other words, if you configure a new index on the peer nodes, a row for that index appears only after you send data to that index.
</p>
<a name="updatepeerconfigurations"></a><h2> <a name="updatepeerconfigurations_update_common_peer_configurations_and_apps"><span class="mw-headline" id="Update_common_peer_configurations_and_apps"> Update common peer configurations and apps</span></a></h2>
<p>The peer update process described in this topic ensures that all peer nodes share a common set of key configuration files. You must manually invoke this process to distribute and update common files, including apps, to the peer nodes. The process also runs automatically when a peer joins the cluster.
</p><p>For information on peer configuration files, see <a href="#managecommonconfigurations" class="external text">"Manage common configurations across all peers"</a>. That topic details exactly which files must be identical across all peers. In brief, the configuration files that must be identical in most circumstances are <code><font size="2">indexes.conf</font></code>, <code><font size="2">props.conf</font></code>, and <code><font size="2">transforms.conf</font></code>. Other configuration files can also be identical, depending on the needs of your system.  Since apps usually include versions of those key files, you should also maintain a common set of apps across all peers.
</p><p>The set of configuration files and apps common to all peers, which is managed from the master and distributed to the peers in a single operation, is called the <b>configuration bundle</b>. The process used to distribute the configuration bundle is known as the configuration bundle method.
</p><p>To distribute new or edited configuration files or apps across all the peers, you add the files to the configuration bundle on the master and tell the master to distribute the files to the peers. 
</p>
<h3> <a name="updatepeerconfigurations_structure_of_the_configuration_bundle"><span class="mw-headline" id="Structure_of_the_configuration_bundle"> Structure of the configuration bundle </span></a></h3>
<p>The configuration bundle consists of the set of files and apps common to all peer nodes.
</p>
<h4><font size="3"><b><i> <a name="updatepeerconfigurations_on_the_master"><span class="mw-headline" id="On_the_master">On the master</span></a></i></b></font></h4>
<p>On the master, the configuration bundle resides under the <code><font size="2">$SPLUNK_HOME/etc/master-apps</font></code> directory. The set of files under that directory constitute the configuration bundle. They are always distributed as a group to all the peers. The directory has this structure:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/master-apps/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_cluster/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;local/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> The <code><font size="2">/_cluster</font></code> directory is a special location for configuration files that need to be distributed across all peers:
<ul><li> The <code><font size="2">/_cluster/default</font></code> subdirectory contains a default version of <code><font size="2">indexes.conf</font></code>. Do not add any files to this directory and do not change any files in it. This peer-specific default <code><font size="2">indexes.conf</font></code> has a higher precedence than the standard default <code><font size="2">indexes.conf</font></code>, located under <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>.
</li><li> The <code><font size="2">/_cluster/local</font></code> subdirectory is where you can put new or edited configuration files that you want to distribute to the peers.
</li><li> <b>For 5.0/5.0.1 upgrades:</b> In Splunk versions 5.0 and 5.0.1, the <code><font size="2">/_cluster</font></code> directory was named <code><font size="2">/cluster</font></code> (no underscore). When you upgrade a 5.0/5.0.1 master node to 5.0.2 or later, its <code><font size="2">/cluster</font></code> directory is automatically  renamed to <code><font size="2">/_cluster</font></code>. When you then restart the master following completion of the upgrade, it performs a rolling restart on its peer nodes and pushes the new bundle, with the renamed <code><font size="2">/_cluster</font></code> directory, to the peers. The <code><font size="2">slave-apps</font></code> directory on all the peer nodes (including  any 5.0/5.0.1 peers) will then contain the renamed directory.
</li></ul></li><li> The <code><font size="2">/&lt;app-name&gt;</font></code> subdirectories are optional. They provide a way to distribute any app to the peer nodes. Create and populate them as needed. For example, to distribute "appBestEver" to the peer nodes, place a copy of that app in its own subdirectory: <code><font size="2">$SPLUNK_HOME/etc/master-apps/appBestEver</font></code>.
</li><li> The master only pushes the contents of subdirectories under <code><font size="2">master-apps</font></code>. It will not push any standalone files directly under <code><font size="2">master-apps</font></code>. For example, it will not push the standalone file <code><font size="2">/master-apps/file1</font></code>. Therefore, be sure to place any standalone configuration files in the <code><font size="2">/_cluster/local</font></code> subdirectory.
</li></ul><p>You explicitly tell the master when you want it to distribute the latest configuration bundle to the peers. In addition, when a peer registers with the master (for example, when the peer joins the cluster), the master distributes the current configuration bundle to it. 
</p><p><b>Caution:</b> When the master distributes the bundle to the peers, it distributes the entire bundle, overwriting the entire contents of any configuration bundle previously distributed to the peers.
</p><p>The <code><font size="2">master-apps</font></code> location is only for peer node files. The master does not use the files in that directory for its own configuration needs.
</p>
<h4><font size="3"><b><i> <a name="updatepeerconfigurations_on_the_peers"><span class="mw-headline" id="On_the_peers">On the peers</span></a></i></b></font></h4>
<p>On the peers, the distributed configuration bundle resides under <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>.  This directory is created soon after a peer is enabled, when the peer initially gets the latest bundle from the master.
Except for the different name for the top-level directory, the structure and contents of the configuration bundle are the same as on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/slave-apps/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_cluster/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;local/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app-name&gt;/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br></font></code>
</div>
<p><b>Important:</b> Leave the downloaded files in this location and do not edit them. If you later distribute an updated version of a configuration file or app to the peers, it will overwrite any earlier version in <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>. You want this to occur, because all peers in the cluster must be using the same versions of the files in that directory.
</p><p>For the same reason, do not add any files or subdirectories directly to <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>. The directory gets overwritten each time the master redistributes the configuration bundle.
</p><p>When Splunk evaluates configuration files, the files in the <code><font size="2">$SPLUNK_HOME/etc/slave-apps/[_cluster|&lt;app-name&gt;]/local</font></code> subdirectories have the highest precedence. For information on configuration file precedence, see "Configuration file precedence" in the <i>Admin Manual</i>.
</p>
<h3> <a name="updatepeerconfigurations_distribute_the_configuration_bundle"><span class="mw-headline" id="Distribute_the_configuration_bundle">Distribute the configuration bundle </span></a></h3>
<p>To distribute new or changed files and apps across all peers, do the following:
</p><p><b>1.</b> Prepare the files and apps and test them. 
</p><p><b>2.</b> Move the files and apps into the configuration bundle on the master node. 
</p><p><b>3.</b> Tell the master to apply the bundle to the peers. 
</p><p>The master pushes the entire bundle to the peers. This overwrites the contents of the peers' current bundle.
</p>
<h4><font size="3"><b><i> <a name="updatepeerconfigurations_1._prepare_the_files_and_apps_for_the_configuration_bundle"><span class="mw-headline" id="1._Prepare_the_files_and_apps_for_the_configuration_bundle">1. Prepare the files and apps for the configuration bundle</span></a></i></b></font></h4>
<p>Make the necessary edits to the files you want to distribute to the peers.  It is advisable that you then test the files, along with any apps, on a standalone test indexer to confirm that they are working correctly, before distributing them to the set of peers. Try to combine all updates in a single bundle, to reduce the impact on the work of the peer nodes.
</p><p>Read the topics <a href="#managecommonconfigurations" class="external text">"Manage common configurations across all peers"</a> and <a href="#configurethepeerindexes" class="external text">"Configure the peer indexes in an indexer cluster"</a> for information on how to configure the files.
</p><p><b>Important:</b> If the configuration bundle subdirectories contain any <code><font size="2">indexes.conf</font></code> files that define new indexes, you must explicitly set each index's <code><font size="2">repFactor</font></code> attribute to <code><font size="2">auto</font></code>. This is necessary for <code><font size="2">indexes.conf</font></code> files that reside in app subdirectories, as well as any <code><font size="2">indexes.conf</font></code> file in the <code><font size="2">_cluster</font></code> subdirectory. See <a href="#configurethepeerindexes_the_indexes.conf_repfactor_attribute" class="external text">"The indexes.conf repFactor attribute"</a> for details.
</p>
<h4><font size="3"><b><i> <a name="updatepeerconfigurations_2._move_the_files_to_the_master_node"><span class="mw-headline" id="2._Move_the_files_to_the_master_node">2. Move the files to the master node</span></a></i></b></font></h4>
<p>When you are ready to distribute the files and apps, copy them to <code><font size="2">$SPLUNK_HOME/etc/master-apps/</font></code> on the master:
</p>
<ul><li> Put apps directly under that directory. For example, <code><font size="2">$SPLUNK_HOME/etc/master-apps/&lt;app-name&gt;</font></code>.
</li><li> Put standalone files in the <code><font size="2">$SPLUNK_HOME/etc/master-apps/_cluster/local</font></code> subdirectory.
</li></ul><h4><font size="3"><b><i> <a name="updatepeerconfigurations_3._apply_the_bundle_to_the_peers"><span class="mw-headline" id="3._Apply_the_bundle_to_the_peers">3. Apply the bundle to the peers</span></a></i></b></font></h4>
<p>To apply the configuration bundle to the peers, you can use Splunk Web or the CLI.
</p>
<h5> <a name="updatepeerconfigurations_use_splunk_web_to_apply_the_bundle"><span class="mw-headline" id="Use_Splunk_Web_to_apply_the_bundle">Use Splunk Web to apply the bundle </span></a></h5>
<p>To apply the configuration bundle to the peers, go to the master node dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p><b>3.</b> Click the <b>Edit</b> button on the upper right corner of the dashboard and then select the <b>Distribute Configuration Bundle</b> option.
</p><p>A dashboard appears with information on the last successful push. It also contains a button, <b>Distribute Configuration Bundle.</b>
</p><p><b>4.</b> Click the <b>Distribute Configuration Bundle</b> button. 
</p><p>A pop-up window warns you that the distribution might, under certain circumstances, initiate a restart of all the peer nodes. For information on which configuration changes cause a peer restart, see <a href="#updatepeerconfigurations_restart_or_reload_after_configuration_bundle_changes.3f" class="external text">"Restart or reload after configuration bundle changes?"</a>. 
</p><p><b>5.</b> Click <b>Push Changes</b> to continue.
</p><p>The screen provides information on the distribution progress.  Once the distribution completes or aborts, the screen indicates the result. In the case of an aborted distribution, it indicates which peers could not receive the distribution. Each peer must successfully receive and apply the distribution. If any peer is unsuccessful, none of the peers will apply the bundle.
</p><p>Once the process completes successfully, the peers will be using the new set of configurations, now located in their local  <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>. 
</p><p><b>Important:</b> Leave the files in <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>.
</p><p>For more details on the internals of the distribution process, read the next section on applying the bundle through the CLI.
</p>
<h5> <a name="updatepeerconfigurations_use_the_cli_to_apply_the_bundle"><span class="mw-headline" id="Use_the_CLI_to_apply_the_bundle">Use the CLI to apply the bundle</span></a></h5>
<p><b>1.</b> To apply the configuration bundle to the peers, run this CLI command on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply cluster-bundle<br></font></code>
</div>
<p>It responds with this warning message:
</p>
<div class="samplecode">
<code><font size="2"><br>Warning: Under some circumstances, this command will initiate a rolling restart <br>of all peers. This depends on the contents of the configuration bundle. For <br>details, refer to the documentation. Do you wish to continue? [y/n]:<br></font></code>
</div>
<p>For information on which configuration changes cause a rolling restart, see <a href="#updatepeerconfigurations_restart_or_reload_after_configuration_bundle_changes.3f" class="external text">"Restart or reload after configuration bundle changes?"</a>. 
</p><p><b>2.</b> To proceed, you need to respond to the message with <code><font size="2">y</font></code>.  You can avoid this message entirely by appending the flag <code><font size="2">--answer-yes</font></code> to the command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply cluster-bundle --answer-yes<br></font></code>
</div>
<p>The <code><font size="2">splunk apply cluster-bundle</font></code> command causes the master to distribute the new configuration bundle to the peers, which then individually validate the bundle.  During this process, each peer validates the settings for all <code><font size="2">indexes.conf</font></code> files in the bundle. After all peers successfully validate the bundle, the master coordinates a rolling restart of all the peer nodes, if necessary. 
</p><p>The download and validation process usually takes just a few seconds to complete.  If any peer is unable to validate the bundle, it sends a message to the master, and the master displays the error on its dashboard in Splunk Web. The process will not continue to the next phase - reloading or restarting the peers - unless all peers successfully validate the bundle.
</p><p>If validation is not successful, you must fix any problems noted by the master and rerun <code><font size="2">splunk apply cluster-bundle</font></code>.
</p><p>Once validation is complete, the master tells the peers to reload or, if necessary, it initiates a rolling restart of all the peers.  For details on how a rolling restart works, see <a href="#restartthecluster_the_rolling-restart_command" class="external text">"The rolling-restart command"</a>.  
</p><p>When the process is complete, the peers will be using the new set of configurations, which will be located in their local  <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>. 
</p><p><b>Important:</b> Leave the files in <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>.
</p><p>Once an app has been distributed to the set of peers, you launch and manage it on each peer in the usual manner, with Splunk Web. See      "Managing app configurations and properties" in the <i>Admin Manual.</i>
</p><p><b>Note:</b> The <code><font size="2">apply cluster-bundle</font></code> command takes an optional flag, <code><font size="2">--skip-validation</font></code>, for use in cases where a problem exists in the validation process.  You should only use this flag under the direction of Splunk Support and after ascertaining that the bundle is valid. Do not use this flag to circumvent the validation process unless you know what you are doing.
</p>
<h5> <a name="updatepeerconfigurations_use_the_cli_to_view_the_status_of_the_bundle_update_process"><span class="mw-headline" id="Use_the_CLI_to_view_the_status_of_the_bundle_update_process"> Use the CLI to view the status of the bundle update process </span></a></h5>
<p>To see how the cluster bundle update is proceeding, run this command from the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk show cluster-bundle-status<br></font></code>
</div>
<p>This command tells you whether bundle validation succeeded or failed. It also indicates the restart status of each peer.
</p>
<h3> <a name="updatepeerconfigurations_settings_that_you_should_not_distribute_through_the_configuration_bundle"><span class="mw-headline" id="Settings_that_you_should_not_distribute_through_the_configuration_bundle">Settings that you should not distribute through the configuration bundle </span></a></h3>
<p>The <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code> directory on the peers is read-only. This is necessary and beneficial behavior, because each time you distribute a new bundle, the directory gets overwritten in its entirety. You would thus otherwise lose any changes made to settings in that directory. Also, the cluster relies on the settings in that directory being identical across all peers.
</p><p>Therefore, if you distribute a setting through the configuration bundle method that the peer needs to update automatically in some way, the peer will do so by creating a new version of the app under <code><font size="2">$SPLUNK_HOME/etc/apps</font></code>. Since you cannot have two apps with the same name, this generates "unexpected duplicate app" errors in <code><font size="2">splunkd.log</font></code>.
</p><p>A common cause of this behavior is distributing SSL passwords through the configuration bundle. Splunk Enterprise overwrites the password with an encrypted version upon restart. But if you distribute the setting through the configuration bundle, the peers cannot overwrite the unencrypted password in its bundle location under <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>. Therefore, upon restart after bundle push, they instead write the encrypted version to <code><font size="2">$SPLUNK_HOME/etc/apps</font></code>, in an app directory with the same name as its name under <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>.  
</p><p>For example, do not push the following setting in <code><font size="2">inputs.conf</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>[SSL]<br>password = &lt;your_password&gt;<br></font></code>
</div>
<p>If the setting is in an app directory called "newapp" in the configuration bundle, upon restart the peer will create a "newapp" directory under <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> and put the setting there. This results in duplicate "newapp" apps.
</p>
<h3> <a name="updatepeerconfigurations_distribution_of_the_bundle_when_a_peer_starts_up"><span class="mw-headline" id="Distribution_of_the_bundle_when_a_peer_starts_up">Distribution of the bundle when a peer starts up</span></a></h3>
<p>After you initially configure a Splunk instance as a peer node, you need to restart it manually in order for it to join the cluster, as described in <a href="#enablethepeernodes" class="external text">"Enable the peer nodes"</a>. During this restart, the peer connects with the master, downloads the current configuration bundle, validates the bundle locally, and then restarts again. The peer will only join the cluster if bundle validation succeeds. This same process also occurs when an offline peer comes back online.
</p><p>If validation fails, the user needs to fix the errors and run <code><font size="2">splunk apply cluster-bundle</font></code> from the master.
</p>
<h3> <a name="updatepeerconfigurations_restart_or_reload_after_configuration_bundle_changes.3f"><span class="mw-headline" id="Restart_or_reload_after_configuration_bundle_changes.3F"> Restart or reload after configuration bundle changes?</span></a></h3>
<p>Some changes to files in the configuration bundle require that the peers restart. In other cases, the peers can just reload, avoiding any interruption to indexing or searching. The bundle reload phase on the peers determines whether a restart is required and directs the master to initiate a rolling restart of the peers only if necessary. 
</p><p>Reload occurs when:
</p>
<ul><li> You add a new sourcetype in <code><font size="2">props.conf</font></code>.
</li><li> You add or update any <code><font size="2">TRANSFORMS-&lt;class&gt;</font></code> stanzas in <code><font size="2">props.conf</font></code>.
</li><li> You add any new stanzas in <code><font size="2">transforms.conf</font></code>.
</li><li> You make any of these changes in <code><font size="2">indexes.conf</font></code>:
<ul><li> Adding new index stanzas 
</li><li> Enabling or disabling an index with no data
</li><li> Changing any attributes not listed as requiring restart
</li></ul></li></ul><p>Restart occurs when:
</p>
<ul><li> The configuration bundle contains changes to any configuration files besides <code><font size="2">indexes.conf</font></code>, <code><font size="2">props.conf</font></code>, or <code><font size="2">transforms.conf</font></code>.
</li><li> You make any changes to <code><font size="2">props.conf</font></code> or <code><font size="2">transforms.conf</font></code>, other than those specified in the reload list, above.
</li><li> You make any of these <code><font size="2">indexes.conf</font></code> changes:
<ul><li> Adding or removing a volume
</li><li> Enabling or disabling an index with data
</li><li> Removing an index 
</li><li> Changing any of these attributes: <code><font size="2">homePath, coldPath, thawedPath, bloomHomePath, summaryHomePath, tstatsHomePath, repFactor, rawChunkSizeBytes, minRawFileSyncSecs, syncMeta, maxConcurrentOptimizes, coldToFrozenDir</font></code>
</li></ul></li></ul><h3> <a name="updatepeerconfigurations_use_deployment_server_to_distribute_the_apps_to_the_master"><span class="mw-headline" id="Use_deployment_server_to_distribute_the_apps_to_the_master">Use deployment server to distribute the apps to the master </span></a></h3>
<p>Although you cannot use deployment server to directly distribute apps to the peers, you can use it to distribute apps to the master node's configuration bundle location. Once the apps are in that location, the master can distribute them to the peer nodes, using the configuration bundle method described in this topic.
</p><p>In addition to the deployment server, you can also use third party distributed configuration management software, such as Puppet or Chef, to distribute apps to the master.
</p><p>To use the deployment server to distribute files to the configuration bundle on the master:
</p><p><b>1.</b> Configure the master as a client of the deployment server, as described in "Configure deployment clients" in <i>Updating Splunk Enterprise Instances</i>.
</p><p><b>2.</b> On the master, edit deploymentclient.conf and set the <code><font size="2">repositoryLocation</font></code> attribute to the <code><font size="2">master-apps</font></code> location:
</p>
<code><font size="2"><br>[deployment-client]<br>serverRepositoryLocationPolicy = rejectAlways<br>repositoryLocation = $SPLUNK_HOME/etc/master-apps<br></font></code>
<p><b>3.</b> On the deployment server, create and populate one or more deployment apps for download to the master's configuration bundle. Make sure that the apps follow the structural requirements for the configuration bundle, as outlined earlier in the current topic. See "Create deployment apps" in <i>Updating Splunk Enterprise Instances</i> for information on creating deployment apps.
</p><p><b>4.</b> Create one or more server classes that map the master to the deployment apps in the usual way.
</p><p><b>5.</b> Each server class must include the <code><font size="2">stateOnClient = noop</font></code> setting:
</p>
<code><font size="2"><br>[serverClass:&lt;serverClassName&gt;]<br>stateOnClient = noop<br></font></code>
<p><b>Note:</b> Do not override this setting at the app stanza level.
</p><p><b>6.</b> Download the apps to the master node.  
</p><p>Once the master receives the new or updated deployment apps in the configuration bundle, you can distribute the bundle to the peers, using the method described in the current topic.
</p><p><b>Important:</b> Take steps to ensure that the master does not restart automatically after receiving the deployment apps. Specifically, when defining deployment app behavior, do not change the value of the <code><font size="2">restartSplunkd</font></code> setting from its default of <code><font size="2">false</font></code> in <code><font size="2">serverclass.conf</font></code>.  If you are using forwarder management to define your server classes, make sure that the field <b>Restart splunkd</b> on the <b>Edit App</b> screen is not checked.
</p><p>For detailed information on the deployment server and how to perform the various operations necessary, read the <i>Updating Splunk Enterprise Instances</i> manual.
</p>
<a name="managesinglepeerconfigurations"></a><h2> <a name="managesinglepeerconfigurations_manage_configurations_on_a_peer-by-peer_basis"><span class="mw-headline" id="Manage_configurations_on_a_peer-by-peer_basis"> Manage configurations on a peer-by-peer basis</span></a></h2>
<p>Most configurations need to be the same across all peers. See <a href="#managecommonconfigurations" class="external text">"Manage common configurations across all peers"</a>. For limited purposes, such as testing, you can handle some configurations on a peer-by-peer basis. 
</p>
<h3> <a name="managesinglepeerconfigurations_configure_data_inputs"><span class="mw-headline" id="Configure_data_inputs"> Configure data inputs </span></a></h3>
<p>Forwarders are the recommended way to handle data inputs to peers. For information on configuring this process, read <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a>. 
</p><p>If you want to input data directly to a peer, without a forwarder, you can configure your inputs on the peer in the same way as for any indexer. For more information, read "Configure your inputs" in the <i>Getting Data In</i> manual.
</p><p><b>Important:</b> Although you can configure inputs on a peer-by-peer basis, consider whether your needs allow you to use a single set of inputs across all peers. This should be possible if all data is channeled through forwarders, and the receiving ports on all peers are the same. If that is the case, you can use the master node to manage a common <code><font size="2">inputs.conf</font></code> file, as described in <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a>.
</p>
<h3> <a name="managesinglepeerconfigurations_add_an_index_to_a_single_peer"><span class="mw-headline" id="Add_an_index_to_a_single_peer"> Add an index to a single peer </span></a></h3>
<p>If you need to add an index to a single peer, you can do so by creating a separate <code><font size="2"> indexes.conf</font></code> file on the peer. However, the data in the new index will remain only on that peer and will not get replicated. The main use case for this is to perform some sort of local testing or monitoring, possibly involving an app that you download to only that one peer. The peer-specific <code><font size="2">indexes.conf</font></code> supplements, but does not replace, the common versions of the file that all peers get. 
</p><p>If you create a version of <code><font size="2">indexes.conf</font></code> for a single peer, you can put it in any of the acceptable locations for an indexer, as discussed in "About configuration files" and "Configuration file directories" in the <i>Admin Manual</i>. The one place where you cannot put the file is under <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code>, which is where the configuration bundle resides on the peer. If you put it there, it will get overwritten the next time the peer downloads a configuration bundle.
</p><p><b>Important:</b> If you add a local index, leave its <code><font size="2">repFactor</font></code> attribute set to the default value of 0.  Do not set it to <code><font size="2">auto</font></code>. If you do set it to <code><font size="2">auto</font></code>, the peer will attempt to replicate the index's data to other peers in the cluster. Since the other peers will not be configured for the new index, there will be nowhere on those peers to store the replicated data, resulting in various, potentially serious, problems. In addition, when the master next attempts to push a configuration bundle to the peers, the peer with the incorrectly configured index will return a bundle validation error to the master, preventing the master from successfully applying the bundle to the set of peers.
</p>
<h3> <a name="managesinglepeerconfigurations_make_other_configuration_changes"><span class="mw-headline" id="Make_other_configuration_changes"> Make other configuration changes</span></a></h3>
<p>If you need to make some other configuration changes specific to an individual peer, you can configure the peer in the usual way for any Splunk Enterprise instance, clustered or not. You can use Splunk Web or the CLI, or you can directly edit configuration files.
</p>
<h3> <a name="managesinglepeerconfigurations_restart_the_peer"><span class="mw-headline" id="Restart_the_peer"> Restart the peer</span></a></h3>
<p>As with any indexer, you sometimes need to restart a peer after you change its configuration. Unlike non-clustered indexers, however, do not use the CLI <code><font size="2">splunk restart</font></code> command to restart the peer. Instead, use the restart capability in Splunk Web. For detailed information on how to restart a cluster peer, read <a href="#restartthecluster_restart_a_single_peer" class="external text">"Restart a single peer"</a>.
</p><p>For information on configuration changes that require a restart, see <a href="#enableclustersindetail_restart_after_modifying_server.conf.3f" class="external text">"Restart after modifying server.conf?"</a> and <a href="#updatepeerconfigurations_restart_or_reload_after_configuration_bundle_changes.3f" class="external text">"Restart or reload after configuration bundle changes?"</a>.
</p>
<h1>Configure the search head</h1><a name="configurethesearchhead"></a><h2> <a name="configurethesearchhead_search_head_configuration_overview"><span class="mw-headline" id="Search_head_configuration_overview"> Search head configuration overview</span></a></h2>
<p>Configuration of the search head in an indexer cluster falls into these categories:
</p>
<ul><li> <a href="#configurethesearchhead_cluster_node_configuration" class="external text">Cluster node configuration</a>. The basic configuration of the search head node occurs during initial deployment of the indexer cluster. You can edit the configuration later.  
</li></ul><ul><li> <a href="#configurethesearchhead_advanced_features_and_topologies" class="external text">Advanced features and topologies</a>. These features, such as mounted bundles, are available to all search heads, whether or not they are participating in an indexer cluster. 
</li></ul><ul><li> <a href="#configurethesearchhead_combined_searches" class="external text">Combined searches</a>. You can combine searches across multiple clusters or across clustered and non-clustered search peers.
</li></ul><p><b>Important:</b> This chapter discusses independent search heads that function as nodes in an indexer cluster. For information on how to incorporate search heads that are members of a <b>search head cluster</b> into an indexer cluster, see "Integrate the search head cluster with an indexer cluster"  in the <i>Distributed Search</i> manual. In addition, see the "Configure search head clustering" chapter in the <i>Distributed Search</i> manual. 
</p>
<h3> <a name="configurethesearchhead_cluster_node_configuration"><span class="mw-headline" id="Cluster_node_configuration">Cluster node configuration</span></a></h3>
<p>Basic configuration of a Splunk Enterprise instance as a search head cluster node occurs when you initially deploy the indexer cluster.  You can edit the configuration later.
</p>
<h4><font size="3"><b><i> <a name="configurethesearchhead_perform_the_initial_configuration"><span class="mw-headline" id="Perform_the_initial_configuration"> Perform the initial configuration</span></a></i></b></font></h4>
<p>You configure and enable the search head at the same time that you enable the other cluster nodes, as described in <a href="#enablethesearchhead" class="external text">"Enable the search head"</a>. The cluster's set of peer nodes become <b>search peers</b> of the search head. For basic functionality, you do not need to set any other configurations. 
</p>
<h4><font size="3"><b><i> <a name="configurethesearchhead_edit_the_configuration"><span class="mw-headline" id="Edit_the_configuration">Edit the configuration</span></a></i></b></font></h4>
<p>There are two main reasons for editing the basic search head configuration for a particular cluster:
</p>
<ul><li> <b>Redirect the search head to another master for the same cluster.</b> This can be useful in the case where a master fails but you have a stand-by master for that cluster which you can redirect the search head to. For information on stand-by masters, see <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.
</li></ul><ul><li> <b>Change the search head's security key for the cluster.</b> Only change the key if you are also changing it for all other nodes in the cluster. The key must be the same across all instances in a cluster.
</li></ul><p>To edit the search head's cluster node configuration, use one of these methods:
</p>
<ul><li> Edit the configuration from the search head node dashboard in Splunk Web. See <a href="#configuresearchheadwithdashboard" class="external text">"Configure the search head with the dashboard"</a>.
</li></ul><ul><li> Edit the search head's <code><font size="2">server.conf</font></code> file. See <a href="#configuresearchheadwithserverconf" class="external text">"Configure the search head with server.conf"</a>. 
</li></ul><ul><li> Use the CLI. See <a href="#configuresearchheadwithcli" class="external text">"Configure the search head with the CLI"</a>.
</li></ul><h4><font size="3"><b><i> <a name="configurethesearchhead_configure_multisite_search_heads"><span class="mw-headline" id="Configure_multisite_search_heads">Configure multisite search heads</span></a></i></b></font></h4>
<p>For additions and differences when configuring multisite search heads, see <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster"</a> and <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h3> <a name="configurethesearchhead_advanced_features_and_topologies"><span class="mw-headline" id="Advanced_features_and_topologies">Advanced features and topologies</span></a></h3>
<p>To implement some advanced features of distributed search, such as mounted bundles, you must edit <code><font size="2">distsearch.conf</font></code> on the search head. 
</p><p>For instructions on how to perform advanced configuration, read the <i>Distributed Search</i> manual. That book focuses on  environments with non-clustered indexers, but you configure most advanced search head features in the same way when working with indexer clusters, except as described here. 
</p>
<h4><font size="3"><b><i> <a name="configurethesearchhead_search_heads_running_on_an_indexer_cluster_compared_to_search_heads_running_against_non-clustered_indexers"><span class="mw-headline" id="Search_heads_running_on_an_indexer_cluster_compared_to_search_heads_running_against_non-clustered_indexers"> Search heads running on an indexer cluster compared to search heads running against non-clustered indexers </span></a></i></b></font></h4>
<p>Most settings and capabilities are the same for search heads running on an indexer cluster and those running against non-clustered indexers. 
</p><p>The main difference is that, for indexer clusters, search heads and search peers are automatically connected to each other as part of the cluster enablement process. You do not perform any configuration in <code><font size="2">distsearch.conf</font></code> to enable automatic discovery.
</p><p>A few attributes in <code><font size="2">distsearch.conf</font></code> are not valid for search heads in indexer clusters. A search head in an indexer cluster ignores these attributes:
</p>
<div class="samplecode">
<code><font size="2"><br>servers<br>disabled_servers<br>heartbeatMcastAddr<br>heartbeatPort<br>heartbeatFrequency<br>ttl<br>checkTimedOutServersFrequency<br>autoAddServers<br></font></code>
</div>
<p>As when running against non-clustered indexers, search head access to search peers is controlled through public key authentication. However, you do not need to distribute the keys manually. The search head in an indexer cluster automatically pushes its public key to the search peers.
</p>
<h4><font size="3"><b><i> <a name="configurethesearchhead_mounted_bundles_and_search_peer_configurations"><span class="mw-headline" id="Mounted_bundles_and_search_peer_configurations">Mounted bundles and search peer configurations</span></a></i></b></font></h4>
<p>Most <code><font size="2">distsearch.conf</font></code> settings are valid only for search heads. However, to implement mounted bundles, you need to distribute a small <code><font size="2">distsearch.conf</font></code> file to the search peers. For indexer clusters, you should use the master node to distribute this file to the peers. For information on how to use the master to manage peer configurations, read <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations and apps"</a> in this manual. For information on how to configure mounted bundles, read the "Mount the knowledge bundle" chapter in the <i>Distributed Search</i> manual.
</p>
<h4><font size="3"><b><i> <a name="configurethesearchhead_how_the_distributed_search_page_works_with_indexer_clusters"><span class="mw-headline" id="How_the_Distributed_Search_page_works_with_indexer_clusters">How the Distributed Search page works with indexer clusters</span></a></i></b></font></h4>
<p>Do not use the Distributed Search page on Splunk Web to configure a search head in an indexer cluster or to add peers to the cluster. You can, however, use that page to view the list of search peers.
</p>
<h3> <a name="configurethesearchhead_combined_searches"><span class="mw-headline" id="Combined_searches">Combined searches</span></a></h3>
<p>To search across multiple indexer clusters, see <a href="#configuremulti-clustersearch" class="external text">"Search across multiple indexer clusters"</a>.
</p><p>To search across both clustered and non-clustered search peers, see <a href="#configurehybridsearch" class="external text">"Configure hybrid search"</a>.
</p>
<a name="configuresearchheadwithdashboard"></a><h2> <a name="configuresearchheadwithdashboard_configure_the_search_head_with_the_dashboard"><span class="mw-headline" id="Configure_the_search_head_with_the_dashboard"> Configure the search head with the dashboard</span></a></h2>
<p>You can edit the search head node configuration through its dashboard. To access the dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p>The dashboard includes a number of menu items and actions that affect the configuration.
</p><p>For information on using this dashboard to view indexer cluster status, see <a href="#viewthesearchheaddashboard" class="external text">"View the search head dashboard"</a>.
</p>
<h3> <a name="configuresearchheadwithdashboard_change_configuration_for_a_particular_cluster"><span class="mw-headline" id="Change_configuration_for_a_particular_cluster"> Change configuration for a particular cluster</span></a></h3>
<p>To change the search head's configuration for a particular indexer cluster, select the <b>Edit Configuration</b> action on the cluster's row: 
</p>
<ul><li> To change the master, edit the <b>Master IP address or hostname</b> and <b>Master port</b> fields.
</li></ul><ul><li> To change the secret key, edit the <b>Security key</b> field.
</li></ul><h3> <a name="configuresearchheadwithdashboard_join_another_indexer_cluster"><span class="mw-headline" id="Join_another_indexer_cluster">Join another indexer cluster</span></a></h3>
<p>To connect the search head to another cluster, see <a href="#configuremulti-clustersearch" class="external text">"Search across multiple indexer clusters"</a>.
</p>
<h3> <a name="configuresearchheadwithdashboard_remove_the_search_head_from_a_cluster"><span class="mw-headline" id="Remove_the_search_head_from_a_cluster"> Remove the search head from a cluster </span></a></h3>
<p>To remove the search head from an indexer cluster, select the <b>Remove Cluster</b> action on the row for that cluster. This disassociates the search head from that cluster, but leaves it connected to all other clusters, if any.
</p><p>To remove the search head from all clusters, select <b>Disable Clustering</b> from the <b>Edit</b> menu on the upper right corner of the dashboard.
</p>
<h3> <a name="configuresearchheadwithdashboard_other_edits"><span class="mw-headline" id="Other_edits"> Other edits </span></a></h3>
<p>If you need to change this instance to some other node type, like a peer node, you can do so through the <b>Edit</b> button on the upper right side of the dashboard.
</p><p><b>Caution:</b> It is extremely unlikely that you will want to change the node type for nodes in an active cluster. Consider the consequences carefully before doing so.
</p><p><b>Note:</b> The <b>Edit</b> button is disabled for multisite clusters.
</p>
<a name="configuresearchheadwithserverconf"></a><h2> <a name="configuresearchheadwithserverconf_configure_the_search_head_with_server.conf"><span class="mw-headline" id="Configure_the_search_head_with_server.conf"> Configure the search head with server.conf</span></a></h2>
<h3> <a name="configuresearchheadwithserverconf_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a>. This topic explains the basics of indexer cluster configuration.  It provides details on issues that are common to all cluster node types.
</li></ul><h3> <a name="configuresearchheadwithserverconf_enable_a_search_head"><span class="mw-headline" id="Enable_a_search_head">Enable a search head</span></a></h3>
<p>The following example shows the basic settings that you must configure when enabling a search head node. The configuration attributes shown here correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>master_uri = https://10.152.31.202:8089<br>mode = searchhead<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the search head's cluster master resides at <code><font size="2">10.152.31.202:8089</font></code>.
</li><li> the instance is a cluster search head.
</li><li> the security key is "whatever".
</li></ul><h3> <a name="configuresearchheadwithserverconf_edit_the_search_head_settings"><span class="mw-headline" id="Edit_the_search_head_settings">Edit the search head settings</span></a></h3>
<p>You can change these settings later, if necessary. For example, to change the cluster's security key, you change the <code><font size="2">pass4SymmKey</font></code> value on each node.
</p><p>You can also configure the search head to search across multiple indexer clusters or across clustered and non-clustered search peers. See:
</p>
<ul><li> <a href="#configuremulti-clustersearch" class="external text">"Search across multiple indexer clusters"</a>
</li><li> <a href="#configurehybridsearch" class="external text">"Configure hybrid search"</a>
</li></ul><a name="configuresearchheadwithcli"></a><h2> <a name="configuresearchheadwithcli_configure_the_search_head_with_the_cli"><span class="mw-headline" id="Configure_the_search_head_with_the_CLI"> Configure the search head with the CLI</span></a></h2>
<h3> <a name="configuresearchheadwithcli_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#usethecli" class="external text">"Configure and manage the indexer cluster with the CLI"</a>. This topic explains the basics of indexer cluster configuration with the CLI.  It provides details on issues that are common to all cluster node types.
</li></ul><h3> <a name="configuresearchheadwithcli_enable_a_search_head"><span class="mw-headline" id="Enable_a_search_head">Enable a search head</span></a></h3>
<p>The following example shows the basic settings that you typically configure when enabling a search head. The configuration attributes correspond to fields on the <b>Enable clustering</b> page of Splunk Web. 
</p><p>To enable an instance as a search head, set <code><font size="2">mode</font></code> to "searchhead". You also need to specify the <code><font size="2">master_uri</font></code>. Optionally, you can specify the security key (<code><font size="2">secret</font></code>): 
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode searchhead -master_uri https://10.160.31.200:8089 -secret your_key<br><br>splunk restart<br></font></code>
</div>
<h3> <a name="configuresearchheadwithcli_edit_the_search_head_settings"><span class="mw-headline" id="Edit_the_search_head_settings">Edit the search head settings</span></a></h3>
<p>You can also use the CLI to edit the configuration later. 
</p><p><b>Important:</b> When you first enable a search head, you use the <code><font size="2">splunk edit cluster-config</font></code> command. To change the search head configuration, you must instead use the <code><font size="2">splunk edit cluster-master</font></code> command. 
</p><p>For example, to change the security key (<code><font size="2">secret</font></code>), use this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-master https://10.160.31.200:8089 &nbsp;-secret newsecret123<br></font></code>
</div>
<p><b>Important:</b> The <code><font size="2">splunk edit cluster-master</font></code> command always takes the current master URI:port value as its initial parameter. For example, this command connects the search head to a different master by setting a new value for the <code><font size="2">-master_uri</font></code> parameter, but it provides the value for the old master as its initial parameter:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-master https://10.160.31.200:8089 &nbsp;-master_uri https://10.160.31.555:8089<br></font></code>
</div>
<p>Refer to the CLI clustering help, along with the  server.conf specification file, for the list of configurable settings.
</p>
<a name="configuremulti-clustersearch"></a><h2> <a name="configuremulti-clustersearch_search_across_multiple_indexer_clusters"><span class="mw-headline" id="Search_across_multiple_indexer_clusters"> Search across multiple indexer clusters</span></a></h2>
<p>You can configure a search head to search across multiple indexer clusters. The method you use depends on whether the clusters are single-site or multisite.
</p>
<h3> <a name="configuremulti-clustersearch_configure_multi-cluster_search_for_single-site_indexer_clusters"><span class="mw-headline" id="Configure_multi-cluster_search_for_single-site_indexer_clusters">Configure multi-cluster search for single-site indexer clusters</span></a></h3>
<p>To configure multi-cluster search:
</p><p><b>1.</b> Configure the search head for one of the clusters in the usual way, as described in <a href="#enablethesearchhead" class="external text">"Enable the search head"</a>.
</p><p><b>2.</b> Point the search head at the master for the new cluster. You can do this with Splunk Web, through the CLI, or by editing the search head's <code><font size="2">server.conf</font></code> file. 
</p>
<h4><font size="3"><b><i> <a name="configuremulti-clustersearch_in_splunk_web"><span class="mw-headline" id="In_Splunk_Web">In Splunk Web</span></a></i></b></font></h4>
<p>In Splunk Web, configure multi-cluster search from the search head dashboard:
</p><p><b>1.</b> Select the <b>Add cluster to be searched</b> button on the uppper right corner of the dashboard.
</p><p><b>2.</b> Fill out the fields in the pop-up window:
</p>
<ul><li> <b>Master IP Address or Hostname</b>. Enter the IP address or hostname of the master for the cluster you want to join. For example: <code><font size="2">https://10.152.31.202</font></code>.
</li><li> <b>Master Port</b>. Enter the master's management port. For example: <code><font size="2">8089</font></code>. 
</li><li> <b>Security Key</b>. This is the secret key that authenticates communication between the master and the peers and search heads. The key must be the same across all instances within a cluster. If the master has a secret key, you must enter it here. The search head can use a different key for each cluster.
</li></ul><p>To remove the search head from a cluster, see <a href="#configuresearchheadwithdashboard_remove_the_search_head_from_a_cluster" class="external text">"Remove the search head from a cluster"</a>.
</p>
<h4><font size="3"><b><i> <a name="configuremulti-clustersearch_through_the_cli"><span class="mw-headline" id="Through_the_CLI">Through the CLI</span></a></i></b></font></h4>
<p>In the CLI, you can configure multi-cluster search with these commands:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add cluster-master &lt;master_uri:port&gt;<br>splunk edit cluster-master &lt;master_uri:port&gt;<br>splunk remove cluster-master &lt;master_uri:port&gt;<br>splunk list cluster-master<br></font></code>
</div>
<p>You do not need to restart the search head after running these commands.
</p><p>For example, to add the search head to a cluster whose master is located at https://10.160.31.200:8089, run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add cluster-master https://10.160.31.200:8089<br></font></code>
</div>
<p>For more information on any command, see its CLI help.
</p>
<h4><font size="3"><b><i> <a name="configuremulti-clustersearch_by_editing_server.conf"><span class="mw-headline" id="By_editing_server.conf"> By editing server.conf</span></a></i></b></font></h4>
<p>You can configure multi-cluster search in the search head's <code><font size="2">server.conf</font></code> file by specifying a comma-separated list of master node references in the <code><font size="2">master_uri</font></code> attribute, followed by individual stanzas for each master. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = searchhead<br>master_uri = clustermaster:east, clustermaster:west<br><br>[clustermaster:east]<br>master_uri=https://SplunkMaster01.example.com:8089<br>pass4SymmKey=someSecret<br><br>[clustermaster:west]<br>master_uri=https://SplunkMaster02.example.com:8089<br></font></code>
</div>
<p>In this example, the search head will use the <code><font size="2">pass4SymmKey</font></code> "someSecret" when communicating with SplunkMaster01 and no <code><font size="2">pass4SymmKey</font></code> when communicating with SplunkMaster02.
</p><p>After you edit <code><font size="2">server.conf</font></code>, you must restart the search head for the changes to take effect.
</p><p>For details on configuring multi-cluster search, see the server.conf specification file.
</p>
<h3> <a name="configuremulti-clustersearch_configure_multi-cluster_search_for_multisite_indexer_clusters"><span class="mw-headline" id="Configure_multi-cluster_search_for_multisite_indexer_clusters">Configure multi-cluster search for multisite indexer clusters </span></a></h3>
<p>A search head can search across multiple multisite clusters or a combination of single-site and multisite clusters. To configure this, you need to specify the search head's <code><font size="2">site</font></code> attribute when connecting it to a multisite cluster. 
</p>
<h4><font size="3"><b><i> <a name="configuremulti-clustersearch_through_the_cli_2"><span class="mw-headline" id="Through_the_CLI_2">Through the CLI</span></a></i></b></font></h4>
<p>In the CLI, you configure multi-cluster search with the <code><font size="2">splunk add cluster-master</font></code> command. When adding a multisite cluster, include the search head's <code><font size="2">site</font></code> value:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk add cluster-master &lt;master_uri:port&gt; -site site&lt;n&gt;<br></font></code>
</div>
<p>You do not need to restart the search head after running this command.
</p>
<h4><font size="3"><b><i> <a name="configuremulti-clustersearch_by_editing_server.conf_2"><span class="mw-headline" id="By_editing_server.conf_2"> By editing server.conf</span></a></i></b></font></h4>
<p>To configure multi-cluster search for a multisite cluster, you need to set two multisite-specific attributes: <code><font size="2">site</font></code> and <code><font size="2">multisite</font></code>. The locations of these attributes vary, depending on a few factors.
</p><p><b>If the search head will be searching across only multisite clusters, and the search head is on the same site in each cluster,</b> put the <code><font size="2">site</font></code> attribute under the <code><font size="2">[general]</font></code> stanza and the <code><font size="2">multisite</font></code> attribute under each <code><font size="2">[clustermaster]</font></code> stanza:
</p>
<div class="samplecode">
<code><font size="2"><br>[general]<br>site=site1<br><br>[clustering]<br>mode = searchhead<br>master_uri = clustermaster:multieast, clustermaster:multiwest<br><br>[clustermaster:multieast]<br>multisite=true<br>master_uri=https://SplunkMaster01.example.com:8089<br><br>[clustermaster:multiwest]<br>multisite=true<br>master_uri=https://SplunkMaster02.example.com:8089<br></font></code>
</div>
<p><b>If the search head will be searching across only multisite clusters, and the search head is on a different site in each cluster,</b> put both the <code><font size="2">site</font></code> and the <code><font size="2">multisite</font></code> attributes under the <code><font size="2">[clustermaster]</font></code> stanzas:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = searchhead<br>master_uri = clustermaster:multieast, clustermaster:multiwest<br><br>[clustermaster:multieast]<br>multisite=true<br>master_uri=https://SplunkMaster01.example.com:8089<br>site=site1<br><br>[clustermaster:multiwest]<br>multisite=true<br>master_uri=https://SplunkMaster02.example.com:8089<br>site=site2<br></font></code>
</div>
<p><b>If the search head will be searching across a combination of single-site and multisite clusters,</b> put both the <code><font size="2">site</font></code> and the <code><font size="2">multisite</font></code> attributes under the <code><font size="2">[clustermaster]</font></code> stanza for any multisite clusters. In this example, the search head searches across two clusters, only one of which is multisite:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = searchhead<br>master_uri = clustermaster:multi, clustermaster:single<br><br>[clustermaster:multi]<br>multisite=true<br>master_uri=https://SplunkMaster01.example.com:8089<br>site=site1<br><br>[clustermaster:single]<br>master_uri=https://SplunkMaster02.example.com:8089<br></font></code>
</div>
<p>After you edit <code><font size="2">server.conf</font></code>, you must restart the search head for the changes to take effect.
</p><p>For more information on multisite cluster configuration, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<a name="configurehybridsearch"></a><h2> <a name="configurehybridsearch_configure_hybrid_search"><span class="mw-headline" id="Configure_hybrid_search"> Configure hybrid search</span></a></h2>
<p>You can search across both clustered and non-clustered search peers. To configure a hybrid search:
</p><p><b>1.</b> Configure an indexer cluster search head in the standard fashion, as described in <a href="#enablethesearchhead" class="external text">"Enable the search head"</a>.
</p><p><b>2.</b> Use Splunk Web or the CLI to add one or more non-clustered search peers, as described in "Add search peers to the search head" in <i>Distributed Search</i>.
</p><p>Note the following:
</p>
<ul><li> You must specify the non-clustered search peers through either Splunk Web or the CLI. Due to authentication issues, you cannot specify the search peers by directly editing <code><font size="2">distsearch.conf</font></code>.  When you add a search peer with Splunk Web or the CLI, Splunk prompts you for public key credentials. It has no way of obtaining those credentials when you add a search peer by directly editing <code><font size="2">distsearch.conf</font></code>. For more information on public keys and distributed search, read "Add search peers to the search head" in <i>Distributed Search</i>.
</li></ul><ul><li> Hybrid search is not compatible with search head pooling.
</li></ul><ul><li> An indexer can be either a cluster peer, in which case, it is automatically a search peer for that cluster's search head, or a non-clustered search peer, with an entry in the search head's <code><font size="2">distsearch.conf</font></code> file. It cannot be both. If you mistakenly configure an indexer as both a cluster peer and a non-clustered search peer, the search head's Distributed Search page in Splunk Web will contain two entries for the peer, and the status for one entry will read, "Peer member of cluster and distsearch.conf".  To remediate, disable or delete the entry for that peer in <code><font size="2">distsearch.conf</font></code>.
</li></ul><h1>Deploy and configure a multisite indexer cluster</h1><a name="multisitedeploymentoverview"></a><h2> <a name="multisitedeploymentoverview_multisite_indexer_cluster_deployment_overview"><span class="mw-headline" id="Multisite_indexer_cluster_deployment_overview"> Multisite indexer cluster deployment overview</span></a></h2>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>. That topic provides a general overview of deployment for both single-site and <b>multisite</b> indexer clusters. The topic you are reading now describes only the multisite differences.
</li></ul><p><b>Important:</b> This chapter assumes that you are deploying independent search heads in the multisite indexer cluster. For information on how to incorporate search heads that are members of a <b>search head cluster</b>, see "Integrate the search head cluster with an indexer cluster"  in the <i>Distributed Search</i> manual.
</p>
<h3> <a name="multisitedeploymentoverview_migrating_from_a_single-site_cluster.3f"><span class="mw-headline" id="Migrating_from_a_single-site_cluster.3F"> Migrating from a single-site cluster?</span></a></h3>
<p>To migrate from a single-site to a multisite indexer cluster, read <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</p>
<h3> <a name="multisitedeploymentoverview_deploy_a_multisite_indexer_cluster"><span class="mw-headline" id="Deploy_a_multisite_indexer_cluster"> Deploy a multisite indexer cluster </span></a></h3>
<p>To deploy a multisite cluster, you configure the set of nodes for each site:
</p>
<ul><li> A single master resides on one of the sites and controls the entire multisite cluster.
</li><li> A set of peer nodes resides on each site.
</li><li> A search head resides on each site that searches cluster data. If you want all searches to be local,&Acirc;&nbsp;you must install a search head on each site. This is known as <b>search affinity.</b>
</li></ul><p>For example, to set up a two-site cluster with three peers and one search head on each site, you install and configure these instances:
</p>
<ul><li> One master node on one of the sites, either site 1 or site 2
</li><li> Three peer nodes on site 1
</li><li> Three peer nodes on site 2
</li><li> One search head on site 1
</li><li> One search head on site 2
</li></ul><p><b>Note:</b> The master itself is not actually a member of any site, aside from its physical location. However, each master has a built-in search head, and that search head requires that you set a site attribute in the master's configuration. You must specify a site for the master, even if you never use its built-in search head. Note that the search head is for testing only. Do not use it for production purposes.
</p>
<h3> <a name="multisitedeploymentoverview_configure_multisite_nodes"><span class="mw-headline" id="Configure_multisite_nodes"> Configure multisite nodes </span></a></h3>
<p>To deploy and configure multisite cluster nodes, you must directly edit <code><font size="2">server.conf</font></code> or use the CLI.  You cannot use Splunk Web.
</p>
<h4><font size="3"><b><i> <a name="multisitedeploymentoverview_multisite-specific_configuration_settings"><span class="mw-headline" id="Multisite-specific_configuration_settings">Multisite-specific configuration settings </span></a></i></b></font></h4>
<p>When you deploy a multisite cluster, you configure the same settings as for single-site, along with some additional settings to specify the set of sites and the location of replicated and searchable copies across the sites.
</p><p><b>On the master</b>, you:
</p>
<ul><li> Enable the cluster for multisite.
</li><li> Enumerate the set of sites for the cluster.
</li><li> Set a multisite replication factor.
</li><li> Set a multisite search factor.
</li></ul><p><b>On each cluster node</b>, you:
</p>
<ul><li> Identify the site that the node resides on.
</li></ul><h4><font size="3"><b><i> <a name="multisitedeploymentoverview_configure_with_server.conf"><span class="mw-headline" id="Configure_with_server.conf">Configure with server.conf</span></a></i></b></font></h4>
<p>To configure a multisite master node with <code><font size="2">server.conf</font></code>, see <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</p>
<h4><font size="3"><b><i> <a name="multisitedeploymentoverview_configure_with_the_cli"><span class="mw-headline" id="Configure_with_the_CLI">Configure with the CLI</span></a></i></b></font></h4>
<p>To configure a multisite master node with the CLI, see <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>
</p>
<a name="multisitesearchaffinity"></a><h2> <a name="multisitesearchaffinity_implement_search_affinity_in_a_multisite_indexer_cluster"><span class="mw-headline" id="Implement_search_affinity_in_a_multisite_indexer_cluster"> Implement search affinity in a multisite indexer cluster</span></a></h2>
<p>One of the key benefits of multisite indexer clustering is that it allows you to configure a cluster so that search heads get search results only from data stored on their local sites. This reduces network traffic while still providing access to the entire set of data, because each site contains a full copy of the data. This benefit is known as <b>search affinity</b>.
</p><p>For example, say you have two data centers in California, one in San Francisco and the other in Los Angeles. You set up a two-site cluster, with each site corresponding to a data center.  Search affinity allows you to reduce long-distance network traffic. Search heads at the San Francisco data center get results only from the peers in San Francisco, while search heads in Los Angeles get results only from their local peers.
</p>
<h3> <a name="multisitesearchaffinity_how_search_affinity_works"><span class="mw-headline" id="How_search_affinity_works"> How search affinity works </span></a></h3>
<p>For those sites that you want to support search affinity, you must configure multisite clustering so that the site has a full set of searchable data and  a local search head. The search head on any particular site then gets data only from its local site, as long as that site is <b>valid</b>. 
</p><p>If a local peer holding some of the searchable data goes down and the site temporarily loses its valid state, the search head will, if necessary, access data from peers on remote sites while the local site is undergoing bucket fixing. During this time, the search head will still get as much of the data as possible from the local site. 
</p><p>Once the site regains its valid state, new searches again occur across only the local site.
</p><p>For more details on how the cluster handles search affinity, see <a href="#multisitearchitecture" class="external text">"Multisite indexer cluster architecture"</a>.
</p>
<h3> <a name="multisitesearchaffinity_implement_search_affinity"><span class="mw-headline" id="Implement_search_affinity"> Implement search affinity</span></a></h3>
<p>Search affinity is always enabled with multisite clusters.  However, you must perform a few steps to take advantage of it. Specifically, you must ensure that both the searchable data and the search heads are available locally. 
</p><p>To implement search affinity:
</p><p><b>1.</b> Configure the site search factor so that you have at least one searchable copy on each site where you require search affinity. 
</p><p>One way to do this is to explicitly specify a search factor for each site that requires search affinity. For example, a four-site cluster with <code><font size="2">site_search_factor = origin:1, site1:1, site2:1, total:3</font></code> ensures that both site1 and site2 have searchable copies of every bucket. The third set of searchable copies will be spread across the two non-explicit sites, with no guarantee that either site will have a full set of searchable copies. Thus, search affinity is enabled for only site1 and site2.
</p><p>There are also ways to configure the site search factor to ensure that all sites have searchable copies, even without explicitly specifying some or all of them. For example, a three-site cluster with <code><font size="2">site_search_factor = origin:1, total:3</font></code> guarantees one searchable copy per site, and thus enables search affinity for each site.
</p><p>For more information on how replication and search factors distribute copies across sites, see <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a> and <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>. 
</p><p><b>2.</b> Deploy a search head on each site where you require search affinity.
</p>
<a name="multisiteconffile"></a><h2> <a name="multisiteconffile_configure_multisite_indexer_clusters_with_server.conf"><span class="mw-headline" id="Configure_multisite_indexer_clusters_with_server.conf"> Configure multisite indexer clusters with server.conf</span></a></h2>
<h3> <a name="multisiteconffile_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#multisitedeploymentoverview" class="external text">"Multisite indexer cluster deployment overview"</a>. This topic provides important background information about configuring a multisite cluster.
</li></ul><ul><li> <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a>. This topic explains the basics of cluster configuration.  It focuses on single-site clusters, but most of the information is relevant to multisite clusters as well.
</li></ul><h3> <a name="multisiteconffile_how_multisite_configuration_differs_from_single-site_configuration"><span class="mw-headline" id="How_multisite_configuration_differs_from_single-site_configuration"> How multisite configuration differs from single-site configuration </span></a></h3>
<p>You configure  multisite indexer clusters in a similar way to how you configure clusters for a single site, with the exception of a few new attributes:
</p><p><b>On all multisite cluster nodes (master/peers/search heads):</b>
</p>
<ul><li> The <code><font size="2">site</font></code> attribute specifies the site that a node resides on. 
</li></ul><p><b>On the master node and search head:</b>
</p>
<ul><li> The <code><font size="2">multisite</font></code> attribute indicates that the master or search head is participating in a multisite cluster.
</li></ul><p><b>On the master node only:</b>
</p>
<ul><li> The <code><font size="2">available_sites</font></code> attribute names the sites that the master is managing.
</li><li> The <code><font size="2">site_replication_factor</font></code> replaces the standard <code><font size="2">replication_factor</font></code> used with single-site clusters. For details, see <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</li><li> The <code><font size="2">site_search_factor</font></code> replaces the standard <code><font size="2">search_factor</font></code> used with single-site clusters. For details, see <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>.
</li></ul><p><b>Important:</b> If you are migrating a cluster from single-site to multisite, you must keep the existing <code><font size="2">replication_factor</font></code> and <code><font size="2">search_factor</font></code> attributes for the existing single-site buckets, while also adding the new multisite <code><font size="2">site_replication_factor</font></code> and <code><font size="2">site_search_factor</font></code> attributes for the new multisite buckets. See <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</p>
<h3> <a name="multisiteconffile_configure_multisite_cluster_nodes"><span class="mw-headline" id="Configure_multisite_cluster_nodes"> Configure multisite cluster nodes</span></a></h3>
<p>To configure a multisite cluster, you configure the nodes for each site, editing each node's <code><font size="2">server.conf</font></code> file. For details on the clustering attributes, read the server.conf specification.
</p>
<h4><font size="3"><b><i> <a name="multisiteconffile_site_values"><span class="mw-headline" id="Site_values"> Site values</span></a></i></b></font></h4>
<p>Site values identify the site on which a node resides. You assign a site value to each node in a multisite cluster. To do this, you set the <code><font size="2">site</font></code> attribute in the node's <code><font size="2">[general]</font></code> stanza.
</p><p>Site values have the syntax: 
</p>
<code><font size="2"><br>site&lt;n&gt;<br></font></code>
<p>where &lt;n&gt; is an integer in the range of 1 to 63: site1, site2, site3, ....
</p><p>For example:
</p>
<code><font size="2"><br>site=site1<br></font></code>
<h4><font size="3"><b><i> <a name="multisiteconffile_configure_the_master_node"><span class="mw-headline" id="Configure_the_master_node">Configure the master node </span></a></i></b></font></h4>
<p>You configure the key attributes for the entire cluster on the master node. Here is an example of a multisite configuration for a master node:
</p>
<div class="samplecode">
<code><font size="2"><br>[general]<br>site = site1<br><br>[clustering]<br>mode = master<br>multisite = true<br>available_sites = site1,site2<br>site_replication_factor = origin:2,total:3<br>site_search_factor = origin:1,total:2<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is located on site1.
</li><li> the instance is a cluster master node.
</li><li> the cluster is multisite.
</li><li> the cluster consists of two sites: site1 and site2.
</li><li> the cluster's replication factor is the default "origin:2,total:3".
</li><li> the cluster's search factor is "origin:1,total:2".
</li><li> the cluster's secret key is "whatever".
</li></ul><p>Note the following:  
</p>
<ul><li> You specify the <code><font size="2">site</font></code> attribute in the <code><font size="2">[general]</font></code> stanza.  You specify all other multisite attributes in the <code><font size="2">[clustering]</font></code> stanza.
</li><li> You can locate the master on any site in the cluster, but each cluster has only one master.
</li><li> You must set <code><font size="2">multisite=true</font></code>.
</li><li> You must list all cluster sites in the <code><font size="2">available_sites</font></code> attribute.
</li><li> You must set a <code><font size="2">site_replication_factor</font></code> and a <code><font size="2">site_search_factor</font></code>. For details, see <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a> and <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>.
</li><li> The <code><font size="2">pass4SymmKey</font></code> attribute, which sets the secret key, is optional. See <a href="#enableclustersindetail" class="external text">"Configure the indexer cluster with server.conf"</a> for details.
</li></ul><p><b>Important:</b> When the master starts up for the first time, it blocks indexing on the peers until you enable and restart the full replication factor number of peers.  For example, given a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, site3:3, total:8", the master blocks indexing until there are at least eight peers in total across all sites, including at least one in site1, two in site2, and three in site3.
</p><p>Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p>
<h4><font size="3"><b><i> <a name="multisiteconffile_configure_the_peer_nodes"><span class="mw-headline" id="Configure_the_peer_nodes">Configure the peer nodes </span></a></i></b></font></h4>
<p>To configure a peer node in a multisite cluster, you set a <code><font size="2">site</font></code> attribute in the <code><font size="2">[general]</font></code> stanza. All other configuration settings are identical to a peer in a single-site cluster.
</p><p>Here is an example configuration for a multisite peer node:
</p>
<div class="samplecode">
<code><font size="2"><br>[general]<br>site = site1<br><br>[replication_port://9887]<br><br>[clustering]<br>master_uri = https://10.152.31.202:8089<br>mode = slave<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is located in site1. A peer can belong to only a single site.
</li><li> the peer will use port 9887 to listen for replicated data streamed from the other peers. You can specify any available, unused port as the replication port. Do not re-use the management or receiving ports. 
</li><li> the peer's cluster master is located at <code><font size="2">10.152.31.202:8089</font></code>.
</li><li> the instance is a cluster peer ("slave") node.
</li><li> the secret key is "whatever".
</li></ul><h4><font size="3"><b><i> <a name="multisiteconffile_configure_the_search_heads"><span class="mw-headline" id="Configure_the_search_heads"> Configure the search heads </span></a></i></b></font></h4>
<p>Multisite search heads provide search affinity. For information, see <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster"</a>.
</p><p>To configure a search head in a multisite cluster, you set a <code><font size="2">site</font></code> attribute in the <code><font size="2">[general]</font></code> stanza and a <code><font size="2">multisite</font></code> attribute in the <code><font size="2">[clustering]</font></code> stanza. All other configuration settings are identical to a search head in a single-site cluster.
Here is an example configuration for a multisite search head node:
</p>
<div class="samplecode">
<code><font size="2"><br>[general]<br>site = site1<br><br>[clustering]<br>multisite = true<br>master_uri = https://10.152.31.202:8089<br>mode = searchhead<br>pass4SymmKey = whatever<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is located in site1. A search head can belong to only a single site per master.
</li><li> the search head is a member of a multisite cluster.
</li><li> the search head's cluster master is located at <code><font size="2">10.152.31.202:8089</font></code>.
</li><li> the instance is a cluster search head.
</li><li> the secret key is "whatever".
</li></ul><p><b>Note:</b> You can also use <code><font size="2">server.conf</font></code> to enable multi-cluster search, in which the search head searches across multiple clusters, multisite or single-site.  For searching across multiple multisite clusters, you can configure the search head to be a member of a different site on each cluster. For details, see <a href="#configuremulti-clustersearch_configure_multi-cluster_search_for_multisite_clusters" class="external text">"Configure multi-cluster search for multisite clusters"</a>.
</p><p>When reconfiguring a search head that is up-and-running, Splunk recommends that you use the CLI command described in <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>, rather than editing <code><font size="2">server.conf</font></code> directly. If you use the CLI, you do not need to restart the search head.
</p>
<h3> <a name="multisiteconffile_restart_the_cluster_nodes"><span class="mw-headline" id="Restart_the_cluster_nodes">Restart the cluster nodes</span></a></h3>
<h4><font size="3"><b><i> <a name="multisiteconffile_after_initial_configuration"><span class="mw-headline" id="After_initial_configuration"> After initial configuration</span></a></i></b></font></h4>
<p>After configuring instances as multisite cluster nodes, you need to restart all of them (master, peers, and search head) for the changes to take effect. You can do this by invoking the CLI <code><font size="2">restart</font></code> command on each node:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk restart<br></font></code>
</div>
<p><b>Important:</b> When the master starts up for the first time, it blocks indexing on the peers until you enable and restart the full replication factor number of peers.  For example, given a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, site3:3, total:8", the master blocks indexing until there are at least eight peers in total across all sites, including at least one in site1, two in site2, and three in site3.
</p><p>Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p>
<h4><font size="3"><b><i> <a name="multisiteconffile_after_changing_the_configuration"><span class="mw-headline" id="After_changing_the_configuration">After changing the configuration</span></a></i></b></font></h4>
<h5> <a name="multisiteconffile_on_the_master"><span class="mw-headline" id="On_the_master">On the master</span></a></h5>
<p>You must restart the master after you change any of the following attributes:
</p>
<ul><li> <code><font size="2">multisite</font></code>
</li><li> <code><font size="2">available_sites</font></code>
</li><li> <code><font size="2">site_replication_factor</font></code>
</li><li> <code><font size="2">site_search_factor</font></code>
</li></ul><p>After you restart the master, you must also initiate a rolling-restart of the cluster peers. If you don't, the cluster will be in an indeterminate state. For information on the <code><font size="2">splunk rolling-restart</font></code> command, see <a href="#restartthecluster_the_rolling-restart_command" class="external text">"The rolling-restart command"</a>.
</p><p>You do not need to restart if you change the <code><font size="2">site</font></code> value on a master.
</p>
<h5> <a name="multisiteconffile_on_the_peers"><span class="mw-headline" id="On_the_peers">On the peers</span></a></h5>
<p>If you change the <code><font size="2">site</font></code> value on a peer, you must restart it for the change to take effect.
</p><p><b>Important:</b>  Although you can use the CLI <code><font size="2">restart</font></code> command when you initially enable an instance as a cluster peer node, you should not use it for subsequent restarts. The <code><font size="2">restart</font></code> command is not compatible with index replication once replication has begun. For more information, including a discussion of safe restart methods, see <a href="#restartthecluster_restart_a_single_peer" class="external text">"Restart a single peer"</a>.
</p>
<h5> <a name="multisiteconffile_on_the_search_head"><span class="mw-headline" id="On_the_search_head">On the search head</span></a></h5>
<p>You do not need to restart if you change the <code><font size="2">site</font></code> value on a search head.
</p>
<a name="multisitecli"></a><h2> <a name="multisitecli_configure_multisite_indexer_clusters_with_the_cli"><span class="mw-headline" id="Configure_multisite_indexer_clusters_with_the_CLI"> Configure multisite indexer clusters with the CLI</span></a></h2>
<h3> <a name="multisitecli_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before reading this topic, see:
</p>
<ul><li> <a href="#multisitedeploymentoverview" class="external text">"Multisite indexer cluster deployment overview"</a>. This topic provides important background information about configuring a multisite cluster.
</li></ul><ul><li> <a href="#usethecli" class="external text">"Configure the indexer cluster with the CLI"</a>. This topic explains the basics of using the CLI to configure a cluster.  It focuses on single-site clusters, but most of its information is relevant to multisite clusters as well.
</li></ul><ul><li> <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>. This topic provides useful information on configuring a multisite cluster, including details on the attributes corresponding to the command-line options described in the current topic.
</li></ul><h3> <a name="multisitecli_configure_multisite_cluster_nodes"><span class="mw-headline" id="Configure_multisite_cluster_nodes"> Configure multisite cluster nodes</span></a></h3>
<p>You configure instances as multisite cluster nodes with the <code><font size="2">splunk edit cluster-config</font></code> command. After enabling an instance, you must restart it.
</p>
<h4><font size="3"><b><i> <a name="multisitecli_site_values"><span class="mw-headline" id="Site_values"> Site values</span></a></i></b></font></h4>
<p>Site values identify the site on which a node resides. You assign a site value to each node in a multisite cluster. 
</p><p>Site values have the syntax: 
</p>
<code><font size="2">site&lt;n&gt;<br></font></code>
<p>where &lt;n&gt; is an integer in the range of 1 to 63: site1, site2, site3, ....
</p>
<h4><font size="3"><b><i> <a name="multisitecli_configure_the_master_node"><span class="mw-headline" id="Configure_the_master_node">Configure the master node </span></a></i></b></font></h4>
<p>Here is an example of a multisite configuration for a master mode:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode master -multisite true -available_sites site1,site2 -site site1 -site_replication_factor origin:2,total:3 -site_search_factor origin:1,total:2<br><br>splunk restart<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is a cluster master node.
</li><li> the cluster is multisite.
</li><li> the cluster consists of two sites: site1 and site2.
</li><li> the master is located on site1. 
</li><li> the cluster's replication factor is the default "origin:2,total:3".
</li><li> the cluster's search factor is "origin:1,total:2".
</li></ul><p>Note the following:
</p>
<ul><li> Each cluster has only one master.
</li><li> You must set <code><font size="2">multisite</font></code> to <code><font size="2">true</font></code> for multisite cluster masters.
</li><li> You must list all cluster sites with the <code><font size="2">available_sites</font></code> attribute.
</li><li> You must set a <code><font size="2">site_replication_factor</font></code> and a <code><font size="2">site_search_factor</font></code>. For details, see <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a> and <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>.
</li></ul><p><b>Important:</b> When the master starts up for the first time, it blocks indexing on the peers until you enable and restart the full replication factor number of peers.  For example, given a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, site3:3, total:8", the master blocks indexing until there are at least eight peers in total across all sites, including at least one in site1, two in site2, and three in site3.
</p><p>Do not restart the master while it is waiting for the peers to join the cluster. If you do, you will need to restart the peers a second time.
</p><p><b>Note:</b> You do not need to restart the master if you later change its <code><font size="2">site</font></code> value.
</p>
<h4><font size="3"><b><i> <a name="multisitecli_configure_the_peer_nodes"><span class="mw-headline" id="Configure_the_peer_nodes">Configure the peer nodes </span></a></i></b></font></h4>
<p>To configure a peer node in a multisite cluster, you set a <code><font size="2">site</font></code> attribute. All other configuration settings are identical to a peer in a single-site cluster.
</p><p>Here is an example configuration for a multisite peer node:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode slave -site site1 -master_uri https://10.160.31.200:8089 -replication_port 9887 <br><br>splunk restart<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is a cluster peer ("slave") node.
</li><li> the instance is located in site1. A peer can belong to only a single site.
</li><li> the peer's cluster master is located at <code><font size="2">10.160.31.200:8089</font></code>.
</li><li> the peer will use port 9887 to listen for replicated data streamed from the other peers. You can specify any available, unused port as the replication port. Do not re-use the management or receiving ports.
</li></ul><p><b>Note:</b> You do not need to restart the peer if you later change its <code><font size="2">site</font></code> value.
</p>
<h4><font size="3"><b><i> <a name="multisitecli_configure_the_search_heads"><span class="mw-headline" id="Configure_the_search_heads"> Configure the search heads </span></a></i></b></font></h4>
<p>To configure a search head for a multisite cluster, set the <code><font size="2">site</font></code> parameter. All other settings are the same as for a search head in a single-site cluster. 
</p><p>You use different commands to configure a search head initially and to change its configuration later.
</p>
<h5> <a name="multisitecli_to_initially_configure_a_search_head:"><span class="mw-headline" id="To_initially_configure_a_search_head:">To initially configure a search head:</span></a></h5>
<p>Use the <code><font size="2">splunk edit cluster-config</font></code> command. Here is an example configuration for a multisite search head:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode searchhead -site site1 -master_uri https://10.160.31.200:8089 &nbsp;<br><br>splunk restart<br></font></code>
</div>
<p>This example specifies that:
</p>
<ul><li> the instance is a cluster search head.
</li><li> the search head is located in site1.  A search head belongs to exactly one site in each cluster.
</li><li> the search head's cluster master is located at <code><font size="2">10.160.31.200:8089</font></code>.
</li></ul><p><b>Note:</b> When you specify the <code><font size="2">site</font></code> parameter, the command automatically sets <code><font size="2">multisite=true</font></code> in the search head's <code><font size="2">server.conf</font></code> file. You do not need to explicitly pass a <code><font size="2">multisite</font></code> parameter.
</p>
<h5> <a name="multisitecli_to_edit_the_search_head_configuration_later:"><span class="mw-headline" id="To_edit_the_search_head_configuration_later:">To edit the search head configuration later:</span></a></h5>
<p>Use the <code><font size="2">splunk edit cluster-master</font></code> command, not the <code><font size="2">splunk edit cluster-config</font></code> command. 
</p><p>For example, assume you initially configure a single-site search head using the <code><font size="2">splunk edit cluster-config</font></code> command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode searchhead -master_uri https://10.160.31.200:8089 &nbsp;<br><br>splunk restart<br></font></code>
</div>
<p>To reconfigure the search head for a multisite cluster, use the <code><font size="2">splunk edit cluster-master</font></code> command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-master https://10.160.31.200:8089 &nbsp;-site site1 <br></font></code>
</div>
<p><b>Important:</b> The <code><font size="2">splunk edit cluster-master</font></code> command always takes the current master URI:port value as its initial parameter. For more examples, see <a href="#configuresearchheadwithcli" class="external text">"Configure the indexer cluster search head with the CLI"</a>.
</p><p>For information on configuring a multisite search head for multi-cluster search, see <a href="#configuremulti-clustersearch_configure_multi-cluster_search_for_multisite_clusters" class="external text">"Configure multi-cluster search for multisite clusters"</a>.
</p><p><b>Note:</b> You do not need to restart the search head if you later change its <code><font size="2">site</font></code> value.
</p>
<a name="sitereplicationfactor"></a><h2> <a name="sitereplicationfactor_configure_the_site_replication_factor"><span class="mw-headline" id="Configure_the_site_replication_factor"> Configure the site replication factor</span></a></h2>
<h3> <a name="sitereplicationfactor_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before attempting to configure the site replication factor, you must be familiar with:
</p>
<ul><li> The basic, single-site replication factor. See <a href="#basicclusterarchitecture" class="external text">"The basics of indexer cluster architecture"</a> and  <a href="#thereplicationfactor" class="external text">"Replication factor"</a>.
</li><li> Multisite cluster configurations. See <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</li></ul><h3> <a name="sitereplicationfactor_what_is_a_site_replication_factor.3f"><span class="mw-headline" id="What_is_a_site_replication_factor.3F"> What is a site replication factor? </span></a></h3>
<p>To implement multisite indexer clustering, you must configure a site replication factor. This replaces the standard replication factor, which is specific to single-site deployments. You specify the site replication factor on the master node, as part of the basic configuration of the cluster.
</p><p>The site replication factor provides site-level control over the location of bucket copies, in addition to providing control over the total number of copies across the entire cluster. For example, you can specify that a two-site cluster maintain a total of three copies of all buckets, with one site maintaining two copies and the  second site maintaining one copy. 
</p><p>You can also specify a replication policy based on which site originates the bucket. That is, you can configure the replication factor so that a site receiving external data maintains a greater number of copies of buckets for that data than for data that it does not originate. For example, you can specify that each site maintains two copies of all data that it originates but only one copy of data originating on another site.
</p>
<h3> <a name="sitereplicationfactor_syntax"><span class="mw-headline" id="Syntax">Syntax</span></a></h3>
<p>You configure the site replication factor with the <code><font size="2">site_replication_factor</font></code> attribute in the master's server.conf file. The attribute resides in the <code><font size="2">[clustering]</font></code> stanza, in place of the single-site <code><font size="2">replication_factor</font></code> attribute. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = master<br>multisite=true<br>available_sites=site1,site2<br>site_replication_factor = origin:2,total:3<br>site_search_factor = origin:1,total:2<br></font></code>
</div>
<p>You can also use the CLI to configure the site replication factor. See <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>.
</p><p><b>Warning:</b> You must configure the <code><font size="2">site_replication_factor</font></code> attribute correctly.  Otherwise, the master will not start.
</p><p>Here is the formal syntax:
</p>
<div class="samplecode">
<code><font size="2"><br>site_replication_factor = origin:&lt;n&gt;, [site1:&lt;n&gt;,] [site2:&lt;n&gt;,] ..., total:&lt;n&gt;<br></font></code>
</div>
<p>where:
</p>
<ul><li> <code><font size="2">&lt;n&gt;</font></code> is a positive integer indicating the number of copies of a bucket.
</li><li> <code><font size="2">origin:&lt;n&gt;</font></code> specifies the minimum number of copies of a bucket that will be held on the site originating the data in that bucket (that is, the site where the data first entered the cluster). When a site is originating the data, it is known as the "origin" site.
</li><li> <code><font size="2">site1:&lt;n&gt;, site2:&lt;n&gt;, ...,</font></code> indicates the minimum number of copies that will be held at each specified site. The identifiers "site1", "site2", and so on, are the same as the <code><font size="2">site</font></code> attribute values specified on the peer nodes. 
</li><li> <code><font size="2">total:&lt;n&gt;</font></code> specifies the total number of copies of each bucket, across all sites in the cluster.
</li></ul><p>Note the following:
</p>
<ul><li> This attribute specifies the per-site replication policy. It is specified globally and applies to all buckets in all indexes.
</li></ul><ul><li> This attribute is valid only if <code><font size="2">mode=master</font></code> and <code><font size="2">multisite=true</font></code>. Under those conditions, it supersedes any <code><font size="2">replication_factor</font></code> attribute.
</li></ul><ul><li> The <code><font size="2">origin</font></code> and <code><font size="2">total</font></code> values are required.
</li></ul><ul><li> Site values (<code><font size="2">site1:&lt;n&gt;, site2:&lt;n&gt;, ...</font></code>) are optional. A site that is specified here is known as an "explicit" site. A site that is not specified is known as a "non-explicit" site. 
</li></ul><ul><li> Here is how the cluster determines the minimum number of copies a site gets:<br><br><ul><li> When a site is functioning as origin, the minimum number of copies the site gets is the greater of either its site value, if any, or <code><font size="2">origin</font></code>.<br><br></li><li> When a site is not functioning as origin, the site value, if any, determines the minimum number of copies the site gets.<br><br></li><li>A non-explicit site is not guaranteed any copies except when it is functioning as the origin site.
</li></ul></li></ul><dl><dd> For example, in a four-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, site3:3, total:8", site1 gets two copies of any data that it originates and one copy of data that any other site originates.  Site2 gets two copies of data, whether or not it originates it. Site3 gets three copies of data, whether or not it originates it. The non-explicit site4 gets two copies of data that it originates, two copies of data that site2 or site3 originates, and one copy of data that site1 originates. (Site4 gets the number of copies necessary to ensure that the number of copies of each bucket meets the <code><font size="2">total</font></code> value of 8.) Here is how you calculate site4's number of copies, according to where the data originates:
</dd></dl><div class="samplecode">
<code><font size="2"><br>originate at site1 -&gt; 2 copies in site1, 2 copies in site2, 3 copies in site3, 1 copy in site4 =&gt; total=8<br>originate at site2 -&gt; 1 copy in site1, 2 copies in site2, 3 copies in site3, 2 copies in site4 =&gt; total=8<br>originate at site3 -&gt; 1 copy in site1, 2 copies in site2, 3 copies in site3, 2 copies in site4 =&gt; total=8<br>originate at site4 -&gt; 1 copy in site1, 2 copies in site2, 3 copies in site3, 2 copies in site4 =&gt; total=8<br></font></code>
</div>
<ul><li> When specifying the <code><font size="2">site_replication_factor</font></code>, here is how you determine the minimum required value for <code><font size="2">total</font></code>, based on the site and <code><font size="2">origin</font></code> values:<br><br><ul><li> If there are some non-explicit sites, then the <code><font size="2">total</font></code> value must be at least the sum of all explicit site and <code><font size="2">origin</font></code> values.<br><br>For example, with a three-site cluster and "site_replication_factor = origin:2, site1:1, site2:2", the <code><font size="2">total</font></code> must be at least 5: 2+1+2=5. For another three-site cluster with "site_replication_factor = origin:2, site1:1, site3:3", the <code><font size="2">total</font></code> must be at least 6: 2+1+3=6.<br><br></li><li> If all sites are explicit, then the <code><font size="2">total</font></code> must be at least the minimum value necessary to fulfill the dictates of the site and <code><font size="2">origin</font></code> values.<br><br>For example, with a three-site cluster and "site_replication_factor = origin:1, site1:1, site2:1, site3:1", the <code><font size="2">total</font></code> must be at least 3, because that configuration never requires more than three copies. For a three-site cluster and "site_replication_factor = origin:2, site1:1, site2:1, site3:1", the <code><font size="2">total</font></code> must be at least 4, because one of the sites will always be the origin and thus require two copies, while the other sites will each require only one. For a three-site cluster and "site_replication_factor = origin:3, site1:1, site2:2", site3:3", the <code><font size="2">total</font></code> must be at least 8, to cover the case where site1 is the origin.<br><br>The easiest way to determine this is to substitute the origin value for the smallest site value and then sum the site values (including the substituted origin value). So, in the last example ("site_replication_factor = origin:3, site1:1, site2:2", site3:3"), site1 has the smallest value, which is 1. You substitute the origin's 3 for that 1 and then add the site2 and site3 values: 3+2+3=8.
</li></ul></li></ul><ul><li> Because the <code><font size="2">total</font></code> value can be greater than the total set of explicit values, the cluster needs a strategy to handle any "remainder" bucket copies. Here is the strategy:<br><br><ul><li> If copies remain to be assigned after all site and origin values have been satisfied, those remainder copies are distributed across all sites, with preference given to sites with less or no copies, so that the distribution is as even as possible. Assuming that there are enough remainder copies available, each site will have at least one copy of the bucket.<br><br>For example, given a four-site cluster with "site_replication_factor = origin:1, site1:1, site4:2, total:4", if site1 is the origin, there will be one remainder copy. This copy gets distributed randomly to either site2 or site3. However, if site2 is the origin, it gets one copy, leaving no remainder copies to distribute to site3.<br><br>In another example, given a four-site cluster where "site_replication_factor = origin:2, site1:2, site4:2, total:7", if site1 is the origin, there are three remainder copies to distribute. Because site2 and site3 have no explicitly assigned copies, the three copies are distributed between them, with each site getting at least one copy. If site2 is the origin, however, it gets two copies and site3 gets the one remainder copy.<br><br>This entire process depends on the availability of a sufficient number of peers on each site. If a site does not have enough peers available to accept additional copies, the copies go to sites with available peers. In any case, at least one copy will be distributed or reserved for each site, assuming enough copies are available.<br><br>Here are a few more examples:<br><br><ul><li> A three-site cluster with "origin:1, total:3": The distribution guarantees one copy per site.<br><br></li><li> A three-site cluster with "origin:1, total:4": The distribution guarantees one copy per site, with one additional copy going to a site that has at least two peers.<br><br></li><li> A three-site cluster with "origin:1, total:9", where site1 has only one peer and site2 and site3 have 10 peers each: The distribution guarantees one copy per site, with the six remaining copies distributed evenly between site2 and site3.<br><br></li></ul></li><li> If all peers on one non-explicit site are down and there are still remainder copies after all other non-explicit sites have received a copy, the cluster will reserve one of the remainder copies for that site, pending the return of its peers. During that time, the <code><font size="2">site_replication_factor</font></code> cannot be met, because the total number of copies distributed will be one less than the specified <code><font size="2">total</font></code> value, due to the copy that is being held in reserve for the site with the downed peers.<br><br>For example, given a four-site cluster with "site_replication_factor = origin:1, site1:1, site4:2, total:5", if site1 is the origin, there will be two remainder copies to distribute between site2 and site3. If all the peers on site2 are down, one remainder copy goes to site3 and the other copy will be held in reserve until one of the site2 peers rejoins the cluster. During that time, the <code><font size="2">site_replication_factor</font></code> is not met. However, given a four-site cluster with "site_replication_factor = origin:1, site1:1, site4:2, total:4" (the only difference from the last example being that the <code><font size="2">total</font></code> value is 4 instead of 5), if site1 is the origin, there will be only one remainder copy, which will go to either site2 or site3. If all the peers on site2 are down, the copy will go to site3 and no copy will be held in reserve for site2. The <code><font size="2">site_replication_factor</font></code> is met in this example, because no copies are being held in reserve for site2.
</li></ul></li></ul><ul><li> Each site must deploy a set of peers at least as large as the greater of the origin value or its site value.<br><br>For example, given a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, site3:3, total:8", the sites must have at least the following number of peers: site1: 2 peers; site2: 2 peers; site3: 3 peers.
</li></ul><ul><li> The total number of peers deployed across all sites must be greater than or equal to the <code><font size="2">total</font></code> value.
</li></ul><ul><li> If you are migrating from a single-site cluster, the <code><font size="2">total</font></code> value must be at least as large as the <code><font size="2">replication_factor</font></code> for the single-site cluster. See <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</li></ul><ul><li> The attribute defaults to: "origin:2, total:3."
</li></ul><h3> <a name="sitereplicationfactor_examples"><span class="mw-headline" id="Examples">Examples</span></a></h3>
<ul><li> <b>A cluster of two sites (site1, site2), with the default "site_replication_factor = origin:2, total:3":</b>  For any given bucket, the site originating the data stores two copies. The remaining site stores one copy.
</li></ul><ul><li> <b>A cluster of three sites (site1, site2, site3), with the default "site_replication_factor = origin:2, total:3":</b>  For any given bucket, the site originating the data stores two copies. One of the two non-originating sites, selected at random, stores one copy, and the other doesn't store any copies. 
</li></ul><ul><li> <b>A cluster of three sites (site1, site2, site3), with "site_replication_factor = origin:1, site1:1, site2:1, site3:2, total:5":</b>  For all buckets, site1 and site2 store a minimum of one copy each and site3 stores two copies. The fifth copy gets distributed to either site1 or site2, because those sites have fewer assigned copies than site3.
</li></ul><ul><li> <b>A cluster of three sites (site1, site2, site3), with "site_replication_factor = origin:2, site1:1, site2:1, total:4":</b>  Site1 stores two copies of any bucket it is originating and one copy of any other bucket. Site2 follows the same pattern. Site3, whose site value is not explicitly defined, follows the same pattern.
</li></ul><ul><li> <b>A cluster of three sites (site1, site2, site3), with "site_replication_factor = origin:2, site1:1, site2:2, total:5":</b>  Site1 stores two copies of any bucket it originates, one or two copies of any bucket site2 originates, and one copy of any bucket that site3 originates. Site2 stores two copies of any bucket, whether or not it originates it. Site3, whose site value is not explicitly defined, stores two copies of any bucket it originates, one copy of any bucket site1 originates, and one or two copies of any bucket site2 originates. (When site2 originates a bucket, one copy remains after initial assignments. The master assigns this randomly to site1 or site3.)
</li></ul><ul><li> <b>A cluster of three sites with "site_replication_factor = origin:1, total:4":</b> Four copies of each bucket are distributed randomly across all sites, with each site getting at least one copy.
</li></ul><a name="sitesearchfactor"></a><h2> <a name="sitesearchfactor_configure_the_site_search_factor"><span class="mw-headline" id="Configure_the_site_search_factor"> Configure the site search factor</span></a></h2>
<h3> <a name="sitesearchfactor_read_this_first"><span class="mw-headline" id="Read_this_first"> Read this first </span></a></h3>
<p>Before attempting to configure the site search factor, you must be familiar with:
</p>
<ul><li> The basic, single-site search factor. See <a href="#basicclusterarchitecture" class="external text">"The basics of cluster architecture"</a> and <a href="#thesearchfactor" class="external text">"Search factor"</a>.
</li><li> The site replication factor. See <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</li><li> Multisite cluster configurations. See <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>.
</li></ul><h3> <a name="sitesearchfactor_what_is_a_site_search_factor.3f"><span class="mw-headline" id="What_is_a_site_search_factor.3F"> What is a site search factor? </span></a></h3>
<p>To implement multisite indexer clustering, you must configure a site search factor. This replaces the standard search factor, which is specific to single-site deployments. You specify the site search factor on the master node, as part of the basic configuration of the cluster.
</p><p>The site search factor provides site-level control over the location of searchable bucket copies, in addition to providing control over the total number of searchable copies across the entire cluster. For example, you can specify that a two-site cluster maintain a total of three searchable copies of all buckets, with one site maintaining two copies and the second site maintaining one copy. 
</p><p>You can also specify a search policy based on which site originates the bucket. That is, you can configure the search factor so that a site receiving external data maintains a greater number of searchable copies of buckets for that data than for data that it does not originate. For example, you can specify that each site maintains two searchable copies of all data that it originates but only one copy of data originating on another site.
</p><p>The site search factor helps determine whether the cluster has search affinity. See <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster"</a>.
</p>
<h3> <a name="sitesearchfactor_syntax"><span class="mw-headline" id="Syntax">Syntax</span></a></h3>
<p>The syntax for <code><font size="2">site_search_factor</font></code> and <code><font size="2">site_replication_factor</font></code> are identical, except for the additional requirement that the values and explicit sites in <code><font size="2">site_search_factor</font></code> be a subset of those in <code><font size="2">site_replication_factor</font></code>.  This section describes the syntax in full detail.
</p><p>You configure the site search factor with the <code><font size="2">site_search_factor</font></code> attribute in the master's server.conf file. The attribute resides in the <code><font size="2">[clustering]</font></code> stanza, in place of the single-site <code><font size="2">search_factor</font></code> attribute. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>[clustering]<br>mode = master<br>multisite=true<br>available_sites=site1,site2<br>site_replication_factor = origin:2,total:3<br>site_search_factor = origin:1,total:2<br></font></code>
</div>
<p>You can also use the CLI to configure the site search factor. See <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>.
</p><p><b>Warning:</b> You must configure the <code><font size="2">site_search_factor</font></code> attribute correctly.  Otherwise, the master will not start.
</p><p>Here is the formal syntax:
</p>
<div class="samplecode">
<code><font size="2"><br>site_search_factor = origin:&lt;n&gt;, [site1:&lt;n&gt;,] [site2:&lt;n&gt;,] ..., total:&lt;n&gt;<br></font></code>
</div>
<p>where:
</p>
<ul><li> <code><font size="2">&lt;n&gt;</font></code> is a positive integer indicating the number of searchable copies of a bucket.
</li><li> <code><font size="2">origin:&lt;n&gt;</font></code> specifies the minimum number of searchable copies of a bucket that will be held on the site originating the data in that bucket (that is, the site where the data first entered the cluster). When a site is originating the data, it is known as the "origin" site.
</li><li> <code><font size="2">site1:&lt;n&gt;, site2:&lt;n&gt;, ...,</font></code> indicates the minimum number of searchable copies that will be held at each specified site. The identifiers "site1", "site2", and so on, are the same as the <code><font size="2">site</font></code> attribute values specified on the peer nodes. 
</li><li> <code><font size="2">total:&lt;n&gt;</font></code> specifies the total number of searchable copies of each bucket, across all sites in the cluster.
</li></ul><p>Note the following:
</p>
<ul><li> This attribute specifies the per-site searchable copy policy. It is specified globally and applies to all buckets in all indexes.
</li></ul><ul><li> This attribute is valid only if <code><font size="2">mode=master</font></code> and <code><font size="2">multisite=true</font></code>. Under those conditions, it supersedes any <code><font size="2">search_factor</font></code> attribute.
</li></ul><ul><li> The <code><font size="2">origin</font></code> and <code><font size="2">total</font></code> values are required.
</li></ul><ul><li> Site values (<code><font size="2">site1:&lt;n&gt;, site2:&lt;n&gt;, ...</font></code>) are optional. A site that is specified here is known as an "explicit" site. A site that is not specified is known as a "non-explicit" site. 
</li></ul><ul><li> To determine the minimum number of searchable copies a site gets, use the same rules as for determining the minimum number of replicated copies a site gets through <code><font size="2">site_replication_factor</font></code>. See <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</li></ul><ul><li> To determine the minimum required value for <code><font size="2">total</font></code>, use the same rules as for determining the minimum <code><font size="2">total</font></code> value for the <code><font size="2">site_replication_factor</font></code>. See <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</li></ul><ul><li> Because the <code><font size="2">total</font></code> value can be greater than the total set of explicit values, the cluster needs a strategy to handle any "remainder" searchable bucket copies. The strategy follows the strategy for remainder replicated copies, described in <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</li></ul><ul><li> All values must be less than or equal to their corresponding values in the <code><font size="2">site_replication_factor</font></code>.<br><br>For example, if you have a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, total:5", then, in  <code><font size="2">site_search_factor</font></code>, the <code><font size="2">origin</font></code> value cannot exceed 2, the <code><font size="2">site1</font></code> value cannot exceed 1, the <code><font size="2">site2</font></code> value cannot exceed 2, and the <code><font size="2">total</font></code> value cannot exceed 5. 
</li></ul><ul><li> If a site value is explicit in <code><font size="2">site_search_factor</font></code>, it must also be explicit in <code><font size="2">site_replication_factor</font></code>. However, an explicit site value in <code><font size="2">site_replication_factor</font></code> does not need be explicit in <code><font size="2">site_search_factor</font></code>.<br><br>For example, if you have a three-site cluster with "site_replication_factor = origin:2, site1:1, site2:2, total:5" (with a non-explicit site3), you can specify "site_search_factor = origin:1, site2:2, total:4" (removing the explicit site1), but you cannot specify "site_search_factor = origin:1, site1:1, site2:2, site3:1, total:4" (making the non-explicit site3 explicit).
</li></ul><ul><li> For search affinity, you must configure the <code><font size="2">site_search_factor</font></code> so that you have at least one searchable copy on each site where you require search affinity. Only explicit sites adhere to search affinity.
</li></ul><ul><li> If you are migrating from a single-site cluster, the <code><font size="2">total</font></code> value must be at least as large as the <code><font size="2">search_factor</font></code> for the single-site cluster. See <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</li></ul><ul><li> The attribute defaults to: "origin:1, total:2."
</li></ul><h3> <a name="sitesearchfactor_examples"><span class="mw-headline" id="Examples">Examples</span></a></h3>
<p>For examples of site search factor syntax, refer to the examples in <a href="#sitereplicationfactor_examples" class="external text">"Configure the site replication factor"</a>. The syntax for specifying origin/site/total values in <code><font size="2">site_search_factor</font></code> is identical to <code><font size="2">site_replication_factor</font></code>.
</p>
<a name="migratetomultisite"></a><h2> <a name="migratetomultisite_migrate_an_indexer_cluster_from_single-site_to_multisite"><span class="mw-headline" id="Migrate_an_indexer_cluster_from_single-site_to_multisite"> Migrate an indexer cluster from single-site to multisite</span></a></h2>
<p>You can migrate an indexer cluster from single-site to multisite. After the migration, the cluster holds both single-site and multi-site buckets. It maintains them separately, following these rules:
</p>
<ul><li> Single-site buckets (those existing at the time of migration) continue to respect their single-site replication and search factors. You cannot convert them to multisite.
</li><li> Multisite buckets (those created after migration) follow the multisite replication and search factor policies. 
</li></ul><h3> <a name="migratetomultisite_perform_the_multisite_migration"><span class="mw-headline" id="Perform_the_multisite_migration"> Perform the multisite migration </span></a></h3>
<p><b>Important:</b> The migration process does not alter the version of Splunk Enterprise that the instances are running on. To migrate to a multisite cluster, the instances must be running version 6.1 or higher. Therefore, before migrating to multisite, you might need to upgrade your single-site cluster. Follow the appropriate procedure in <a href="#upgradeacluster" class="external text">"Upgrade an indexer cluster"</a>.
</p><p>To migrate a single-site cluster to multisite, configure each node for multisite:
</p><p><b>1.</b> Configure the master node for multisite and restart it, following the instructions in <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>. For example: 
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode master -multisite true -available_sites site1,site2 -site site1 -site_replication_factor origin:2,total:3 -site_search_factor origin:1,total:2<br><br>splunk restart<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> Do not remove the existing single-site attributes for replication factor and search factor, <code><font size="2">replication_factor</font></code> and <code><font size="2">search_factor</font></code>. The master needs them to handle the migrated buckets.
</li></ul><ul><li> The <code><font size="2">total</font></code> values for <code><font size="2">site_replication_factor</font></code> and <code><font size="2">site_search_factor</font></code> must be at least as large as <code><font size="2">replication_factor</font></code> and <code><font size="2">search_factor</font></code>, respectively.
</li></ul><ul><li> If the number of peers on any site is less than the single-site <code><font size="2">replication_factor</font></code> or <code><font size="2">search_factor</font></code>, you must reduce the values of those attributes to match the least number of peers on any site. For example, if <code><font size="2">replication_factor</font></code> is 3 and <code><font size="2">search_factor</font></code> is 2, and one of the sites has only 2 peers, you must change <code><font size="2">replication_factor</font></code> to 2. Otherwise, the migrated buckets might not meet the replication and search factors, due to the way the cluster replicates migrated buckets. See <a href="#bucketreplicationissues_multisite_cluster_does_not_meet_its_replication_or_search_factors" class="external text">"Multisite cluster does not meet its replication or search factors."</a>
</li></ul><p><b>2.</b> Set maintenance mode on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk enable maintenance-mode<br></font></code>
</div>
<p>This step prevents unnecessary bucket fix-ups. See <a href="#usemaintenancemode" class="external text">"Use maintenance mode"</a>.
</p><p>To confirm that the master has entered maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. 
</p><p><b>3.</b> Configure the existing peer nodes for multisite.  For each peer, specify its master node and site.  For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -site site1 <br></font></code>
</div>
<p>You will be prompted to restart the peer.
</p><p>Do this for each peer, specifying the site for that peer.
</p><p><b>4.</b> If you want to add new peers to the cluster, follow the instructions in <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode slave -site site1 -master_uri https://10.160.31.200:8089 -replication_port 9887 <br><br>splunk restart<br></font></code>
</div>
<p>Do this for each new peer that you want to add to the cluster.
</p><p><b>5.</b> Configure the search heads for multisite. For each search head, specify its master node and site. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-master https://10.160.31.200:8089 -site site1 <br></font></code>
</div>
<p>Do this for each search head, specifying the site for that search head.
</p><p><b>6.</b> If you want to add new search heads to the cluster, follow the instructions in <a href="#multisitecli" class="external text">"Configure multisite indexer clusters with the CLI"</a>. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -mode searchhead -site site1 -master_uri https://10.160.31.200:8089 &nbsp;<br><br>splunk restart<br></font></code>
</div>
<p>Do this for each new search head that you want to add to the cluster.
</p><p><b>7.</b> Disable maintenance mode on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk disable maintenance-mode<br></font></code>
</div>
<p>To confirm that the master has left maintenance mode, run <code><font size="2">splunk show maintenance-mode</font></code>. 
</p><p>You can view the master dashboard to verify that all cluster nodes are up and running.
</p><p>During the migration, the cluster tags each single-site bucket with a site value.
</p><p><b>Note:</b> You can also configure a multisite cluster by directly editing <code><font size="2">server.conf</font></code>.  See <a href="#multisiteconffile" class="external text">"Configure multisite indexer clusters with server.conf"</a>
</p>
<h3> <a name="migratetomultisite_how_the_cluster_migrates_and_maintains_single-site_buckets"><span class="mw-headline" id="How_the_cluster_migrates_and_maintains_single-site_buckets"> How the cluster migrates and maintains single-site buckets</span></a></h3>
<p>Buckets in multisite clusters include a property that identifies the origin site. Buckets in single-site clusters do not include that property. So, when a cluster migrates from single-site to multisite, it must tag each single-site bucket with an origin site value. Since the bucket name includes the GUID of the originating peer, the cluster always knows the originating peer. With that information, it infers an origin site for the bucket:
</p>
<ul><li> If the originating peer still exists in the cluster, the cluster assumes that the bucket originated on the site that the originating peer has been assigned to. It sets the bucket's origin to that site.
</li><li> If the originating peer is no longer in the cluster, the cluster assumes that the site with the most copies of the bucket is the origin site. It sets the bucket's origin to that site.
</li></ul><p>Here is how the cluster uses the inferred origin site to maintain the single-site bucket going forward, to handle any necessary fix-up so that the bucket continues to meet the single-site replication and search factors:
</p>
<ul><li> If the cluster needs to replicate additional copies of the bucket to fulfill the replication factor, it only replicates within the bucket's inferred origin site.
</li><li> If the cluster needs to make a non-searchable copy of the bucket searchable to fulfill the search factor, it might do so on a non-origin site, if a non-searchable copy of that bucket already exists on some other site.
</li></ul><p>The cluster will never create a new copy of the bucket on a non-origin site.
</p>
<h1>View indexer cluster status</h1><a name="howtomonitoracluster"></a><h2> <a name="howtomonitoracluster_view_the_master_dashboard"><span class="mw-headline" id="View_the_master_dashboard">View the master dashboard </span></a></h2>
<p>This dashboard provides detailed information on the status of the entire indexer cluster. You can also get information on each of the master's peer nodes from here. 
</p><p>For information on the other clustering dashboards, read:
</p>
<ul><li> <a href="#viewthepeerdashboard" class="external text">"View the peer dashboard"</a> 
</li><li> <a href="#viewthesearchheaddashboard" class="external text">"View the search head dashboard"</a> 
</li></ul><h3> <a name="howtomonitoracluster_access_the_master_dashboard"><span class="mw-headline" id="Access_the_master_dashboard"> Access the master dashboard</span></a></h3>
<p>To view the dashboard for the master node:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p>You can only view this dashboard on an instance that has been enabled as a master.
</p>
<h3> <a name="howtomonitoracluster_view_the_master_dashboard_2"><span class="mw-headline" id="View_the_master_dashboard_2"> View the master dashboard</span></a></h3>
<p>The master dashboard contains these sections:
</p>
<ul><li> <a href="#howtomonitoracluster_cluster_overview" class="external text">Cluster overview</a> 
</li><li> <a href="#howtomonitoracluster_peers_tab" class="external text">Peers tab</a> 
</li><li> <a href="#howtomonitoracluster_indexes_tab" class="external text">Indexes tab</a> 
</li><li> <a href="#howtomonitoracluster_search_heads_tab" class="external text">Search Heads tab</a> 
</li></ul><p>Here is how the dashboard initially appears, with the <b>Peers</b> tab open:
</p><p><img alt="Master dashboard 6.1.png" src="images/8/8d/Master_dashboard_6.1.png" width="700" height="342"></p>
<h4><font size="3"><b><i> <a name="howtomonitoracluster_cluster_overview"><span class="mw-headline" id="Cluster_overview">Cluster overview</span></a></i></b></font></h4>
<p>The cluster overview summarizes the health of your cluster. It tells you:
</p>
<ul><li> whether the cluster's data is fully searchable; that is, whether all buckets in the cluster have a <b>primary</b> copy.
</li><li> whether the search and replication factors have been met.
</li><li> how many peers are searchable.
</li><li> how many indexes are searchable.
</li></ul><p>Depending on the health of your cluster, it might also provide warning messages such as:
</p>
<ul><li> Some data is not searchable.
</li><li> Replication factor not met.
</li><li> Search factor not met.
</li></ul><p>For details on the information presented in the cluster overview, browse the tabs underneath. 
</p><p>On the upper right side of the dashboard, there are three buttons:
</p>
<ul><li> <b>Edit.</b> For information on this button, see <a href="#configuremasterwithdashboard" class="external text">"Configure the master with the dashboard."</a>. This button is disabled for multisite clusters.
</li></ul><ul><li> <b>More Info.</b> This button provides details on the master node configuration:
<ul><li> <b>Name</b>. The master's <code><font size="2">serverName</font></code>, as specified in the master's <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> file.
</li><li> <b>Replication Factor.</b> The cluster's <b>replication factor</b>.
</li><li> <b>Search Factor.</b> The cluster's <b>search factor</b>.
</li><li> <b>Generation ID.</b> The cluster's current <b>generation ID</b>.
</li></ul></li><li> <b>Documentation.</b>
</li></ul><h4><font size="3"><b><i> <a name="howtomonitoracluster_peers_tab"><span class="mw-headline" id="Peers_tab">Peers tab</span></a></i></b></font></h4>
<p>For each peer, the master dashboard lists:
</p>
<ul><li> <b>Peer Name.</b> The peer's <code><font size="2">serverName</font></code>, as specified in the peer's <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> file.
</li><li> <b>Fully searchable.</b> This column indicates whether the peer currently has a complete set of primaries and is fully searchable.
</li><li> <b>Site.</b> (For multisite only.) This column displays the site value for each peer.
</li><li> <b>Status.</b> The peer's status. For more information about the processes discussed here, read <a href="#takeapeeroffline" class="external text">"Take a peer offline"</a>. Possible values include:
<ul><li> <code><font size="2">Up</font></code>
</li><li> <code><font size="2">Pending</font></code>. This occurs when the master does not receive three consecutive heartbeats from the peer. This is a temporary state, and a peer rarely stays in it for long.
</li><li> <code><font size="2">Detention</font></code>. A peer goes into detention when it hits resource constraints (for example, it runs out of disk space). While in detention, a peer does not accept inputs but it can still participate in searches.
</li><li> <code><font size="2">Restarting</font></code>.  When you run the <code><font size="2">offline</font></code> command, the peer enters this state temporarily after it leaves the <code><font size="2">ReassigningPrimaries</font></code> state. It remains in this state for the <code><font size="2">restart_timeout</font></code> period (60 seconds by default). If you do not restart the peer within this time, it then moves to the <code><font size="2">Down</font></code> state. The peer also enters this state during rolling restarts or if restarted via Splunk Web.
</li><li> <code><font size="2">ShuttingDown</font></code>. The master detects that the peer is shutting down.
</li><li> <code><font size="2">ReassigningPrimaries</font></code>. A peer enters this state temporarily when you run the <code><font size="2">offline</font></code> command. 
</li><li> <code><font size="2">Down</font></code>. The peer enters this state when it goes offline: either you ran the version of the <code><font size="2">offline</font></code> command and the peer shut down for longer than the <code><font size="2">restart_timeout</font></code> period (60 seconds by default), or the peer went offline for some other reason (for instance, it crashed).
</li></ul></li><li> <b>Buckets.</b> The number of buckets for which the peer has copies.
</li></ul><p>To get more information for any peer, click on the arrow to the left of the peer name. These fields appear:
</p>
<ul><li> <b>Location.</b> The peer's IP address and port number.
</li><li> <b>Last Heartbeat.</b> The time of the last heartbeat the master received from the peer.
</li><li> <b>Replication port.</b> The port on which the peer receives replicated data from other peers. 
</li><li> <b>Base generation ID.</b> The peer's base generation ID, which is equivalent to the cluster's generation ID at the moment that the peer last joined the cluster. This ID will be less or equal to the cluster's current generation ID. So, if a peer joined the cluster at generation 1 and has stayed in the cluster ever since, its base generation ID remains 1, even though the cluster might have incremented its current generation ID to, say, 5.
</li><li> <b>GUID.</b> The peer's GUID.
</li></ul><p><b>Note:</b> After a peer goes down, it continues to appear on the list of peers, although its status changes to "Down" or "GracefulShutdown." To remove the peer from the master's list, see <a href="#removepeerfrommasterlist" class="external text">"Remove a peer from the master's list."</a>
</p>
<h4><font size="3"><b><i> <a name="howtomonitoracluster_indexes_tab"><span class="mw-headline" id="Indexes_tab">Indexes tab</span></a></i></b></font></h4>
<p>For each index, the master dashboard lists:
</p>
<ul><li> <b>Index Name.</b> The name of the index. Internal indexes are preceded by an underscore (_).
</li><li> <b>Fully searchable.</b> Is the index fully searchable? In other words, does it have at least one searchable copy of each bucket? If even one bucket in the index does not have a searchable copy, this field will report the index as non-searchable. 
</li><li> <b>Searchable Data Copies.</b> The number of complete searchable copies of the index that the cluster has. 
</li><li> <b>Replicated Data Copies.</b> The number of copies of the index that the cluster has. Each copy must be complete, with no buckets missing. 
</li><li> <b>Buckets.</b> The number of buckets in the index.
</li><li> <b>Cumulative Raw Data Size.</b> The size of the index, excluding hot buckets.
</li></ul><p>The list of indexes include the internal indexes, <b>_audit</b> and <b>_internal</b>. As you would expect in a cluster, these internal indexes contain the combined data generated by <i>all</i> peers in the cluster. If you need to search for the data generated by a single peer, you can search on the peer's host name.
</p><p>This tab also reveals a button with the label, <b>Bucket Status.</b> If you click on it, you go to the Bucket Status dashboard. See <a href="#howtomonitoracluster_bucket_status_dashboard" class="external text">"Bucket status dashboard."</a>.
</p><p><b>Note:</b> A new index appears here only after it contains some data. In other words, if you configure a new index on the peer nodes, a row for that index appears only after you send data to that index.
</p>
<h4><font size="3"><b><i> <a name="howtomonitoracluster_search_heads_tab"><span class="mw-headline" id="Search_heads_tab">Search heads tab</span></a></i></b></font></h4>
<p>For each search head accessing this cluster, the master dashboard lists:
</p>
<ul><li> <b>Search head name.</b> The search head's <code><font size="2">serverName</font></code>, as specified in its <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> file.
</li><li> <b>Site.</b> (For multisite only.) This column displays the site value for each search head.
</li><li> <b>Status.</b> Is the search head up?
</li></ul><p><b>Note:</b> The list includes the master node as one of the search heads. Although the master has search head capabilities, you should only use those capabilities for debugging purposes. The resources of the master must be dedicated to fulfilling its critical role of coordinating cluster activities. Under no circumstances should the master be employed as a production search head. Also, unlike a dedicated search head, the search head on the master cannot be configured for multi-cluster search; it can only search its own cluster.
</p><p>To get more information for any search head, click on the arrow to the left of the search head name. These fields appear:
</p>
<ul><li> <b>Location.</b> The search head's server name and port number.
</li><li> <b>GUID.</b> The search head's GUID.
</li></ul><h3> <a name="howtomonitoracluster_view_the_bucket_status_dashboard"><span class="mw-headline" id="View_the_bucket_status_dashboard">View the bucket status dashboard</span></a></h3>
<p>The Bucket Status dashboard provides status for the buckets in the cluster. It contains three tabs:
</p>
<ul><li> Fixup Tasks - In Progress
</li><li> Fixup Tasks - Pending
</li><li> Indexes with Excess Buckets
</li></ul><h4><font size="3"><b><i> <a name="howtomonitoracluster_fixup_tasks_-_in_progress"><span class="mw-headline" id="Fixup_Tasks_-_In_Progress">Fixup Tasks - In Progress</span></a></i></b></font></h4>
<p>This tab provides a list of buckets that are currently being fixed. For example, if a bucket has too few copies, fixup activities must occur to return the cluster to a <b>valid</b> and <b>complete</b> state. While those activities are occurring, the bucket appears on this list.
</p>
<h4><font size="3"><b><i> <a name="howtomonitoracluster_fixup_tasks_-_pending"><span class="mw-headline" id="Fixup_Tasks_-_Pending">Fixup Tasks - Pending</span></a></i></b></font></h4>
<p>This tab provides a list of buckets that are waiting to be fixed. You can filter the fixup tasks by search factor, replication factor, and generation.
</p><p>For more information on bucket fixup activities, see <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer goes down."</a>
</p>
<h4><font size="3"><b><i> <a name="howtomonitoracluster_indexes_with_excess_buckets"><span class="mw-headline" id="Indexes_with_Excess_Buckets">Indexes with Excess Buckets</span></a></i></b></font></h4>
<p>This tab provides a list of indexes with excess bucket copies.  It enumerates both buckets with excess copies and buckets with excess searchable copies. It also enumerates the total excess copies in each category. For example, if your index "new" has one bucket with three excess copies, one of which is searchable, and a second bucket with one excess copy, which is non-searchable, the row for "new" will report:
</p>
<ul><li> 2 buckets with excess copies
</li><li> 1 bucket with excess searchable copies
</li><li> 4 total excess copies
</li><li> 1 total excess searchable copies
</li></ul><p>If you want to remove the excess copies for a single index, click the <b>Remove</b> button on the right side of the row for that index.
</p><p>If you want to remove the excess copies for all indexes, click the <b>Remove All Excess Buckets</b> button.
</p><p>For more information on excess bucket copies, see <a href="#removeextrabucketcopies" class="external text">"Remove excess bucket copies from the indexer cluster."</a>
</p>
<a name="viewthepeerdashboard"></a><h2> <a name="viewthepeerdashboard_view_the_peer_dashboard"><span class="mw-headline" id="View_the_peer_dashboard"> View the peer dashboard</span></a></h2>
<p>The indexer cluster peer dashboard provides detailed information on the status of a single peer node. 
</p><p>For a single view with information on all the peers in a cluster, use the master node dashboard instead, as described in <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p>
<h3> <a name="viewthepeerdashboard_access_the_peer_dashboard"><span class="mw-headline" id="Access_the_peer_dashboard"> Access the peer dashboard </span></a></h3>
<p>To view the peer dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p>You can only view this dashboard on an instance that has been enabled as a peer.
</p>
<h3> <a name="viewthepeerdashboard_view_the_dashboard"><span class="mw-headline" id="View_the_dashboard"> View the dashboard</span></a></h3>
<p>Here is how the dashboard looks:
</p><p><img alt="Peer dashboard 6.1.png" src="images/4/49/Peer_dashboard_6.1.png" width="700" height="152"></p><p>The dashboard provides information on the peer's status:
</p>
<ul><li> <b>Name</b>. The peer's <code><font size="2">serverName</font></code>, as specified in its <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> file.
</li><li> <b>Replication port.</b> The port on which the peer receives replicated data from other peers. 
</li><li> <b>Master location.</b>  The master node's IP address and port number.
</li><li> <b>Base Generation ID.</b> The peer's base generation ID, which is equivalent to the cluster's <b>generation ID</b> at the moment that the peer last joined the cluster. This ID will be less or equal to the cluster's current generation ID. So, if a peer joined the cluster at generation 1 and has stayed in the cluster ever since, its base generation ID remains 1, even though the cluster might have incremented its current generation ID to, say, 5.
</li></ul><h3> <a name="viewthepeerdashboard_configure_the_peer"><span class="mw-headline" id="Configure_the_peer">Configure the peer</span></a></h3>
<p>The <b>Edit</b> button at the top right of the peer dashboard offers several options for changing the configuration of the peer. See <a href="#configurepeerswithdashboard" class="external text">"Configure peer nodes with the dashboard."</a>
</p><p><b>Note:</b> The <b>Edit</b> button is disabled for multisite clusters.
</p>
<a name="viewthesearchheaddashboard"></a><h2> <a name="viewthesearchheaddashboard_view_the_search_head_dashboard"><span class="mw-headline" id="View_the_search_head_dashboard"> View the search head dashboard</span></a></h2>
<p>This dashboard provides detailed information on the status of the search head in an indexer cluster. 
</p>
<h3> <a name="viewthesearchheaddashboard_access_the_dashboard"><span class="mw-headline" id="Access_the_dashboard">Access the dashboard</span></a></h3>
<p>To access the dashboard:
</p><p><b>1.</b> Click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p>You can only view this dashboard on an instance that has been enabled as a cluster search head.
</p>
<h3> <a name="viewthesearchheaddashboard_view_the_dashboard"><span class="mw-headline" id="View_the_dashboard">View the dashboard</span></a></h3>
<p>Here is how the search head dashboard looks:
</p><p><img alt="Search head dashboard 6.1.png" src="images/a/a7/Search_head_dashboard_6.1.png" width="700" height="168"></p><p>The dashboard lists the master nodes for all clusters the search head belongs to, along with some information on the status of each cluster. 
</p><p>For more information on the master node and its cluster, click the arrow at the far left of each row.
</p><p>You can get information on the search head itself by selecting the <b>More Info</b> button on the upper right corner of the dashboard:
</p>
<ul><li> <b>Name</b>. The search head's <code><font size="2">serverName</font></code>, as specified in its <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> file.
</li></ul><h3> <a name="viewthesearchheaddashboard_configure_the_search_head"><span class="mw-headline" id="Configure_the_search_head"> Configure the search head </span></a></h3>
<p>The dashboard offers several options for acting on the search head or otherwise changing its configuration. See <a href="#configuresearchheadwithdashboard" class="external text">"Configure the search head with the dashboard."</a>
</p>
<h3> <a name="viewthesearchheaddashboard_view_information_on_search_peers"><span class="mw-headline" id="View_information_on_search_peers"> View information on search peers</span></a></h3>
<p>You can also view information on the search head's <b>search peers</b> (identical, in clustering, to the set of cluster peer nodes) from the search head's Distributed Search page in Splunk Web: 
</p><p><b>1.</b> On the search head, click <b>Settings</b> in the upper right corner of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed environment</b> section, click <b>Distributed search</b>. 
</p><p><b>3.</b> Click <b>Search peers</b> to view the set of search peers.
</p><p><b>Caution:</b> Do not use the Distributed Search page in Splunk Web to change your search head configuration or to add peers. For information on how to configure an indexer cluster search head correctly, see  <a href="#configurethesearchhead" class="external text">"Search head configuration overview"</a>.
</p>
<h1>Manage the indexer cluster</h1><a name="takeapeeroffline"></a><h2> <a name="takeapeeroffline_take_a_peer_offline"><span class="mw-headline" id="Take_a_peer_offline"> Take a peer offline</span></a></h2>
<p>Use the CLI <code><font size="2">splunk offline</font></code> command to take a peer offline. By using the <code><font size="2">offline</font></code> command, you minimize any disruption to your searches. 
</p><p>During and after a peer goes offline, the cluster performs actions to regain its <b>valid</b> and <b>complete</b> states:
</p>
<ul><li> A <b>valid</b> cluster has <b>primary</b> copies of all its buckets and therefore is able to handle search requests across the entire set of data. In the case of a multisite cluster, a valid cluster also has primary copies for every site with search affinity.
</li><li> A <b>complete</b> cluster has <b>replication factor</b> number of copies of all its buckets, with <b>search factor</b> number of searchable copies. It therefore meets the designated requirements for failure tolerance. A complete cluster is also a valid cluster.
</li></ul><h3> <a name="takeapeeroffline_the_offline_command"><span class="mw-headline" id="The_offline_command">The offline command</span></a></h3>
<p>The CLI <code><font size="2">offline</font></code> command handles peer shutdown. It takes the peer down gracefully, allowing most in-progress searches to complete while also returning the cluster quickly to the valid state. In this way, it essentially eliminates disruption to existing or future searches. The <code><font size="2">offline</font></code> command also initiates further remedial bucket-fixing activities to return the cluster to a complete state. 
</p><p><b>Note:</b> The peer goes down after a maximum of 5-10 minutes, even if searches are still in progress. 
</p><p>For the offline command to work as intended, the search factor must be at least 2. This is why: With a search factor of 2, there is always a spare searchable copy of the data. If a peer with searchable data goes offline, new searches can immediately switch to the spare searchable copy of that data, thus minimizing the time that the cluster is not fully searchable. On the other hand, if the search factor is just 1, the master must make non-searchable copies searchable before searches can again run across the full set of data.  That can potentially take quite a while, as described in <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_goes_offline" class="external text">"Estimate the cluster recovery time when a peer goes offline"</a>.
</p><p><b>Important:</b> When taking down a peer, use the <code><font size="2">offline</font></code> command, not the <code><font size="2">stop</font></code> command. The <code><font size="2">offline</font></code> command stops the peer, but it does so in a way that minimizes any disruption to your searches.
</p>
<h4><font size="3"><b><i> <a name="takeapeeroffline_take_a_peer_down_with_the_offline_command"><span class="mw-headline" id="Take_a_peer_down_with_the_offline_command"> Take a peer down with the offline command </span></a></i></b></font></h4>
<p>Here is the syntax for the <code><font size="2">offline</font></code> command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk offline<br></font></code>
</div>
<p>You run this command directly on the peer. 
</p><p>When you run this command, the peer shuts down after the master finishes coordinating the necessary activities to return the cluster to a valid state (by reassigning primaries, as necessary), up to a maximum of 5-10 minutes. Until it shuts down, the peer continues to participate in any in-progress searches.
</p><p>After the peer shuts down, you have 60 seconds (by default) to complete any maintenance and bring the peer back online. If the peer does not return to the cluster within this time, the master initiates bucket-fixing activities to return the cluster to a complete state. If you need more than 60 seconds, you can extend the time the master waits for the peer to come back online by configuring the <code><font size="2">restart_timeout</font></code> attribute, as described in <a href="#takeapeeroffline_extend_the_restart_period" class="external text">"Extend the restart period"</a>.
</p><p>If you are performing an operation that involves taking many peers offline temporarily, you should consider invoking maintenance mode during the operation. See <a href="#usemaintenancemode" class="external text">"Use maintenance mode"</a>.
</p><p>For detailed information on the processes that occur when a peer goes offline, read <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down"</a>.
</p><p>After a peer goes down, it continues to appear on the list of peers on the master dashboard, although its status changes to "Down." To remove the peer from the master's list, see <a href="#removepeerfrommasterlist" class="external text">"Remove a peer from the master's list."</a>
</p>
<h4><font size="3"><b><i> <a name="takeapeeroffline_extend_the_restart_period"><span class="mw-headline" id="Extend_the_restart_period"> Extend the restart period</span></a></i></b></font></h4>
<p>If you need to perform maintenance on a peer and you expect the time required to exceed the master's <code><font size="2">restart_timeout</font></code> period (set to 60 seconds by default), you can change the value of that setting. Run this CLI command on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -restart_timeout &lt;seconds&gt;<br></font></code>
</div>
<p>For example, this command resets the timeout period to 900 seconds (15 minutes):
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -restart_timeout 900<br></font></code>
</div>
<p>You can run this command on the fly. You do not need to restart the master after it runs.
</p><p>You can also change this value in <code><font size="2">server.conf</font></code> on the master.
</p>
<h3> <a name="takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_goes_offline"><span class="mw-headline" id="Estimate_the_cluster_recovery_time_when_a_peer_goes_offline"> Estimate the cluster recovery time when a peer goes offline </span></a></h3>
<p>When a peer goes offline for a period that exceeds <code><font size="2">restart_timeout</font></code>, the master coordinates activities among the remaining peers to fix the buckets and return the cluster to a complete state.  For example, if the peer going offline is storing copies of 10 buckets and five of those copies are searchable, the master instructs peers to:
</p>
<ul><li> Stream copies of those 10 buckets to other peers, so that the cluster regains a full complement of bucket copies (to match the replication factor).
</li><li> Make five non-searchable bucket copies searchable, so that the cluster regains a full complement of searchable bucket copies (to match the search factor).
</li></ul><p>This activity can take some time to complete. Exactly how long depends on many factors, such as:
</p>
<ul><li> <b>System considerations</b>, such as CPU specifications, storage type, interconnect type.
</li><li> <b>Amount of other indexing currently being performed</b> by the peers that are tasked with making buckets searchable.
</li><li> <b>The size and number of buckets</b> stored on the offline peer.
</li><li> <b>The size of the index files</b> on the searchable copies stored on the offline peer. (These index files can vary greatly in size relative to rawdata size, depending on factors such as amount of segmentation.) For information on the relative sizes of rawdata and index files, read <a href="#systemrequirements_storage_considerations" class="external text">"Storage considerations"</a>.
</li><li> <b>The search factor.</b> This determines how quickly the cluster can convert non-searchable copies to searchable. If the search factor is at least 2, the cluster can convert non-searchable copies to searchable by copying index files to the non-searchable copies from the remaining set of searchable copies. If the search factor is 1, however, the cluster must convert non-searchable copies by rebuilding the index files, a much slower process. (For information on the types of files in a bucket, see <a href="#bucketsandclusters_data_files" class="external text">"Data files"</a>.)
</li></ul><p>Despite these variable factors, you can make a rough determination of how long the process will take. Assuming you are using Splunk Enterprise reference hardware, here are some basic estimates of how long the two main activities take:
</p>
<ul><li> To stream 10GB (rawdata and/or index files) from one peer to another across a LAN takes about 5-10 minutes. 
</li><li> The time required to rebuild the index files on a non-searchable bucket copy containing 4GB of rawdata depends on a number of factors such as the size of the resulting index files, but 30 minutes is a reasonable approximation to start with. Rebuilding index files is necessary if the search factor is 1, meaning that there are no copies of the index files available to stream. A non-searchable bucket copy consisting of 4GB rawdata can grow to a size approximating 10GB once the index files have been added. As described earlier, the actual size depends on numerous factors.
</li></ul><a name="usemaintenancemode"></a><h2> <a name="usemaintenancemode_use_maintenance_mode"><span class="mw-headline" id="Use_maintenance_mode"> Use maintenance mode</span></a></h2>
<p>Certain conditions can generate errors during hot bucket replication and cause the source peer to roll the bucket. While this behavior is generally beneficial to the health of the indexer cluster, it can result in many small buckets across the cluster, if errors occur frequently. Situations that can generate an unacceptable number of small buckets include persistent network problems or repeated offlining of peers. 
</p><p>To stop this behavior, you can temporarily put the cluster into maintenance mode. This can be useful for system maintenance work that generates repeated network errors, such as network reconfiguration. Similarly, if you need to upgrade your peers or otherwise temporarily offline several peers, you can invoke maintenance mode to forestall bucket rolling during that time.
</p><p>Maintenance mode works the same for single-site and multisite clusters. It has no notion of sites.
</p><p><b>Warning:</b> While the cluster is in maintenance mode, the master will not enforce replication factor or search factor policies. The only bucket fix-up that occurs during maintenance mode is that the master will attempt, when necessary, to reassign primaries to available searchable bucket copies. So, the cluster can be operating under a valid but incomplete status. See <a href="#clusterstates" class="external text"><b>Indexer cluster states</b></a> to understand the implications of this.
</p><p><b>Note:</b> The CLI commands <code><font size="2">apply cluster-bundle</font></code> and <code><font size="2">rolling-restart</font></code> incorporate maintenance mode functionality into their behavior by default, so there's no reason to invoke it explicitly when running those commands. A message stating that maintenance mode is on will appear on the master dashboard when you invoke these actions. 
</p>
<h3> <a name="usemaintenancemode_enable_maintenance_mode"><span class="mw-headline" id="Enable_maintenance_mode"> Enable maintenance mode </span></a></h3>
<p>Put the cluster into maintenance mode before starting maintenance activity. Once you have finished with maintenance, you should disable maintenance mode.
</p><p>To invoke maintenance mode, run this CLI command on the master node:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk enable maintenance-mode<br></font></code>
</div>
<p>When you run the <code><font size="2">enable</font></code> command, a message warning of the effects of maintenance mode appears and requires confirmation that you want to continue.
</p><p><b>Important:</b> Maintenance mode does not persist across master restarts. If the master restarts, you need to re-enable maintenance mode.
</p>
<h3> <a name="usemaintenancemode_disable_maintenance_mode"><span class="mw-headline" id="Disable_maintenance_mode"> Disable maintenance mode </span></a></h3>
<p>To return to the standard bucket-rolling behavior, run:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk disable maintenance-mode<br></font></code>
</div>
<h3> <a name="usemaintenancemode_determine_maintenance_mode_status"><span class="mw-headline" id="Determine_maintenance_mode_status">Determine maintenance mode status </span></a></h3>
<p>To determine whether maintenance mode is on, run:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk show maintenance-mode<br></font></code>
</div>
<p>A returned value of <code><font size="2">1</font></code> indicates that maintenance mode is on. <code><font size="2">0</font></code> indicates that maintenance mode is off.
</p>
<a name="restartthecluster"></a><h2> <a name="restartthecluster_restart_the_entire_indexer_cluster_or_a_single_peer_node"><span class="mw-headline" id="Restart_the_entire_indexer_cluster_or_a_single_peer_node"> Restart the entire indexer cluster or a single peer node</span></a></h2>
<p>This topic describes how to restart the entire indexer cluster (unusual) or a single peer node.
</p><p>When you restart a master or peer node, the master rebalances the primary bucket copies across the set of peers, as described in <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a>. 
</p><p>For information on configuration changes that require a restart, see <a href="#enableclustersindetail_restart_after_modifying_server.conf.3f" class="external text">"Restart after modifying server.conf?"</a> and <a href="#updatepeerconfigurations_restart_or_reload_after_configuration_bundle_changes.3f" class="external text">"Restart or reload after configuration bundle changes?"</a>. 
</p>
<h3> <a name="restartthecluster_restart_the_entire_cluster"><span class="mw-headline" id="Restart_the_entire_cluster"> Restart the entire cluster </span></a></h3>
<p>You ordinarily do not need to restart the entire cluster.  If you change a master's configuration, you restart just the master. If you update a set of common peer configurations, the master restarts just the set of peers, and only when necessary, as described in  <a href="#updatepeerconfigurations" class="external text">"Update common peer configurations"</a>.
</p><p>If, for any reason, you do need to restart both the master and the peer nodes:
</p><p><b>1.</b> Restart the master node, as you would any instance. For example, run this CLI command on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk restart<br></font></code>
</div>
<p><b>2.</b> Once the master restarts, wait until all the peers re-register with the master, and the master dashboard indicates that all peers and indexes are searchable. See <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p><p><b>3.</b> Restart the peers as a group, by running this CLI command on the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rolling-restart cluster-peers<br></font></code>
</div>
<p>If you need to restart the search head, you can do so at any time, as long as the rest of the cluster is running.
</p>
<h3> <a name="restartthecluster_the_rolling-restart_command"><span class="mw-headline" id="The_rolling-restart_command"> The rolling-restart command</span></a></h3>
<p>The <code><font size="2">splunk rolling-restart</font></code> command performs a phased restart of all the peer nodes, so that the cluster as a whole can continue to perform its functions during the restart process. 
</p><p><b>Caution:</b> Do not invoke the <code><font size="2">splunk rolling-restart</font></code> command unless absolutely necessary. Restarting the set of peers can result in prolonged amounts of bucket-fixing.
</p><p>You invoke the <code><font size="2">splunk rolling-restart</font></code> command from the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rolling-restart cluster-peers<br></font></code>
</div>
<p>The master also automatically initiates a rolling restart, when necessary, after distributing a configuration bundle to the peer nodes. For details on this process, see <a href="#updatepeerconfigurations_distribute_the_configuration_bundle" class="external text">"Distribute the configuration bundle"</a>.  
</p><p>The rolling restart works like this: The master issues a restart message to approximately 10% (by default) of the peer nodes at a time. (If there are less than 10 peers in the cluster, it issues the restart to one peer at a time.) Once those peers restart and contact the master, the master then issues a restart message to another 10% of the peers, and so on, until all the peers have restarted. This method helps ensure that load-balanced forwarders sending data to the cluster always have a peer available to receive the data.
</p><p>At the end of the rolling restart, the master rebalances the cluster primary buckets.  See <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a>.  
</p><p><b>Note:</b> The master restarts the peers in random order. In the case of multsite clusters, this operation is not site-aware.
</p><p>By default, the master issues the restart command to 10% of the peers at a time. However, the percentage is configurable through the <code><font size="2">percent_peers_to_restart</font></code> attribute in the <code><font size="2">[clustering]</font></code> stanza of <code><font size="2">server.conf</font></code>. For convenience, you can configure this attribute with the CLI <code><font size="2">splunk edit cluster-config</font></code> command. For example, to change the restart behavior so that the master restarts 20% of the peers at a time, run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -percent_peers_to_restart 20<br></font></code>
</div>
<p>To cause the master to restart all peers at once, run the command with a value of <code><font size="2">100</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit cluster-config -percent_peers_to_restart 100<br></font></code>
</div>
<p>This can be useful under certain circumstances, such as when no users are actively searching and no forwarders are actively sending data to the cluster. It minimizes the time required to complete the restart.
</p><p>After changing the <code><font size="2">percent_peers_to_restart</font></code> attribute, you still need to run the <code><font size="2">splunk rolling-restart</font></code> command to initiate the actual restart.
</p><p><b>Note:</b> During a rolling restart, there is no guarantee that the cluster will be fully searchable.
</p>
<h3> <a name="restartthecluster_restart_a_single_peer"><span class="mw-headline" id="Restart_a_single_peer"> Restart a single peer</span></a></h3>
<p>You might occasionally have need to restart a single peer; for example, if you change certain configurations on only that peer. 
</p><p>There are two ways that you can safely restart a single peer:
</p>
<ul><li> Through Splunk Web (<b>Settings&gt;Server Controls</b>).
</li><li> With the CLI command <code><font size="2">offline</font></code>, followed by <code><font size="2">start</font></code>.
</li></ul><p>When you use Splunk Web or the <code><font size="2">offline</font></code>/<code><font size="2">start</font></code> commands to restart a peer, the master waits 60 seconds (by default) before assuming that the peer has gone down for good. This allows sufficient time for the peer to come back on-line and prevents the cluster from performing unnecessary remedial activities. 
</p><p><b>Note:</b> The actual time that the master waits is determined by the value of the master's <code><font size="2">restart_timeout</font></code> attribute in server.conf. The default for this attribute is  60 seconds. If you need the master to wait for a longer period, you can change the <code><font size="2">restart_timeout</font></code> value, as described in <a href="#takeapeeroffline_extend_the_restart_period" class="external text">"Extend the restart period"</a>.
</p><p>The <code><font size="2">offline</font></code>/<code><font size="2">start</font></code> restart method has an advantage over the Splunk Web method in that it waits for in-progress searches to complete before stopping the peer. In addition, since it involves a two-step process, you can use it if you need the peer to remain down briefly while you perform some maintenance. 
</p><p>For information on the <code><font size="2">offline</font></code> command, read <a href="#takeapeeroffline" class="external text">"Take a peer offline"</a>.
</p><p><b>Warning:</b> Do not use the CLI <code><font size="2">restart</font></code> command to restart the peer. If you use the <code><font size="2">restart</font></code> command, the master will not be aware that the peer is restarting. Instead, after waiting a default 60 seconds for the peer to send a heartbeat, the master will initiate the usual remedial actions that occur when a peer goes down, such as adding its bucket copies to other peers. The actual time the master waits is determined by the master's <code><font size="2">heartbeat_timeout</font></code> attribute. It is inadvisable to change its default value of 60 seconds without consultation.
</p>
<a name="rebalancethecluster"></a><h2> <a name="rebalancethecluster_rebalance_the_indexer_cluster_primary_buckets"><span class="mw-headline" id="Rebalance_the_indexer_cluster_primary_buckets"> Rebalance the indexer cluster primary buckets</span></a></h2>
<p>When you start or restart a master or peer node, the master rebalances the set of <b>primary</b> bucket copies across the peers, in an attempt to spread the primary copies as equitably as possible. Ideally, if you have four peers and 300 buckets, each peer would hold 75 primary copies. The purpose of rebalancing is to even the search load across the set of peers.
</p>
<h3> <a name="rebalancethecluster_how_rebalancing_works"><span class="mw-headline" id="How_rebalancing_works"> How rebalancing works</span></a></h3>
<p>To achieve rebalancing, the master reassigns the primary state from existing bucket copies to <b>searchable</b> copies of the same buckets on other peers, as necessary. This rebalancing is a best-effort attempt; there is no guarantee that full, perfect rebalance will result. 
</p><p>Rebalancing occurs automatically whenever a peer or master node joins or rejoins the cluster.  In the case of a rolling restart, rebalancing occurs once, at the end of the process.
</p><p><b>Note:</b> Even though rebalancing occurs when a new peer joins the cluster, that peer won't participate in the rebalancing, because it does not yet have any bucket copies. The rebalancing takes place among any existing peers that have searchable bucket copies.
</p><p>When rebalancing a bucket, the master just reassigns the primary state from one searchable copy to another searchable copy of the same bucket, if there is one and if, by doing so, the balance of primaries across peers will improve. It does not cause peers to stream bucket copies, and it does not cause peers to make unsearchable copies searchable. If an existing peer does not have any searchable copies, it will not gain any primaries during rebalancing.
</p>
<h3> <a name="rebalancethecluster_initiate_rebalancing_manually"><span class="mw-headline" id="Initiate_rebalancing_manually"> Initiate rebalancing manually </span></a></h3>
<p>If you want to initiate the process manually, you can either restart a peer or hit the <code><font size="2">/services/cluster/master/control/control/rebalance_primaries</font></code> REST endpoint on the master. For example, run this command on the master node: 
</p>
<div class="samplecode">
<code><font size="2"><br>curl -k -u admin:pass --request POST \<br>&nbsp;&nbsp;https://localhost:8089/services/cluster/master/control/control/rebalance_primaries<br></font></code>
</div>
<p>For more information, refer to the REST API documentation for cluster/master/control.
</p>
<h3> <a name="rebalancethecluster_rebalance_a_multisite_cluster"><span class="mw-headline" id="Rebalance_a_multisite_cluster">Rebalance a multisite cluster</span></a></h3>
<p>There are a few differences in how rebalancing works in a multisite cluster. In a multisite cluster, multiple sites typically have full sets of primary copies. When you rebalance the cluster, the rebalancing occurs independently for each site. For example, in a two-site cluster, the cluster separately rebalances the primaries in site1 and site2. It does not shift primaries between the two sites.
</p><p>The start or restart of a peer on any site triggers rebalancing on all sites.  For example, if you restart a peer on site1 in a two-site cluster, rebalancing occurs on both site1 and site2.
</p>
<h3> <a name="rebalancethecluster_summary"><span class="mw-headline" id="Summary">Summary</span></a></h3>
<p>Cluster rebalancing is the rebalancing of the primary assignments across existing searchable copies in the cluster. 
</p><p>It occurs under these circumstances: 
</p>
<ul><li> A peer joins or rejoins a cluster.
</li><li> At the end of a rolling restart.
</li><li> A master rejoins the cluster.
</li><li> You manually hit the <code><font size="2">rebalance_primaries</font></code> REST endpoint on the master.
</li></ul><a name="removeextrabucketcopies"></a><h2> <a name="removeextrabucketcopies_remove_excess_bucket_copies_from_the_indexer_cluster"><span class="mw-headline" id="Remove_excess_bucket_copies_from_the_indexer_cluster"> Remove excess bucket copies from the indexer cluster</span></a></h2>
<p>When a peer goes down but later returns to the indexer cluster, any bucket copies that the peer retained while down are once again available to the cluster. This can result in the cluster maintaining excess copies of some buckets, as described in the topic <a href="#whathappenswhenapeercomesbackup" class="external text">"What happens when a peer node comes back up"</a>.
</p><p>In effect, a returning peer can cause the cluster to store more copies of some buckets than are needed to fulfill the replication factor and, possibly, the search factor as well. It can sometimes be useful to keep the extra copies around, as that topic explains, but you can save disk space by instead removing them.
</p><p>You can view and remove excess bucket copies from the Master dashboard or from the CLI.
</p>
<h3> <a name="removeextrabucketcopies_use_the_master_dashboard"><span class="mw-headline" id="Use_the_Master_dashboard">Use the Master dashboard</span></a></h3>
<p>To view or remove excess bucket copies:
</p><p><b>1.</b> On the master node, click <b>Settings</b> on the upper right side of Splunk Web.
</p><p><b>2.</b> In the <b>Distributed Environment</b> group, click <b>Indexer clustering</b>. 
</p><p>This takes you to the Master dashboard.
</p><p><b>3.</b> Select the Indexes tab.
</p><p><b>4.</b> Click the <b>Bucket Status</b> button.
</p><p>This takes you to the Bucket Status dashboard.
</p><p><b>5.</b> Select the Indexes with Excess Buckets tab.
</p><p>This tab provides a list of indexes with excess bucket copies.  It enumerates both buckets with excess copies and buckets with excess searchable copies. It also enumerates the total excess copies in each category. For example, if your index "new" has one bucket with three excess copies, one of which is searchable, and a second bucket with one excess copy, which is non-searchable, the row for "new" will report:
</p>
<ul><li> 2 buckets with excess copies
</li><li> 1 bucket with excess searchable copies
</li><li> 4 total excess copies
</li><li> 1 total excess searchable copies
</li></ul><p>If you want to remove the excess copies for a single index, click the <b>Remove</b> button on the right side of the row for that index.
</p><p>If you want to remove the excess copies for all indexes, click the <b>Remove All Excess Buckets</b> button.
</p>
<h3> <a name="removeextrabucketcopies_use_the_cli"><span class="mw-headline" id="Use_the_CLI">Use the CLI </span></a></h3>
<p>The Splunk CLI has two commands that help manage and remove excess bucket copies. You can run these commands either across the entire set of indexes or on just a single index.
</p>
<h4><font size="3"><b><i> <a name="removeextrabucketcopies_determine_whether_the_cluster_has_extra_copies"><span class="mw-headline" id="Determine_whether_the_cluster_has_extra_copies">Determine whether the cluster has extra copies</span></a></i></b></font></h4>
<p>To find out how many buckets have extra copies, including extra searchable copies, run this command from the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk list excess-buckets [index-name]<br></font></code>
</div>
<p>The output from <code><font size="2">splunk list excess-buckets</font></code> looks like this:
</p>
<div class="samplecode">
<code><font size="2"><br>index=_audit<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of buckets=4<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess replication copies=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess searchable copies=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess replication copies across all buckets=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess searchable copies across all buckets=0<br>index=_internal<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of buckets=4<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess replication copies=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess searchable copies=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess replication copies across all buckets=0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess searchable copies across all buckets=0<br>index=main<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of buckets=5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess replication copies=5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of buckets with excess searchable copies=5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess replication copies across all buckets=10<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total number of excess searchable copies across all buckets=5<br></font></code>
</div>
<h4><font size="3"><b><i> <a name="removeextrabucketcopies_remove_extra_bucket_copies"><span class="mw-headline" id="Remove_extra_bucket_copies">Remove extra  bucket copies </span></a></i></b></font></h4>
<p>To remove all extra bucket copies from the cluster (or from one index on the cluster), run this command from the master:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk remove excess-buckets [index-name]<br></font></code>
</div>
<p>The master determines which peers to remove the extra copies from.  This is not configurable, and the extras will not necessarily be removed from the peer that has most recently returned to the cluster.
</p>
<a name="removepeerfrommasterlist"></a><h2> <a name="removepeerfrommasterlist_remove_a_peer_from_the_master.27s_list"><span class="mw-headline" id="Remove_a_peer_from_the_master.27s_list">Remove a peer from the master's list </span></a></h2>
<p>After a peer goes down, it remains on the master's list of peer nodes. For example, it continues to appear on the master dashboard, although the status changes to "Down" or "GracefulShutdown", depending on how it went down. If you subsequently restart the master, the peer gets removed from the master's list.  You can also use the CLI <code><font size="2">splunk remove cluster-peers</font></code> command to remove the peer without a master restart:
</p>
<div class="samplecode">
<code><font size="2"><br>./splunk remove cluster-peers -peers &lt;guid&gt;,&lt;guid&gt;,&lt;guid&gt;,...<br></font></code>
</div>
<p>Note the following:
</p>
<ul><li> This command removes one or more peers from the master's list of peers.
</li><li> All peers removed must be in the "Down" or "GracefulShutdown" state.
</li><li> You specify the peers with a comma-separated list of GUIDs, one per peer. 
</li><li> The GUIDs can be specified with or without hyphens. For example: 4EB4D230-CB8B-4DEB-AD68-CF9209A6868A and 4EB4D230CB8B4DEBAD68CF9209A6868A are both valid.
</li><li> If any GUID on the list is invalid, because one of the GUIDs does not correlate to a downed peer, the master aborts the entire operation.
</li></ul><p>For information on the master dashboard's list of peers, see <a href="#howtomonitoracluster" class="external text">"View the master dashboard"</a>.
</p>
<h1>Manage a multisite indexer cluster</h1><a name="mastersitefailure"></a><h2> <a name="mastersitefailure_handle_master_site_failure"><span class="mw-headline" id="Handle_master_site_failure"> Handle master site failure</span></a></h2>
<p>If the site holding the master node fails, you can bring up a new master on one of the remaining sites.  In the meantime, the cluster continues to function as best it can. The peers continue to stream data to other peers based on the list of target peers they were using at the time the master went down.  If some of their target peers go down (as would likely be the case in a site failure), they remove them from their lists of streaming targets and continue to stream data to any peers remaining on their lists. 
</p><p>You should prepare for the possibility of master site failure by configuring a stand-by master on at least one additional site.
</p><p>Do the following:
</p><p><b>1.</b> Configure a stand-by master for at least one of the sites not hosting the current master. For the details on how to do this, see <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.  <b>This is a preparatory step. You must do this before the need arises.</b>  
</p><p><b>2.</b> When the master site goes down, bring up a stand-by master on one of the remaining sites, as described in  <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.
</p><p><b>3.</b> Restart indexing on the cluster, following the instructions in <a href="#restartindexing" class="external text">"Restart indexing after master restart or site failure"</a>. 
</p><p>The new master now serves as a full substitute for the old master.
</p><p><b>Note:</b> If the site later comes back up, you need to point the peers on that site to the new master.  See <a href="#handlemasternodefailure_ensure_that_the_peer_and_search_head_nodes_can_find_the_new_master" class="external text">"Ensure that the peer and search head nodes can find the new master"</a>.
</p>
<a name="restartindexing"></a><h2> <a name="restartindexing_restart_indexing_after_master_restart_or_site_failure"><span class="mw-headline" id="Restart_indexing_after_master_restart_or_site_failure"> Restart indexing after master restart or site failure</span></a></h2>
<p>When a master restarts, it blocks indexing until enough peers exist across the indexer cluster to fulfill the replication factor. In a basic, single-site cluster, this is usually  desired behavior.  However, in the case of a multisite cluster, you might want to restart indexing even though you do not have enough available peers to fulfill all aspects of the site replication factor (for example, in the case of site failure).  
</p><p>The two cases where this need typically arises are:
</p>
<ul><li> A site goes down and you later need to restart the master for any reason.
</li><li> The site with the master goes down and you bring up a stand-by master on another site.
</li></ul><p>If a site goes down but the master, running on another site, remains up, indexing continues as usual, because the master only runs the check at start-up.
</p><p>Run the <code><font size="2">splunk set indexing-ready</font></code> command on the master to unblock indexing when replication factor number of peers are not available:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set indexing-ready -auth admin:changeme<br></font></code>
</div>
<p>For example, assume you have a three-site cluster configured with "site_replication_factor = origin:1, site1:2, site2:2, site3:2, total:7", with the master located on site1. If site2 goes down and you subsequently restart the master, the master blocks indexing after it restarts, because it is waiting to hear from a minimum of two peers on site2 ("site2:2"). In this situation, you can use the command to restart indexing on the remaining sites. 
</p><p>Similarly, if site1, which has the master, goes down and you bring up a stand-by master on site2, the new master initially blocks indexing because site1 is not available. You can then use the command  to tell the new master to restart indexing.
</p><p><b>Important:</b> You must run the <code><font size="2">splunk set indexing-ready</font></code> command every time you restart the master under the listed circumstances. The command unblocks indexing only for the current restart.
</p><p><b>Note:</b> Although this command is designed with site failure in mind, you can also use it to restart indexing on a single-site cluster prior to the replication factor number of peers being available. In that circumstance, however, it is usually better just to wait until the replication number of peers rejoin the cluster.
</p>
<a name="converttosinglesite"></a><h2> <a name="converttosinglesite_convert_a_multisite_indexer_cluster_to_single-site"><span class="mw-headline" id="Convert_a_multisite_indexer_cluster_to_single-site"> Convert a multisite indexer cluster to single-site</span></a></h2>
<p>You can convert a multisite indexer cluster to a basic, single-site cluster. When you do so, all peers and search heads become part of the implicit single site. 
</p><p><b>1.</b> Stop all cluster nodes (master/peers/search heads).
</p><p><b>2.</b> On the master, edit <code><font size="2">server.conf</font></code>:
</p>
<dl><dd><b>a.</b> Set <code><font size="2">multisite</font></code> to <code><font size="2">false</font></code>.
</dd></dl><dl><dd><b>b.</b> Set the single-site <code><font size="2">replication_factor</font></code> and <code><font size="2">search_factor</font></code> attributes to implement the desired replication behavior.
</dd></dl><dl><dd><b>c.</b> Remove the <code><font size="2">site</font></code> attribute.
</dd></dl><p><b>3.</b> On each peer and search head, edit <code><font size="2">server.conf</font></code>:
</p>
<dl><dd><b>a.</b> Remove the <code><font size="2">site</font></code> attribute.
</dd></dl><p><b>4.</b> Start the master node.
</p><p><b>5.</b> Start the peer nodes and search heads.
</p><p>Note the following:
</p>
<ul><li> The master ignores any multisite attributes remaining in <code><font size="2">server.conf</font></code> (<code><font size="2">site_replication_factor</font></code> and so on).
</li></ul><ul><li> Upon restart, the master picks a primary copy for each bucket. 
</li></ul><ul><li> After the conversion, any bucket copies that exceed the single-site replication factor remain in place.  For information on removing these extra copies, see <a href="#removeextrabucketcopies" class="external text">"Remove excess bucket copies"</a>.
</li></ul><ul><li> Future bucket replication and bucket fixing will follow the values set for the single-site replication and search factors. 
</li></ul><p>For information on how to convert a single-site cluster to multisite, see <a href="#migratetomultisite" class="external text">"Migrate an indexer cluster from single-site to multisite"</a>.
</p>
<a name="moveapeertoanewsite"></a><h2> <a name="moveapeertoanewsite_move_a_peer_to_a_new_site"><span class="mw-headline" id="Move_a_peer_to_a_new_site"> Move a peer to a new site</span></a></h2>
<p>Use this procedure to relocate a peer to another site. This can be useful if a peer was shipped to the wrong site and the mistake was discovered only after the peer was deployed on the cluster.
</p><p><b>1.</b>  Take the peer offline with the <code><font size="2">offline</font></code> command, as described in <a href="#takeapeeroffline" class="external text">"Take a peer offline"</a>. The master will reassign the bucket copies handled by this peer to other peers in the same site.
</p><p><b>2.</b> Ship the peer's server to the new site.
</p><p><b>3.</b> Delete the entire Splunk Enterprise installation from the server, including its index database with all bucket copies.
</p><p><b>4.</b> Reinstall Splunk Enterprise on the server, re-enable clustering, and set the peer's site value to the new site location.
</p><p>The peer rejoins the cluster as a new peer.
</p>
<h1>How indexer clusters work</h1><a name="basicconcepts"></a><h2> <a name="basicconcepts_basic_indexer_cluster_concepts_for_advanced_users"><span class="mw-headline" id="Basic_indexer_cluster_concepts_for_advanced_users"> Basic indexer cluster concepts for advanced users</span></a></h2>
<p>To understand how a cluster functions, you need to be familiar with a few concepts:
</p>
<ul><li> <a href="#thereplicationfactor" class="external text"><b>Replication factor.</b></a> This specifies how many copies of the data the cluster maintains. It influences the cluster's resiliency, its ability to withstand multiple node failures.
</li><li> <a href="#thesearchfactor" class="external text"><b>Search factor.</b></a> This specifies how many copies of the data are <b>searchable</b>. It influences how quickly a cluster can recover from a downed node.
</li><li> <a href="#bucketsandclusters" class="external text"><b>Buckets.</b></a> These are the basic storage containers for indexes. They correspond to subdirectories in the indexer's database.
</li><li> <a href="#clusterstates" class="external text"><b>Cluster states.</b></a> These states describe the health of the cluster.
</li></ul><p>You can find an overview to these concepts in the introductory topic on cluster architecture, <a href="#basicclusterarchitecture" class="external text">"Basic indexer cluster architecture"</a>. Topics in the chapter you are now reading provide more detail.
</p>
<a name="thereplicationfactor"></a><h2> <a name="thereplicationfactor_replication_factor"><span class="mw-headline" id="Replication_factor"> Replication factor</span></a></h2>
<p>As part of setting up an indexer cluster, you specify the number of copies of data that you want the cluster to maintain. Peer nodes store incoming data in <b>buckets</b>, and the cluster maintains multiple copies of each bucket. The cluster stores each bucket copy on a separate peer node. The number of copies of each bucket that the cluster maintains is the <b>replication factor</b>.
</p>
<h3> <a name="thereplicationfactor_replication_factor_and_cluster_resiliency"><span class="mw-headline" id="Replication_factor_and_cluster_resiliency">Replication factor and cluster resiliency</span></a></h3>
<p>The cluster can tolerate a failure of (replication factor - 1) peer nodes. For example, to ensure that your system can tolerate a failure of two peers, you must configure a replication factor of 3, which means that the cluster stores three identical copies of each bucket on separate nodes. With a replication factor of 3, you can be certain that all your data will be available if no more than two peer nodes in the cluster fail. With two nodes down, you still have one complete copy of data available on the remaining peers. 
</p><p>By increasing the replication factor, you can tolerate more peer node failures. With a replication factor of 2, you can tolerate just one node failure; with a replication factor of 3, you can tolerate two concurrent failures; and so on.
</p><p>The trade-off is that you need to store and process all those copies of data. Although the replicating activity doesn't consume much processing power, still, as the replication factor increases, you need to run more indexers and provision more storage for the indexed data. On the other hand, since data replication itself requires little processing power, you can take advantage of the multiple indexers in a cluster to ingest and index more data. Each indexer in the cluster can function as both originating indexer ("source peer") and replication target ("target peer"). It can index incoming data and also store copies of data from other indexers in the cluster.
</p>
<h3> <a name="thereplicationfactor_example:_replication_factor_in_action"><span class="mw-headline" id="Example:_Replication_factor_in_action">Example: Replication factor in action </span></a></h3>
<p>In the following diagram, one peer is receiving data from a forwarder, which it processes and then streams to two other peers.The cluster will contain three complete copies of the peer's data, one copy on each peer. 
</p><p><img alt="Replication factor 3 60.png" src="images/5/5f/Replication_factor_3_60.png" width="700" height="554"></p><p><b>Note:</b> This diagram represents a highly simplified version of peer replication, where all data is entering the system through a single peer. There are a few issues that add complexity to a real-life scenario:
</p>
<ul><li> In most clusters, each of the peer nodes would be functioning as both source and target peer, receiving external data from a forwarder, as well as replicated data from other peers. 
</li><li> To accommodate horizontal scaling, a cluster with a replication factor of 3 could consist of many more peers than three. At any given time, each source peer would be streaming copies of its data to two target peers, but each time it started a new hot bucket, its set of target peers could potentially change. 
</li></ul><p>Later topics in this chapter describe in detail how clusters process data.
</p>
<h3> <a name="thereplicationfactor_replication_factor_in_multisite_clusters"><span class="mw-headline" id="Replication_factor_in_multisite_clusters"> Replication factor in multisite clusters</span></a></h3>
<p>A multisite cluster uses a special version of the replication factor, the site replication factor. This determines not only the number of copies that the entire cluster maintains but also the number of copies that each site maintains.  For information on the site replication factor, see <a href="#sitereplicationfactor" class="external text">"Configure the site replication factor"</a>.
</p>
<a name="thesearchfactor"></a><h2> <a name="thesearchfactor_search_factor"><span class="mw-headline" id="Search_factor"> Search factor</span></a></h2>
<p>When you configure the master node, you designate a <b>search factor</b>. The search factor determines the number of <b>searchable</b> copies of data the indexer cluster maintains. In other words, the search factor determines the number of searchable copies of each <b>bucket</b>. The default value for the search factor is 2, meaning that the cluster maintains two searchable copies of all data. The search factor must be less than or equal to the <b>replication factor</b>.
</p>
<h3> <a name="thesearchfactor_searchable_and_non-searchable_bucket_copies"><span class="mw-headline" id="Searchable_and_non-searchable_bucket_copies">Searchable and non-searchable bucket copies</span></a></h3>
<p>The difference between a searchable and a <b>non-searchable</b> copy of a bucket is this: The searchable copy contains both the data itself and some very extensive index files that the peer node uses to search the data. The non-searchable copy contains just the data. Even the data stored in the non-searchable copy, however, has undergone initial processing and is stored in a form that makes it possible to create the index files later, if necessary. For more information on the files that constitute Splunk Enterprise indexes, read the subtopic <a href="#bucketsandclusters_data_files" class="external text">"Data files"</a>.
</p>
<h3> <a name="thesearchfactor_search_recovery_from_peer_node_failure"><span class="mw-headline" id="Search_recovery_from_peer_node_failure">Search recovery from peer node failure</span></a></h3>
<p>With a search factor of at least 2, the cluster is able to continue searching with little interruption if a peer node goes down. For example, say you specify a replication factor of 3 and a search factor of 2. The cluster will maintain three copies of all buckets on separate peers across the cluster, and two copies of each bucket will be searchable. Then, if a peer goes down and it contains a bucket copy that has been participating in searches, a searchable copy of that bucket on another peer can immediately step in and start participating in searches. 
</p><p>On the other hand, if the cluster's search factor is only 1 and a peer goes down, there will be a significant lag before searching can resume across the full set of cluster data. Although non-searchable copies of the buckets can be made searchable, doing so takes time, because the index files must first be built from the raw data file. The processing time can be significant if the peer that went down was storing a large quantity of searchable data. For help estimating the time needed to make non-searchable copies searchable, look <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_gets_decommissioned" class="external text">here</a>.
</p><p>The reason you might want to limit the number of searchable copies on your cluster is because searchable data occupies a lot more storage than non-searchable data. The trade-off, therefore, is between quick access to all your data in case of peer node failure versus increased storage requirements. For help estimating the relative storage sizes of searchable and non-searchable data, read <a href="#systemrequirements_storage_considerations" class="external text">"Storage considerations"</a>. For most needs, the default search factor of 2 represents the right trade-off.
</p>
<h3> <a name="thesearchfactor_search_factor_in_multisite_clusters"><span class="mw-headline" id="Search_factor_in_multisite_clusters"> Search factor in multisite clusters</span></a></h3>
<p>A multisite cluster uses a special version of the search factor, the site search factor. This determines not only the number of  searchable copies that the entire cluster maintains but also the number of searchable copies that each site maintains.  For information on the site search factor, see <a href="#sitesearchfactor" class="external text">"Configure the site search factor"</a>.
</p>
<a name="bucketsandclusters"></a><h2> <a name="bucketsandclusters_buckets_and_indexer_clusters"><span class="mw-headline" id="Buckets_and_indexer_clusters"> Buckets and indexer clusters</span></a></h2>
<p>Splunk Enterprise stores indexed data in <b>buckets</b>, which are directories containing both the data and index files into the data. An index typically consists of many buckets, organized by age of the data. 
</p><p>The indexer cluster replicates data on a bucket-by-bucket basis. The original bucket copy and its replicated copies on other peer nodes contain identical sets of data, although only <b>searchable</b> copies also contain the index files.
</p><p>In a cluster, copies of buckets originating from a single source peer can be spread across many target peers. For example, if you have five peers in your cluster and a replication factor of 3 (a typical scenario for horizontal scaling), the cluster will maintain three copies of each bucket (the original copy on the source peer and replicated copies on two target peers). Each time the source peer starts a new hot bucket, the master gives the peer a new set of target peers to replicate data to. Therefore, while the original copies will all be on the source peer, the replicated copies of those buckets will be randomly spread across the other peers.This behavior is not configurable. The one certainty is that you will never have two copies of the same bucket on the same peer. In the case of a multisite cluster, you can also configure the site location of the replicated copies, but you still cannot specify the actual peer location.
</p><p>The following diagram shows the scenario just described - five peers, a replication factor of 3, and seven original source buckets, with their copies spread across all the peers. To reduce complexity, the diagram only shows the buckets for data originating from one peer. In a real-life scenario, most, if not all, of the other peers would also be originating data and replicating it to other peers on the cluster.
</p><p><img alt="Bucket replication new 60.png" src="images/6/65/Bucket_replication_new_60.png" width="700" height="558"></p><p><br>
In this diagram, 1A is a source bucket. 1B and 1C are copies of that bucket. The diagram uses the same convention with 2A/B/C, 3A/B/C, and so on.
</p><p>You need a good grasp of buckets to understand cluster architecture. The rest of this section describes some bucket concepts of particular importance for a clustered deployment. For a thorough introduction to buckets, read  <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>.
</p>
<h3> <a name="bucketsandclusters_data_files"><span class="mw-headline" id="Data_files">Data files</span></a></h3>
<p>There are two key types of files in a bucket:
</p>
<ul><li> The processed external data in compressed form (<b>rawdata</b>)
</li><li> Indexes that point to the rawdata (<b>index files</b>)
</li></ul><p>Buckets contain a few other types of files as well, but these are the ones that are most important to understand.
</p><p>Rawdata is not actually "raw" data, as the term might be defined by a dictionary. Rather, it consists of the external data after it has been processed into events. The processed data is stored in a compressed rawdata journal file. As a journal file, the rawdata file, in addition to containing the <b>event data</b>, contains all information necessary to generate the associated index files, if they are missing.
</p><p>All bucket copies, both <b>searchable</b> and <b>non-searchable</b>, contain rawdata files. Searchable copies also contain index files.
</p><p>When a peer node receives a block of data from a forwarder, it processes the data and adds it to the rawdata file in its local hot bucket. It also indexes it, creating the associated index files. In addition, it streams copies of just the processed rawdata to each of its target peers, which then adds it to the rawdata file in its own copy of the bucket. The rawdata in both the original and the replicated bucket copies are identical. 
</p><p>If the cluster has a search factor of 1, the target peers store only the rawdata in the bucket copies. They do not generate index files for the data. By not storing the index files on the target peers, you limit storage requirements. Because the rawdata is stored as a journal file, if the peer maintaining the original, fully indexed data goes down, one of the target peers can step in and generate the indexes from its copy of the rawdata.
</p><p>If the cluster has a search factor greater than 1, some or all of the target peers also create index files for the data. For example, say you have a replication factor of 3 and a search factor of 2. In that case, the source peer streams its rawdata to two target peers. One of those peers then uses the rawdata to create index files, which it stores in its copy of the bucket. That way, there will be two searchable copies of the data (the original copy and the replicated copy with the index files). As described in <a href="#thesearchfactor" class="external text">"Search factor"</a>, this allows the cluster to recover more quickly in case of peer node failure. For more information on searchable bucket copies, see <a href="#bucketsandclusters_bucket_searchability_and_primacy_states" class="external text">"Bucket searchability"</a> later in this topic.
</p><p>See these topics for more information on bucket files: 
</p>
<ul><li> For information on how bucket files get regenerated when a peer goes down, read <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down"</a>.
</li><li> For information on the relative sizes of rawdata and index files, read <a href="#systemrequirements_storage_considerations" class="external text">"Storage considerations"</a>.
</li></ul><h3> <a name="bucketsandclusters_bucket_stages"><span class="mw-headline" id="Bucket_stages">Bucket stages</span></a></h3>
<p>As a bucket ages, it rolls through several stages:
</p>
<ul><li> hot
</li><li> warm
</li><li> cold 
</li><li> frozen
</li></ul><p>For detailed information about these stages, read <a href="#howsplunkstoresindexes" class="external text">"How the indexer stores indexes"</a>.
</p><p>For the immediate discussion of cluster architecture, you just need a basic understanding of these bucket stages. A hot bucket is a bucket that's still being written to. When an indexer finishes writing to a hot bucket (for example, because the bucket reaches a maximum size), it rolls the bucket to warm and begins writing to a new hot bucket. Warm buckets are readable (for example, for searching) but the indexer does not write new data to them. Eventually, a bucket rolls to cold and then to frozen, at which point it gets archived or deleted.
</p><p>There are a couple other details that are important to keep in mind:
</p>
<ul><li> Hot/warm and cold buckets are stored in separately configurable locations. 
</li><li> The filename of a warm or cold bucket includes the time range of the data in the bucket. For detailed information on bucket naming conventions, read  <a href="#howsplunkstoresindexes_what_the_index_directories_look_like" class="external text">"What the index directories look like"</a>.
</li><li> Searches occur across hot, warm, and cold buckets.
</li><li> The conditions that cause buckets to roll are configurable, as described in <a href="#configureindexstorage" class="external text">"Configure index storage"</a>.
</li><li> For storage hardware information, such as help on estimating storage requirements, read <a href="#systemrequirements_storage_considerations" class="external text">"Storage considerations"</a>.
</li></ul><h3> <a name="bucketsandclusters_bucket_searchability_and_primacy_states"><span class="mw-headline" id="Bucket_searchability_and_primacy_states">Bucket searchability and primacy states </span></a></h3>
<p>A copy of a bucket is either <b>searchable</b> or <b>non-searchable</b>. Since a cluster can maintain multiple searchable copies of a bucket, the cluster needs to be a way to identify which copy participates in a search. To handle this, clusters use the concept of <b>primacy</b>. A searchable bucket copy is either <b>primary</b> or non-primary.
</p><p>A bucket copy is searchable if it contains index files in addition to the rawdata file. The peer receiving the external data indexes the rawdata and also sends copies of the rawdata to its peers. If the search factor is greater than 1, some or all of those peers will also generate index files for the buckets they're replicating. So, for example, if you have a replication factor of 3 and a search factor of 2 and the cluster is <b>complete</b>, the cluster will contain three copies of each bucket. All three copies will contain the rawdata file, and two of the copies (the copy on the source peer and one of the copies on the target peers) will also contain index files and therefore be searchable. The third copy will be non-searchable, but it can be made searchable if necessary. The main reason that a non-searchable copy gets made searchable is because a peer holding a searchable copy of the bucket goes down.
</p><p>A primary copy of a bucket is the searchable copy that participates in a search. A single-site <b>valid</b> cluster has exactly one primary copy of each bucket. That way, one and only one copy of each bucket gets searched. If a node with primary copies goes down, searchable but non-primary copies on other nodes can immediately be designated as primary, thus allowing searches to continue without any need to first wait for new index files to be generated. 
</p><p><b>Note:</b> In the case of a multisite cluster, a valid cluster is a cluster that has a set of primary copies for each site that supports search affinity. In search affinity, search heads perform searches across the peers on their local site. This requires that each site have its own set of primary buckets.
</p><p>Initially, the copy of the bucket on the peer originating the data is the primary copy, but this can change over time. For example, if the peer goes down, the master reassigns primacy from any primary copies on the downed peer to corresponding searchable copies on remaining peers. For more information on this process, read <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down"</a>.
</p><p>Primacy reassignment also occurs when the master rebalances the cluster, in an attempt to achieve a more even distribution of primary copies across the set of peers. Rebalancing occurs under these circumstances: 
</p>
<ul><li> A peer joins or rejoins a cluster.
</li><li> A master rejoins the cluster.
</li><li> You manually hit the <code><font size="2">rebalance_primaries</font></code> REST endpoint on the master.
</li></ul><p>See <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a> for details.
</p><p>The following diagram shows buckets spread across all the peers, as in the previous diagram. The cluster has a replication factor of 3 and a search factor of 2, which means that the cluster maintains two searchable copies of each bucket. Here, the copies of the buckets on the source peer are all primary (and therefore also searchable). The buckets' second searchable (but non-primary) copies are spread among most of the remaining peers in the cluster.
</p><p><img alt="Bucket searchable primacy new 60.png" src="images/8/89/Bucket_searchable_primacy_new_60.png" width="700" height="559"></p><p>The set of primary bucket copies define a cluster's generation, as described in the next section.
</p>
<h3> <a name="bucketsandclusters_generations"><span class="mw-headline" id="Generations">Generations</span></a></h3>
<p>A <b>generation</b> identifies which copies of a cluster's buckets are primary and therefore will participate in a search. 
</p><p><b>Note:</b> The actual set of buckets that get searched also depends on other factors such as the search time range. This is true for any indexer, clustered or not.
</p><p>The generation changes over time, as peers leave and join the cluster. When a peer goes down, its primary bucket copies get reassigned to other peers. The master also reassigns primaries under certain other circumstances, in a process known as "cluster rebalancing".
</p><p>Here is another way of defining a generation: A generation is a snapshot of a <b>valid</b> state of the cluster; "valid" in the sense that every bucket on the cluster has exactly one primary copy. 
</p><p>All peers that are currently registered with the master participate in the current generation. When a peer joins or leaves the cluster, the master creates a new generation. 
</p><p><b>Note:</b> Since the process of reassigning primacy to new bucket copies is not instantaneous, the cluster might quickly go through a number of generations while reassigning primacy due to an event such as a downed peer, particularly in the case where numerous primaries were residing on the downed peer.
</p><p>The generation is a cluster-wide attribute. Its value is the same across all sites in a multisite cluster.
</p>
<h4><font size="3"><b><i> <a name="bucketsandclusters_how_cluster_nodes_use_the_generation"><span class="mw-headline" id="How_cluster_nodes_use_the_generation">How cluster nodes use the generation </span></a></i></b></font></h4>
<p>Here is how the various cluster nodes use generation information:
</p>
<ul><li> The master creates each new generation, and assigns a <b>generation ID</b> to it. When necessary, it communicates the current generation ID to the peers and the search head. It also keeps track of the primary bucket copies for each generation and on which peers they are located.
</li></ul><ul><li> The peers keep track of which of their bucket copies are primary for each generation. The peers retain primacy information across multiple generations.
</li></ul><ul><li> For each search, the search head uses the generation ID that it gets from the master to determine which peers to search across.
</li></ul><h4><font size="3"><b><i> <a name="bucketsandclusters_when_the_generation_changes"><span class="mw-headline" id="When_the_generation_changes">When the generation changes</span></a></i></b></font></h4>
<p>The generation changes under these circumstances:
</p>
<ul><li> The master comes online.
</li><li> A peer joins the cluster.
</li><li> A peer goes down, either intentionally (through the CLI <code><font size="2">offline</font></code> command) or unintentionally (by crashing). When a peer goes down, the master reassigns primacy from bucket copies on the downed node to searchable copies of the same buckets on the remaining nodes and creates a new generation.
</li><li> Whenever rebalancing of the primary copies occurs, such as when you manually hit the <code><font size="2">rebalance_primaries</font></code> REST endpoint on the master. For information on rebalancing, see <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a>.
</li></ul><p>The master does not create a new generation merely when a bucket rolls from hot to warm, thus causing a new hot bucket to get created (unless the bucket rolled for one of the reasons listed above). In that situation, the set of peers doesn't change. The search head only needs to know which peers are part of the generation; that is, which peers are currently participating in the cluster. It does not need to know which bucket copies on a particular peer are primary; the peer itself keeps track of that information.
</p>
<h4><font size="3"><b><i> <a name="bucketsandclusters_how_the_generation_is_used_in_searches"><span class="mw-headline" id="How_the_generation_is_used_in_searches">How the generation is used in searches</span></a></i></b></font></h4>
<p>The search heads poll the master for the latest generation information at regular intervals. When the generation changes, the master gives the search heads the new generation ID and a list of the peers that belong to that generation. Each search head, in turn, gives the peers the ID whenever it initiates a search. The peers use the ID to identify which of their buckets are primary for that search.
</p><p>Usually, a search occurs over the most recent generation of primary bucket copies.  In the case of long-running searches, however, it is possible that a search could be running across an earlier generation. This situation typically occurs because a peer went down in the middle of the search. This allows the long-running search to complete, even though some data might be missing due to the downed peer node. The alternative would be to start the search over again, which you can always do manually if necessary.
</p>
<h4><font size="3"><b><i> <a name="bucketsandclusters_why_a_downed_peer_causes_the_generation_to_change"><span class="mw-headline" id="Why_a_downed_peer_causes_the_generation_to_change">Why a downed peer causes the generation to change</span></a></i></b></font></h4>
<p>The reason that a downed peer causes the master to create a new generation is because, when a peer goes down, the master reassigns the downed peer's primary copies to copies on other peers. A copy that was not primary for a previous generation becomes primary in the new generation. By knowing the generation ID associated with a search, a peer is able to determine which of its buckets are primary for that search.
</p><p>For example, the diagram that follows shows the same simplified version of a cluster as earlier, after the source node holding all the primary copies has gone down and the master has directed the remaining peers in fixing the buckets. First, the master reassigned primacy to the remaining searchable copy of each bucket. Next, it directed the peers to make their non-searchable copies searchable, to make up for the missing set of searchable copies. Finally, it directed the replication of a new set of non-searchable copies (1D, 2D, etc.), spread among the remaining peers. 
</p><p>Even though the source node went down, the cluster was able to fully recover both its <b>complete</b> and <b>valid</b> states, with replication factor number (3) of total bucket copies, search factor number (2) of searchable bucket copies, and exactly one primary copy of each bucket. This represents a different generation from the previous diagram, because primary copies have moved to different peers.
</p><p><img alt="Bucket generation 2 60.png" src="images/c/ca/Bucket_generation_2_60.png" width="700" height="559"></p><p><br><b>Note:</b> This diagram only shows the buckets originating from one of the peers. A more complete version of this diagram would show buckets originating from several peers as they have migrated around the cluster.
</p>
<h3> <a name="bucketsandclusters_how_the_cluster_handles_frozen_buckets"><span class="mw-headline" id="How_the_cluster_handles_frozen_buckets"> How the cluster handles frozen buckets </span></a></h3>
<p>In the case of a standalone indexer, when a bucket rolls to frozen, the indexer deletes it from its <code><font size="2">colddb</font></code> directory.  Depending on its retirement policy, the indexer might copy it to an archive directory before deleting it. See <a href="#automatearchiving" class="external text">"Archive indexed data."</a> 
</p><p>In the case of an indexer cluster, the handling of a frozen bucket requires the removal, with optional archiving, of all copies from the peers on which they reside. It is possible that the bucket will not freeze simultaneously across all its copies, because peers can vary in their retirement policies. To prevent any unnecessary bucket fix-up subsequent to one copy being frozen, each peer notifies the master when it freezes a bucket. 
</p><p>When the master receives the first freeze notification for a particular bucket, it sets a flag to indicate that the bucket is frozen and alerts the other peers that the bucket is frozen. Each peer eventually deletes, and optionally archives, its copy of the bucket according to its own retirement policy. Once all copies have been deleted from the cluster, the master removes the bucket from the list it maintains.
</p>
<a name="clusterstates"></a><h2> <a name="clusterstates_indexer_cluster_states"><span class="mw-headline" id="Indexer_cluster_states"> Indexer cluster states</span></a></h2>
<p>An indexer cluster in good working order is both <b>valid</b> and <b>complete</b>:
</p>
<ul><li> A <b>valid</b> cluster has exactly one <b>primary</b> copy of each bucket. In the case of a multisite cluster, a valid cluster has one full set of primary copies for each site that supports search affinity.
</li></ul><ul><li> A <b>complete</b> cluster has <b>replication factor</b> number of copies of each bucket and  <b>search factor</b> number of <b>searchable</b> copies of each bucket. In the case of a multisite cluster, the number of bucket copies must also fulfill the site-specific requirements for the replication and search factors.
</li></ul><p>Note these points:
</p>
<ul><li> A valid cluster is able to handle search requests across the entire set of data. A valid multisite cluster also meets any inherent search affinity goals.
</li></ul><ul><li> A complete cluster meets the designated requirements for failure tolerance.
</li></ul><ul><li> A complete cluster is also a valid cluster, but a valid cluster is not necessarily complete.
</li></ul><p>In addition, to ensure robust data availability, a cluster must not only be complete, but its search factor must be set to at least 2. This guarantees that a search head can continue to search across the cluster without interruption, if a peer goes down.
</p><p>When a peer node goes down, the master directs the cluster in activities designed to recover both its valid and complete states. In some cases, the cluster might be able to return to a valid state but not to a complete state. (For example, consider a cluster containing three peers, with a replication factor of 3. If one peer goes down, the cluster cannot recover its complete state as long as the peer remains down, but it should be able to recover its valid state.) See <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down"</a> for details on how the cluster recovers from a downed node.
</p>
<a name="howclusteredindexingworks"></a><h2> <a name="howclusteredindexingworks_how_clustered_indexing_works"><span class="mw-headline" id="How_clustered_indexing_works"> How clustered indexing works</span></a></h2>
<p>When discussing how data and messages flow between nodes during indexing, it is useful to distinguish between the two roles that a peer node plays:
</p>
<ul><li> <b>Source node.</b> The source node ingests data from forwarders or other external sources.
</li><li> <b>Target node.</b> The target node receive streams of replicated data from the source nodes.
</li></ul><p>In practice, a single peer functions as both a source and a target node, often simultaneously. 
</p><p><b>Important:</b> In a typical indexer cluster deployment, all the peer nodes are source nodes; that is, each node has its own set of external inputs. This is not a requirement, but it is generally the best practice. There is no reason to reserve some peers for use just as target nodes. The processing cost of storing replicated data is minimal, and, in any case, you cannot currently specify which nodes will receive replicated data. The master determines that on a bucket-by-bucket basis, and the behavior is not configurable.  You must assume that all the peer nodes will serve as targets.
</p><p><b>Note:</b> In addition to replicating external data, each peer replicates its internal indexes to other peers in the same way. To keep things simple, this discussion focuses on external data only.
</p>
<h3> <a name="howclusteredindexingworks_how_the_target_peers_are_chosen"><span class="mw-headline" id="How_the_target_peers_are_chosen"> How the target peers are chosen </span></a></h3>
<p>Whenever the source peer starts a hot bucket, the master node gives it a list of target peers to stream its replicated data to. The list is bucket-specific. If a source peer is writing to several hot buckets, it could be streaming the contents of each bucket to a different set of target peers. 
</p><p>The master chooses the list of target peers randomly.  In the case of multisite clustering, it respects site boundaries, as dictated by the replication factor, but chooses the target peers randomly within those constraints.
</p>
<h3> <a name="howclusteredindexingworks_when_a_peer_node_starts"><span class="mw-headline" id="When_a_peer_node_starts">When a peer node starts</span></a></h3>
<p>These events occur when a peer node starts up:
</p><p><b>1.</b> The peer node registers with the master and receives the latest <b>configuration bundle</b> from the master.
</p><p><b>2.</b> The master rebalances the <b>primary</b> bucket copies across the cluster and starts a new generation.
</p><p><b>3.</b> The peer starts ingesting external data, in the same way as any indexer. It processes the data into events and then appends the data to a rawdata file. It also creates associated index files. It stores these files (both the rawdata and the index files) locally in a hot bucket. This is the primary copy of the bucket.
</p><p><b>4.</b> The master gives the peer a list of target peers for its replicated data. For example, if the replication factor is 3, the master gives the peer a list of two target peers.  
</p><p><b>5.</b> If the search factor is greater than 1, the master also tells the peer which of its target peers should make its copy of the data <b>searchable</b>. For example, if the search factor is 2, the master picks one specific target peer that should make its copy searchable and communicates that information to the source peer.
</p><p><b>6.</b> The peer begins streaming the processed rawdata to the target peers specified by the master. It does not wait until its rawdata file is complete to start streaming its contents; rather, it streams the rawdata in blocks, as it processes the incoming data. It also tells any target peers if they need to make their copies searchable, as communicated to it by the master in step 5.
</p><p><b>7.</b> The target peers receive the rawdata from the source peer and store it in local copies of the bucket.
</p><p><b>8.</b> Any targets  with designated searchable copies start creating the necessary index files. 
</p><p><b>9.</b> The peer continues to stream data to the targets until it rolls its hot bucket.
</p><p><b>Note:</b> The source and target peers rarely communicate with each other through their management ports. Usually, they just send and receive data to each other over their replication ports. The master node manages the overall process.
</p><p>This is just the breakdown for data flowing from a single peer. In a cluster, multiple peers will be both originating and receiving data at any time.
</p>
<h3> <a name="howclusteredindexingworks_when_a_peer_node_rolls_a_hot_bucket"><span class="mw-headline" id="When_a_peer_node_rolls_a_hot_bucket"> When a peer node rolls a hot bucket</span></a></h3>
<p>When a source peer rolls a hot bucket to warm (for example, because the bucket has reached its maximum size), the following sequence of events occurs:
</p><p><b>1.</b> The source peer tells the master and its target peers that it has rolled a bucket.
</p><p><b>2.</b> The target peers roll their copies of the bucket.
</p><p><b>3.</b> The source peer continues ingesting external data as this process is occurring. It indexes the data locally into a new hot bucket and streams the rawdata to a new set of target peers that it gets from the master.
</p><p><b>4.</b> The new set of target peers receive the rawdata for the new hot bucket from the source peer and store it in local copies of the bucket. The targets with designated searchable copies also start creating the necessary index files.
</p><p><b>5.</b> The source peer continues to stream data to the targets until it rolls its next hot bucket. And so on.
</p>
<h3> <a name="howclusteredindexingworks_how_a_peer_node_interacts_with_a_forwarder"><span class="mw-headline" id="How_a_peer_node_interacts_with_a_forwarder"> How a peer node interacts with a forwarder </span></a></h3>
<p>When a peer node gets its data from a forwarder, it processes it in the same way as any indexer getting data from a forwarder. However, in a clustering environment, you should ordinarily enable <b>indexer acknowledgment</b> for each forwarder sending data to a peer. This protects against loss of data between forwarder and peer and is the only way to ensure end-to-end data fidelity. If the forwarder does not get an acknowledgment for a block of data it has sent to a peer, it resends the block.
</p><p>For details on how to set up forwarders to send data to peers, including how to enable indexer acknowledgment, read <a href="#useforwarderstogetyourdata" class="external text">"Use forwarders to get your data into the indexer cluster"</a>. To understand how peers and forwarders process indexer acknowledgment, read the section <a href="#useforwarderstogetyourdata_how_indexer_acknowledgment_works" class="external text">"How indexer acknowledgment works"</a> in that topic.
</p>
<a name="howclusteredsearchworks"></a><h2> <a name="howclusteredsearchworks_how_search_works_in_an_indexer_cluster"><span class="mw-headline" id="How_search_works_in_an_indexer_cluster"> How search works in an indexer cluster</span></a></h2>
<p>In a single-site indexer cluster, the search head performs searches across the entire set of peers. 
</p><p>With a multisite indexer cluster, you can implement <b>search affinity.</b> With search affinity, searches occur across peers on the same site as the search head. This improves network efficiency without reducing access to the full set of cluster data.
</p><p>Under rare circumstances, described later, you might want to initiate a search on a single peer.
</p>
<h3> <a name="howclusteredsearchworks_search_across_a_single-site_cluster"><span class="mw-headline" id="Search_across_a_single-site_cluster"> Search across a single-site cluster</span></a></h3>
<p>Searching across an indexer cluster works in a way similar to how <b>distributed search</b> works with non-clustered indexers. The main difference is that the <b>search head</b> gets its list of <b>search peers</b> from the master node. It also gets a generation ID from the master. After that, it communicates directly with the peers. 
</p><p><b>Note:</b> In an indexer cluster search, the search peers are the set of cluster peers that are currently registered with the master (in other words, the peers that are up-and-running and participating in the cluster).
</p><p>When the search head initiates a search:
</p><p><b>1.</b> The search head contacts the master node.
</p><p><b>2.</b> The master node gives the search head the current generation ID and a list of the peers in that generation (that is, the peers that are currently registered with the master).
</p><p><b>3.</b> The search head communicates with the search peers in the same way as in a distributed search not involving an indexer cluster. It provides the peers with exactly the same information (search request and replication bundle), except that it also gives the search peers the generation ID.
</p><p><b>4.</b> The search peers use the generation ID to identify which of their bucket copies, if any, are <b>primary</b> for the generation and thus need to participate in the search. As in any other search, the peers also use the search's time range to determine whether to search a particular bucket.
</p><p><b>5.</b> The search peers search their primary copies of buckets and send the results back to the search head, which consolidates the results.
</p><p>You can integrate the indexer cluster with a search head cluster, for search head scaling and high availability. See "Integrate the search head cluster with an indexer cluster" in the <i>Distributed Search</i> manual.
</p><p>For details on these and other available features of distributed search, read the <i>Distributed Search</i> manual, starting with "About distributed search". Also, read <a href="#configurethesearchhead" class="external text">"Configure the search head"</a> in this manual to learn about a few configuration differences when dealing with a search head in an indexer cluster.
</p>
<h3> <a name="howclusteredsearchworks_search_locally_in_a_multisite_cluster"><span class="mw-headline" id="Search_locally_in_a_multisite_cluster">Search locally in a multisite cluster</span></a></h3>
<p>In a multisite cluster, you typically put search heads on each site. This allows you to take advantage of search affinity. In search affinity, searches normally run across only peers on the same site as the requesting search head.
</p><p>Search affinity is always enabled with multisite clusters. However, you must perform a few steps to take advantage of it. Specifically, you must ensure that both the searchable data and the search heads are available locally. For information on how to set up search affinity, see <a href="#multisitesearchaffinity" class="external text">"Implement search affinity in a multisite indexer cluster"</a>.
</p><p>Once a site has been configured for search affinity, the actual search process works the same as for single-site clusters. The search head distributes the current generation ID, along with the search and replication bundle, to all peers across the entire cluster. The local peers, however, are the only ones to respond. They search their primary buckets and return results to the search head, using the generation ID to determine which of their bucket copies are primary. 
</p><p><b>Note:</b> Hot bucket data is replicated in blocks, as described in <a href="#howclusteredindexingworks" class="external text">"How clustered indexing works"</a>. If a local search involves a replicated hot bucket copy, where the origin copy is on a different site, there might be a time lag while the local peer waits to get the latest block of hot data from the originating peer.  During this time, the search does not return the latest data. 
</p><p>If some peers on the local site are down and the site therefore does not have a full complement of primaries, remote peers will participate in the search, providing results from any primaries missing from the site. In that case, the search does not adhere to search affinity, in order to maintain access to the full set of data. Once the site returns to a <b>valid</b> state, subsequent searches again adhere to search affinity.
</p>
<h3> <a name="howclusteredsearchworks_search_a_single_peer"><span class="mw-headline" id="Search_a_single_peer">Search a single peer</span></a></h3>
<p>For debugging purposes, you might occasionally need to search a single peer node. You do this by initiating the search directly on the peer, in the usual manner. The search accesses any <b>searchable</b> data on that peer. It does not have access to unsearchable copies of data on the peer or to searchable copies of data on other peers. 
</p><p><b>Note:</b> Keep in mind that there is no way to configure exactly what data will be searchable on any individual peer. However, at a minimum, all data that has entered the cluster through the peer should be searchable on that peer.
</p>
<h3> <a name="howclusteredsearchworks_how_clusters_handle_report_and_data_model_acceleration_summaries"><span class="mw-headline" id="How_clusters_handle_report_and_data_model_acceleration_summaries">How clusters handle report and data model acceleration summaries </span></a></h3>
<p>Indexer clusters do not replicate <b>report acceleration</b> and <b>data model acceleration</b> summaries.
</p><p>These summaries reside on the peers, in their own directories immediately under each index-level directory. For example, for the "index1" index, they reside under <code><font size="2">$SPLUNK_HOME/var/lib/splunk/index1</font></code>. The report acceleration summary directory is named <code><font size="2">summary</font></code> and the data model acceleration summary directory is named <code><font size="2">datamodel_summary</font></code>.
</p><p>A summary correlates with one or more buckets, depending on the summary's time span. When a summary is generated, it resides on the peer that holds the primary copy of the bucket for that time span. If the summary spans multiple buckets, and the primary copies of those buckets reside on multiple peers, then each of those peers will hold the corresponding part of the summary. 
</p><p>If <b>primacy</b> gets reassigned from one copy of a bucket to another (for example, because the peer holding the primary copy fails), the summary does not move to the peer with the new primary copy. Therefore, it becomes unavailable. It will not be available again until the next time Splunk Enterprise attempts to update the summary, finds that it is missing, and then regenerates it. 
</p><p>In multisite clusters, like single-site clusters, the summaries reside with the primary bucket copy. Because a multisite cluster has multiple primaries, one for each site that supports search affinity, the summaries reside with the particular primary that the generating search head accessed when running the search. Due to site affinity, that usually means that the summaries reside on primaries on the same site as the generating search head.
</p>
<a name="howclusterstartworks"></a><h2> <a name="howclusterstartworks_how_indexer_cluster_nodes_start_up"><span class="mw-headline" id="How_indexer_cluster_nodes_start_up"> How indexer cluster nodes start up</span></a></h2>
<p>This topic describes what happens when: 
</p>
<ul><li> the master node starts
</li><li> a peer node joins a new cluster
</li><li> a peer node joins an existing cluster
</li></ul><h3> <a name="howclusterstartworks_when_the_master_node_starts"><span class="mw-headline" id="When_the_master_node_starts"> When the master node starts </span></a></h3>
<p>When a master node comes online (either the first time or subsequently), it begins listening for cluster peers. Each online peer registers with the master, and the master adds it to the cluster. The master waits until the replication factor number of peers register, and then it starts performing its functions.
</p><p><b>When you first deploy the cluster,</b> you must enable the master before enabling the peer nodes, as described in <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>. The master blocks indexing on the peers until you enable and restart the full replication factor number of peers. 
</p><p><b>If you subsequently restart the master,</b> it waits for a quiet period of 60 seconds, so that all peers have an opportunity to register with it. Once the quiet period ends and the replication factor number of peers register with it, the master can start performing its coordinating functions, such as rebalancing the <b>primary</b> bucket copies and telling peers where to stream copies of incoming data. Therefore, you must make sure that there are at least replication factor number of peers running when you restart the master.
</p><p>After the 60 second quiet period is over, you can view the master dashboard for information on the status of the cluster.
</p><p>For more information on what occurs when a master goes down and then restarts, see <a href="#whathappenswhenamasternodegoesdown" class="external text">"What happens when the master node goes down"</a>.
</p>
<h3> <a name="howclusterstartworks_when_a_peer_joins_a_new_cluster"><span class="mw-headline" id="When_a_peer_joins_a_new_cluster">When a peer joins a new cluster</span></a></h3>
<p>When you initially deploy a cluster, you must first enable the master and then enable the peer nodes, as described in <a href="#clusterdeploymentoverview" class="external text">"Indexer cluster deployment overview"</a>. The master blocks indexing on the peers until you enable and restart the full replication factor number of peers. 
</p><p>Each peer registers with the master when it comes online, and the master automatically distributes the latest <b>configuration bundle</b> to it. The peer then validates the configuration bundle locally. The peer will only join the cluster if bundle validation succeeds.
</p><p>The peer starts indexing data after the replication factor number of peers join the cluster.
</p>
<h3> <a name="howclusterstartworks_when_a_peer_joins_an_existing_cluster"><span class="mw-headline" id="When_a_peer_joins_an_existing_cluster">When a peer joins an existing cluster</span></a></h3>
<p>A peer can also come online at some later time, when the cluster is already up and running with a master and the replication factor number of peers. The peer registers with the master when it comes online, and the master distributes the latest configuration bundle to it. The peer validates the configuration bundle locally. The peer joins the cluster only if bundle validation succeeds.
</p><p><b>Note:</b> Adding a new peer to an existing cluster  causes a rebalancing of primary bucket copies across the set of existing peer nodes, as described in <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets"</a>. However, the new peer won't get any of those primary copies, because the master performs rebalancing only across the set of searchable copies, and a new peer has no searchable copies when it starts up. The peer can participate in future bucket replication, but the master does not automatically shift bucket copies or reassign primaries from existing peers to the new peer.
</p>
<a name="whathappenswhenaslavenodegoesdown"></a><h2> <a name="whathappenswhenaslavenodegoesdown_what_happens_when_a_peer_node_goes_down"><span class="mw-headline" id="What_happens_when_a_peer_node_goes_down"> What happens when a peer node goes down</span></a></h2>
<p>A peer node can go down either intentionally (by invoking the CLI <code><font size="2">offline</font></code> command, as described in <a href="#takeapeeroffline" class="external text">"Take a peer offline"</a>) or unintentionally (for example, by a server crashing). 
</p><p>No matter how a peer goes down, the master coordinates remedial activities to recreate a full complement of bucket copies. This process is called <b>bucket fixing</b>. The master keeps track of which bucket copies are on each node and what their states are (<b>primacy</b>,   <b>searchability</b>). When a peer goes down, the master can therefore instruct the remaining peers to fix the cluster's set of buckets, with the aim of returning to the state where the cluster has: 
</p>
<ul><li> Exactly one <b>primary</b> copy of each bucket (the <b>valid</b> state). In a multisite cluster, the valid state means that there is a primary copy on each site that supports search affinity, based on the <code><font size="2">site_search_factor</font></code>.
</li><li> A full set of <b>searchable</b> copies for each bucket, matching the search factor. In the case of a multisite cluster, the number of searchable bucket copies must also fulfill the site-specific requirements for the search factor.
</li><li> A full set of copies (searchable and non-searchable combined) for each bucket, matching the replication factor (the <b>complete</b> state). In the case of a multisite cluster, the number of bucket copies must also fulfill the site-specific requirements for the replication factor.
</li></ul><p>Barring catastrophic failure of multiple nodes, the master can usually recreate a valid cluster. If the cluster (or the site in a multisite cluster) has a search factor of at least 2, it can do so almost immediately. Whether or not a master can also recreate a complete cluster depends on the number of peers still standing compared to the replication factor. At least replication factor number of peers must remain standing for a cluster to be made complete. In the case of a multisite cluster, each site must have available at least the number of peers specified for it in the site replication factor.
</p><p>The time that the cluster takes to return to a complete state can be significant, because it must first stream buckets from one peer to another and make non-searchable bucket copies searchable. See <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_goes_offline" class="external text">"Estimate the cluster recovery time when a peer goes offline"</a> for more information.
</p><p>Besides the remedial steps to fix the buckets, a few other key events happen when a node goes down:
</p>
<ul><li> The master rolls the <b>generation</b> and creates a new <b>generation ID</b>, which it communicates to both peers and search heads when needed.
</li><li> Any peers with copies of the downed node's hot buckets roll those copies to warm.
</li></ul><h3> <a name="whathappenswhenaslavenodegoesdown_when_a_peer_node_gets_taken_offline_intentionally"><span class="mw-headline" id="When_a_peer_node_gets_taken_offline_intentionally">When a peer node gets taken offline intentionally</span></a></h3>
<p>The <code><font size="2">offline</font></code> command removes a peer from the cluster and then stops the peer. It takes the peer down gracefully, allowing most in-progress searches to complete while quickly returning the cluster to a fully searchable state.
</p><p><b>Note:</b> The peer goes down after a maximum of 5-10 minutes, even if searches or remedial activities are still in progress. 
</p><p>For the basics on how to take a peer offline, read the topic <a href="#takeapeeroffline" class="external text">"Take a peer offline."</a>
</p>
<h4><font size="3"><b><i> <a name="whathappenswhenaslavenodegoesdown_the_offline_command"><span class="mw-headline" id="The_offline_command"> The offline command </span></a></i></b></font></h4>
<p>The <code><font size="2">offline</font></code> command has this syntax:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk offline<br></font></code>
</div>
<p>This command causes the following sequence of actions to occur:
</p><p><b>1.</b> <b>Partial shutdown.</b> The peer immediately undergoes a partial shutdown. The peer stops accepting both external inputs and replicated data. It continues to participate in searches for the time being.
</p><p><b>2.</b> <b>Primacy reassignment.</b> The master reassigns primacy from any primary bucket copies on the peer to available searchable copies of those buckets on other peers (on the same site, if a multisite cluster). At the end of this step, which should take just a few moments if the cluster (or the cluster site) has a search factor of at least 2, the cluster returns to the valid state. During this step, the peer's status is <code><font size="2">ReassigningPrimaries</font></code>.
</p><p><b>3.</b> <b>Generation ID rolling.</b> The master rolls the cluster's generation ID. At the end of this step, the peer no longer joins in new searches, but it continues to participate in any in-progress searches. 
</p><p><b>4.</b> <b>Full shutdown.</b> The peer shuts down completely after a maximum of 5-10 minutes, or when in-progress searches and primacy re-assignment activities complete - whichever comes first. It no longer sends heartbeats to the master.
</p><p><b>5.</b> <b>Restart count.</b> After the peer shuts down, the master waits the length of the <code><font size="2">restart_timeout</font></code> attribute (60 seconds by default), set in <code><font size="2">server.conf</font></code>. If the peer comes back online within this period, the master rebalances the cluster's set of primary bucket copies but no further remedial activities occur. During this step, the peer's status is <code><font size="2">Restarting</font></code>. If the peer does not come back up within the timeout period, its status changes to <code><font size="2">Down</font></code>.
</p><p><b>6.</b> <b>Remedial activities.</b> If the peer does not restart within the <code><font size="2">restart_timeout</font></code> period, the master initiates actions to fix the cluster buckets. It tells the remaining peers to replicate copies of the buckets on the offline peer to other peers. It also compensates for any searchable copies of buckets on the offline peer by instructing other peers to make non-searchable copies of those buckets searchable. At the end of this step, the cluster returns to the complete state. If taking the peer offline results in less than replication factor number of remaining peers, the cluster cannot finish this step and cannot return to the complete state. For details on how bucket-fixing works, see <a href="#whathappenswhenaslavenodegoesdown_bucket-fixing_scenarios" class="external text">"Bucket-fixing scenarios."</a> 
</p><p>In the case of a multisite cluster, remedial activities take place within the same site as the offline peer, if possible. For details, see <a href="#whathappenswhenaslavenodegoesdown_bucket-fixing_in_multisite_clusters" class="external text">"Bucket-fixing in multisite clusters."</a>
</p>
<h3> <a name="whathappenswhenaslavenodegoesdown_when_a_peer_node_goes_down_unintentionally"><span class="mw-headline" id="When_a_peer_node_goes_down_unintentionally">When a peer node goes down unintentionally</span></a></h3>
<p>When a peer node goes down for any reason besides the <code><font size="2">offline</font></code> command, it stops sending the periodic heartbeat to the master. This causes the master to detect the loss and initiate remedial action. The master coordinates essentially the same actions as when the peer gets taken offline intentionally, except for the following:
</p>
<ul><li> The downed peer does not continue to participate in ongoing searches.
</li><li> The master waits only for the length of the heartbeat timeout (by default, 60 seconds) before reassigning primacy and initiating bucket-fixing actions.
</li></ul><p>Searches can continue across the cluster after a node goes down; however, searches will provide only partial results until the cluster regains its valid state. 
</p><p>In the case of a multisite cluster, when a peer goes down on one site, the site loses its search affinity, if any, until it regains its valid state. During that time, searches continue to provide full results through involvement of remote peers.
</p>
<h3> <a name="whathappenswhenaslavenodegoesdown_bucket-fixing_scenarios"><span class="mw-headline" id="Bucket-fixing_scenarios">Bucket-fixing scenarios</span></a></h3>
<p>To replace bucket copies on a downed peer, the master coordinates bucket-fixing activities among the peers. Besides replacing all the bucket copies, the cluster must ensure that it has a full complement of primary and searchable copies.
</p><p>Bucket fixing involves three activities:
</p>
<ul><li> <b>Compensating for any primary copies</b> on the downed peer by assigning primary status to searchable copies of those buckets on other peers.
</li><li> <b>Compensating for any searchable copies</b> by converting non-searchable copies of those buckets on other peers to searchable.
</li><li> <b>Replacing all bucket copies</b> (searchable and non-searchable) by streaming a copy of each bucket to a peer that doesn't already have a copy of that bucket.
</li></ul><p>For example, assume that the downed peer had 10 bucket copies, and five of those were searchable, and two of the searchable copies were primary. The cluster must:
</p>
<ul><li> Reassign primary status to two searchable copies on other peers.
</li><li> Convert five non-searchable bucket copies on other peers to searchable.
</li><li> Stream 10 bucket copies from one standing peer to another. 
</li></ul><p>The first activity - converting a searchable copy of a bucket from non-primary to primary - happens very quickly, because searchable bucket copies already have index files and so there's virtually no processing involved. (This assumes that there is a spare searchable copy available, which requires the search factor to be at least 2. If not, the cluster has to first make a non-searchable copy searchable before it can assign primary status to the copy.) 
</p><p>The second activity - converting a non-searchable copy of a bucket to searchable - takes some time, because the peer must copy the bucket's index files from a searchable copy on another peer (or, if there's no other searchable copy of that bucket, then the peer must rebuild the bucket's index files from the rawdata file). For help estimating the time needed to make non-searchable copies searchable, read <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_goes_offline" class="external text">"Estimate the cluster recovery time when a peer goes offline"</a>.
</p><p>The third activity - streaming copies from one peer to another - can also take a significant amount of time (depending on the amount of data to be streamed), as described in <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_goes_offline" class="external text">"Estimate the cluster recovery time when a peer goes offline"</a>.
</p><p>The two examples below illustrate how a master 1) recreates a valid and complete cluster and 2) creates a valid but incomplete cluster when insufficient nodes remain standing. The process operates the same whether the peer goes down intentionally or unintentionally.
</p><p>Remember these points: 
</p>
<ul><li> A cluster is valid when there is one primary searchable copy of every bucket. Any search across a valid cluster provides a full set of search results.
</li></ul><ul><li> A cluster is complete when the cluster has replication factor number of copies of buckets with search factor number of searchable copies.
</li></ul><ul><li> A cluster can be valid but incomplete if there are searchable copies of all buckets, but there are less than replication factor number of copies of buckets. So, if a cluster with a replication factor of 3 has exactly three peer nodes and one of those peers goes down, the cluster can be made valid but it cannot be made complete, since, with just two standing nodes, it is not possible to fulfill the replication factor by maintaining three sets of buckets.
</li></ul><h4><font size="3"><b><i> <a name="whathappenswhenaslavenodegoesdown_example:_fixing_buckets_to_create_a_valid_and_complete_cluster"><span class="mw-headline" id="Example:_Fixing_buckets_to_create_a_valid_and_complete_cluster"> Example: Fixing buckets to create a valid and complete cluster</span></a></i></b></font></h4>
<p>Assume:
</p>
<ul><li> The peer went down unintentionally (that is, not in response to an <code><font size="2">offline</font></code> command).
</li></ul><ul><li> The downed peer is part of a cluster with these characteristics:
<ul><li> 5 peers, including the downed peer
</li><li> replication factor = 3
</li><li> search factor = 2
</li></ul></li><li> The downed peer holds these bucket copies:
<ul><li> 3 primary copies of buckets
</li><li> 10 searchable copies (including the primary copies)
</li><li> 20 total bucket copies (searchable and non-searchable combined)
</li></ul></li></ul><p>When the peer goes down, the master sends messages to various of the remaining peers as follows:
</p><p><b>1.</b> For each of the three primary bucket copies on the downed peer, the master identifies a peer with another searchable copy of that bucket and tells that peer to mark the copy as primary. 
</p><p>When this step finishes, the cluster regains its valid state, and any subsequent searches provide a full set of results.
</p><p><b>2.</b> For each of the 10 searchable bucket copies on the downed peer, the master identifies 1) a peer with a searchable copy of that bucket and 2) a peer with a non-searchable copy of the same bucket. It then tells the peer with the searchable copy to stream the bucket's index files to the second peer. When the index files have been copied over, the non-searchable copy becomes searchable.
</p><p><b>3.</b> For each of the 20 total bucket copies on the downed peer, the master identifies 1) a peer with a copy of that bucket and 2) a peer that does not have a copy of the bucket. It then tells the peer with the copy to stream the bucket's rawdata to the second peer, resulting in a new, non-searchable copy of that bucket. 
</p><p>When this last step finishes, the cluster regains its complete state.
</p>
<h4><font size="3"><b><i> <a name="whathappenswhenaslavenodegoesdown_example:_fixing_buckets_to_create_a_valid_but_incomplete_cluster"><span class="mw-headline" id="Example:_Fixing_buckets_to_create_a_valid_but_incomplete_cluster">Example: Fixing buckets to create a valid but incomplete cluster </span></a></i></b></font></h4>
<p>Assume:
</p>
<ul><li> The peer went down unintentionally (that is, <i>not</i> in response to an <code><font size="2">offline</font></code> command).
</li></ul><ul><li> The downed peer is part of a cluster with these characteristics:
<ul><li> 3 peers, including the downed peer
</li><li> replication factor = 3
</li><li> search factor = 1
</li></ul></li><li> The downed peer holds these bucket copies:
<ul><li> 5 primary copies of buckets
</li><li> 5 searchable copies (the same as the number of primary copies; because the search factor = 1, all searchable copies must also be primary.)
</li><li> 20 total bucket copies (searchable and non-searchable combined)
</li></ul></li></ul><p>Since the cluster has just three peers and a replication factor of 3, the downed peer means that the cluster can no longer fulfill the replication factor and therefore cannot be made complete.
</p><p>When the peer goes down, the master sends messages to various of the remaining peers as follows:
</p><p><b>1.</b> For each of the five searchable, primary bucket copies on the downed node, the master first identifies a peer with a non-searchable copy and tells the peer to make the copy searchable. The peer then begins building index files for that copy. (Because the search factor is 1, there are no other searchable copies of those buckets on the remaining nodes. Therefore, it is not possible for the remaining peers to make non-searchable bucket copies searchable by streaming index files from another searchable copy. Rather, they must employ the slower process of creating index files from the rawdata file of a non-searchable copy.)
</p><p><b>2.</b> The master then tells the peers from step 1 to mark the five, newly searchable copies as primary. Unlike the previous example, the step of designating other bucket copies as primary cannot occur until non-searchable copies have been made searchable. Because the cluster's search factor = 1, there are no standby searchable copies.  
</p><p>Once step 2 completes, the cluster regains its valid state. Any subsequent searches provide a full set of results.
</p><p><b>3.</b> For the 20 bucket copies on the downed node, the master cannot initiate any action to make replacement copies (so that the cluster will again have the three copies of each bucket, as specified by the replication factor), because there are too few peers remaining to hold the copies. 
</p><p>Since the cluster cannot recreate a full set of replication factor number of copies of buckets, the cluster remains in an incomplete state.
</p>
<h4><font size="3"><b><i> <a name="whathappenswhenaslavenodegoesdown_pictures.2c_too"><span class="mw-headline" id="Pictures.2C_too">Pictures, too</span></a></i></b></font></h4>
<p>The following diagram shows a cluster of five peers, with a replication factor of 3 and a search factor of 2. The primary bucket copies reside on the source peer receiving data from a forwarder, with searchable and non-searchable copies of that data spread across the other peers.
</p><p><img alt="Bucket searchable primacy new 60.png" src="images/8/89/Bucket_searchable_primacy_new_60.png" width="700" height="559"></p><p><b>Note:</b> This is a highly simplified diagram. To reduce complexity, it shows only the buckets for data originating on one peer. In a real-life scenario, most, if not all, of the other peers would also be originating data and replicating it to other peers on the cluster. 
</p><p>The next diagram shows the same simplified version of the cluster, after the source node holding all the primary copies has gone down and the master has finished directing the remaining peers in fixing the buckets:
</p><p><img alt="Bucket generation 2 60.png" src="images/c/ca/Bucket_generation_2_60.png" width="700" height="559"></p><p>The master directed the cluster in a number of activities to recover from the downed peer:
</p><p><b>1.</b> The master reassigned primacy from bucket copies on the downed peer to searchable copies on the remaining peers. When this step finished, it rolled the generation ID.
</p><p><b>2.</b> It directed the peers in making a set of non-searchable copies searchable, to make up for the missing set of searchable copies. 
</p><p><b>3.</b> It directed the replication of a new set of non-searchable copies (1D, 2D, etc.), spread among the remaining peers. 
</p><p>Even though the source node went down, the cluster was able to fully recover both its complete and valid states, with replication factor number of total buckets, search factor number of searchable bucket copies, and exactly one primary copy of each bucket. This represents a different generation from the previous diagram, because primary copies have moved to different peers.
</p>
<h3> <a name="whathappenswhenaslavenodegoesdown_bucket-fixing_in_multisite_clusters"><span class="mw-headline" id="Bucket-fixing_in_multisite_clusters"> Bucket-fixing in multisite clusters</span></a></h3>
<p>The processes that multisite clusters use to handle node failure have some significant differences from single-site clusters. See <a href="#multisitearchitecture_multisite_clusters_and_node_failure" class="external text">"Multisite clusters and node failure."</a>
</p>
<h3> <a name="whathappenswhenaslavenodegoesdown_view_bucket-fixing_status"><span class="mw-headline" id="View_bucket-fixing_status"> View bucket-fixing status</span></a></h3>
<p>You can view the status of bucket fixing from the Master dashboard.  See <a href="#howtomonitoracluster" class="external text">"View the master dashboard."</a>
</p>
<a name="whathappenswhenapeercomesbackup"></a><h2> <a name="whathappenswhenapeercomesbackup_what_happens_when_a_peer_node_comes_back_up"><span class="mw-headline" id="What_happens_when_a_peer_node_comes_back_up"> What happens when a peer node comes back up</span></a></h2>
<p>A peer node can go down either intentionally (through the CLI <code><font size="2">offline</font></code> command) or unintentionally (for example, by a server crashing). When the peer goes down, the cluster undertakes remedial activities, also known as <b>bucket fixing</b>, as described in the topic, <a href="#whathappenswhenaslavenodegoesdown" class="external text">"What happens when a peer node goes down."</a> The topic you're now reading describes what happens when and if the peer later returns to the cluster.
</p><p>When a peer comes back up, it starts sending heartbeats to the master. The master recognizes it and adds it back into the cluster. If the peer still has intact bucket copies from its earlier activities in the cluster, the master adds those copies to the counts it maintains of buckets. The master also rebalances the cluster, which can result in searchable bucket copies on the peer, if any, being assigned <b>primary</b> status. For information on rebalancing, see <a href="#rebalancethecluster" class="external text">"Rebalance the indexer cluster primary buckets."</a> 
</p><p><b>Note:</b> When the peer connects with the master, it checks to see whether it already has the current version of the <b>configuration bundle</b>. If the bundle has changed since it went down, the peer downloads the latest configuration bundle, validates it locally, and restarts. The peer rejoins the cluster only if bundle validation succeeds.
</p>
<h3> <a name="whathappenswhenapeercomesbackup_how_the_master_counts_buckets"><span class="mw-headline" id="How_the_master_counts_buckets">How the master counts buckets</span></a></h3>
<p>To understand what happens when a peer returns to the cluster, you must first understand how the master tracks bucket copies.
</p><p>The master maintains counts for each bucket in the cluster. For each bucket, it knows:
</p>
<ul><li> how many copies of the bucket exist on the cluster.
</li><li> how many <b>searchable</b> copies of the bucket exist on the cluster.
</li></ul><p>The master also ensures that there's always exactly one <b>primary</b> copy of a given bucket.
</p><p>With multisite clusters, the master keeps track of copies and searchable copies for each site, as well as for the cluster as a whole. It also ensures that each site with an explicit search factor has exactly one primary copy of each bucket.
</p><p>These counts allow the master to determine whether the cluster is <b>valid</b> and <b>complete</b>. For a single-site cluster, this means that the cluster has:
</p>
<ul><li> Exactly one primary copy of each bucket.
</li><li> A full set of searchable copies for each bucket, matching the search factor.
</li><li> A full set of copies (searchable and non-searchable) for each bucket, matching the replication factor.
</li></ul><p>For a multisite cluster, a valid and complete cluster has: 
</p>
<ul><li> Exactly one primary copy of each bucket for each site with an explicit search factor.
</li><li> A full set of searchable copies for each bucket, matching the search factor for each site as well as for the cluster as a whole.
</li><li> A full set of copies (searchable and non-searchable) for each bucket, matching the replication factor for each site as well as for the cluster as a whole.
</li></ul><h3> <a name="whathappenswhenapeercomesbackup_bucket-fixing_and_the_copies_on_the_peer"><span class="mw-headline" id="Bucket-fixing_and_the_copies_on_the_peer"> Bucket-fixing and the copies on the peer</span></a></h3>
<p>When a peer goes down, the master directs the remaining peers in bucket-fixing activities. Eventually, if the bucket fixing is successful, the cluster returns to a complete state. 
</p><p>If the peer later returns to the cluster, the master adds its bucket copies to its counts (assuming that the copies were not destroyed by whatever problem caused the peer to go down in the first place). The consequences vary somewhat depending on whether bucket-fixing activity has completed by the time the peer comes back up.
</p>
<h4><font size="3"><b><i> <a name="whathappenswhenapeercomesbackup_if_bucket-fixing_is_finished"><span class="mw-headline" id="If_bucket-fixing_is_finished"> If bucket-fixing is finished </span></a></i></b></font></h4>
<p>If bucket-fixing has already completed and the cluster is in a complete state, the copies from the returned peer are just extras. For example, assume the replication factor is 3 and the cluster has fixed all the buckets so that there are again three copies of each bucket in the cluster, including the ones that the downed peer was maintaining before it went down. When the downed peer then comes back up with its copies intact, the master just adds those copies to the count, so that instead of three copies, there will be four copies of some buckets. Similarly, there could be an excess of searchable bucket copies if the returned peer was maintaining some searchable bucket copies. These extra copies might come in handy later, if another peer maintaining copies of some of those buckets goes down.
</p>
<h4><font size="3"><b><i> <a name="whathappenswhenapeercomesbackup_if_bucket-fixing_is_still_underway"><span class="mw-headline" id="If_bucket-fixing_is_still_underway"> If bucket-fixing is still underway</span></a></i></b></font></h4>
<p>If the cluster is still replacing the copies that were lost when the peer went down, the return of the peer can curtail the bucket-fixing. Once the master has added the copies on the returned peer to its counts, it knows that the cluster is complete and valid, and so it will no longer direct the other peers to make copies of those buckets. However, any peers that are currently in the middle of some bucket-fixing activity, such as copying buckets or making copies searchable, will complete their work on those copies.  Since bucket-fixing is time-intensive, it is worthwhile to bring a downed peer back online as soon as possible, particularly if the peer was maintaining a large number of bucket copies.
</p>
<h3> <a name="whathappenswhenapeercomesbackup_remove_extra_bucket_copies"><span class="mw-headline" id="Remove_extra_bucket_copies">Remove extra bucket copies</span></a></h3>
<p>If the returning peer results in extra copies of some buckets, you can save disk space by removing the extra copies. For details, read the topic, <a href="#removeextrabucketcopies" class="external text">"Remove excess bucket copies from the indexer cluster."</a>
</p>
<a name="whathappenswhenamasternodegoesdown"></a><h2> <a name="whathappenswhenamasternodegoesdown_what_happens_when_the_master_node_goes_down"><span class="mw-headline" id="What_happens_when_the_master_node_goes_down"> What happens when the master node goes down</span></a></h2>
<p>The master is essential to the proper running of an indexer cluster, with its role as coordinator for much of the cluster activity. However, if the master goes down, the peers and search head have default behaviors that allow them to function fairly normally, at least for a while. Nevertheless, you should treat a downed master as a serious failure.
</p><p>To deal with the possibility of a downed master, you can configure a stand-by master that can take over if needed. For details, see <a href="#handlemasternodefailure" class="external text">"Replace the master node on the indexer cluster"</a>.
</p>
<h3> <a name="whathappenswhenamasternodegoesdown_when_a_master_goes_down"><span class="mw-headline" id="When_a_master_goes_down">When a master goes down</span></a></h3>
<p>If a master goes down, the cluster can continue to run as usual, as long as there are no other failures. Peers can continue to ingest data, stream copies to other peers, replicate buckets, and respond to search requests from the search head.
</p><p>When a peer rolls a hot bucket, it normally contacts the master to get a list of target peers to stream its next hot bucket to. However, if a peer rolls a hot bucket while the master is down, it will just start streaming its next hot bucket to the same set of peers that it used as targets for the previous hot bucket.
</p><p>Eventually, problems will begin to arise. For example, if a peer goes down and the master is still down, there will be no way to coordinate the necessary remedial <b>bucket-fixing</b> activity. Or if, for some reason, a peer is unable to connect with one of its target peers, it has no way of getting another target. 
</p><p>The search head can also continue to function without a master, although eventually the searches will be accessing incomplete sets of data. (For example, if a peer with primary bucket copies goes down, there's no way to transfer primacy to copies on other peers, so those buckets will no longer get searched.) The search head will use the last generation ID that it got before the master went down. It will display a warning if one or more peers in the last generation are down.
</p>
<h3> <a name="whathappenswhenamasternodegoesdown_when_the_master_comes_back_up"><span class="mw-headline" id="When_the_master_comes_back_up">When the master comes back up</span></a></h3>
<p>Peers continue to send heartbeats indefinitely, so that, when the master comes back up, they will be able to detect it and reconnect. 
</p><p>When the master comes back up, it waits for a quiet period of 60 seconds, so that all peers have an opportunity to re-register with it. Once the quiet period ends, the master has a complete view of the state of the cluster, including the state of peer nodes and buckets. Assuming that at least the replication factor number of peers have registered with it, the master initiates any necessary bucket-fixing activities to ensure that the cluster is <b>valid</b> and <b>complete</b>. In addition, it rebalances the cluster and updates the generation ID as needed. 
</p><p>Bucket fixing can take some time to complete, because it involves copying buckets and making non-searchable copies searchable. For help estimating the time needed to complete the bucket-fixing activity, look <a href="#takeapeeroffline_estimate_the_cluster_recovery_time_when_a_peer_gets_decommissioned" class="external text">here</a>.
</p><p>After the 60 second quiet period is over, you can view the master dashboard for accurate information on the status of the cluster.
</p><p><b>Note:</b> You must make sure that at least replication factor number of peers are running when you restart the master.
</p>
<h1>Troubleshoot indexers and clusters of indexers</h1><a name="bucketissues"></a><h2> <a name="bucketissues_non-clustered_bucket_issues"><span class="mw-headline" id="Non-clustered_bucket_issues"> Non-clustered bucket issues</span></a></h2>
<p>This section tells you how to deal with an assortment of bucket problems that can exist independent of clustering. 
</p>
<h3> <a name="bucketissues_rebuild_all_buckets"><span class="mw-headline" id="Rebuild_all_buckets"> Rebuild all buckets </span></a></h3>
<p>The indexer usually handles crash recovery without your intervention. If an indexer goes down unexpectedly, some recently received data might not be searchable. When you restart the indexer, it will automatically run the <code><font size="2">fsck</font></code> command in the background. This command diagnoses the health of your buckets and rebuilds search data as necessary. 
</p><p><b>Caution:</b> It is unlikely that you will need to run <code><font size="2">fsck</font></code> manually. This is a good thing, because to run it manually, you must stop the indexer, and the command can take several hours to complete if your indexes are large. During that time your data will be inaccessible. However, if Splunk Support directs you to run it, the rest of this section tells you how to do so. 
</p><p>To run <code><font size="2">fsck</font></code> manually, you must first stop the indexer. Then run <code><font size="2">fsck</font></code> against any affected buckets. To run <code><font size="2">fsck</font></code> against buckets in all indexes, use this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk fsck repair --all-buckets-all-indexes<br></font></code>
</div>
<p>This will rebuild all types of buckets (hot/warm/cold) in all indexes.
</p><p><b>Note:</b> The <code><font size="2">fsck</font></code> command only rebuilds buckets created by version 4.2 or later of Splunk Enterprise.
</p><p>To learn more about the <code><font size="2">fsck</font></code> command, including a list of all options available, enter:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk fsck --help<br></font></code>
</div>
<p>The <code><font size="2">fsck repair</font></code> command can take several hours to run, depending on the size of your indexes If you determine that you only need to rebuild a few buckets, you can run the <code><font size="2">rebuild</font></code> command on just those buckets, as described in the next section, <a href="#bucketissues_rebuild_a_single_bucket" class="external text">"Rebuild a single bucket."</a>
</p><p>If you just want to diagnose the state of your indexes (without taking any immediate remedial action),  run:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk fsck scan --all-buckets-all-indexes<br></font></code>
</div>
<p><b>Note:</b> You cannot use <code><font size="2">splunk fsck</font></code> to repair a single bucket. Instead, use the <code><font size="2">splunk rebuild</font></code> command.
</p>
<h3> <a name="bucketissues_rebuild_a_single_bucket"><span class="mw-headline" id="Rebuild_a_single_bucket"> Rebuild a single bucket </span></a></h3>
<p>If the index and metadata files in a bucket (version 4.2 and later) somehow get corrupted, you can rebuild the bucket from the raw data file alone. Use this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk rebuild &lt;bucket directory&gt;<br></font></code>
</div>
<p>The indexer automatically deletes the old index and metadata files and rebuilds them. You don't need to delete any files yourself.
</p><p>You must stop the indexer before running the <code><font size="2">rebuild</font></code> command.
</p><p>Note:
</p>
<ul><li> Rebuilding a bucket does not count against your license. 
</li><li> The time required to rebuild a bucket can be significant.  Depending on various system considerations, such as your hardware specifications, it can take anywhere from half an hour to a few hours to rebuild a 10 GB bucket.
</li></ul><h3> <a name="bucketissues_recover_invalid_pre-4.2_hot_buckets"><span class="mw-headline" id="Recover_invalid_pre-4.2_hot_buckets">Recover invalid pre-4.2 hot buckets </span></a></h3>
<p>A hot bucket becomes an invalid hot (<code><font size="2">invalid_hot_&lt;ID&gt;</font></code>) bucket when the indexer detects that the metadata files (<code><font size="2">Sources.data, Hosts.data, SourceTypes.data</font></code>) are corrupt or incorrect. Incorrect data usually signifies incorrect time ranges; it can also mean that event counts are incorrect.
</p><p>The indexer ignores invalid hot buckets. Data does not get added to such buckets, and they cannot be searched. Invalid buckets also do not count when determining bucket limit values such as <code><font size="2">maxTotalDataSizeMB</font></code>. This means that invalid buckets do not negatively affect the flow of data through the system, but it also means that they can result in disk storage that exceeds the configured maximum value.
</p><p>To recover an invalid hot bucket, use the <code><font size="2">recover-metadata</font></code> command:
</p><p><b>1.</b> Make backup copies of the metadata files, <code><font size="2">Sources.data, Hosts.data, SourceTypes.data</font></code>.
</p><p><b>2.</b> Rebuild the metadata from the raw data information:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk cmd recover-metadata path_to_your_hot_buckets/invalid_hot_&lt;ID&gt;<br></font></code>
</div>
<p><b>3.</b> If successful, rename the bucket as it would normally be named.
</p>
<h3> <a name="bucketissues_rebuild_index-level_bucket_manifests"><span class="mw-headline" id="Rebuild_index-level_bucket_manifests">Rebuild index-level bucket manifests</span></a></h3>
<p>It is rare that you might have reason to rebuild index-level manifests, but if you need to, the indexer provides a few commands that do just that.  
</p><p><b>Caution:</b> You should only use these commands if Splunk Support directs you to. Do not rebuild the manifests on your own.
</p><p>The two index-level manifest files are <code><font size="2">.bucketManifest</font></code> and <code><font size="2">.metaManifest</font></code>. The <code><font size="2">.bucketManifest</font></code> file contains a list of all buckets in the index. You might need to rebuild this if, for example, you manually copy a bucket into an index. The <code><font size="2">.metaManifest</font></code> file contains a list of buckets that have contributed to the index-level metadata file.
</p><p>The following command rebuilds the <code><font size="2">.bucketManifest</font></code> and <code><font size="2">.metaManifest</font></code> files and all <code><font size="2">*.data</font></code> files in the homePath for the main index <b>only</b>. It does <b>not</b> rebuild metadata for individual buckets:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk _internal call /data/indexes/main/rebuild-metadata-and-manifests<br></font></code>
</div>
<p>If you only want to rebuild the <code><font size="2">.metaManifest</font></code> and <code><font size="2">homePath/*.data</font></code> files, use this command instead:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk _internal call /data/indexes/main/rebuild-metadata<br></font></code>
</div>
<p>If you only want to rebuild the <code><font size="2">.bucketManifest</font></code> file, use this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk _internal call /data/indexes/main/rebuild-bucket-manifest<br></font></code>
</div>
<p>You can use the asterisk (*) wildcard to rebuild manifests for all indexes. For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk _internal call /data/indexes/*/rebuild-metadata-and-manifests<br></font></code>
</div>

<a name="bucketreplicationissues"></a><h2> <a name="bucketreplicationissues_bucket_replication_issues"><span class="mw-headline" id="Bucket_replication_issues"> Bucket replication issues</span></a></h2>
<h3> <a name="bucketreplicationissues_network_issues_impede_bucket_replication"><span class="mw-headline" id="Network_issues_impede_bucket_replication">Network issues impede bucket replication</span></a></h3>
<p>If there are problems with the connection between peer nodes such that a source peer is unable to replicate a hot bucket to a target peer, the source peer rolls the hot bucket and start a new hot bucket. If it still has problems connecting with the target peer, it rolls the new hot bucket, and so on. 
</p><p>To prevent a situation from arising where a prolonged failure causes the source peer to generate a large quantity of small hot buckets, the source peer, after a configurable number of replication errors to a single target peer, stops rolling hot buckets in response to the connection problem with that target peer.  The default is three replication errors. The following banner message then appears one or more times in the master node's dashboard, depending on the number of source peers encountering errors:
</p>
<div class="samplecode">
<code><font size="2"><br>Search peer &lt;search peer&gt; has the following message: Too many streaming errors to target=&lt;target <br>peer&gt;. Not rolling hot buckets on further errors to this target. (This condition might exist with <br>other targets too. Please check the logs.)<br></font></code>
</div>
<p>While the network problem persists, there might not be replication factor number of  copies available for the most recent hot buckets.
</p>
<h3> <a name="bucketreplicationissues_configure_the_allowable_number_of_replication_errors"><span class="mw-headline" id="Configure_the_allowable_number_of_replication_errors">Configure the allowable number of replication errors</span></a></h3>
<p>To adjust the allowable number of replication errors, you can configure the <code><font size="2">max_replication_errors</font></code> attribute in <code><font size="2">server.conf</font></code> on the source peer.  However, it is unlikely that you will need to change the attribute from its default of 3, because replication errors that can be attributed to a single network problem are bunched together and only count as one error. The  "Too many streaming errors" banner message might still appear, but it can be ignored.
</p><p><b>Note:</b> The bunching of replication errors is a change introduced in release 6.0. With this change, the number of errors will be unlikely to exceed the default value of 3, except in unusual conditions.
</p>
<h3> <a name="bucketreplicationissues_evidence_of_replication_failure_on_the_source_peer"><span class="mw-headline" id="Evidence_of_replication_failure_on_the_source_peer">Evidence of replication failure on the source peer</span></a></h3>
<p>Evidence of replication failure appears in the source peer's <code><font size="2">splunkd.log</font></code>, with a reference to the failed target peer(s). You can locate the relevant lines in the log by searching on "CMStreamingErrorJob". For example, this <code><font size="2">grep</font></code> command finds that there have been 15 streaming errors to the peer with the GUID "B3D35EF4-4BC8-4D69-89F9-3FACEDC3F46E": 
</p>
<div class="samplecode">
<code><font size="2"><br>grep CMStreamingErrorJob ../var/log/splunk/splunkd.log* | cut -d' ' -f10 | sort |uniq -c | sort -nr<br>15 failingGuid=B3D35EF4-4BC8-4D69-89F9-3FACEDC3F46E <br></font></code>
</div>
<h3> <a name="bucketreplicationissues_unable_to_disable_and_re-enable_a_peer"><span class="mw-headline" id="Unable_to_disable_and_re-enable_a_peer"> Unable to disable and re-enable a peer </span></a></h3>
<p>When you disable an indexer as a peer, any hot buckets that were on the peer at the time it was disabled are rolled to warm and named using the standalone bucket convention.  If you later re-enable the peer, a problem arises because the master remembers those buckets as clustered and expects them to be named according to the clustered bucket convention, but instead they are named using the convention for standalone buckets. Because of this naming discrepancy, the peer cannot rejoin the cluster.
</p><p>To work around this issue, you must clean the buckets or otherwise remove the standalone buckets on the peer before re-enabling it.
</p>
<h3> <a name="bucketreplicationissues_multisite_cluster_does_not_meet_its_replication_or_search_factors"><span class="mw-headline" id="Multisite_cluster_does_not_meet_its_replication_or_search_factors"> Multisite cluster does not meet its replication or search factors </span></a></h3>
<p>The symptom is a message that the multisite cluster does not meet its replication or search factors. This message can appear, for example, on the master dashboard. This condition occurs immediately after bringing up a multisite cluster. 
</p><p>Compare the values for the single-site <code><font size="2">replication_factor</font></code> and <code><font size="2">search_factor</font></code> attributes to the number of peers that you have on each site. (If you did not explicitly set the single-site replication and search factors, then they default to 3 and 2, respectively.) If the number of peers on any site is less than the smaller of the two attribute values, set both of those attributes to match the least number of peers on any site.
</p>
<a name="configurationbundleissues"></a><h2> <a name="configurationbundleissues_configuration_bundle_issues"><span class="mw-headline" id="Configuration_bundle_issues"> Configuration bundle issues</span></a></h2>
<p>This topic describes problems that can arise when pushing a configuration bundle from the master to the peer nodes.
</p>
<h3> <a name="configurationbundleissues_bundle_validation_failure_when_pushing_a_very_large_bundle"><span class="mw-headline" id="Bundle_validation_failure_when_pushing_a_very_large_bundle">Bundle validation failure when pushing a very large bundle </span></a></h3>
<p>If you attempt to push a very large bundle (&gt;200MB), bundle validation might fail due to various timeouts. To remediate:
</p><p><b>1.</b> Edit the master's <code><font size="2">server.conf</font></code> file to include these settings:
</p>
<div class="samplecode">
<code><font size="2"><br>[sslConfig]<br>allowSslCompression = false<br><br>[clustering]<br>heartbeat_timeout = 600<br></font></code>
</div>
<p><b>2.</b> Restart the master.
</p><p><b>3.</b> Push a bundle that includes some updated <code><font size="2">server.conf</font></code> settings to the peers:
</p><p><b>a.</b> Move to a temporary location any apps that you added to the bundle since the last successful push.
</p><p><b>b.</b> Create <code><font size="2">$SPLUNK_HOME/etc/master-apps/_cluster/local/server.conf</font></code> on the master with these settings:
</p>
<div class="samplecode">
<code><font size="2"><br>[general]<br>useHTTPClientCompression = true<br><br>[clustering]<br>heartbeat_period = 30<br>cxn_timeout = 300<br>send_timeout = 300<br>rcv_timeout = 300<br></font></code>
</div>
<p><b>c.</b> Push the bundle to the peers:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk apply cluster-bundle<br></font></code>
</div>
<p>This will initiate a rolling restart of all peers.
</p><p><b>4.</b> Re-add into the bundle the apps that you deleted from the bundle in step 3a, leaving the new <code><font size="2">server.conf</font></code> file there as well.
</p><p><b>5.</b> Push the expanded bundle to the peers.
</p>
</body><script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

        <script src="js/index.js"></script></html>
