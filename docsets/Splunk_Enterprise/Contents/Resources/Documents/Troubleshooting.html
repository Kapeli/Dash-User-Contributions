<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="http://ogp.me/ns/fb#" charset="utf-8"><head><meta charset="UTF-8"><title></title>
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" type="text/css" href="css/main.css">
<link rel="stylesheet" href="css/style.css">
<style>
html,body {
margin: 0px;
padding: 10px;
width: 210mm;
max-width: 210mm;
overflow-x: hidden;
}
pre {
	width: 100%;
	overflow-x: hidden;
}
</style></head><body><h1>Welcome to the Troubleshooting Manual</h1><a name="whatsinhere"></a><div class="all-questions"><h2> <a name="whatsinhere_what.27s_in_here.3f"><span class="mw-headline" id="What.27s_in_here.3F"> What's in here? </span></a></h2>
<p>Welcome to the Splunk Enterprise Troubleshooting Manual!
</p><p>Use this manual to troubleshoot your instance of Splunk.
</p><p>As with all Splunk docs, use the comment box feedback link at the bottom of each page to make any suggestions.
</p><p>Here's a brief description of each chapter you see in the left navigation bar:
</p>
<h3> <a name="whatsinhere_first_steps"><span class="mw-headline" id="First_steps"> First steps </span></a></h3>
<p>Get oriented here. <a href="#intrototroubleshootingsplunk" class="external text">Find some tips</a> about where to start with your troubleshooting.
</p>
<h3> <a name="whatsinhere_splunk_log_files"><span class="mw-headline" id="Splunk_log_files"> Splunk log files </span></a></h3>
<p><a href="#whatsplunklogsaboutitself" class="external text">Splunk Enterprise logs all sorts of things about itself</a>. Find out what, where, and how in this section.
</p>
<h3> <a name="whatsinhere_contact_splunk_support"><span class="mw-headline" id="Contact_Splunk_Support"> Contact Splunk Support </span></a></h3>
<p>We're here to help! If you're stuck, do contact us! <a href="#contactsplunksupport" class="external text">Details and tips in this section</a>.
</p>
<h3> <a name="whatsinhere_some_common_scenarios"><span class="mw-headline" id="Some_common_scenarios"> Some common scenarios </span></a></h3>
<p><a href="#cantfinddata" class="external text">This section</a> includes some of the most common scenarios we see in Splunk Support, with suggestions about what to do. Much more material is in the works here!
</p>
<h4><font size="3"><b><i> <a name="whatsinhere_make_a_pdf_of_this_manual"><span class="mw-headline" id="Make_a_PDF_of_this_manual"> Make a PDF of this manual </span></a></i></b></font></h4>
<p>If you'd like a PDF of this manual, click the red  <b>Download the Troubleshooting Manual as PDF</b> link in the table of contents bar on the left side of this page. A PDF version of the manual is generated on the fly for you, and you can save it or print it out to read later.
</p>
<h1>First steps</h1><a name="intrototroubleshootingsplunk"></a><h2> <a name="intrototroubleshootingsplunk_intro_to_troubleshooting_splunk_enterprise"><span class="mw-headline" id="Intro_to_troubleshooting_Splunk_Enterprise"> Intro to troubleshooting Splunk Enterprise</span></a></h2>
<p>This topic is intended as a first step in either diagnosing your Splunk Enterprise problem yourself or asking for help.
</p>
<h3> <a name="intrototroubleshootingsplunk_narrow_down_the_problem"><span class="mw-headline" id="Narrow_down_the_problem"> Narrow down the problem </span></a></h3>
<p>For example, if the error occurs in a dashboard or alert, check the underlying search first to see whether the error appears there. When troubleshooting searches, it's almost always best to remove the dashboard layer as soon as possible.
</p><p>For another example, does the problem exist in one app but not the other? With one user but not admins?
</p><p>Basically, is there any case for which this <i>does</i> work?
</p>
<h3> <a name="intrototroubleshootingsplunk_did_the_error_start_occurring_after_the_product_was_functioning_normally.3f"><span class="mw-headline" id="Did_the_error_start_occurring_after_the_product_was_functioning_normally.3F"> Did the error start occurring after the product was functioning normally? </span></a></h3>
<p><b>Yes</b>! So what has changed? Remember to think of both Splunk and non-Splunk factors. Was there a server outage? Network problems? Has any configuration or topology changed?
</p><p><b>No</b>, it never functioned normally. Check the operating environment and installation. Start with the system requirements in the Installation Manual.
</p>
<h3> <a name="intrototroubleshootingsplunk_resources_to_help_you"><span class="mw-headline" id="Resources_to_help_you"> Resources to help you </span></a></h3>
<h4><font size="3"><b><i> <a name="intrototroubleshootingsplunk_configurations"><span class="mw-headline" id="Configurations"> Configurations </span></a></i></b></font></h4>
<p>Splunk has configuration files in several locations, with rules about which files take precedence over each other. Use btool to check which settings your Splunk instance is using. Read about <a href="#usebtooltotroubleshootconfigurations" class="external text">btool in this manual</a>.
</p><p>The *.conf files are case-sensitive. Check settings and values against the spec and example configuration files in the Admin manual.
</p><p>There are also a lot of settings in the .conf files that aren't exposed in Splunk Web. It's best to leave these alone unless you know what changing these settings might do.
</p>
<h4><font size="3"><b><i> <a name="intrototroubleshootingsplunk_splunk_log_files"><span class="mw-headline" id="Splunk_log_files"> Splunk log files </span></a></i></b></font></h4>
<p>Splunk has various internal log files that can help you diagnose problems. Read about the <a href="#whatsplunklogsaboutitself" class="external text">log files in this manual</a>.
</p>
<h4><font size="3"><b><i> <a name="intrototroubleshootingsplunk_understand_how_your_data_gets_into_splunk"><span class="mw-headline" id="Understand_how_your_data_gets_into_Splunk"> Understand how your data gets into Splunk </span></a></i></b></font></h4>
<p>The Distributed Deployment Manual has a high-level overview of the Splunk data pipeline, breaking it into input, parsing, indexing, and search segments. 
</p><p>For more detail on each segment, see this Community Wiki article about how indexing works.
</p>
<h4><font size="3"><b><i> <a name="intrototroubleshootingsplunk_i.27ve_figured_out_exactly_where_the_problem_is"><span class="mw-headline" id="I.27ve_figured_out_exactly_where_the_problem_is"> I've figured out exactly where the problem is </span></a></i></b></font></h4>
<p>Hey, well done!
</p><p>Check the (continuously growing) chapter in this manual on <a href="#cantfinddata" class="external text">some of the most common symptoms and solutions</a>.
</p><p>If you need additional help or opinions, ask the Splunk community! The Community Wiki, Splunk Answers, and the #splunk IRC channel on efnet are available to everyone and provide a great resource.
</p>
<h3> <a name="intrototroubleshootingsplunk_test_potential_fixes_or_workarounds"><span class="mw-headline" id="Test_potential_fixes_or_workarounds"> Test potential fixes or workarounds </span></a></h3>
<p>Once you've found a way to fix the problem, test it! Test any noninvasive changes first. Then, test any changes that would create minor interruptions. Make sure no new issues arise from your tested solution.
</p><p>Always test invasive or major changes in a sandbox environment before moving them to your production system! Your sandbox should be an independent system that mirrors the affected environment.
</p>
<h3> <a name="intrototroubleshootingsplunk_stuck.3f"><span class="mw-headline" id="Stuck.3F"> Stuck? </span></a></h3>
<p>If you get stuck at any point, contact Splunk Support. Don't forget to send a diag! Read about making a diag in this manual.
</p>
<a name="checksplunkversion"></a><h2> <a name="checksplunkversion_determine_which_version_of_splunk_enterprise_you.27re_running"><span class="mw-headline" id="Determine_which_version_of_Splunk_Enterprise_you.27re_running"> Determine which version of Splunk Enterprise you're running </span></a></h2>
<h3> <a name="checksplunkversion_in_splunk_web"><span class="mw-headline" id="In_Splunk_Web"> In Splunk Web </span></a></h3>
<p>Click the <b>About</b> link at the bottom left of most pages in Splunk Web to view a JavaScript overlay with the version and build numbers.
</p><p><img alt="About6.png" src="images/4/47/About6.png" width="300" height="63"></p>
<h3> <a name="checksplunkversion_at_the_command_line"><span class="mw-headline" id="At_the_command_line"> At the command line </span></a></h3>
<p>Use one minus or two minuses; Splunk gets it either way:
</p>
<code><font size="2"><br>&gt; ./splunk --version<br>Splunk 6.0 (build 181491)</font></code>
<p>or
</p>
<code><font size="2">&gt; ./splunk -version<br>Splunk 6.0 (build 181491)<br></font></code>
<h3> <a name="checksplunkversion_from_the_files"><span class="mw-headline" id="From_the_files"> From the files </span></a></h3>
<p>You can get the version information from the file <code><font size="2">$SPLUNK_HOME/etc/splunk.version</font></code>
</p>
<code><font size="2"><br>&gt; cat $SPLUNK_HOME/etc/splunk.version <br>VERSION=6.0<br>BUILD=181491<br>PRODUCT=splunk<br>PLATFORM=Darwin-x86_64<br></font></code>
<h3> <a name="checksplunkversion_in_splunk_search"><span class="mw-headline" id="In_Splunk_Search"> In Splunk Search </span></a></h3>
<p>Splunk Enterprise indexes the splunk.version file into the _internal index and sends it along to the indexer by forwarders.
</p><p>Here's a search that shows you how many installs you have of each Splunk Enterprise version:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2"> index=_internal sourcetype=splunk_version | dedup host | top VERSION
</font></code><br></div>

<a name="usebtooltotroubleshootconfigurations"></a><h2> <a name="usebtooltotroubleshootconfigurations_use_btool_to_troubleshoot_configurations"><span class="mw-headline" id="Use_btool_to_troubleshoot_configurations"> Use btool to troubleshoot configurations</span></a></h2>
<p>The Splunk Enterprise configuration file system supports many overlapping configuration files in many different locations. How these configuration files interact with  and take precedence over one another is described in "Configuration file precedence" in the Admin Manual. This flexibility can occasionally make it hard to figure out exactly which configuration value Splunk Enterprise is using.
</p><p>To help you out, Splunk provides <code><font size="2">btool</font></code>. This is a command line tool that can help you troubleshoot configuration file issues or just see what values are being used by your Splunk Enterprise installation.
</p><p>Btool displays merged on-disk configurations. To view in-memory configurations, query the REST endpoint /services/properties/
</p><p><b>Note:</b> <code><font size="2">btool</font></code> is not tested by Splunk and is not officially supported or guaranteed. That said, it's what our Support team uses when trying to troubleshoot your issues.
</p>
<h3> <a name="usebtooltotroubleshootconfigurations_investigate_configuration_values_of_your_entire_splunk_installation"><span class="mw-headline" id="Investigate_configuration_values_of_your_entire_Splunk_installation">Investigate configuration values of your entire Splunk installation</span></a></h3>
<p>You can run <code><font size="2">btool</font></code> to see all the configuration values in use by your Splunk instance. 
</p><p>From <code><font size="2">$SPLUNK_HOME/bin</font></code> type:
</p><p><code><font size="2">./splunk cmd btool &lt;conf_file_prefix&gt; list</font></code>
</p><p>where <code><font size="2">&lt;conf_file_prefix&gt;</font></code> is the name of the configuration file you're interested in (minus the <code><font size="2">.conf</font></code> extension). The <code><font size="2">list</font></code> literal specifies that you want to list the options.
</p><p>For example, to see what settings transforms.conf is using, type:
</p><p><code><font size="2">./splunk cmd btool transforms list</font></code>
</p><p>You probably want to send the results of btool into a text file that you can peruse then delete, like this:
</p><p><code><font size="2">./splunk cmd btool transforms list &gt; /tmp/transformsconfigs.txt</font></code>
</p><p>or if not to a file, at least pipe to grep like this:
</p><p><code><font size="2">./splunk cmd btool server list --debug | grep '\[' </font></code>
</p><p>which determines which server.conf stanzas are being recognized.
</p><p>Piping to a file is handy for all use cases of btool, but for simplicity we'll only explicitly mention it this once.
</p>
<h3> <a name="usebtooltotroubleshootconfigurations_investigate_configuration_values_in_one_app"><span class="mw-headline" id="Investigate_configuration_values_in_one_app">Investigate configuration values in one app</span></a></h3>
<p>You can also run <code><font size="2">btool</font></code> for a specific app in your Splunk installation. It will list all the configuration values in use by that app for a given configuration file.
</p><p>To run <code><font size="2">btool</font></code>, go to <code><font size="2">$SPLUNK_HOME/bin</font></code> and type:
</p><p><code><font size="2">./splunk cmd btool --app=&lt;app_name&gt; &lt;conf_file_prefix&gt; list</font></code>
</p><p>where <code><font size="2">&lt;app_name&gt;</font></code> is the name of the app you want to see the configurations for.
</p><p>For example, if you want to know what configuration options are being used in <code><font size="2">props.conf</font></code> by the Search app, type:
</p><p><code><font size="2">./splunk cmd btool --app=search props list</font></code>
</p><p>This returns a list of the <code><font size="2">props.conf</font></code> settings currently being used for the Search app.
</p><p>The app name is not required. In fact, it's often a good idea <b>not</b> to specify the app when using <code><font size="2">btool</font></code>. In the case of <code><font size="2">btool</font></code>, insight into all of your configurations can be helpful.
</p>
<h3> <a name="usebtooltotroubleshootconfigurations_learn_where_configuration_values_come_from"><span class="mw-headline" id="Learn_where_configuration_values_come_from">Learn where configuration values come from</span></a></h3>
<p>Another thing you can do with <code><font size="2">btool</font></code> is find out from which specific app Splunk is pulling its configuration parameters for a given configuration file. To do this, add the <code><font size="2">--debug</font></code> flag to <code><font size="2">btool</font></code> like in this example for props.conf:
</p><p><code><font size="2">./splunk cmd btool props list --debug</font></code>
</p><p><br>
Read about <code><font size="2">btool</font></code> syntax in <a href="#commandlinetoolsforusewithsupport" class="external text">"Command line tools for use with Support"</a>.
</p>
<h3> <a name="usebtooltotroubleshootconfigurations_additional_resources"><span class="mw-headline" id="Additional_resources"> Additional resources </span></a></h3>
<p>Watch a video on using btool to troubleshoot configuration issues by a Splunk Support engineer.
</p><p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has using btool.
</p>
<a name="splunkonsplunkapp"></a><h2> <a name="splunkonsplunkapp_splunk_on_splunk_app"><span class="mw-headline" id="Splunk_on_Splunk_app"> Splunk on Splunk app </span></a></h2>
<p>Splunk on Splunk (SoS) is an app that uses Splunk Enterprise diagnostic tools to analyze and troubleshoot your configuration. SoS contains views and tooling that allow you to do the following:
</p>
<ul><li> View, search and compare Splunk Enterprise configuration files.
</li><li> Detect and expose errors and anomalies in your installation, including inspection of crash logs.
</li><li> Measure indexing performance and expose event processing bottlenecks.
</li><li> View details of scheduler and user-driven search activity.
</li><li> Analyze Splunk Enterprise data volume metrics.
</li></ul><p>Download the Splunk on Splunk app from Splunkbase.
</p><p>For information about installing and configuring the Splunk on Splunk app, see the Splunk on Splunk documentation.
</p>
<h3> <a name="splunkonsplunkapp_looking_at_your_splunk_enterprise_installation_with_splunk_on_splunk"><span class="mw-headline" id="Looking_at_your_Splunk_Enterprise_installation_with_Splunk_on_Splunk"> Looking at your Splunk Enterprise installation with Splunk on Splunk  </span></a></h3>
<p>Each view offers help to explain the significance of the different charts and panels shown, as well as the searches that populate them.
</p><p>The SoS app (version 1.0) contains the following views:
</p><p><b>Home</b>: Provides an introduction to SoS.
</p><p><b>Configuration File Viewer</b>: Provides a layered view of the Splunk Enterprise configuration files, allowing you to search and compare files side by side.
</p><p><b>Errors</b>: Parses Splunk Enterprise internal logs to help expose errors and abnormal behavior. Contains dedicated search controls to help you locate the source of problems.
</p><p><b>Warnings</b>: Detects known problems that may exist on your Splunk Enterprise instance.
</p><p><b>Crash Log Viewer</b>: Detects and displays recent crash logs and correlates them with Splunk Enterprise log files.
</p><p><b>Indexing Performance</b>: Tracks indexing performance, allowing you to correlate measured latency and data volume with the size of event processing queues.
</p><p><b>Search Detail Activity</b>: Displays CPU utilization for all searches, providing various ways to analyze and compare searches.
</p><p><b>UI and User Search Activity</b>: Provides analysis of dashboards most viewed, user activity, time to completion, and other related data for searches.
</p><p><b>Scheduler Activity</b>: Shows a variety of performance and usage metrics for the search scheduler. Also, displays statistics on alert actions associated with scheduled searches.
</p><p><b>Metrics</b>: Shows license usage over time as well as different breakdowns of indexing throughput (per source, per source type, per host, per index) recorded in metrics.log. Also provides statistics on incoming and outgoing network traffic.
</p>
<h1>Splunk Enterprise log files</h1><a name="whatsplunklogsaboutitself"></a><h2> <a name="whatsplunklogsaboutitself_what_splunk_logs_about_itself"><span class="mw-headline" id="What_Splunk_logs_about_itself"> What Splunk logs about itself </span></a></h2>
<p>Splunk Enterprise keeps track of its activity by logging to various files in <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code>.  
</p><p>The Splunk Enterprise internal log files are rolled based on size. You can change the default log rotation size by editing <code><font size="2">$SPLUNK_HOME/etc/log.cfg</font></code>.
</p><p>Search these files in Splunk Web by typing:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal</font></code><br></div>
<h3> <a name="whatsplunklogsaboutitself_internal_logs"><span class="mw-headline" id="Internal_logs"> Internal logs </span></a></h3>
<p>Here is a list, with descriptions, of the internal logs in <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code>. Splunk's internal logs are useful for troubleshooting or metric analysis.
</p><p>Note that some log files are not created until your Splunk instance uses them, for example crawl.log.
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr bgcolor="#D9EAED"><th bgcolor="#C0C0C0"> Log file name
</th><th bgcolor="#C0C0C0"> Useful for?
</th></tr><tr><td valign="center" align="left"> audit.log
</td><td valign="center" align="left"> Information about user activity, most interestingly about a user logging in (or failing to log in), modifying a setting, or running a search. For example, if you're looking for information about a saved search, audit.log matches the name of a saved search (savedsearch_name) with its search ID (search_id), user, and time. With the search_id, you can look up that particular search elsewhere, like in the <a href="#whatsplunklogsaboutitself_splunk_search_logs" class="external text">search dispatch directory</a>. Read about audit events in the Securing Splunk Manual.
</td></tr><tr><td valign="center" align="left"> btool.log
</td><td valign="center" align="left"> Log of btool activity. Read about <a href="#usebtooltotroubleshootconfigurations" class="external text">btool in this manual</a>.
</td></tr><tr><td valign="center" align="left">conf.log
</td><td valign="center" align="left">Contains messages about configuration replication related to search head clustering.
</td></tr><tr><td valign="center" align="left"> crawl.log
</td><td valign="center" align="left"> Log of crawl activity. Read about crawl in the Getting Data In Manual. Crawl is now deprecated.
</td></tr><tr><td valign="center" align="left"> django_access.log
</td><td valign="center" align="left">Django HTTP request log (equivalent to web_access.log) for the Django Bindings component of the Splunk Web Framework.
</td></tr><tr><td valign="center" align="left"> django_error.log
</td><td valign="center" align="left"> Raw Django error output from Splunk Web Framework (not really meant to be human readable). Used with link on error screens to see the full error in Splunk Web.
</td></tr><tr><td valign="center" align="left"> django_service.log
</td><td valign="center" align="left"> General Django related messages from Splunk Web Framework (equivalent to web_service.log)
</td></tr><tr><td valign="center" align="left"> export_metrics.log
</td><td valign="center" align="left"> Log of metrics related to exporting data with Hadoop Connect.
</td></tr><tr><td valign="center" align="left"> first_install.log
</td><td valign="center" align="left"> Shows version number.
</td></tr><tr><td valign="center" align="left"> inputs.log
</td><td valign="center" align="left">Inputs found by crawl. This log file will be empty unless you use the crawl command.
</td></tr><tr><td valign="center" align="left">intentions.log
</td><td valign="center" align="left"><b>Intentions</b> activity. Read about intentions in the Developing Views and Apps for Splunk Web Manual.
</td></tr><tr><td valign="center" align="left"> license_audit.log
</td><td valign="center" align="left"> Deprecated. Look at license_usage.log instead of here.
</td></tr><tr><td valign="center" align="left"> license_usage.log
</td><td valign="center" align="left"> Indexed volume in bytes per pool, index, source, sourcetype, and host. Starting in 4.2, license_usage.log is available only on a Splunk license master.
</td></tr><tr><td valign="center" align="left"> metrics.log
</td><td valign="center" align="left"> Contains periodic snapshots of Splunk performance and system data, including information about CPU usage by internal processors and queue usage in Splunk's data processing. The metrics.log file is a sampling of the top ten items in each category in 30 second intervals, based on the size of _raw. It can be used for limited analysis of volume trends for data inputs. For more information about metrics.log, see <a href="#aboutmetricslog" class="external text">About metrics.log</a> and <a href="#metricslog" class="external text">Work with metrics.log</a> in this manual.
</td></tr><tr><td valign="center" align="left"> migration.log
</td><td valign="center" align="left"> A log of events during install and migration.  Specifies which files were altered during upgrade.
</td></tr><tr><td valign="center" align="left"> mongod.log
</td><td valign="center" align="left"> Contains runtime messages from the Splunk Enterprise app key value store.
</td></tr><tr><td valign="center" align="left"> python.log
</td><td valign="center" align="left"> Python events within Splunk. Useful for debugging REST endpoints, communication with splunkd, PDF Report Server App, Splunk Web display issues, sendmail (email alerts), and scripted inputs. With web_service.log, one of the few Splunk logs that uses "WARNING" instead of "WARN" for second most verbose logging level.
</td></tr><tr><td valign="center" align="left"> remote_searches.log
</td><td valign="center" align="left"> Messages from StreamedSearch channel. This code is executed on the search peers when a search head makes a search request. So this file contains useful information on indexers regarding searches they're participating in.
</td></tr><tr><td valign="center" align="left"> scheduler.log
</td><td valign="center" align="left"> All actions (successful or unsuccessful) performed by the splunkd search and alert scheduler. Typically, this shows scheduled search activity.
</td></tr><tr><td valign="center" align="left"> searches.log
</td><td valign="center" align="left"> Beginning with Splunk 5, no longer used. Instead, use the following search syntax: <code><font size="2">|  history</font></code>. This shows all the searches that have been run, plus stats for the searches.
</td></tr><tr><td valign="center" align="left"> searchhistory.log
</td><td valign="center" align="left"> No longer used.
</td></tr><tr><td valign="center" align="left"> splunkd.log
</td><td valign="center" align="left"> The primary log written to by the Splunk server. May be requested by Splunk Support for troubleshooting purposes. Any <code><font size="2">stderr</font></code> messages generated by scripted inputs, scripted search commands, and so on, are logged here.
</td></tr><tr><td valign="center" align="left"> splunkd_access.log
</td><td valign="center" align="left"> Any action done from splunkd through the UI is logged here, including splunkweb, the CLI, all POST GET actions, deleted saved searches, and other programs accessing the REST endpoints. Also logs the time taken to respond to the requests. Search job artifacts logged here include size of data returned with search.  sourcetype="splunkd_access"
</td></tr><tr><td valign="center" align="left"> splunkd_stderr.log
</td><td valign="center" align="left"> The Unix standard error device for the server. Typically this contains (for *nix) times of healthy start and stop events, as well as various errors like exceptions, assertions, and errors generated by libraries and the operating system.
</td></tr><tr><td valign="center" align="left"> splunkd_stdout.log
</td><td valign="center" align="left"> The Unix standard output device for the server.
</td></tr><tr><td valign="center" align="left"> splunkd_ui_access.log
</td><td valign="center" align="left"> Starting in 6.2, contains a significant portion of the types of events that used to be logged in web_access.log.
</td></tr><tr><td valign="center" align="left"> splunkd-utility.log
</td><td valign="center" align="left"> This log is written to by the prereq-checking utils <code><font size="2">splunkd clone-prep-clear-config</font></code>, <code><font size="2">splunkd validatedb</font></code>, <code><font size="2">splunkd check-license</font></code>, <code><font size="2">splunkd check-transforms-keys</font></code>, and <code><font size="2">splunkd rest</font></code> (for offline CLI). Each util logs Splunk version, some basic config, and current OS limits like max number of threads, and then messages specific to the util. Consult this log file when splunkd didn't start.
</td></tr><tr><td valign="center" align="left"> web_access.log
</td><td valign="center" align="left"> Requests made of Splunk Web, in an Apache access_log format. Much of the types of events logged here are logged in splunkd_ui_access.log starting in 6.2.
</td></tr><tr><td valign="center" align="left"> web_service.log
</td><td valign="center" align="left"> Primary log written by splunkweb. Records actions made by splunkweb. This and python.log are the only logs that, in second most verbose logging level, write messages with "WARNING" instead of Splunk log files' usual "WARN."
</td></tr></table><h3> <a name="whatsplunklogsaboutitself_introspection_logs"><span class="mw-headline" id="Introspection_logs"> Introspection logs </span></a></h3>
<p>Splunk Enterprise platform instrumentation refers to data that your Splunk Enterprise deployment logs in the _introspection index. It gathers data about your Splunk instance and operating system and writes it to log files that you can search later to aid in troubleshooting a variety of problems. You can also view the data at REST endpoints.
</p><p>Read more in "<a href="#abouttheplatforminstrumentationframework" class="external text">About Splunk Enterprise platform instrumentation</a>" in this manual.
</p>
<h3> <a name="whatsplunklogsaboutitself_splunk_search_logs"><span class="mw-headline" id="Splunk_search_logs"> Splunk search logs</span></a></h3>
<p>Splunk also creates search logs. Note that these are not indexed to _internal.
</p><p>Each search has its own directory for all information specific to the search, including its search logs. The search's directory is named with (among other parameters) the search_id. (Match a search to its search_id in audit.log.) You'll find the search directory in <code><font size="2">$SPLUNK_HOME/var/run/splunk/dispatch/</font></code>.
</p><p>If you have any long-running real-time searches, you might want to adjust the maximum size of your search logs. These logs are rotated when they reach a default maximum size of 25 MB. Splunk keeps up to five of them for each search, so the total log size for a search can conceivably grow as large as 125 MB.
</p><p>Most searches are unlikely to generate logs anywhere near 25 MB in size; however, it can become an issue if you have ongoing real-time searches.
</p><p>To adjust the log size, edit <code><font size="2">$SPLUNK_HOME/etc/log-searchprocess.cfg</font></code>.
</p>
<h3> <a name="whatsplunklogsaboutitself_debug_mode"><span class="mw-headline" id="Debug_mode"> Debug mode </span></a></h3>
<p>Splunk has a debugging parameter. Read about <a href="#enabledebuglogging" class="external text">enabling debug logging in this manual</a>.
</p><p>Except where noted above, Splunk's internal logging levels are <code><font size="2">DEBUG INFO WARN ERROR FATAL</font></code> (from most to least verbose).
</p><p><b>Note:</b> Running Splunk with debugging turned on outputs a large amount of information.  Make sure you do not leave debugging on for any significant length of time.
</p>
<h3> <a name="whatsplunklogsaboutitself_use_splunk_web_to_manage_logs"><span class="mw-headline" id="Use_Splunk_Web_to_manage_logs"> Use Splunk Web to manage logs </span></a></h3>
<p>To view and manage logs, you can use Splunk Web:
</p><p><b>1.</b> Navigate to <b>Settings &gt; System settings &gt; System logging</b>. This generates a list of log channels and their status.
</p><p><b>2.</b> To change the logging level for a particular log channel, click on that channel. This brings up a page specific to that channel.
</p><p><b>3.</b> On the log channel's page, you can change its logging level. 
</p><p>When you change the logging level, note the following:
</p>
<ul><li> The change is immediate and dynamic.
</li><li> The change is not persistent; it goes away when Splunk is restarted. 
</li></ul><p><b>Settings &gt; System settings &gt; System logging</b> is meant only for dynamic and temporary changes to Splunk log files. For permanent changes, use <code><font size="2">$SPLUNK_HOME/etc/log.cfg</font></code> instead.
</p>
<h3> <a name="whatsplunklogsaboutitself_included_data_models"><span class="mw-headline" id="Included_data_models"> Included data models </span></a></h3>
<p>Splunk comes with two sample data models. These data models are constructed from Splunk's internal logs. By interacting with them, you can learn about Splunk's log files and about data models in one fell swoop. 
</p><p>To access the internal log data models, click <b>Pivot</b>. By default, you should see two data models, "Splunk's Internal Audit Logs - SAMPLE" and "Splunk's Internal Server Logs - SAMPLE."
</p><p>Note that there is a known issue in the alerts section of the "Splunk's Internal Server Logs" data model.
</p>
<a name="enabledebuglogging"></a><h2> <a name="enabledebuglogging_enable_debug_logging"><span class="mw-headline" id="Enable_debug_logging"> Enable debug logging </span></a></h2>
<p>Splunk's internal logging levels are <code><font size="2">DEBUG INFO WARN ERROR FATAL</font></code> (from most to least verbose). This topic gives a few popular options for how you might want to put Splunk into debug mode.
</p><p>Be warned, Splunk's debug mode is <b>extremely verbose</b>. All the extra chatter might obscure something that might have helped you diagnose your problem. And running Splunk in debug mode for any length of time will make your internal log files really pretty unwieldy. Running debug mode is not recommended on production systems.
</p>
<h3> <a name="enabledebuglogging_enable_debug_logging_on_all_of_splunkd.log"><span class="mw-headline" id="Enable_debug_logging_on_all_of_splunkd.log"> Enable debug logging on all of splunkd.log </span></a></h3>
<p>Splunk has a debugging parameter (<code><font size="2">--debug</font></code>) that you can use when starting Splunk from the CLI in *nix. This command outputs logs to <code><font size="2">$SPLUNK_HOME/var/log/splunk/splunkd.log</font></code>. To enable debug logging from the command line:
</p>
<ul><li> Navigate to <code><font size="2">$SPLUNK_HOME/bin</font></code>.
</li><li> Stop Splunk, if it is running.
</li><li> Save your existing <code><font size="2">splunkd.log</font></code> file by moving it to a new filename, like <code><font size="2">splunkd.log.old</font></code>.
</li><li> Restart Splunk in debug mode with <code><font size="2">splunk start --debug</font></code>. 
</li><li> When you notice the problem, stop Splunk.
</li><li> Move the new <code><font size="2">splunkd.log</font></code> file elsewhere and restore your old one.
</li><li> Stop or restart Splunk normally (without the --debug flag) to disable debug logging.
</li></ul><p>Specific areas can be enabled to collect debugging details over a longer period with minimal performance impact. See the category settings in the file <code><font size="2">$SPLUNK_HOME/etc/log.cfg</font></code> to set specific log levels without enabling a large number of categories as with --debug. Restart Splunk after changing this file.
</p><p><b>Important:</b> Changes to <code><font size="2">$SPLUNK_HOME/etc/log.cfg</font></code> are overwritten if you upgrade your version of Splunk.
</p><p><b>Note:</b> Not all messages marked WARN or ERROR indicate actual problems with Splunk; some indicate that a feature is not being used.
</p><p>Note also that this option is not available on Windows. To enable debugging on Splunk running on Windows, enable debugging on a specific processor in Splunk Web or using log.cfg.
</p>
<h3> <a name="enabledebuglogging_enable_debug_logging_for_a_specific_processor_within_splunkd.log"><span class="mw-headline" id="Enable_debug_logging_for_a_specific_processor_within_splunkd.log"> Enable debug logging for a specific processor within splunkd.log </span></a></h3>
<h4><font size="3"><b><i> <a name="enabledebuglogging_in_splunk_web"><span class="mw-headline" id="In_Splunk_Web"> In Splunk Web </span></a></i></b></font></h4>
<p>You can enable these DEBUG settings via Splunk Web if you have admin privileges. Navigate to <b>Settings &gt; System settings &gt; System logging</b>. Search for the processor names using the text box. Click on the processor name to change the logging level to DEBUG. You do not need to restart Splunk. In fact, these changes will not persist if you restart the Splunk instance.
</p>
<h4><font size="3"><b><i> <a name="enabledebuglogging_in_log.cfg"><span class="mw-headline" id="In_log.cfg"> In log.cfg </span></a></i></b></font></h4>
<p>If you want the processors to be in DEBUG on startup, or if you want to turn on debugging for a few processors or for a lightweight forwarder (with no Splunk Web), edit the <code><font size="2">$SPLUNK_HOME/etc/log.cfg</font></code> file directly. Back up your log.cfg file before making any changes.
</p><p>In $SPLUNK_HOME/etc/log.cfg, find the category.* entry that relates to the processor you are interested in, and change the INFO or WARN string to DEBUG. There will not always be an existing entry for the processor you are interested in, and it may take some digging through the logs or documentation to find the correct one.
</p>
For example, to see how often Splunk is checking on a particular file, put 'FileInputTracker' in DEBUG. Update the existing entry to read <code><font size="2">category.FileInputTracker=DEBUG</font></code>
<p>Or for investigating problems monitoring files, use the FileInputTracker and selectProcessor categories.
</p><p>Restart Splunk. Now every time Splunk checks the inputs file, it will be recorded in $SPLUNK_HOME/var/log/splunk/splunkd.log. Remember to change these settings back when you are finished investigating.
</p><p>If a default level is not specified for a category, the logging level defaults to your rootCategory setting.
</p><p><b>Note</b>: Leave category.loader at INFO. This is what gives us our build and system info.
</p><p>To change the maximum size of a log file before it rolls, change the <code><font size="2">maxFileSize</font></code> value (in bytes) for the desired file:
</p>
<code><font size="2"><br>appender.A1=RollingFileAppender<br>appender.A1.fileName=${SPLUNK_HOME}/var/log/splunk/splunkd.log<br>appender.A1.maxFileSize=250000000<br>appender.A1.maxBackupIndex=5<br>appender.A1.layout=PatternLayout<br>appender.A1.layout.ConversionPattern=%d{%m-%d-%Y&nbsp;%H:%M:%S.%l}&nbsp;%-5p&nbsp;%c -&nbsp;%m%n<br></font></code>
<h5> <a name="enabledebuglogging_about_precedence"><span class="mw-headline" id="About_precedence"> About precedence </span></a></h5>
<p>If you have duplicate lines in log.cfg, the last line takes precedence. For example,
</p>
<code><font size="2"><br>category.databasePartitionPolicy=INFO<br>category.databasePartitionPolicy=DEBUG<br></font></code>
<p>will give you DEBUG, but in the other order it will not.
</p><p>The other log-*.cfg files behave similarly when you add categories. To set
only some things in a search.log into debug mode, then in log-searchprocess.cfg just add a new category line after the rootCategory:
</p><p>rootCategory=INFO,searchprocessAppender
category.whatever=DEBUG
appender.searchprocessAppender=RollingFileAppender
</p><p>This leaves everything else as it was, which means only the debug messages you want are generated. Putting rootCategory into DEBUG mode makes the dispatch directories huge, so it's not a good choice for long-running debug.
</p>
<h4><font size="3"><b><i> <a name="enabledebuglogging_log-local.cfg"><span class="mw-headline" id="log-local.cfg"> log-local.cfg </span></a></i></b></font></h4>
<p>You can put <code><font size="2">log.cfg</font></code> settings into a local file, <code><font size="2">log-local.cfg</font></code> file, residing in the same directory as <code><font size="2">log.cfg</font></code>. The settings in <code><font size="2">log-local.cfg</font></code> take precedence. And unlike <code><font size="2">log.cfg</font></code>, the <code><font size="2">log-local.cfg</font></code> file doesn't get overwritten on upgrade.
</p>
<h4><font size="3"><b><i> <a name="enabledebuglogging_with_endpoints"><span class="mw-headline" id="With_endpoints"> With endpoints </span></a></i></b></font></h4>
<p>In Splunk 4.1 and later, you can access a debugging endpoint that shows status information about monitored files:
</p>
<code><font size="2">https://your-splunk-server:8089/services/admin/inputstatus/TailingProcessor:FileStatus</font></code>
<h4><font size="3"><b><i> <a name="enabledebuglogging_enable_debug_messages_from_the_cli_.284.1.4_and_later_versions.29"><span class="mw-headline" id="Enable_debug_messages_from_the_CLI_.284.1.4_and_later_versions.29"> Enable debug messages from the CLI (4.1.4 and later versions) </span></a></i></b></font></h4>
<code><font size="2"><br>./splunk _internal call /services/server/logger/TailingProcessor -post:level DEBUG<br></font></code>
<p><b> Note</b>: This search will return the message "HTTP Status: 200". This is not an error and is normal.
</p><p><br><b>From 4.2, you can also set this way;</b>
</p>
<code><font size="2"><br>./splunk set log-level TailingProcessor -level DEBUG<br></font></code>
<h3> <a name="enabledebuglogging_enable_debug_logging_for_search_processes"><span class="mw-headline" id="Enable_debug_logging_for_search_processes"> Enable debug logging for search processes </span></a></h3>
<p>Search processes obey the etc/log-searchprocess.cfg rules. Similar to splunkd, they can be overridden in etc/log-searchprocess-local.cfg.
</p><p>All loggers can be set to DEBUG by adding a line such as
</p>
<code><font size="2">rootCategory=DEBUG,searchprocessAppender</font></code>
<p>Specific loggers can be set to debug as well, for example:
</p>
<code><font size="2"><br>category.UnifiedSearch=DEBUG<br>category.IndexScopedSearch=DEBUG<br></font></code>
<p>This change takes effect immediately for all searches started after the change.
</p>
<h3> <a name="enabledebuglogging_debug_splunk_web"><span class="mw-headline" id="Debug_Splunk_Web"> Debug Splunk Web </span></a></h3>
<p>Change the logging level for the splunkweb process by editing the file:
</p>
<code><font size="2"><br>$SPLUNK_HOME/etc/log.cfg <br></font></code> or if you have created your own 
<code><font size="2">$SPLUNK_HOME/etc/log-local.cfg<br></font></code>
<p>Locate the <code><font size="2">[python]</font></code> stanza and change the contents to:
</p>
<code><font size="2"><br>[python]<br>splunk = DEBUG<br># other lines should be removed<br></font></code>
<p>The logging component names are hierarchical so setting the top level <code><font size="2">splunk</font></code> component will affect all loggers unless a more specific setting is provided, like <code><font size="2">splunk.search = INFO</font></code>.
</p><p>Restart the splunkweb process with the command <code><font size="2">./splunk restart splunkweb</font></code>. The additional messages are output in the file <code><font size="2">$SPLUNK_HOME/var/log/splunk/web_service.log</font></code>.
</p>
<a name="aboutmetricslog"></a><h2> <a name="aboutmetricslog_about_metrics.log"><span class="mw-headline" id="About_metrics.log"> About metrics.log</span></a></h2>
<p>This topic is an overview of metrics.log.
</p>
<ul><li> To learn about other log files, read "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk logs about itself</a>."
</li><li> For an example using metrics.log, read "<a href="#metricslog" class="external text">Troubleshoot inputs with metrics.log</a>."
</li></ul><h3> <a name="aboutmetricslog_what_information_is_in_metrics.log.3f"><span class="mw-headline" id="What_information_is_in_metrics.log.3F"> What information is in metrics.log? </span></a></h3>
<p>Metrics.log has a variety of introspection information for reviewing product behavior.
</p><p>First, metrics.log is a periodic report, taken every 30 seconds or so, of recent Splunk software activity. 
</p><p>By default, metrics.log reports the top 10 results for each type. You can change that number of series from the default by editing the value of <code><font size="2">maxseries</font></code> in the <code><font size="2">[metrics]</font></code> stanza in <code><font size="2">limits.conf</font></code>. 
</p>
<h3> <a name="aboutmetricslog_structure_of_the_lines"><span class="mw-headline" id="Structure_of_the_lines"> Structure of the lines </span></a></h3>
<p>Here is a sample line from metrics.log:
</p><p><code><font size="2"> 01-27-2010 15:43:54.913 INFO  Metrics - group=pipeline, name=parsing, processor=utf8, cpu_seconds=0.000000, executes=66, cumulative_hits=301958</font></code>
</p><p>First, boiler plate: the timestamp, the "severity," which is always <code><font size="2">INFO</font></code> for metrics events, and then the kind of event, "<code><font size="2">Metrics</font></code>." 
</p><p>The next field is the "<code><font size="2">group</font></code>." This indicates what kind of metrics data it is. There are a few groups in the file, including:
</p>
<ul><li> pipeline
</li><li> queue
</li><li> thruput
</li><li> udpin_connections
</li><li> mpool
</li></ul><h4><font size="3"><b><i> <a name="aboutmetricslog_pipeline_messages"><span class="mw-headline" id="Pipeline_messages"> Pipeline messages </span></a></i></b></font></h4>
<p>Pipeline messages are reports on the Splunk pipelines, which are the strung-together pieces of "machinery" that process and manipulate events flowing into and out of the Splunk system. You can see how many times data reached a given machine in the Splunk system (executes), and you can see how much cpu time each machine used (cpu_seconds).
</p><p>Plotting totals of cpu seconds by processor can show you where the cpu time is going in indexing activity. Looking at numbers for executes can give you an idea of data flow. For example if you see:
</p>
<code><font size="2"> group=pipeline, name=merging, processor=aggregator, ..., executes=998<br>&nbsp;group=pipeline, name=merging, processor=readerin, ... , executes=698<br>&nbsp;group=pipeline, name=merging, processor=regexreplacement, ..., executes=698<br>&nbsp;group=pipeline, name=merging, processor=sendout, ... , executes=698<br></font></code>
<p>then it's pretty clear that a large portion of your items aren't making it past the aggregator. This might indicate that many of your events are multiline and are being combined in the aggregator before being passed along.
</p><p>Read more about Splunk's data pipeline in "How data moves through Splunk" in the Distributed Deployment Manual.
</p>
<h4><font size="3"><b><i> <a name="aboutmetricslog_queue_messages"><span class="mw-headline" id="Queue_messages"> Queue messages </span></a></i></b></font></h4>
<p>Queue messages look like
</p><p><code><font size="2">  ... group=queue, name=parsingqueue, max_size=1000, filled_count=0, empty_count=8, current_size=0, largest_size=2, smallest_size=0</font></code>
</p><p>Most of these values aren't interesting. But current_size, especially considered in aggregate, across events, can tell you which portions of Splunk indexing are the bottlenecks. If current_size remains near zero, then probably the Splunk (indexing) system is not being taxed in any way. If the queues remain near 1000, then more data is being fed into the system (at the time) than it can process in total.
</p><p>Sometimes you will see messages such as
<code><font size="2"> ... group=queue, name=parsingqueue, blocked!!=true, max_size=1000,
</font></code></p>
<code><font size="2">filled_count=0, empty_count=8, current_size=0, largest_size=2, smallest_size=0</font></code><br><p>This message contains the "blocked" string, indicating that it was full, and someone tried to add more, and couldn't. A queue will become unblocked as soon as the code pulling items out of it pulls an item. Many blocked queue messages in a sequence indicate that data is not flowing at all for some reason. A few scattered blocked messages indicate that flow control is operating, and is normal for a busy indexer.
</p><p>If you want to look at the queue data in aggregate, graphing the average of current_size is probably a good starting point.
</p><p>There are queues in place for data going into the parsing pipeline, and for data between parsing and indexing. Each networking output also has its own queue, which can be useful to determine whether the data is able to be sent promptly, or alternatively whether there's some network or receiving system limitation.
</p><p>Generally, filled_count and empty_count cannot be productively used for inferences.
</p>
<h4><font size="3"><b><i> <a name="aboutmetricslog_thruput_messages"><span class="mw-headline" id="Thruput_messages"> Thruput messages </span></a></i></b></font></h4>
<p>Thruput messages (similar to the English word "throughput") come in a few varieties.
</p><p>Thruput is measured in the indexing pipeline. If your data is not reaching this pipeline for some reason, it will not appear in this data.
</p><p>First there is a catchall line, which looks like this:
</p>
<div class="samplecode">
<p>... group=thruput, name=index_thruput, instantaneous_kbps=0.287598, instantaneous_eps=1.000000, average_kbps=0.270838, total_k_processed=74197, load_average=1.345703M&lt;/code&gt;
</p>
</div>
<p>This is the best line to look at when tuning performance or evaluating indexing load. It tries to capture the total indexing data load.
</p><p><b>Note:</b> In thruput lingo, "kbps" does not mean kilobits per second, it means kilo<i>bytes</i> per second. The industry standard term would be to write this something like KBps.
</p><p>The <code><font size="2">average_kbps</font></code> in the <code><font size="2">group=thruput</font></code> catchall indicates the average since Splunk Enterprise started.
</p><p><code><font size="2">instantaneous_kbps</font></code> indicates the average kbps for the reporting interval (equivalent to kbps for the breakouts below.)
</p><p>The most useful figure to look at in aggregate is probably <code><font size="2">instantaneous_kbps</font></code> over time.
</p><p>Following the catchall, there  can be variety of breakouts of the indexing thruput, including lines like:
</p>
 <div class="samplecode">
<p>... group=per_host_thruput, series="jombook.splunk.com", kbps=0.261530, eps=1.774194, kb=8.107422, ev=2606, avg_age=420232.710668, max_age=420241
</p>
<code><font size="2">... group=per_index_thruput, series="_internal", kbps=0.261530, eps=1.774194, kb=8.107422, ev=2606, avg_age=420232.710668, max_age=420241<br>... group=per_source_thruput, series="/applications/splunk4/var/log/splunk/metrics.log", kbps=0.261530, eps=1.774194, kb=8.107422, ev=2606, avg_age=420232.710668, max_age=420241<br>... group=per_sourcetype_thruput, series="splunkd", kbps=0.261530, eps=1.774194, kb=8.107422, ev=2606, avg_age=420232.710668, max_age=420241<br></font></code>
</div>
<p>In thruput messages the data load is broken out by host, index, source, and source type. This can be useful for answering two questions:  
</p>
<ul><li> Which data categories are busy?
</li><li> When were my data categories busy?
</li></ul><p>The series value identifies the host or index, etc. The kb value indicates the number of kilobytes processed since the last sample. Graphing kb in aggregate can be informative. The summary indexing status dashboard uses this data, for example.
</p>
<ul><li> <code><font size="2">ev</font></code> is a simple total count of events during the sampling period.
</li><li> <code><font size="2">kbps</font></code>, as before, indicates kilobytes per second averaged over the sampling period.
</li></ul><p>The <code><font size="2">avg_age</font></code> and <code><font size="2">max_age</font></code> refer to the difference between the time that the event was seen by the thruput processor in the indexing queue, and the time when the event occurred (or more accurately, the time that Splunk decided the event occurred).
</p>
<ul><li> <code><font size="2">max_age</font></code> is the largest difference between the current time and the perceived time of the events coming through the thruput processor.
</li><li> <code><font size="2">avg_age</font></code> is the average difference between the current time and the perceived time of the events coming through the thruput processor.
</li></ul><p><b>Note:</b> The <code><font size="2">per_x_thruput</font></code> categories are not complete. Remember that by default metrics.log shows the 10 busiest of each type, for each sampling window. If you have 2000 active forwarders, you cannot expect to see the majority of them in this data. You can adjust the sampling quantity, but this will increase the chattiness of metrics.log and the resulting indexing load and _internal index size. The sampling quantity is adjustable in limits.conf, <code><font size="2">[metrics] maxseries = num</font></code>.
</p><p>Ignore the <code><font size="2">eps</font></code> value, as it has accuracy issues.
</p>
<h4><font size="3"><b><i> <a name="aboutmetricslog_udpin_messages"><span class="mw-headline" id="udpin_messages"> udpin messages </span></a></i></b></font></h4>
<p>udpin_connections lines are essentially metering on udp input.
</p>
<code><font size="2">group=udpin_connections, 2514, sourcePort=2514, _udp_bps=0.00, _udp_kbps=0.00,<br>_udp_avg_thruput=0.00, _udp_kprocessed=0.00, _udp_eps=0.00<br></font></code>
<p>Some should be relatively self-explanatory.
</p>
<ul><li> bps: bytes per second
</li><li> kbps: kilobytes per specond
</li><li> eps: events (packets) per second
</li><li> _udp_kprocessed is a running total of kilobytes processed since udp input processor start (typically since splunk start, but if reconfigured may reset).
</li></ul><p>Don't have info on avg_thruput at this time.
</p><p>Be aware that it's quite achievable to max out the ability of the operating system, let alone Splunk, to handle UDP packets at high rates. This data might be useful to determine if any data is coming in at all, and at what times it rises. There is no guarantee that all packets sent to this port will be received and thus metered.
</p>
<h4><font size="3"><b><i> <a name="aboutmetricslog_mpool_messages"><span class="mw-headline" id="mpool_messages"> mpool messages </span></a></i></b></font></h4>
<p>The mpool lines represent memory used by the Splunk indexer code only
(not any other pipeline components). This information is probably not useful to anyone other than Splunk developers.
</p>
<code><font size="2">group=mpool, max_used_interval=4557, max_used=53878, avg_rsv=180, capacity=268435456, used=0<br></font></code>
<ul><li> <code><font size="2">max_used_interval</font></code> represents the number of bytes used during the reporting interval (since the last output).
</li><li> <code><font size="2">max_used</font></code> represents the maximum amount of memory, in bytes, in use at any time during the component's lifetime (most likely since last starting Splunk).
</li><li> <code><font size="2">avg_rsv</font></code> is the average size of a memory allocation across the run time of the system.
</li><li> <code><font size="2">capacity</font></code> is the limit on memory use for the indexer.
</li><li> <code><font size="2">used</font></code> is the current indexer's current memory use.
</li></ul><p>In this case we can see that some memory is sometimes in use, although at the time of the sample, none is in use, and that generally the use is low.
</p>
<h4><font size="3"><b><i> <a name="aboutmetricslog_map.2c_pipelineinputchannel_name_messages"><span class="mw-headline" id="map.2C_pipelineinputchannel_name_messages"> map, pipelineinputchannel name messages </span></a></i></b></font></h4>
<p>These messages are primarily debugging information over the Splunk internal cache of processing state and configuration data for a given data stream (host, source, or source type).
</p>
<code><font size="2">group=map, name=pipelineinputchannel, current_size=29, inactive_channels=4,<br>new_channels=0, removed_channels=0, reclaimed_channels=0, timedout_channels=0,<br>abandoned_channels=0<br></font></code>
<ul><li> <code><font size="2">current_size</font></code> is the number of total channels loaded in the system at the end of the sampling period.
</li><li> <code><font size="2">inactive_channels</font></code> is the number of channels that have no entries in any pipeline referring to them (typically for recently seen data but not for data currently being processed) at the end of the sampling period.
</li><li> <code><font size="2">new_channels</font></code> is the number of channels created during the sampling period, meaning that new data streams arrived, or a data stream that was aged out was created again.
</li><li> <code><font size="2">removed_channels</font></code> is the number of channels destructed during the sampling period, which means that enough pressure existed to push these channels out of set (there were too many other new data streams).
</li><li> <code><font size="2">reclaimed_channels</font></code> is the number of channels that were repurposed during the sampling period. This will happen for reasons similar to <code><font size="2">new_channels</font></code>, based on the size of the utilization, etc.
</li><li> <code><font size="2">timedout_channels</font></code> is the number of channels that became unused for a long enough time to be considered stale and the information to be culled. Typically a timedout file or data source hasn't been producing data for some time.
</li><li> <code><font size="2">abandoned_channels</font></code> is the number of channels that were terminated by Splunk forwarding, where the forwarder stopped communicating to an indexer, so the indexer shut them down.
</li></ul><h4><font size="3"><b><i> <a name="aboutmetricslog_subtask_seconds_messages"><span class="mw-headline" id="subtask_seconds_messages"> subtask_seconds messages </span></a></i></b></font></h4>
<p>Currently there is only one type of these messages, name=indexer, task=indexer_service. The message describes how the indexer has spent time over each interval window.
</p>
<code><font size="2">group=subtask_seconds, name=indexer, task=indexer_service, <br>replicate_semislice=0.000000, throttle_optimize=0.00015, &nbsp;flushBlockSig=0.000000,<br>retryMove_1hotBkt=0.000000, size_hotBkt=0.000000, roll_hotBkt=0.000000, <br>chillOrFreeze=0.000000, update_checksums=0.000000, fork_recovermetadata=0.000000,<br>rebuild_metadata=0.000300, update_bktManifest=0.000000, service_volumes=0.000105,<br>update_bktManifest=0.000000, service_volumes=0.000105, service_maxSizes=0.000000<br>service_externProc=0.000645<br></font></code>
<ul><li> The <code><font size="2">throttle_optimize</font></code> subtask represents time that the indexer spends waiting for splunk_optimize processes to reduce the count of .tsidx files to a reasonable level within hot buckets. Because splunk_optimize can in some cases run more slowly merging .tsidx files than the indexer runs while generating them, this flow-control state must exist for splunk_optimize to catch up. When this throttling occurs, messages are logged to splunkd.log in category DatabasePartitionPolicy similar to "idx=&lt;idxname&gt; Throttling indexer, too many tsidx files in bucket."
</li><li> The <code><font size="2">rebuild_metadata</font></code> subtask represents time that the indexer spends generating new copies of .data files in hot buckets. This can mean writing out Hosts.data, Sourcetypes.data, Sources.data, or Strings.data. Typically the largest cost is for Strings.data in configurations where a large amount of data is directed to this file. Setting the <code><font size="2">MetaData</font></code> log channel to DEBUG can provide more information on which buckets and files might be involved in a slowdown.
</li></ul><a name="metricslog"></a><h2> <a name="metricslog_troubleshoot_inputs_with_metrics.log"><span class="mw-headline" id="Troubleshoot_inputs_with_metrics.log"> Troubleshoot inputs with metrics.log </span></a></h2>
<p>This topic is an example of a problem you can solve using metrics.log.
</p>
<ul><li> To learn about metrics.log, read "<a href="#aboutmetricslog" class="external text">About metrics.log</a>."
</li><li> To learn about other log files, read "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk logs about itself</a>."
</li></ul><h3> <a name="metricslog_example:_troubleshoot_data_inputs"><span class="mw-headline" id="Example:_Troubleshoot_data_inputs"> Example: Troubleshoot data inputs </span></a></h3>
<p>You might want to identify a data input that has suddenly begun to generate uncharacteristically large numbers of  events. If this input is hidden in a large quantity of similar data, it can be difficult to determine which one is actually the problem. You can find it by searching the internal index (add <code><font size="2">index=_internal</font></code> to your search) or just look in <code><font size="2">metrics.log</font></code> itself in <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code>.
</p><p>There's a lot more in metrics.log than just volume data, but for now let's focus on investigating data inputs. 
</p><p>For incoming events, the amount of data processed is in the <code><font size="2">thruput</font></code> group, as in <code><font size="2">per_host_thruput</font></code>. In this example, you're only indexing data from one host,  so <code><font size="2">per_host_thruput</font></code> actually can tell us something useful: that right now host "grumpy" indexes around  8k in a 30-second period. Since there is only one host, you can add it all up and get a good picture of what you're indexing, but if you had more than 10 hosts you would only get a sample.
</p>
<code><font size="2"><br>03-13-2008 10:49:57.634 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.245401, eps=1.774194, kb=7.607422<br>03-13-2008 10:50:28.642 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.237053, eps=1.612903, kb=7.348633<br>03-13-2008 10:50:59.648 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.217584, eps=1.548387, kb=6.745117<br>03-13-2008 10:51:30.656 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.245621, eps=1.741935, kb=7.614258<br>03-13-2008 10:52:01.661 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.311051, eps=2.290323, kb=9.642578<br>03-13-2008 10:52:32.669 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.296938, eps=2.322581, kb=9.205078<br>03-13-2008 10:53:03.677 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.261593, eps=1.838710, kb=8.109375<br>03-13-2008 10:53:34.686 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.263136, eps=2.032258, kb=8.157227<br>03-13-2008 10:54:05.692 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.261530, eps=1.806452, kb=8.107422<br>03-13-2008 10:54:36.699 INFO Metrics - group=per_host_thruput, series="grumpy", kbps=0.313855, eps=2.354839, kb=9.729492<br></font></code>
<p>For example, you might know that <code><font size="2">access_common</font></code> is a popular source type for events on this Web server, so it would give you a good idea of what was happening:
</p>
<code><font size="2"><br>03-13-2008 10:51:30.656 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.022587, eps=0.193548, kb=0.700195<br>03-13-2008 10:52:01.661 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.053585, eps=0.451613, kb=1.661133<br>03-13-2008 10:52:32.670 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.031786, eps=0.419355, kb=0.985352<br>03-13-2008 10:53:34.686 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.030998, eps=0.387097, kb=0.960938<br>03-13-2008 10:54:36.700 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.070092, eps=0.612903, kb=2.172852<br>03-13-2008 10:56:09.722 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.023564, eps=0.290323, kb=0.730469<br>03-13-2008 10:56:40.730 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.006048, eps=0.096774, kb=0.187500<br>03-13-2008 10:57:11.736 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.017578, eps=0.161290, kb=0.544922<br>03-13-2008 10:58:13.748 INFO Metrics - group=per_sourcetype_thruput, series="access_common", kbps=0.025611, eps=0.225806, kb=0.793945<br></font></code>
<p>But you probably have more than 10 source types, so at any particular time some other one could spike and <code><font size="2">access_common</font></code> wouldn't be reported. <code><font size="2">per_index_thruput</font></code> and <code><font size="2">per_source_thruput</font></code> work similarly.
</p><p>With this in mind, let's examine the standard saved search "KB indexed per hour last 24 hours".
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">
<p>index=_internal metrics group=per_index_thruput NOT debug NOT sourcetype=splunk_web_access | timechart fixedrange=t span=1h sum(kb) | rename sum(kb) as totalKB
</p>
</font></code><br></div>
<p>This means: look in the internal index for metrics data of group <code><font size="2">per_index_thruput</font></code>, ignore some internal stuff and make a report showing the sum of the kb values. For cleverness, we'll also rename the output to something meaningful, "totalKB". The result looks like this:
</p>
<code><font size="2"><br>sum of kb vs. time for results in the past day<br>_time totalKB<br>1 03/12/2008 11:00:00 922.466802<br>2 03/12/2008 12:00:00 1144.674811<br>3 03/12/2008 13:00:00 1074.541995<br>4 03/12/2008 14:00:00 2695.178730<br>5 03/12/2008 15:00:00 1032.747082<br>6 03/12/2008 16:00:00 898.662123<br></font></code>
<p>Those totalKB values just come from the sum of kb over a one hour interval. If you like, you can change the search and get just the ones from grumpy:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">
index=_internal metrics grumpy group=per_host_thruput | timechart fixedrange=t span=1h sum(kb) | rename sum(kb) as totalKB</font></code><br></div>
<code><font size="2"><br>sum of kb vs. time for results in the past day<br>_time totalKB<br>1 03/12/2008 11:00:00 746.471681<br>2 03/12/2008 12:00:00 988.568358<br>3 03/12/2008 13:00:00 936.092772<br>4 03/12/2008 14:00:00 2529.226566<br>5 03/12/2008 15:00:00 914.945313<br>6 03/12/2008 16:00:00 825.353518<br></font></code>
<p>We see that grumpy was unusually active in the 2 pm time bin. With this knowledge, we can start to hunt down the culprit by, for example, source type or host.
</p>
<h3> <a name="metricslog_answers"><span class="mw-headline" id="Answers"> Answers </span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has about working with metrics.log.
</p>
<a name="aboutaccesslogs"></a><h2> <a name="aboutaccesslogs_about_access_logs"><span class="mw-headline" id="About_access_logs"> About access logs</span></a></h2>
<p>Splunkd and splunkweb both produce access logs in a format similar to common Apache webserver access log formats.
</p><p>Splunkd produces splunkd_access.log, and splunkweb records logs in web_access.log. Both log files are close approximations of the Apache combined log format.
</p><p>Apache formats are described briefly in the Apache HTTP Server documentation. For example, see Apache 2.4 log file documentation.
</p>
<h3> <a name="aboutaccesslogs_splunkd_access.log"><span class="mw-headline" id="splunkd_access.log"> splunkd_access.log </span></a></h3>
<p>Here is a typical line in splunkd_access.log:
</p><p><code><font size="2">127.0.0.1 - - [21/Oct/2014:13:50:25.662 -0700] "GET /services/server/info?output_mode=json HTTP/1.1" 200 1566 - - - 1ms</font></code>
</p><p>These fields are
</p><p><code><font size="2">&lt;address&gt; - &lt;user&gt; [&lt;time&gt;] "&lt;request&gt;" &lt;status&gt; &lt;response_size&gt; - - - &lt;duration&gt;</font></code>
</p><p>address: The IP address from which the HTTP client socket appears to originate. Typically these requests originate from splunkweb and come over the localhost/loopback address.  
</p><p>The second field is a placeholder for the unused identd field.
</p><p>user: The splunk user, if any, making the request.  System accesses on behalf of no particular user will appear as "-".
</p><p>timestamp: This is the time that splunkd finished reading in the request. However, the log event is written out when the http server finishes writing the response, so these timestamps can be out of order.
</p><p>request: The HTTP request made by the client consisting of an action, a URL, and a protocol version.
</p><p>status: The HTTP status returned as part of the response.
</p><p>response_size: The size of the body of the response in bytes
</p><p>Three additional placeholders. (If you know what these stand in for, send docs feedback below!)
</p><p>duration: The time it took from the completion of reading the request to completely writing out the response. This value is logged explicitly in milliseconds.
</p><p>Between the definitions for timestamp and duration, you can infer the response completion time by adding duration to the timestamp.
</p>
<h3> <a name="aboutaccesslogs_web_access.log"><span class="mw-headline" id="web_access.log"> web_access.log </span></a></h3>
<p>A web access line is similar:
</p><p><code><font size="2">127.0.0.1 - admin [21/Oct/2014:14:05:05.044 -0700] "GET /en-US/api/message/index HTTP/1.1" 200 341 "http://mcp.sv.splunk.com:62100/en-US/manager/search/saved/searches" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:32.0) Gecko/20100101 Firefox/32.0" - 5446ca810b7fb1d8551110 11ms</font></code>
</p><p><br>
Here the format is:
</p><p><code><font size="2">&lt;address&gt; - &lt;user&gt; [&lt;time&gt;] "&lt;request&gt;" &lt;status&gt; &lt;response_size&gt; "&lt;referer&gt;" "&lt;user agent&gt;" - &lt;session_id&gt; &lt;duration&gt;</font></code>
</p><p>Address, user, time, request, status, response_size, and duration are the same as in splunkd_access.log. The new components here are:
</p><p>referer: referer [sic] is the URL that the client told us provided the link to the URL that was accessed.
</p><p>user agent: The string the http client used to identify itself.
</p><p>session_id: This represents the splunkweb session. Can be used to follow a stream of requests from a particular client.  These sessions are transient starting in Splunk Enterprise 6.2.0.
</p>
<h3> <a name="aboutaccesslogs_splunkd_ui_access.log"><span class="mw-headline" id="splunkd_ui_access.log"> splunkd_ui_access.log </span></a></h3>
<p>Starting in Splunk Enterprise 6.2.0, splunkd handles requests from the browser that  splunkweb handled pre-6.2.0. This file represents those requests. The format is identical to web_access.log.
</p>
<h1>Platform Instrumentation</h1><a name="abouttheplatforminstrumentationframework"></a><h2> <a name="abouttheplatforminstrumentationframework_about_splunk_enterprise_platform_instrumentation"><span class="mw-headline" id="About_Splunk_Enterprise_platform_instrumentation"> About Splunk Enterprise platform instrumentation </span></a></h2>
<p>Splunk Enterprise platform instrumentation refers to data that Splunk Enterprise logs and uses to populate the <code><font size="2">_introspection</font></code> index. It generates data about your Splunk instance and environment and writes that data to log files to aid in reporting on system resource utilization and troubleshooting problems with your Splunk Enterprise deployment. You can also view the latest instrumentation data at REST endpoints.
</p><p>Platform instrumentation is included in Splunk Enterprise as an add-on, sometimes referred to as the "introspection_generator_addon."
</p>
<h3> <a name="abouttheplatforminstrumentationframework_what_data_does_splunk_enterprise_record_in_these_introspection_log_files.3f"><span class="mw-headline" id="What_data_does_Splunk_Enterprise_record_in_these_introspection_log_files.3F"> What data does Splunk Enterprise record in these introspection log files? </span></a></h3>
<p>The introspection files contain data about:
</p>
<ul><li> Operating system resource usage for Splunk Enterprise processes, broken down by process. 
</li><li> Operating system resource usage for the entire host (i.e., all system and user processes). 
</li><li> Disk object data.
</li><li> KV store performance data.
</li></ul><p>See "<a href="#whatdatagetslogged" class="external text">What data gets logged</a>" for more information.
</p>
<h3> <a name="abouttheplatforminstrumentationframework_where_is_this_data_written.3f"><span class="mw-headline" id="Where_is_this_data_written.3F"> Where is this data written? </span></a></h3>
<p>Events are written to two log files in <code><font size="2">$SPLUNK_HOME/var/log/introspection</font></code>. Non-forwarders tail these log files and place results into the local <code><font size="2">_introspection</font></code> index. Forwarders, which have no local indexes, forward these events to indexers.
</p><p>The two log files are <code><font size="2">disk_objects.log</font></code> and <code><font size="2">resource_usage.log</font></code>. See "<a href="#whatdatagetslogged" class="external text">What gets logged</a>" for a breakdown of what data goes into which file.
</p><p>To find platform instrumentation events, qualify your searches:
</p>
<ul><li> Find introspection data: <div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_introspection</font></code><br></div>
</li><li> To find introspection data from a forwarder or another instance in your deployment, qualify your search with the remote host name.
</li></ul><h3> <a name="abouttheplatforminstrumentationframework_how_does_this_feature_affect_my_splunk_deployment.3f"><span class="mw-headline" id="How_does_this_feature_affect_my_Splunk_deployment.3F"> How does this feature affect my Splunk deployment? </span></a></h3>
<p>If you are upgrading from a Splunk Enterprise version pre-6.1, expect the new log files to use a bit of disk space (an estimated 300 MB). The _introspection index's disk usage, on the other hand, varies from deployment to deployment.
</p><p>Each log file has a maximum size of 25 Mb. You can change this limit in log.cfg. You can have up to six instances of each file, according to your log rotation policy. That is, resource_usage.log, resource_usage.log.1, ... resource_usage.log.5, and the same for disk_objects.log. Thus, the introspection log files by default can take up to 300 MB of disk space.
</p><p>This feature is implemented as an auxiliary low-profile long-running process. This process is where resource usage (RU) introspection data is collected. Collecting disk object (DO) introspection data requires no extra I/O, as it leverages information that other parts of splunkd have already collected and cached.
</p><p>See the upgrade docs in the Installation Manual for upgrade information.
</p><p>See "<a href="#configurepif" class="external text">Configure platform instrumentation</a>" for instructions on tuning this feature.
</p>
<h3> <a name="abouttheplatforminstrumentationframework_supported_platforms"><span class="mw-headline" id="Supported_platforms"> Supported platforms </span></a></h3>
<ul><li> Windows
<ul><li> x86-64: Server 2008, Server 2008 R2, Server 2012
</li><li> x86-32: Server 2008, Server 2008 R2
</li></ul></li><li> Linux
<ul><li> x86-64: RHEL with 2.6+ kernel
</li><li> x86-32: RHEL with 2.6+ kernel
</li></ul></li><li> Solaris
<ul><li> x86-64: 10, 11
</li><li> SPARC: 10, 11
</li></ul></li></ul><a name="whatdatagetslogged"></a><h2> <a name="whatdatagetslogged_what_gets_logged.3f"><span class="mw-headline" id="What_gets_logged.3F"> What gets logged? </span></a></h2>
<p>This topic describes the contents of log files  that are tailed to populate the <code><font size="2">_introspection</font></code> index. For the log files that populate <code><font size="2">_internal</font></code>, see "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk logs about itself</a>" in this manual.
</p><p>These log files comply with the Common Information Model (CIM). See the the CIM add-on documentation for more information.
</p><p>"Extra field" indicates a field that is not logged by default. Read more about configuring polling intervals and enabling this feature on a universal forwarder in "<a href="#configurepif" class="external text">Configure platform instrumentation</a>."
</p>
<h3> <a name="whatdatagetslogged_per-process_resource_usage_data"><span class="mw-headline" id="Per-process_resource_usage_data"> Per-process resource usage data </span></a></h3>
<p>Exposes OS resource usage info for just Splunk processes, broken down by process. Splunk processes include splunkd, splunkweb, Splunk search processes, splunkd-launched (fsck, splunk-optimize), and modular or scripted inputs launched on behalf of splunkd.
</p><p>These fields are available:
</p>
<ul><li> in the log file <code><font size="2">$SPLUNK_HOME/var/log/introspection/resource_usage.log</font></code>
</li><li> in an indexer's <code><font size="2">_introspection</font></code> index
</li><li> at the endpoint <code><font size="2">server/status/resource-usage/splunk-processes</font></code>.
</li></ul><h4><font size="3"><b><i> <a name="whatdatagetslogged_data_available_for_all_splunk_enterprise_processes"><span class="mw-headline" id="Data_available_for_all_Splunk_Enterprise_processes"> Data available for all Splunk Enterprise processes </span></a></i></b></font></h4>
<p>Access information about operating system resource utilization, broken down by Splunk Enterprise processes. Four fields here are "extra" fields, not logged by default. Read about populating extra fields in "<a href="#configurepif" class="external text">Configure platform instrumentation</a>."
</p><p>See the list of output fields at system/server/status/resource-usage/splunk-processes in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_additional_data_available_only_for_search_processes"><span class="mw-headline" id="Additional_data_available_only_for_search_processes"> Additional data available only for search processes </span></a></i></b></font></h4>
<p>Splunk Enterprise can log all the above data for search processes (except <code><font size="2">args</font></code>). In addition, it logs some additional information about search processes, in a subsection called <code><font size="2">search_props</font></code>.
</p><p>See the list of output fields at system/server/status/resource-usage/splunk-processes in the REST API Reference Manual. The search process fields are embedded within the larger process table, at the <code><font size="2">search_props</font></code> entry.
</p>
<h3> <a name="whatdatagetslogged_hostwide_resource_usage_data"><span class="mw-headline" id="Hostwide_resource_usage_data"> Hostwide resource usage data</span></a></h3>
<p>Access host-level, dynamic CPU utilization and paging information.
</p><p>These fields are available:
</p>
<ul><li> in the log file <code><font size="2">resource_usage.log</font></code>
</li><li> in an indexer's <code><font size="2">_introspection</font></code> index
</li><li> at the endpoint <code><font size="2">server/status/resource-usage/hostwide</font></code>.
</li></ul><p>See the list of output fields at system/server/status/resource-usage/hostwide in the REST API Reference Manual.
</p>
<h3> <a name="whatdatagetslogged_disk_object_data"><span class="mw-headline" id="Disk_object_data"> Disk object data </span></a></h3>
<p>These fields are available in the log file <code><font size="2">$SPLUNK_HOME/var/log/introspection/disk_objects.log</font></code>
</p><p>Additionally, the latest snapshot of these field values are available at endpoints as itemized below.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_server.2finfo"><span class="mw-headline" id="server.2Finfo"> server/info </span></a></i></b></font></h4>
<p>Splunk Enterprise server configuration information (static server characteristics; dynamic characteristics go under <code><font size="2">server/status</font></code>).
</p><p>See the list of output fields at system/server/info in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_data.2findex-volumes"><span class="mw-headline" id="data.2Findex-volumes"> data/index-volumes </span></a></i></b></font></h4>
<p>Lists the Splunk Enterprise volume(s).
</p><p>See the list of output fields at data/index-volumes in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_data.2findex-volumes.2f.7bname.7d"><span class="mw-headline" id="data.2Findex-volumes.2F.7BName.7D"> data/index-volumes/{Name} </span></a></i></b></font></h4>
<p>Characterizes persisted objects at the volume level.
</p><p>See the list of output fields at index/data/index-volumes/{Name} in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_data.2findexes-extended"><span class="mw-headline" id="data.2Findexes-extended"> data/indexes-extended </span></a></i></b></font></h4>
<p>Provides information about Splunk Enterprise index <b>buckets</b>.
</p><p>See the list of output fields at index/data/indexes-extended in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_data.2findexes-extended.2f.7bname.7d"><span class="mw-headline" id="data.2Findexes-extended.2F.7BName.7D"> data/indexes-extended/{Name} </span></a></i></b></font></h4>
<p>Provides bucket-level information for the specified index.
</p><p>See the list of output fields at data/indexes-extended{Name} in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_server.2fstatus.2fdispatch-artifacts"><span class="mw-headline" id="server.2Fstatus.2Fdispatch-artifacts"> server/status/dispatch-artifacts </span></a></i></b></font></h4>
<p>Accesses search job information. 
</p><p>See the list of output fields at server/status/dispatch-artifacts in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_server.2fstatus.2ffishbucket"><span class="mw-headline" id="server.2Fstatus.2Ffishbucket"> server/status/fishbucket </span></a></i></b></font></h4>
<p>Accesses information about the private BTree database. Gives an idea of fishbucket growth. The fishbucket is a directory, <code><font size="2">$SPLUNK_DB/fishbucket/splunk_private_db/</font></code>, that keeps a record about each file input. Most fundamentally, this record keeps track of how far into the file we've read, so that if splunkd is stopped and then restarted, it'll know where in each file input to resume reading.
</p><p>See the list of output fields at server/status/fishbucket in the REST API Reference Manual
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_server.2fstatus.2flimits.2fsearch-concurrency"><span class="mw-headline" id="server.2Fstatus.2Flimits.2Fsearch-concurrency"> server/status/limits/search-concurrency </span></a></i></b></font></h4>
<p>Search concurrency limits for a standalone Splunk Enterprise instance. 
</p><p>See the list of output fields at system/server/status/limits/search-concurrency in the REST API Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="whatdatagetslogged_server.2fstatus.2fpartitions-space"><span class="mw-headline" id="server.2Fstatus.2Fpartitions-space"> server/status/partitions-space </span></a></i></b></font></h4>
<p>Helps track disk usage. These results show only partitions with Splunk disk objects (indexes, volumes, logs, fishbucket, search process artifacts) on them. There is a partitions event for each file system, and each event gives the respective file system type.
</p><p>A file system (or "volume" in Windows) is a logical concept, identified on UNIX by a number called "device ID." A file system has the property of type (format). For example: ZFS, EXT3.
</p><p>A partition is a physical concept, simply a chunk of hard drive (or solid state drive). All we know about a partition is its size. A file system can reside on multiple partitions. Splunk Enterprise does not report at the partition level.
</p><p>See the list of output fields at server/status/partitions-space in the REST API Reference Manual.
</p>
<a name="configurepif"></a><h2> <a name="configurepif_configure_platform_instrumentation"><span class="mw-headline" id="Configure_platform_instrumentation"> Configure platform instrumentation </span></a></h2>
<p>This topic is about log files that are tailed to populate the <code><font size="2">_introspection</font></code> index. Read about this feature in "<a href="#abouttheplatforminstrumentationframework" class="external text">About Splunk Enterprise platform instrumentation</a>."
</p><p>This topic helps you configure the default logging interval and enable or disable logging.
</p>
<h3> <a name="configurepif_what_is_logged.2c_and_how_frequently"><span class="mw-headline" id="What_is_logged.2C_and_how_frequently"> What is logged, and how frequently </span></a></h3>
<p>This table summarizes the default settings:
</p>
<table border="1" cellpadding="5" cellspacing="0" width="100%"><tr><th bgcolor="#C0C0C0">  Instance type </th><th bgcolor="#C0C0C0"> Resource usage </th><th bgcolor="#C0C0C0"> Disk objects: indexes, bucket superdirectories, volumes, search dispatch artifacts </th><th bgcolor="#C0C0C0"> Disk objects: fishbucket, partitions
</th></tr><tr><td valign="center" align="left"> Universal forwarder </td><td valign="center" align="left"> every 600 sec (disabled by default) </td><td valign="center" align="left"> N/A (UFs do not have indexes) </td><td valign="center" align="left"> every 600 sec
</td></tr><tr><td valign="center" align="left"> non-UFs </td><td valign="center" align="left"> every 10 sec </td><td valign="center" align="left"> every 600 sec </td><td valign="center" align="left"> every 600 sec
</td></tr></table><p>See "<a href="#whatdatagetslogged" class="external text">What gets logged</a>" for details about what data is logged.
</p>
<h3> <a name="configurepif_enable_logging_on_a_universal_forwarder"><span class="mw-headline" id="Enable_logging_on_a_universal_forwarder"> Enable logging on a universal forwarder </span></a></h3>
<p>The introspection generator add-on is disabled by default on a universal forwarder. To enable:
in the forwarder's <code><font size="2">$SPLUNK_HOME/etc/apps/introspection_generator_addon/local/app.conf</font></code>, set
</p>
<code><font size="2"><br>[install]<br>state = enabled</font></code>
<h3> <a name="configurepif_enable_the_introspection_generator_add-on_using_deployment_server"><span class="mw-headline" id="Enable_the_introspection_generator_add-on_using_deployment_server"> Enable the introspection generator add-on using deployment server </span></a></h3>
<p>To facilitate the management of collecting introspection logs from Splunk Universal Forwarders, we will use the Splunk Deployment Server to enable the introspection generator add-on.
</p>
<h4><font size="3"><b><i> <a name="configurepif_prerequisites"><span class="mw-headline" id="Prerequisites"> Prerequisites </span></a></i></b></font></h4>
<p>The instructions require the use of a deployment server running Splunk Enterprise 6.2 or later. Additionally, you must have command line access to the deployment server host, as the changes cannot be completed using the <b>Forwarder Management</b> interface provided with the deployment server.
</p><p>The introspection generator add-on is only available on Splunk Enterprise version 6.1 or later. All forwarder instances must be configured as deployment clients to a centralized deployment server.
</p>
<h4><font size="3"><b><i> <a name="configurepif_configure_the_introspection_generator_add-on_on_the_deployment_server"><span class="mw-headline" id="Configure_the_introspection_generator_add-on_on_the_deployment_server"> Configure the introspection generator add-on on the deployment server </span></a></i></b></font></h4>
<ol><li> SSH into the deployment server.
</li><li> Find the Splunk Enterprise installation path on the local machine. The default installation path is: <code><font size="2">/opt/splunk</font></code>
</li><li> Create a new folder: <code><font size="2">$SPLUNK_HOME$/etc/deployment-apps/introspection_generator_addon</font></code>
</li><li> Create a new folder: <code><font size="2">$SPLUNK_HOME$/etc/deployment-apps/introspection_generator_addon/local</font></code>
</li><li> Create an <code><font size="2">app.conf</font></code> file under <code><font size="2">$SPLUNK_HOME$/etc/deployment-apps/introspection_generator_addon/local</font></code>
</li><li> Edit the <code><font size="2">app.conf</font></code> file and enable the add-on by adding:
</li></ol><code><font size="2">[install] <br>state = enabled </font></code>
<p>7. Save the changes. Review the changes to the <code><font size="2">app.conf</font></code> file and the path as a validation step.
</p>
<h4><font size="3"><b><i> <a name="configurepif_review_the_excludefromupdate_command"><span class="mw-headline" id="Review_the_excludeFromUpdate_command"> Review the <code><font size="2">excludeFromUpdate</font></code> command </span></a></i></b></font></h4>
<p>The <code><font size="2">excludeFromUpdate</font></code> prevents the deployment server from overwriting the contents of defined folders in an app. For more examples, see the "serverclass.conf" in the <i>Admin Manual</i>. 
</p><p>For this task, we will use <code><font size="2">excludeFromUpdate</font></code> to enable the introspection generator add-on, while preventing the deployment server from making any changes to the add-on by blocking it from overwriting the contents in the <code><font size="2">app/introspection_generator_addon/default</font></code> and <code><font size="2">app/introspection_generator_addon/bin</font></code> folders.
</p>
<h4><font size="3"><b><i> <a name="configurepif_update_the_serverclass.conf_file.2c_adding_the_app_to_a_serverclass_for_deployment"><span class="mw-headline" id="Update_the_serverclass.conf_file.2C_adding_the_app_to_a_serverclass_for_deployment"> Update the <code><font size="2">serverclass.conf</font></code> file, adding the app to a serverclass for deployment </span></a></i></b></font></h4>
<p>1. Find the primary copy of the <code><font size="2">serverclass.conf</font></code> file. The location and contents will vary between deployments, but some common locations are: <code><font size="2">$SPLUNK_HOME$/etc/system/local/</font></code>, and <code><font size="2">$SPLUNK_HOME$/etc/apps/*/local</font></code>. To use btool to find all <code><font size="2">serverclass.conf</font></code> files referenced on the deployment server, run: <code><font size="2">./splunk btool --debug serverclass list</font></code> and review the output.
</p><p>2. Create a new app definition for deploying the changes to the introspection generator add-on. This task is dependent upon the local environment and how the Splunk administrator has chosen to assign and manage apps deployed to forwarders. Many deployments use one serverclass definition to deploy and manage the most common apps for forwarders. For the purposes of this procedure, all universal forwarders are included under one encompassing serverclass named <code><font size="2">PrimaryForwarders</font></code>. 
</p><p>3. Define the field <code><font size="2">excludeFromUpdate</font></code> command at the app level. 
</p>
<code><font size="2">[serverClass:PrimaryForwarders:app:introspection_generator_addon]<br>excludeFromUpdate = $app_root$/default, $app_root$/bin <br>restartSplunkd = True </font></code>
<p>4. Save the changes. Review the changes to the <code><font size="2">serverclass.conf</font></code> file and the path as a validation step.
</p>
<h4><font size="3"><b><i> <a name="configurepif_reload_the_deployment_server"><span class="mw-headline" id="Reload_the_deployment_server"> Reload the deployment server </span></a></i></b></font></h4>
<p>1. Utilize your enterprise change control system to file the requirements and changes for this procedure. 
</p><p>2. Run <code><font size="2">./splunk reload deploy-server</font></code> to reload the deployment server and present the changes to all forwarder hosts at their next check-in interval. The command can be scripted to run on the deployment server after working hours.
</p>
<h4><font size="3"><b><i> <a name="configurepif_validate_changes_have_been_successfully_deployed"><span class="mw-headline" id="Validate_changes_have_been_successfully_deployed"> Validate changes have been successfully deployed </span></a></i></b></font></h4>
<p>Use the search head to validate the introspection logs are being forwarded. Example: <code><font size="2"> index=_introspection host=&lt;forwarder_host&gt; | stats count by source, component </font></code>
</p>
<h3> <a name="configurepif_populate_.22extra.22_fields"><span class="mw-headline" id="Populate_.22Extra.22_fields"> Populate "Extra" fields </span></a></h3>
<p>Four fields (in per-process resource usage data) are not populated by default but can be turned on. See "<a href="#whatdatagetslogged" class="external text">What gets logged</a>" for information.
</p><p>In server.conf you can tell Splunk Enterprise to acquire the "Extra" fields by setting <code><font size="2">acquireExtra_i_data</font></code> to true. For example:
</p>
<code><font size="2"><br>[introspection:generator:disk_objects]<br>disabled = false<br>acquireExtra_i_data = true<br>collectionPeriodInSecs = 600</font></code>
<h3> <a name="configurepif_increase_the_polling_period"><span class="mw-headline" id="Increase_the_polling_period"> Increase the polling period </span></a></h3>
<h4><font size="3"><b><i> <a name="configurepif_why_might_you_want_to_increase_the_polling_period.3f"><span class="mw-headline" id="Why_might_you_want_to_increase_the_polling_period.3F"> Why might you want to increase the polling period? </span></a></i></b></font></h4>
<p>Search processes are polled every 10 seconds (600 seconds on a universal forwarder) by a low-profile process. For healthy Splunk Enterprise deployments, we do not expect this to cause any performance problems. But on a deployment that is already prone to performance problems such as a slow pooled search head environment, there might be some performance implications.
</p>
<h4><font size="3"><b><i> <a name="configurepif_configure_by_collection_type"><span class="mw-headline" id="Configure_by_collection_type"> Configure by collection type </span></a></i></b></font></h4>
<p>In server.conf you can increase the polling period by collection type (that is, resource usage data or disk object data).
</p><p>The default settings (for anything other than a universal forwarder) are:
</p>
<code><font size="2"><br>[introspection:generator:disk_objects]<br>disabled = false<br>acquireExtra_i_data = false<br>collectionPeriodInSecs = 600<br><br>[introspection:generator:resource_usage]<br>disabled = false<br>acquireExtra_i_data = false<br>collectionPeriodInSecs = 10<br></font></code>
<p>On a universal forwarder, the default resource usage collection period is 600 seconds.
</p>
<h3> <a name="configurepif_disable_logging"><span class="mw-headline" id="Disable_logging"> Disable logging </span></a></h3>
<p>It is possible to disable introspection logging, although in most cases, it's preferable to merely increase the polling interval.
</p>
<h4><font size="3"><b><i> <a name="configurepif_turn_off_all_introspection_logging"><span class="mw-headline" id="Turn_off_all_introspection_logging"> Turn off all introspection logging </span></a></i></b></font></h4>
<p>You can turn off all introspection collection (and subsequent logging) by disabling the Introspection Generator Add-On.
</p><p>In the <code><font size="2">$SPLUNK_HOME/etc/apps/introspection_generator_addon/local/app.conf</font></code> file, set
</p>
<code><font size="2"><br>[install]<br>state = disabled<br></font></code>
<h4><font size="3"><b><i> <a name="configurepif_turn_off_introspection_logging_at_the_component_level"><span class="mw-headline" id="Turn_off_introspection_logging_at_the_component_level"> Turn off introspection logging at the component level </span></a></i></b></font></h4>
<p>In server.conf you can disable, enable, and configure collection by collection type. That is, resource usage data or disk object data.
</p><p>The default settings are:
</p>
<code><font size="2"><br>[introspection:generator:disk_objects]<br>disabled = false<br>acquireExtra_i_data = false<br>collectionPeriodInSecs = 600<br><br>[introspection:generator:resource_usage]<br>disabled = false<br>acquireExtra_i_data = false<br>collectionPeriodInSecs = 10<br></font></code>
<h3> <a name="configurepif_run_resource_usage_logging_from_the_command_line"><span class="mw-headline" id="Run_resource_usage_logging_from_the_command_line"> Run resource usage logging from the command line </span></a></h3>
<p>If you've disabled this logging on your instance, you can still invoke the CLI command. To invoke, at the command line:
</p><p><code><font size="2">$ splunkd instrument-resource-usage [--debug] [--once] [--extra]</font></code>
</p><p>where the flags mean:
</p><p><code><font size="2">--debug</font></code>: Set logging level to DEBUG (this can also be done via <code><font size="2">log-cmdline.cfg</font></code>)
</p><p><code><font size="2">--once</font></code>: Emit one set of introspection data, and then quit
</p><p><code><font size="2">--extra</font></code>: This has the same effect as setting <code><font size="2">acquireExtra_i_data</font></code> to true in the <code><font size="2">server.conf [introspection:generator:resource_usage]</font></code> stanza. See "<a href="#whatdatagetslogged" class="external text">What gets logged</a>" for which fields are not logged by default and require this flag.
</p>
<h3> <a name="configurepif_change_the_location_of_the_introspection_index"><span class="mw-headline" id="Change_the_location_of_the_introspection_index"> Change the location of the _introspection index </span></a></h3>
<p>In indexes.conf you can specify the _introspection index. The default location is in <code><font size="2">$SPLUNK_DB</font></code>:
</p>
<code><font size="2"><br>[_introspection]<br>homePath &nbsp;&nbsp;= $SPLUNK_DB/_introspection/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/_introspection/colddb<br>thawedPath = $SPLUNK_DB/_introspection/thaweddb<br>maxDataSize = 1024<br>frozenTimePeriodInSecs = 1209600<br></font></code>

<a name="sampleplatforminstrumentationsearches"></a><h2> <a name="sampleplatforminstrumentationsearches_sample_platform_instrumentation_searches"><span class="mw-headline" id="Sample_platform_instrumentation_searches"> Sample platform instrumentation searches</span></a></h2>
<p>This topic introduces a few examples of analysis you can perform using Splunk Enterprise platform instrumentation. Read "<a href="#abouttheplatforminstrumentationframework" class="external text">About Splunk Enterprise platform instrumentation</a>" for an introduction to the feature.
</p>
<h3> <a name="sampleplatforminstrumentationsearches_aggregate_median_physical_memory_usage_per_search_type"><span class="mw-headline" id="Aggregate_median_physical_memory_usage_per_search_type"> Aggregate median physical memory usage per search type </span></a></h3>
<p>Use this search to find the median total physical memory used, per search type (ad hoc, scheduled, report acceleration, data model acceleration, or summary indexing) for one host over the last hour:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_introspection host=&lt;hostname&gt; data.search_props.sid=* earliest=-1h | bin _time span=10s|stats latest(data.mem_used) as mem_used by data.search_props.sid, data.search_props.type, _time | stats sum(mem_used) as mem_used by data.search_props.sid, data.search_props.type, _time | timechart median(mem_used) by data.search_props.type</font></code><br></div>
<p>As a stacked column chart, this search produces a visualization that looks like this:
</p><p><img alt="Phys mem per search type per host.png" src="images/1/1e/Phys_mem_per_search_type_per_host.png" width="700" height="193"></p>
<h3> <a name="sampleplatforminstrumentationsearches_current_disk_usage_per_partition_in_use_by_splunk_enterprise"><span class="mw-headline" id="Current_disk_usage_per_partition_in_use_by_Splunk_Enterprise"> Current disk usage per partition in use by Splunk Enterprise </span></a></h3>
<p>Use this search to find the latest value of Splunk Enterprise disk usage per partition and instance:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">| rest  /services/server/status/partitions-space | eval usage = capacity - free
| eval pct_usage = round(usage / capacity * 100, 2) | stats first(fs_type) as fs_type first(usage) as usage first(capacity) as capacity first(pct_usage) as pct_usage by mount_point, splunk_server</font></code><br></div>
<p><img alt="Disk usage per mount point.png" src="images/7/73/Disk_usage_per_mount_point.png" width="700" height="96"></p>
<h3> <a name="sampleplatforminstrumentationsearches_median_cpu_usage_for_the_main_splunkd_process_for_one_host"><span class="mw-headline" id="Median_CPU_usage_for_the_main_splunkd_process_for_one_host"> Median CPU usage for the main splunkd process for one host </span></a></h3>
<p>Use this search to find the median CPU usage of the main splunkd process for one host over the last hour:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_introspection component=PerProcess host=&lt;hostname&gt; data.process=splunkd (data.args="-p * start" OR data.args="service") earliest=-1h | timechart median(data.pct_cpu) as cpu_usage(%)</font></code><br></div>
<p>Fill in "&lt;hostname&gt;" with the "host" metadata field associated with your instance, as recorded in inputs.conf's "host" property. As an area chart, this search produces something like this:
</p><p><img alt="CPU usage splunkd.png" src="images/c/c2/CPU_usage_splunkd.png" width="700" height="185"></p>
<h3> <a name="sampleplatforminstrumentationsearches_median_search_concurrency_by_search_mode_for_all_instances"><span class="mw-headline" id="Median_search_concurrency_by_search_mode_for_all_instances"> Median search concurrency by search mode for all instances </span></a></h3>
<p>Use this search to find the median number of searches running at any given time, split by mode (historical, historical batch, real-time, or real-time indexed):
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_introspection data.search_props.sid=* earliest=-1h | bin _time span=10s|stats dc(data.search_props.sid) as search_count by data.search_props.mode, _time | timechart median(search_count) by data.search_props.mode</font></code><br></div>
<h3> <a name="sampleplatforminstrumentationsearches_peak_splunkweb_file_descriptor_usage_over_time_for_one_instance"><span class="mw-headline" id="Peak_splunkweb_file_descriptor_usage_over_time_for_one_instance"> Peak splunkweb file descriptor usage over time for one instance </span></a></h3>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_introspection component=PerProcess host="&lt;hostname&gt;" (data.process="python*" data.args="*/mrsparkle/root.py*") OR data.process=splunkweb
| timechart max(data.fd_used) as fd_used</font></code><br></div>
<p>Fill in "&lt;hostname&gt;" with the "host" metadata field associated with your instance, as recorded in inputs.conf's "host" property.
</p>
<h1>Contact Splunk Support</h1><a name="contactsplunksupport"></a><h2> <a name="contactsplunksupport_contact_support"><span class="mw-headline" id="Contact_Support"> Contact Support </span></a></h2>
<p>For contact information, see the main Support contact page.
</p><p>Here is some information on tools and techniques Splunk Support uses to diagnose problems. Many of these you can try yourself. 
</p><p><b>Note:</b> Before you send any files or information to Splunk Support, verify that you are comfortable with sending it to us. We try to ensure that no sensitive information is included in any output from the commands below and in "<a href="#anonymizedatasamplestosendtosupport" class="external text">Anonymize data samples to send to Support</a>" in this manual, but we cannot guarantee compliance with your particular security policy.
</p>
<h3> <a name="contactsplunksupport_upload_to_your_case"><span class="mw-headline" id="Upload_to_your_case"> Upload to your case </span></a></h3>
<p><b>Note:</b> Before you upload a diag, make sure the user who uploads the file has read permissions to the diag*.tar.gz file.
</p><p>Upload your supporting case documentation to your Support case here: 
</p>
<ul><li> Enterprise Support: http://www.splunk.com/index.php/track_issues
</li><li> Community Members: https://www.splunk.com/index.php/send_to_splunk
</li></ul><h3> <a name="contactsplunksupport_diagnostic_files"><span class="mw-headline" id="Diagnostic_files"> Diagnostic files </span></a></h3>
<p>The diag command collects basic info about your Splunk server, including Splunk's configuration details (such as the contents of <code><font size="2">$SPLUNK_HOME/etc</font></code> and general details about your index, like the host and source names). It does not include any event data or private information.
</p><p><b>Be sure to run diag as a user with appropriate access to read Splunk files.</b>  On *NIX, typically the user you run the splunk service under, such as 'splunk', while on Windows typically the domain user you run splunk as, or some kind of local administrator if you run as "LocalSystem". 
</p><p>See "<a href="#generateadiag" class="external text">Generate a diag</a>" in this manual for instructions on the diag command.
</p>
<h3> <a name="contactsplunksupport_core_files"><span class="mw-headline" id="Core_Files"> Core Files </span></a></h3>
<p>To collect a core file if Support asks you for one, use <code><font size="2">ulimit</font></code> to remove any maximum file size setting before starting Splunk.
</p><p><code><font size="2"># ulimit -c unlimited</font></code>
</p><p><code><font size="2"># splunk restart</font></code>
</p><p>This setting only affects the processes you start from the shell where you ran the ulimit command.  To find out where core files land in your particular UNIX flavor and version, consult the system documentation.  The below text includes some general rules that may or may not apply.
</p><p>On UNIX, if you start Splunk with the --nodaemon option (<code><font size="2">splunk start --nodaemon</font></code>), it may write the core file to the current directory.  Without the flag the expected location is / (the root of the filesystem tree).  However, various platforms have various rules about where core files go with or without this setting.  Consult your system documentation. If you do start splunk with --nodaemon, you will need to, in another shell, start the web interface manually with <code><font size="2">splunk start splunkweb</font></code>.
</p><p>Depending on your system, the core may be named something like core.1234, where '1234' is the process ID of the crashing program.
</p>
<h3> <a name="contactsplunksupport_ldap_configurations"><span class="mw-headline" id="LDAP_configurations"> LDAP configurations </span></a></h3>
<p>If you are having trouble setting up LDAP, Support will typically need the following information: 
</p>
<ul><li> The <code><font size="2">authentication.conf</font></code> file from <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code>.
</li><li> An ldif for a group you are trying to map roles for.
</li><li> An ldif for a user you are trying to authenticate as.
</li></ul><p>In some instances, a debug <code><font size="2">splunkd.log</font></code> or <code><font size="2">web_service.log</font></code> is helpful.
</p>
<a name="howtofileagreatsupportcase"></a><h2> <a name="howtofileagreatsupportcase_how_to_file_a_great_support_case"><span class="mw-headline" id="How_to_file_a_great_Support_case"> How to file a great Support case</span></a></h2>
<p>When you're contacting Support, you can save time by starting out with everything we'll need!
</p><p>Here are some ideas to get you started.
</p>
<h3> <a name="howtofileagreatsupportcase_describe_the_issue"><span class="mw-headline" id="Describe_the_issue"> Describe the issue </span></a></h3>
<p>Where does the issue occur? On a forwarder? On an indexer?
</p><p>What elements are present for the issue? What's the timeline leading to the error? What processes are running when the error appears?
</p><p>What behavior do you observe, compared to what you expect? Be specific - for example, how late is "late"?
</p><p>Try to classify the problem:
</p>
<ul><li> Is it a searching issue? These include Splunk Web, management, roles, apps, views and dashboards, search language.
</li><li> Is it a back end issue? These problems could include crashing, OS issues, REST API, or SDK.
</li><li> Is it a configuration issue? These include extractions, input configurations, forwarding, apps disabling, or authentication.
</li><li> Is it a performance problem?
</li></ul><h3> <a name="howtofileagreatsupportcase_send_diagnosis_files"><span class="mw-headline" id="Send_diagnosis_files"> Send diagnosis files </span></a></h3>
<p>Most support cases are opened in response to functional problems: Splunk has been configured to do something, but it is behaving in an unexpected way.
</p><p>Splunk Support needs both the context of the problem and insight into the instance that is not performing as expected. That insight comes in the form of a "diag," which is essentially a snapshot of the configuration of the host server, the Splunk instance, and the recent logs of that instance.  
</p><p>Whether your problem is with a forwarder, an indexer, a search head, or a deployment server, send us your diag. If you have a forwarder and a receiver that aren't working together correctly, send us diags of both. (If you have many forwarders, just send one representative forwarder diag.)
</p><p>The diag tarball or .zip does not contain any of your indexed data, but if you have concerns, please go ahead and examine the contents. Read about <a href="#generateadiag" class="external text">making a diag</a> in this manual.
</p><p>Splunk Support might request another diag after recommending a change or update to the instance. This diag can ensure that the change has been applied and verify the impact, if any, to the instance. It is not unusual to have multiple updated diags for a single case.
</p><p>Splunk Support understands that it is not always straightforward to collect a diag from certain machines, due to a variety of restrictions. If this is the case with your environment, detail that in your case and we will adjust our approach and requests accordingly. Review "<a href="#generateadiag" class="external text">Generate a diag</a>" in this manual for options available when generating a diag.
</p>
<a name="generateadiag"></a><h2> <a name="generateadiag_generate_a_diag"><span class="mw-headline" id="Generate_a_diag"> Generate a diag </span></a></h2>
<p>To help diagnose a problem, Splunk Support might request a diagnostic file from you. Diag files give Support insight into how an instance is configured and how it has been operating up to the point that the diag command was issued. 
</p>
<h3> <a name="generateadiag_about_diag"><span class="mw-headline" id="About_diag"> About diag </span></a></h3>
<p>The diag command collects basic information about your Splunk server, including Splunk's configuration details. It gathers information from the server such as server specs, OS version, file system, and current open connections. From the Splunk instance it collects the contents of <code><font size="2">$SPLUNK_HOME/etc</font></code> such as app configurations, internal Splunk log files, and index metadata.
</p><p>Diag does not collect any of your indexed data and we strongly encourage you to examine the tarball to ensure that no proprietary data is included. In some environments, custom app objects, like lookup tables, could potentially contain sensitive data. You can exclude any file or directory from the diag collection by using the <code><font size="2">--exclude</font></code> flag. Read on for more details.
</p><p><b>Note:</b> Before you send any files or information to Splunk Support, verify that you are comfortable with sending it to us. We try to ensure that no sensitive information is included in any output from the commands below and in <a href="#anonymizedatasamplestosendtosupport" class="external text">"Anonymize data samples to send to Support" in this manual</a>, but we cannot guarantee compliance with your particular security policy.
</p>
<h3> <a name="generateadiag_run_diag_with_default_settings"><span class="mw-headline" id="Run_diag_with_default_settings"> Run diag with default settings </span></a></h3>
<p><b>Be sure to run diag as a user with appropriate access to read Splunk files.</b>
</p><p>On *nix:   <code><font size="2">$SPLUNK_HOME/bin</font></code>
</p>
<code><font size="2"><br>./splunk diag<br></font></code>
<p>On Windows:  <code><font size="2">%SPLUNK_HOME%/bin</font></code>
</p>
<code><font size="2"><br>splunk diag<br></font></code>
<p>If you have difficultly running diag in your environment, you can also run the python script directly from the bin directory using cmd.
</p><p>On *nix:
</p>
<code><font size="2"><br>./splunk cmd python $SPLUNK_HOME/lib/python2.7/site-packages/splunk/clilib/info_gather.py<br></font></code>
<p>On Windows:
</p>
<code><font size="2"><br>splunk cmd python&nbsp;%SPLUNK_HOME%\Python-2.7\Lib\site-packages\splunk\clilib\info_gather.py<br></font></code>
<p>For clustered environments, see the <a href="#generateadiag_clustering_diag_steps" class="external text">recommended steps</a> below.
</p><p><b>Note</b>: The python version number may differ in future versions of Splunk Enterprise, affecting this path.
</p><p>This produces <code><font size="2">diag-&lt;server name&gt;-&lt;date&gt;.tar.gz</font></code> in your Splunk home directory, which you can send to Splunk Support for troubleshooting. If you're having trouble with forwarding, Support will probably need to see a diag for both your forwarder and your receiver.
</p>
<h3> <a name="generateadiag_designate_content_for_diag_to_include_or_exclude"><span class="mw-headline" id="Designate_content_for_diag_to_include_or_exclude"> Designate content for diag to include or exclude </span></a></h3>
<p>Diag can be told to leave some files out of the diag. One way to do this is with path exclusions.  At the command line you can use the switch <code><font size="2">--exclude</font></code>. For example:
</p>
<code><font size="2"><br>splunk diag --exclude "*/passwd"<br></font></code>
<p>This is repeatable:
</p>
<code><font size="2"><br>splunk diag --exclude "*/passwd" --exclude "*/dispatch/*"<br></font></code>
<p>A more robust way to exclude content is with <a href="#generateadiag_components" class="external text">components</a>. The following switches select which categories of information should be collected. The components available are: <code><font size="2">index_files, index_listing, dispatch, etc, log, pool</font></code>.
</p>
<code><font size="2"> &nbsp;--collect=list &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Declare an arbitrary set of components to gather, as a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;comma-separated list, overriding any prior choices<br>&nbsp;&nbsp;--enable=component_name<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a component to the work list<br>&nbsp;&nbsp;--disable=component_name<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Remove a component from the work list<br></font></code>
<p>The following switches control the thoroughness with which diag gathers categories of data:
</p>
<code><font size="2"> &nbsp;--all-dumps=bool &nbsp;&nbsp;&nbsp;get every crash .dmp file, as opposed to the default of a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;more useful subset<br>&nbsp;&nbsp;--index-files=level<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Index data file gathering level: manifests, or full,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;meaning manifests + metadata files) [default:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;manifests]<br>&nbsp;&nbsp;--index-listing=level<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Index directory listing level: light (hot buckets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only), or full, meaning manifests + metadata files)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[default: light]<br>&nbsp;&nbsp;--etc-filesize-limit=level<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do not gather files in $SPLUNK_HOME/etc larger than<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this many kilobytes, 0 disables this filter [default:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10000]<br>&nbsp;&nbsp;--log-age=days &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log age to gather: log files over this many days old<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are not included, 0 disables this filter [default: 60]<br></font></code>
<p>Defaults can also be controlled in server.conf. Refer to server.conf.spec in the Admin Manual for more information.
</p>
<h4><font size="3"><b><i> <a name="generateadiag_components"><span class="mw-headline" id="Components"> Components </span></a></i></b></font></h4>
<p>The "enable" and "disable" switches use the following components.
</p><p><b>index_files:</b> Files from the index that describe their contents. (Hosts|Sources|Sourcetypes.data and bucketManifests). User data is not collected. If diag collects index files on larger deployments, it might take a while to run. Read about <b>index files</b> in the Splexicon.
</p><p><b>index_listing:</b> Directory listings of the index contents are gathered, in order to see file names, directory names, sizes, timestamps, and the like.  This information lands in systeminfo.txt
</p><p><b>dispatch:</b> The search dispatch directories. See "<a href="#whatsplunklogsaboutitself_splunk_search_logs" class="external text">What Splunk Enterprise logs about itself</a>."
</p><p><b>etc:</b> The entire contents of the <code><font size="2">$SPLUNK_HOME/etc</font></code> directory, which contains configuration information, including .conf files.
</p><p><b>log:</b> The contents of <code><font size="2">$SPLUNK_HOME/var/log/...</font></code> See "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk Enterprise logs about itself</a>."
</p><p><b>pool:</b> If search head pooling is enabled, the contents of the pool dir.
</p><p><b>searchpeers:</b> Directory listing of the "searchpeers" location, actually the data provided by search*heads* on indexers/search nodes.
</p><p><b>consensus:</b> Search Head Clustering -- Copies of the consensus protocol files used for search head cluster member coordination from var/run/splunk/_raft
</p><p><b>conf_replication_summary:</b> Search Head Clustering -- A directory listing of replication summaries produced by search head clustering
</p><p><b>rest:</b> splunkd httpd REST endpoint gathering.  Collects output of various splunkd urls into xml files to capture system state.  (Off by default due to fragility concerns for initial 6.2 shipment.)
</p><p><b>kvstore:</b> Directory listing of the Splunk key value store files.
</p>
<h3> <a name="generateadiag_run_diag_on_a_remote_node"><span class="mw-headline" id="Run_diag_on_a_remote_node"> Run diag on a remote node </span></a></h3>
<p>If you are not able to SSH into every machine in your deployment, you can still gather diags.
</p><p>First, make sure you have the "get-diag" capability. Admin users have this capability. If admin users want to delegate this responsibility, they can give power users the get-diag capability.
</p><p>You also need login credentials for the remote server.
</p><p>The syntax is:
</p>
<code><font size="2">splunk diag -uri https://&lt;host&gt;:&lt;mgmtPort&gt;</font></code>
<h3> <a name="generateadiag_examples"><span class="mw-headline" id="Examples"> Examples </span></a></h3>
<h4><font size="3"><b><i> <a name="generateadiag_exclude_a_lookup_table"><span class="mw-headline" id="Exclude_a_lookup_table"> Exclude a lookup table </span></a></i></b></font></h4>
<p>These two examples exclude content on the file level. A <b>lookup table</b> can be one of several formats, like .csv, .dat, or text.
</p><p>Exclude all .csv files, or all .dat files, in <code><font size="2">$SPLUNK_HOME</font></code>:
</p><p><code><font size="2">splunk diag --exclude "*.csv"</font></code> or
</p><p><code><font size="2">splunk diag --exclude "*.dat"</font></code>
</p><p><b>Note</b>: These examples will exclude all files of that type, not only lookup tables. If you have .csv or .dat files that will be helpful for Support in troubleshooting your issue, exclude only your lookup tables. That is, write out the files instead of using an asterisk.
</p><p><b>Note</b>: Filenames excluded by the --exclude feature will be listed in the excluded_filelist.txt in the diag to ensure Splunk Support can understand the diag.
</p>
<h4><font size="3"><b><i> <a name="generateadiag_exclude_the_dispatch_directory"><span class="mw-headline" id="Exclude_the_dispatch_directory"> Exclude the dispatch directory </span></a></i></b></font></h4>
<p>This example excludes content on the component level. Exclude the dispatch directory to avoid gathering search artifacts (which can be very costly on a pooled search head):
</p>
<code><font size="2">$SPLUNK_HOME/bin/splunk diag --disable=dispatch</font></code>
<h4><font size="3"><b><i> <a name="generateadiag_exclude_multiple_directories"><span class="mw-headline" id="Exclude_multiple_directories"> Exclude multiple directories </span></a></i></b></font></h4>
<p>To exclude multiple components, use the <code><font size="2">--disable</font></code> flag once for each component.
</p><p>Exclude the dispatch directory and all files in the shared search head pool:
</p>
<code><font size="2">$SPLUNK_HOME/bin/splunk diag --disable=dispatch --disable=pool</font></code>
<p><b>Note:</b> This does not gather a full set of the configuration files in use by that instance. Such a diag is useful only for the logs gathered from <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code>. See "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk Enterprise logs about itself</a>" in this manual.
</p>
<h4><font size="3"><b><i> <a name="generateadiag_gather_only_logs"><span class="mw-headline" id="Gather_only_logs"> Gather only logs </span></a></i></b></font></h4>
<p>To whitelist only the Splunk Enterprise internal log files:
</p>
<code><font size="2">$SPLUNK_HOME/bin/splunk diag --collect=log</font></code>
<h4><font size="3"><b><i> <a name="generateadiag_clustering_diag_steps"><span class="mw-headline" id="Clustering_diag_steps"> Clustering diag steps </span></a></i></b></font></h4>
<p>Our recommended steps for the moment for generating a diag on a Splunk data cluster are:
</p>
<code><font size="2">$SPLUNK_HOME/bin/splunk login<br>...enter username and password here...<br>$SPLUNK_HOME/bin/splunk diag --collect all<br></font></code>
<h3> <a name="generateadiag_save_the_settings_for_diag_in_server.conf"><span class="mw-headline" id="Save_the_settings_for_diag_in_server.conf"> Save the settings for diag in server.conf </span></a></h3>
<p>You can update the default settings for diag in the <code><font size="2">[diag]</font></code> stanza of server.conf.
</p><p><code><font size="2">
[diag]
</font></code></p><p>EXCLUDE-&lt;class&gt; = &lt;glob expression&gt;
</p>
<code><font size="2"> &nbsp;&nbsp;* Specifies a glob / shell pattern to be excluded from diags generated on this instance. <br>&nbsp;&nbsp;&nbsp;* Example: */etc/secret_app/local/*.conf<br></font></code>
<p>
</p><p>Flags that you append to <code><font size="2">splunk diag</font></code> override server.conf settings.
</p>
<h3> <a name="generateadiag_diag_contents"><span class="mw-headline" id="Diag_contents"> Diag contents </span></a></h3>
<p>Primarily, a diag contains server logs, from $SPLUNK_HOME/var/log/splunk, and the configuration files, from $SPLUNK_HOME/etc.
</p><p>Specifically, by pathname, there is:
</p>
<dl><dt> _raft/...
</dt><dd> Files containing the state of the consensus protocol produced by search head clustering from var/run/splunk/_raft
</dd><dt> composite.xml
</dt><dd> The generated file that splunkd uses at runtime to control its component system (pipelines &amp; processors), from var/run/splunk/composite.xml
</dd><dt> diag.log
</dt><dd> A copy of all the messages diag produces to the screen when running, including progress indicators, timing, messages about files excluded by heuristic rules (eg if size heuristic, the setting and the size of the file), errors, exceptions, etc.
</dd><dt> dispatch/...
</dt><dd> A copy of some of the data from the search dispatch directory.  Results files (the output of searches) are not included, nor other similar files (events/*)
</dd><dt> etc/...
</dt><dd> A copy of the contents of the configuration files.  All files and directories under $SPLUNK_HOME/etc/auth are excluded by default.
</dd><dt> excluded_filelist.txt
</dt><dd> A list of files which diag would have included, but did not because of some restriction (exclude rule, size restriction).  This is primarily to confirm the behavior of exclusion rules for customers, and to enable Splunk technical support to understand why they can't see data they are looking for.
</dd><dt> introspection/...
</dt><dd> The log files from $SPLUNK_HOME/var/log/introspection
</dd><dt> log/...
</dt><dd> The log files from $SPLUNK_HOME/var/log/splunk
</dd><dt> rest-collection/...
</dt><dd> Output of several splunkd http endpoints that contain information not available in logs.  File input/monitor/tailing status information, server-level admin banners, clustering status info if on a cluster.
</dd><dt> scripts/...
</dt><dd> A single utility script may exist here for support reasons.  It is identical for every diag.
</dd><dt> systeminfo.txt
</dt><dd> Generated output of various system commands to determine things like available memory, open splunk sockets, size of disk/filesystems, operating system version, ulimits.
</dd><dd> Also contained in systeminfo.txt are listings of  filenames/sizes etc from a few locations.
<ul><li> Some of the splunk index directories (or all of the index directories, if full listing is requested.)
</li><li> The searchpeers directory (replicated files from  search heads)
</li><li> Search Head Clustering -- The summary files used in synchronization from var/run/splunk/snasphot
</li></ul></dd></dl><dl><dt> Typically var/...
</dt><dd> The paths to the indexes are a little 'clever', attempting to resemble the paths actually in use (For example, on windows if an index is in e:\someother\largedrive, that index's files will be in e/someother/largdrive inside the diag).  By default only the .bucketManifest for each index is collected.
</dd></dl><h3> <a name="generateadiag_behavior_on_failure"><span class="mw-headline" id="Behavior_on_failure"> Behavior on failure </span></a></h3>
<p>If for some reason diag should fail, it will:
</p>
<ol><li> Clean up temporary files it created while running
</li><li> leave a copy of the output in a temporary filename it references.
</li></ol><p>Here's a typical example:
</p>
<code><font size="2"><br>jrodman@mcp:~$ splunk/bin/splunk diag<br>[... lots of normal output...]<br>Selected diag name of: diag-mcp-2014-09-24<br>Starting splunk diag...<br>[etc .... etc]<br>Getting index listings...<br>Copying Splunk configuration files...<br>Exception occurred while generating diag, we are deeply sorry.<br>Traceback (most recent call last):<br>&nbsp;&nbsp;File "/opt/splunk/lib/python2.7/site-packages/splunk/clilib/info_gather.py", line 1959, in main<br>&nbsp;&nbsp;&nbsp;&nbsp;create_diag(options, log_buffer)<br>&nbsp;&nbsp;File "/opt/splunk/lib/python2.7/site-packages/splunk/clilib/info_gather.py", line 1862, in create_diag<br>&nbsp;&nbsp;&nbsp;&nbsp;copy_etc(options)<br>&nbsp;&nbsp;File "/opt/splunk/lib/python2.7/site-packages/splunk/clilib/info_gather.py", line 1626, in copy_etc<br>&nbsp;&nbsp;&nbsp;&nbsp;raise Exception("OMG!")<br>Exception: OMG!<br><br>Diag failure, writing out logged messages to '/tmp/diag-fail-F2B94h.txt', please send output + this file to either an existing or new case&nbsp;; http://www.splunk.com/support<br>We will now try to clean out the temp directory...<br></font></code>
<p>For most real errors, diag tries to guess at the original problem, but it also writes out a file for use in bugfixing diag.  Please do send it along, and at least a workaround can often be provided quickly.
</p>
<h3> <a name="generateadiag_additional_resources"><span class="mw-headline" id="Additional_resources"> Additional resources </span></a></h3>
<p>Watch a video on making a diag and using the anonymize command by a Splunk Support engineer.
</p><p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has about diags.
</p>
<a name="anonymizedatasamplestosendtosupport"></a><h2> <a name="anonymizedatasamplestosendtosupport_anonymize_data_samples_to_send_to_support"><span class="mw-headline" id="Anonymize_data_samples_to_send_to_Support"> Anonymize data samples to send to Support </span></a></h2>
<p>Splunk contains an anonymize function.  The anonymizer combs through sample log files or event files to replace identifying data - usernames, IP addresses, domain names, etc. - with fictional values that maintain the same word length, and event type.  For example, it may turn the string <code><font size="2">user=carol@adalberto.com</font></code>  into <code><font size="2">user=plums@wonderful.com</font></code>.  This lets Splunk users share log data without revealing confidential or personal information from their networks.
</p><p>The anonymized file is written to the same directory as the source file, with <code><font size="2">ANON-</font></code>  prepended to its filename.  For example, <code><font size="2">/tmp/messages</font></code>  will be anonymized as <code><font size="2">/tmp/ANON-messages</font></code>.
</p><p>You can anonymize files from Splunk's CLI.  To use Splunk's CLI, navigate to the <code><font size="2">$SPLUNK_HOME/bin/</font></code> directory and use the <code><font size="2">./splunk</font></code> command. 
</p>
<h3> <a name="anonymizedatasamplestosendtosupport_simple_method"><span class="mw-headline" id="Simple_method"> Simple method </span></a></h3>
<p>The easiest way to anonymize a file is with the anonymizer tool's defaults, as shown in the session below. Note that you currently need to have <code><font size="2">$SPLUNK_HOME/bin</font></code> as your current working directory.
</p><p>From the CLI while you are in $SPLUNK_HOME, type the following:
</p><p><code><font size="2">&gt; ./splunk anonymize file -source &lt;/path/to/filename&gt;</font></code>
</p><p>Of course it is always good practice to move the file somewhere safe (like /tmp) before doing this sort of thing. So, for example:
</p>
<code><font size="2"><br>&gt; cp -p /var/log/messages /tmp<br>&gt; cd $SPLUNK_HOME/bin<br>&gt; ./splunk anonymize file -source /tmp/messages<br>Processing files: ['/tmp/messages']<br>Getting named entities<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Processing /tmp/messages<br>Adding named entities to list of public terms: Set(['secErrStr', 'MD_SB_DISKS', 'TTY', 'target', 'precision ', 'lpj', 'ip', 'pci', 'hard', 'last bus', 'override with idebus', 'SecKeychainFindGenericPassword err', 'vector', 'USER', 'irq ', 'com &nbsp;user', 'uid'])<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Processing /tmp/messages for terms.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Calculating replacements for 4672 terms.<br>===================================================<br>Wrote dictionary scrubbed terms with replacements to "/tmp/INFO-mapping.txt"<br>Wrote suggestions for dictionary to "/tmp/INFO-suggestions.txt"<br>===================================================<br>Writing out /tmp/ANON-messages<br>Done.<br></font></code>
<h3> <a name="anonymizedatasamplestosendtosupport_advanced_method"><span class="mw-headline" id="Advanced_method"> Advanced method </span></a></h3>
<p>You can customize the anonymizer by telling it what terms to anonymize, what terms to leave alone, and what terms to use as replacements.  The advanced form of the command is:
</p><p><code><font size="2">
./splunk anonymize file -source &lt;filename&gt; [-public_terms &lt;file&gt;] [-private_terms &lt;file&gt;] [-name_terms &lt;file&gt;] [-dictionary &lt;file&gt;] [-timestamp_config &lt;file&gt;]
</font></code>
</p>
<ul><li>  <code><font size="2">filename</font></code> 
<ul><li> Default:  <code><font size="2">None</font></code> 
</li><li> Path and name of the file to anonymize.
</li></ul></li><li> <code><font size="2">public_terms</font></code>
<ul><li> Default: <code><font size="2">$SPLUNK_HOME/etc/anonymizer/public-terms.txt</font></code>
</li><li> A list of locally-used words that will not be anonymized if they are in the file.  It serves as an appendix to the <code><font size="2">dictionary</font></code>  file.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>2003 2004 2005 2006 abort aborted am apr april aug august auth<br>authorize authorized authorizing bea certificate class com complete<br></font></code>
<ul><li> <code><font size="2">private_terms</font></code>
<ul><li> Default:  <code><font size="2">$SPLUNK_HOME/etc/anonymizer/private-terms.txt</font></code>
</li><li> A list of words that will be anonymized if found in the file, because they may denote confidential information.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>401-51-6244<br>passw0rd<br></font></code>
<ul><li> <code><font size="2">name_terms</font></code>
<ul><li> Default:  <code><font size="2">$SPLUNK_HOME/etc/anonymizer/names.txt</font></code>
</li><li> A global list of common English personal names that Splunk uses to replace anonymized words.  
</li><li> Splunk always replaces a word with a name of the exact same length, to keep each event's data pattern the same. 
</li><li> Splunk uses each name in <code><font size="2">name_terms</font></code> once to replace a character string of equal length throughout the file.  After it runs out of names, it begins using randomized character strings, but still mapping each replaced pattern to one anonymized string.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>charlie<br>claire<br>desmond<br>jack<br></font></code>
<ul><li> <code><font size="2">dictionary</font></code>
<ul><li> Default:  <code><font size="2">$SPLUNK_HOME/etc/anonymizer/dictionary.txt</font></code>
</li><li> A global list of common words that will not be anonymized, unless overridden by entries in the <code><font size="2">private_terms</font></code> file.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>algol<br>ansi<br>arco<br>arpa<br>arpanet<br>ascii<br></font></code>
<ul><li> <code><font size="2">timestamp_config</font></code>
<ul><li> Default:  <code><font size="2">$SPLUNK_HOME/etc/anonymizer/anonymizer-time.ini</font></code>
</li><li> Splunk's built-in file that determines how timestamps are parsed.
</li></ul></li></ul><h4><font size="3"><b><i> <a name="anonymizedatasamplestosendtosupport_output_files"><span class="mw-headline" id="Output_Files"> Output Files </span></a></i></b></font></h4>
<p>Splunk's anonymizer function will create three new files in the same directory as the source file.
</p>
<ul><li> <code><font size="2">ANON-filename</font></code>
<ul><li> The anonymized version of the source file.
</li></ul></li><li> <code><font size="2">INFO-mapping.txt</font></code>
<ul><li> This file contains a list of which terms were anonymized into which strings.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>Replacement Mappings<br>--------------------<br>kb900485 --&gt; LO200231<br>1718 --&gt; 1608<br>transitions --&gt; tstymnbkxno<br>reboot --&gt; SPLUNK<br>cdrom --&gt; pqyvi<br></font></code>
<ul><li> <code><font size="2">INFO-suggestions.txt</font></code>
<ul><li> A report of terms found in the file that, based on their appearance and frequency, you may want to add to <code><font size="2">public_terms.txt</font></code>  or to <code><font size="2">private-terms.txt</font></code> or to <code><font size="2">public-terms.txt</font></code> for more accurate anonymization of your local data.  
</li><li> Here is a sample entry:
</li></ul></li></ul><code><font size="2"><br>Terms to consider making private (currently not scrubbed):<br>['uid', 'pci', 'lpj', 'hard']<br>Terms to consider making public (currently scrubbed):<br>['jun', 'security', 'user', 'ariel', 'name', 'logon', 'for', 'process', 'domain', 'audit']<br></font></code>
<h3> <a name="anonymizedatasamplestosendtosupport_linux_tip:_anonymize_all_log_files_from_a_diag_at_once"><span class="mw-headline" id="Linux_tip:_Anonymize_all_log_files_from_a_diag_at_once"> Linux tip: Anonymize all log files from a diag at once </span></a></h3>
<p>Here are the steps to generate a diagnostic (diag file)  and then anonymize the logs of that diag.
</p><p><b>1.</b> Generate the diag:
For example:
</p>
<code><font size="2"><br>cd $SPLUNK_HOME/bin<br>./splunk diag --exclude "*/passwd"<br></font></code>
<p><b>2.</b> Uncompress the diag.
For example:
</p>
<code><font size="2"><br>cd pathtomyuncompresseddiag/<br>tar xfz &nbsp;my-diag-hostname.tar.gz<br></font></code>
<p><b>3.</b> Run <code><font size="2">anonymize</font></code> on each file of the diag.
If you run this command for all *.log, then make note of the log files that now have a prefix of ANON*.log. 
For example:
</p>
<code><font size="2"><br>find pathtomyuncompresseddiag/ -name \*.log* | xargs -I{} ./splunk anonymize file -source '{}'<br></font></code>
<p><b>4.</b> Keep all the files that now have a prefix of ANON*.log while deleting the non-anonymized versions in the diag directory.
</p><p><b>5.</b> Compress the diag.
</p>
<code><font size="2"><br>tar cfz my-diag-hostname.tar.gz pathtomyuncompresseddiag<br></font></code>
<p><b>6.</b> Upload the diag, adding it to the Support case, with the ADD FILE button in the case.
</p>
<a name="collectpstacks"></a><h2> <a name="collectpstacks_collect_pstacks"><span class="mw-headline" id="Collect_pstacks"> Collect pstacks</span></a></h2>
<p>Support might ask you to gather thread call stacks with pstack, for example if your deployment experiences:
</p>
<ul><li> unexplained high CPU, along with identified threads using high CPU,
</li><li> frozen Splunk that's not doing anything, when it obviously should, or
</li><li> unexplainably slow behavior in splunkd (that is, not limited by disk or CPU).
</li></ul><h3> <a name="collectpstacks_on_.2anix"><span class="mw-headline" id="On_.2Anix"> On *nix </span></a></h3>
<h4><font size="3"><b><i> <a name="collectpstacks_find_or_install_pstack"><span class="mw-headline" id="Find_or_install_pstack"> Find or install pstack </span></a></i></b></font></h4>
<p>Pstack is available on Red Hat and Centos Linux and Solaris by default. Pstack is installable on several other flavors of Linux. 
</p><p>Test whether pstack is installed:
</p>
<code><font size="2"><br>&nbsp;which pstack<br>/usr/bin/pstack<br></font></code>
<p>If you get an error message instead of a location, you might still be able to install pstack. On RHEL and its derivatives (CentOS, Oracle Linux, etc), pstack is part of the gdb package.
</p>
<h5> <a name="collectpstacks_error_on_linux_from_pstack:_no_symbols"><span class="mw-headline" id="Error_on_Linux_from_pstack:_no_symbols"> Error on Linux from pstack: no symbols </span></a></h5>
<p>On Linux flavors that aren't based on RHEL, pstack might be useless for troubleshooting, in that it does not support threads.
</p><p>If you get output from pstack such as:
</p>
<code><font size="2">29175: splunkd -p 8089 start<br>(No symbols found)<br>0x7fd3740e96d9:&nbsp;???? (100, 0, 7fffa6befd00, 100000010, 25bb080, ffffffff00000010) + ffff8001594106da <br></font></code>
<p>Then you probably have the x86-64-specific pstack binary, which is less capable than the redhat gdb-based one, as it does not understand posix threaded applications.
Ensure that the gdb package is installed, and try the gstack command as a substitution for pstack. gstack is available on Ubuntu, for example.  If gstack is not available, a very barebones gstack is provided here:
</p>
<code><font size="2"><br>pid=$1<br>echo 'thread apply all bt' | gdb --quiet -nx /proc/$pid/exe $pid<br></font></code>
<h5> <a name="collectpstacks_gdb"><span class="mw-headline" id="gdb"> gdb </span></a></h5>
<p>Installable on nearly any Unix.
</p>
<code><font size="2"><br># ps aux |grep splunkd<br>root &nbsp;&nbsp;&nbsp;&nbsp;31038 &nbsp;0.5 &nbsp;0.6 245292 104884&nbsp;? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sl &nbsp;&nbsp;Sep07 &nbsp;66:45 splunkd -p 17011 restart<br>root &nbsp;&nbsp;&nbsp;&nbsp;31039 &nbsp;0.0 &nbsp;0.0 &nbsp;47012 &nbsp;7076&nbsp;? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ss &nbsp;&nbsp;Sep07 &nbsp;&nbsp;4:47 splunkd -p 17011 restart<br># gdb -p 31038 &nbsp;#this will freeze splunk temporarily<br>... lots of output you don't care about ...<br>(gdb) &lt;-this is the prompt<br>(gdb) thread apply all bt<br>&lt;... interesting output here...&gt;<br>(gdb) quit # important! otherwise splunk is frozen forever<br>#<br></font></code>
<h4><font size="3"><b><i> <a name="collectpstacks_run_pstack"><span class="mw-headline" id="Run_pstack"> Run pstack </span></a></i></b></font></h4>
<p>To run pstack from the *nix command line,
</p>
<code><font size="2"><br># ps aux |grep splunkd<br>root &nbsp;&nbsp;&nbsp;&nbsp;31038 &nbsp;0.5 &nbsp;0.6 245292 104884&nbsp;? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sl &nbsp;&nbsp;Sep07 &nbsp;66:45 splunkd -p 17011 restart<br>root &nbsp;&nbsp;&nbsp;&nbsp;31039 &nbsp;0.0 &nbsp;0.0 &nbsp;47012 &nbsp;7076&nbsp;? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ss &nbsp;&nbsp;Sep07 &nbsp;&nbsp;4:47 splunkd -p 17011 restart<br># pstack 31038<br>&lt;... output here ...&gt;<br></font></code>
<p>It is usually beneficial to get multiple pstacks separated by 1 second. Here is an example of getting 100 pstacks separated by 1 second and storing them in /tmp:
</p>
<code><font size="2"><br>% i=0; while [ $i -lt 100 ]&nbsp;; do date &gt; /tmp/pstack$i.out; pstack $splunkd_pid &gt;&gt; /tmp/pstack$i.out; let "i+=1"; sleep 1; done<br></font></code>
<p>Note that this script requires bash (<code><font size="2">let</font></code> is not a portable expression).
</p>
<h3> <a name="collectpstacks_on_windows"><span class="mw-headline" id="On_Windows"> On Windows </span></a></h3>
<p>You can gather many pstacks at once, like with *nix:
</p><p>http://wiki.splunk.com/Community:GatherWindowsStacks
</p>
<a name="commandlinetoolsforusewithsupport"></a><h2> <a name="commandlinetoolsforusewithsupport_command_line_tools_for_use_with_support"><span class="mw-headline" id="Command_line_tools_for_use_with_Support"> Command line tools for use with Support </span></a></h2>
<p>This topic contains information on CLI tools to help with troubleshooting Splunk. Most of these tools are invoked using the Splunk CLI command "cmd". You should not use these tools without first consulting with Splunk Support.
</p><p>For general information about using the CLI in Splunk, see "Get help with the CLI" in the Admin Manual.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_cmd"><span class="mw-headline" id="cmd"> cmd </span></a></h3>
<p>Runs the specified utility in <code><font size="2">$SPLUNK_HOME/bin</font></code> with the required environment variables preset.
</p><p>To see which environment variables will be set, run "splunk envvars".
</p><p>Examples:
</p>
<code><font size="2"><br>&nbsp;&nbsp;./splunk cmd btool inputs list<br>&nbsp;&nbsp;./splunk cmd /bin/ls<br></font></code>
<p>Syntax: <code><font size="2">cmd &lt;command&gt; [parameters...]</font></code>
</p><p>Objects:	None
</p><p>Required Parameters:	None
</p><p>Optional Parameters:	None
</p>
<h3> <a name="commandlinetoolsforusewithsupport_btool"><span class="mw-headline" id="btool"> btool </span></a></h3>
<p>View or validate Splunk configuration files, taking into account configuration file layering and user/app context.
</p><p>Syntax:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;btool &lt;CONF_FILE&gt; list [options]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;btool check [options]<br></font></code>
<p>Objects:	None
</p><p>Required Parameters:	None
</p><p>Optional Parameters:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--user=SPLUNK_USER &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View the configuration data visible to the given user<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--app=SPLUNK_APP &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;View the configuration data visible from the given app<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--dir=DIR &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Read configuration data from the given absolute path instead of $SPLUNK_HOME/etc<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--debug &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Print and log extra debugging information<br></font></code>
<p>Examples:
</p><p>List: <code><font size="2">./splunk cmd btool [--app=<i>app_name</i>] <i>conf_file_prefix</i> list [<i>stanza_prefix</i>]</font></code>
</p><p>Add: <code><font size="2">./splunk cmd btool [--app=<i>app_name</i>] <i>conf_file_prefix</i> add</font></code>
</p><p>Delete: <code><font size="2">./splunk cmd btool --app=<i>app_name</i> --user=<i>user_name</i> <i>conf_file_prefix</i> delete <i>stanza_name</i> [<i>attribute_name</i>]</font></code>
</p><p>For more information, read "<a href="#usebtooltotroubleshootconfigurations" class="external text">Use btool to troubleshoot configurations</a>."
</p>
<h3> <a name="commandlinetoolsforusewithsupport_btprobe"><span class="mw-headline" id="btprobe"> btprobe </span></a></h3>
<p>Queries the fishbucket for file records stored by tailing.  For up-to-date usage, run btprobe --help.
</p><p><b>Note:</b> You must specify either -d &lt;dir&gt; or --compute-crc &lt;file&gt;
</p><p>There are two possible ways to invoke this tool:
</p><p><b>1.</b> <code><font size="2"> btprobe [-h or --help] -d &lt;btree directory&gt; [-k &lt;hex key OR ALL&gt; | --file &lt;filename&gt;] [--salt &lt;salt&gt;] [--validate] [--reset] [--bytes &lt;bytes&gt;] [-r]</font></code>
</p><p>This method queries the specified BTree for the given key or file.
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-d &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Directory that contains the btree index. (Required)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-k &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hex crc key or ALL to get all the keys.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--file &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;File to compute the crc from.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rebuild the btree .dat files (i.e., var/lib/splunk/fishbucket/splunk_private_db/ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(One of -k and --file must be specified.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--validate &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Validate the btree to look for errors.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--salt &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Salt the crc if --file param is specified.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--reset &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reset the fishbucket for the given key or file in the btree.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--bytes &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of bytes to read when calculating CRC (default 256).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--sourcetype&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sourcetype to load configurations and check Indexed Extraction<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and compute CRC accordingly.<br></font></code>
<p><b>2.</b> <code><font size="2">btprobe [-h or --help] --compute-crc &lt;filename&gt; [--salt &lt;salt&gt;] [--bytes &lt;bytes&gt;]</font></code>
</p><p><br>
This method computes a crc from the specified file, using the given salt if any.
</p>
<ul><li> Example: <code><font size="2">./btprobe -d /opt/splunk/var/lib/splunk/fishbucket/splunk_private_db -k 0xe8d117ddba85e714 --validate</font></code>
</li><li> Example: <code><font size="2">./btprobe -d /opt/splunk/var/lib/splunk/fishbucket/splunk_private_db --file /var/log/inputfile --salt SOME_SALT</font></code>
</li><li>  Example: <code><font size="2">./btprobe --compute-crc /var/log/inputfile --salt SOME_SALT</font></code>
</li></ul><h3> <a name="commandlinetoolsforusewithsupport_classify"><span class="mw-headline" id="classify"> classify </span></a></h3>
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk cmd classify &lt;path/to/myfile&gt; &lt;mysourcetypename&gt; <br></font></code>
<h3> <a name="commandlinetoolsforusewithsupport_fsck"><span class="mw-headline" id="fsck"> fsck </span></a></h3>
<p>Diagnoses the health of your buckets and can rebuild search data as necessary. 
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--hots] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include hot buckets in scan<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--warms] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include warm buckets in scan<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--colds] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include cold buckets in scan<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--thawed] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include thawed buckets in scan<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--all] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include all types of buckets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--index &lt;index&gt;] only scan specified index (defaults to all)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--mode metadata] only supported mode is 'metadata'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--verbose] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;display diagnostic info while scanning<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--repair] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attempt to repair buckets if errors found<br></font></code>
<p><b>Note:</b> <code><font size="2">./splunk --repair</font></code> will only work with buckets created by Splunk version &gt; 4.2.
</p><p>For more information, read "How Splunk stores indexes" in the Managing Indexers and Clusters Manual.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_locktest"><span class="mw-headline" id="locktest"> locktest </span></a></h3>
<p>./splunk cmd locktest
</p>
<h3> <a name="commandlinetoolsforusewithsupport_locktool"><span class="mw-headline" id="locktool"> locktool </span></a></h3>
<p>./splunk cmd locktool 
</p><p>Usage&nbsp;:
</p><p>lock&nbsp;: [-l | --lock ] [dirToLock] &lt;timeOutSecs&gt;
</p><p>unlock [-u | --unlock ] [dirToUnlock] &lt;timeOutSecs&gt;
</p><p>Acquires and releases locks in the same manner as splunkd. If you were to write an external script to copy db buckets in and out of indexes you should acqure locks on the db colddb and thaweddb directories as you are modifying them and release the locks when you are done.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_parsetest"><span class="mw-headline" id="parsetest"> parsetest </span></a></h3>
<code><font size="2"><br>./splunk cmd parsetest<br><br>Usage: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parsetest "&lt;string&gt;" ["&lt;sourcetype&gt;|source::&lt;filename&gt;|host::&lt;hostname&gt;"]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parsetest file &lt;filename&gt; ["&lt;sourcetype&gt;|host::&lt;hostname&gt;"]<br>Example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parsetest "10/11/2009 12:11:13" "syslog"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parsetest file "foo.log" "syslog"<br></font></code>
<h3> <a name="commandlinetoolsforusewithsupport_pcregextest"><span class="mw-headline" id="pcregextest"> pcregextest </span></a></h3>
<p>Simple utility tool for testing modular regular expressions.
</p>
<code><font size="2"><br>./splunk cmd pcregextest mregex=&lt;regex&gt;<br><br>Usage: pcregextest mregex="query_regex" (name="subregex_value")* (test_str="string to test regex")?<br><br>Example: pcregextest mregex="[[ip:src_]] [[ip:dst_]]" ip="(?&lt;ip&gt;\d+[[dotnum]]{3})" dotnum="\.\d+" test_str="1.1.1.1 2.2.2.2"<br></font></code>
<p>That is, define modular regex in the 'mregex' parameter. Then define all the subregexes referenced in 'mregex'. Finally you can provide a sample string to test the resulting regex against, in 'test_str'.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_regextest"><span class="mw-headline" id="regextest"> regextest </span></a></h3>
<h3> <a name="commandlinetoolsforusewithsupport_searchtest"><span class="mw-headline" id="searchtest"> searchtest </span></a></h3>
<p>./splunk cmd searchtest search
</p>
<h3> <a name="commandlinetoolsforusewithsupport_signtool"><span class="mw-headline" id="signtool"> signtool </span></a></h3>
<p>Sign
</p><p><b>./splunk cmd signtool </b>  [<i>-s | --sign</i>]  [<i>&lt;dir to sign&gt;</i>]
</p><p>Verify
</p><p><b>./splunk cmd signtool </b> [<i>-v | --verify</i>] [<i>&lt;dir to verify&gt;</i>]
</p><p>Using logging configuration at <code><font size="2">/Applications/splunk/etc/log-cmdline.cfg</font></code>.
</p><p>Allows verification and signing splunk index buckets. If you have signing set up in a cold to frozen script. Signtool allows you to verify the signatures of your archives.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_tsidxprobe"><span class="mw-headline" id="tsidxprobe"> tsidxprobe </span></a></h3>
<p>This will take a look at your <b>time-series index files</b> (or "tsidx files"; they are appended with .tsidx) and verify that they meet the necessary format requirements. It should also identify any files that are potentially causing a problem
</p><p>go to the $SPLUNK_HOME/bin directory. Do "source setSplunkEnv".
</p><p>Then use tsidxprobe to look at each of your index files with this little script you can run from your shell (this works with bash):
</p>
<ul><li> for i in `find $SPLUNK_DB -name '*.tsidx'`; do tsidxprobe $i &gt;&gt; tsidxprobeout.txt; done
</li></ul><p>(If you've changed the default datastore path, then this should be in the new location.)
</p><p>The file tsidxprobeout.txt will contain the results from your index files. You should be able to gzip this and attach it to an email and send it to Splunk Support.
</p>
<h3> <a name="commandlinetoolsforusewithsupport_tsidx_scan.py"><span class="mw-headline" id="tsidx_scan.py"> tsidx_scan.py </span></a></h3>
<p>(4.2.2+)
This utility script searches for tsidx files at a specified starting location, runs tsidxprobe for each one, and outputs the results to a file. 
</p><p>From $SPLUNK_HOME/bin, call it like this:
</p><p>splunk cmd python tsidx_scan.py [path]
</p><p>Example: 
</p><p>splunk cmd python tsidx_scan.py /opt/splunk/var/lib/splunk
</p><p>If you omit the optional path, the scan starts at $SPLUNK_DB
</p><p>The output is written to the file tsidxprobe.YYYY-MM-DD.txt in the current directory.
</p>
<h1>Common front end scenarios</h1><a name="cantfinddata"></a><h2> <a name="cantfinddata_i_can.27t_find_my_data.21"><span class="mw-headline" id="I_can.27t_find_my_data.21"> I can't find my data! </span></a></h2>
<p>Are you searching for events and not finding them, or looking at a dashboard and seeing "No result data"? Here are a few common mistakes to check.
</p>
<h3> <a name="cantfinddata_are_you_running_splunk_free.3f"><span class="mw-headline" id="Are_you_running_Splunk_Free.3F"> Are you running Splunk Free? </span></a></h3>
<p>Splunk Free does not support multiple user accounts, distributed searching, or alerting.
</p><p>Saved searches that were previously scheduled by other users are still available, and you can run them manually as required. You can also view, move, or modify them in Splunk Web or in savedsearches.conf. 
</p><p>Review this topic about object ownership and this topic about configuration file precedence in the Admin Manual for information about where Splunk writes knowledge objects such as scheduled searches.
</p>
<h3> <a name="cantfinddata_was_the_data_added_to_a_different_index.3f"><span class="mw-headline" id="Was_the_data_added_to_a_different_index.3F"> Was the data added to a different index?</span></a></h3>
<p>Some apps, like the *nix and Windows apps, write input data to a specific index (in the case of *nix and Windows, that is the "os" index). If you're not finding data that you're certain is in Splunk, be sure that you're looking at the right index. You may want to add the "os" index to the list of default indexes for the role you're using. For more information about roles, refer to the topic about roles in the Securing Splunk Enterprise manual. For information about troubleshooting data input issues, see "Troubleshoot the input process" in the Getting Data In manual.
</p>
<h3> <a name="cantfinddata_do_your_permissions_allow_you_to_see_the_data.3f"><span class="mw-headline" id="Do_your_permissions_allow_you_to_see_the_data.3F"> Do your permissions allow you to see the data? </span></a></h3>
<p>Your permissions can vary depending on the index privileges or search filters.
Read more about adding and editing roles in Securing Splunk.
</p>
<h3> <a name="cantfinddata_what_about_issues_related_to_time.3f"><span class="mw-headline" id="What_about_issues_related_to_time.3F"> What about issues related to time? </span></a></h3>
<p>Double check the time range that you're searching. Are you sure the events exist in that time window? Try increasing the time window for your search.
</p><p>You might also want to try a real-time search over all time for some part of your data, like a source type or string.
</p><p>The indexer might be incorrectly timestamping for some reason. Read about timestamping in the Getting Data In Manual.
</p>
<h3> <a name="cantfinddata_are_you_using_forwarders.3f"><span class="mw-headline" id="Are_you_using_forwarders.3F"> Are you using forwarders? </span></a></h3>
<p>Check that your data is in fact being forwarded. Here are some searches to get you started. You can run all these searches, except for the last one, from the Splunk default Search app. The last search you run from the CLI to access the forwarder. A forwarder does not have a user interface:
</p>
<ul><li> Are my forwarders connecting to my receiver? Which IP addresses are connecting to Splunk as inputs, and how many times is each IP logged in metrics.log?
</li></ul><div class="inlineQuery splunk_search_4_3"><code><font size="2">
<p>index=_internal source=*metrics.log* tcpin_connections | stats count by sourceIp
</p>
</font></code><br></div>
<ul><li> What output queues are set up?
</li></ul><div class="inlineQuery splunk_search_4_3"><code><font size="2">
<p>index=_internal source=*metrics.log* group=queue tcpout | stats count by name
</p>
</font></code><br></div>
<ul><li> What hosts (not forwarder/TCP inputs) have logged an event to Splunk in the last 10 minutes? (Including rangemap.)
</li></ul><div class="inlineQuery splunk_search_4_3"><code><font size="2">
<p>| metadata type=hosts index=netops | eval diff=now()-recentTime | where diff &lt; 600 | convert ctime(*Time) | stats count  | rangemap  field=count low=800-2000 elevated=100-799 high=50-99 server=0-49
</p>
</font></code><br></div>
<ul><li> Where is Splunk trying to forward data to? From the Splunk CLI issue the following command:
</li></ul><div class="samplecode"><code><font size="2"><br>$SPLUNK_HOME/bin/splunk search 'index=_internal source=*metrics.log* destHost | dedup destHost'<br></font></code></div>
<ul><li> If you need to see if the socket is getting established you can look at the forwarder's log of this in splunkd.log "Connected to idx=&lt;ip&gt;:&lt;port&gt;" , and on the receiving side if you set the log category TcpInputConn to INFO or lower you can see messages "Connection in cooked mode from src=&lt;ip&gt;:&lt;port&gt;
</li></ul><p>Read up on forwarding in the Forwarding Data Manual.
</p>
<h3> <a name="cantfinddata_are_you_using_search_heads.3f"><span class="mw-headline" id="Are_you_using_search_heads.3F"> Are you using search heads? </span></a></h3>
<p>Check that your search heads are searching the indexers that contain the data you're looking for. Read about distributed search in the Distributed Search Manual.
</p>
<h3> <a name="cantfinddata_are_you_still_logged_in_and_under_your_license_usage.3f"><span class="mw-headline" id="Are_you_still_logged_in_and_under_your_license_usage.3F"> Are you still logged in and under your license usage? </span></a></h3>
<p>If you have several (3 for Splunk Free or 5 for Enterprise) license violations within a rolling 30 day window, Splunk will prevent you from searching your data. 
</p><p>Note, however, that Splunk will continue to index your data, and no data will be lost. You will also still be able to search the _internal index to troubleshoot your problem. Read about license violations in the Admin Manual.
</p>
<h3> <a name="cantfinddata_are_you_using_a_scheduled_search.3f"><span class="mw-headline" id="Are_you_using_a_scheduled_search.3F"> Are you using a scheduled search? </span></a></h3>
<p>Are you SURE your time range is correct? (You wouldn't be the first!) Search over all time to double check.
</p><p>Are you sure the incoming data is indexed when you expect and not lagging? 
To determine if there is a lag between the event's timestamp and indexed time is to manually run the scheduled search with the added syntax of:   
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">
<p>| eval time=_time | eval itime=_indextime | eval lag=(itime - time)/60 | stats avg(lag), min(lag), max(lag) by index host sourcetype
</p>
</font></code><br></div>
<p>For example there is an indexing lag of up to 90 minutes, if you run a scheduled search every 20 minutes, you might not see the most recent data yet (but if you run the same search 70 minutes later, the data will be there).   
</p><p>It could also be a scheduler problem. The Knowledge Manager Manual has a topic on configuring priority of scheduled searches.
</p><p>Other common problems with scheduled searches are searches getting rewritten, saved, run incorrectly, or run not as expected. Investigate scheduled searches in audit.log and the search's dispatch directory: read about these tools in "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk logs about itself</a>" in this manual.
</p>
<h3> <a name="cantfinddata_check_your_search_query"><span class="mw-headline" id="Check_your_search_query"> Check your search query </span></a></h3>
<ul><li> Are you using NOT, AND, or OR? Check your logic.
</li><li> How about double quotes? Read more about Search language syntax in the Search Reference Manual.
</li><li> Are you using views and drilldowns? Splunk Web might be rewriting the search incorrectly via the <b>intentions</b> functionality.
</li><li> Double check that you're using the correct index, source, sourcetype, and host.
</li><li> Are you correctly using escape characters when needed?
</li><li> Are your subsearches ordered correctly?
</li><li> Are your subsearches being passed the correct fields?
</li></ul><h3> <a name="cantfinddata_are_you_extracting_fields.3f"><span class="mw-headline" id="Are_you_extracting_fields.3F"> Are you extracting fields? </span></a></h3>
<ul><li> Check your regex. One way to test regexes interactively is in Splunk using the rex command.
</li><li> Do you have privileges for extracting and sharing fields? Read about sharing fields in the Knowledge Manager Manual.
</li><li> Are your extractions applied for the correct source, sourcetype, and host?
</li></ul><h3> <a name="cantfinddata_additional_resources"><span class="mw-headline" id="Additional_resources"> Additional resources </span></a></h3>
<p>Watch a video on troubleshooting missing forwarder data by a Splunk Support engineer.
</p><p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has.
</p><p>If you get stuck at any point, contact Splunk Support. Don't forget to send a diag! Read about making a diag in this manual.
</p>
<a name="sooomanydispatchdirectories"></a><h2> <a name="sooomanydispatchdirectories_too_many_search_jobs"><span class="mw-headline" id="Too_many_search_jobs"> Too many search jobs </span></a></h2>
<p>A real-time (all-time) scheduled search will spawn many search jobs, populating the search dispatch directory, when there is no suppression (aka alert throttling). This can negatively effect search performance.
</p>
<h3> <a name="sooomanydispatchdirectories_symptom"><span class="mw-headline" id="Symptom"> Symptom </span></a></h3>
<p>Splunk Web displays a yellow banner warning of too many search jobs in the dispatch directory.
</p>
<h3> <a name="sooomanydispatchdirectories_remedies"><span class="mw-headline" id="Remedies"> Remedies </span></a></h3>
<p>First, check that for any real-time all-time scheduled searches, you've configured alert throttling. Configure throttling in <b>Settings &gt; Searches and Reports</b>. Read more about throttling in "Define per-result alerts" in the <i>Alerting Manual</i>.
</p><p>Already throttled alerts and still getting the warning? A second step you can take is to make alert expiration shorter than the default of 24 hours. If you can, change "alert expiration time" from 24 hours to 1 hour (or less, if you need your alert triggered very frequently).
</p><p>The Splunk on Splunk App, version 3.0+, has a helpful view, Dispatch Directory Inspector. The view provides details on search artifacts, including breaking down the disk usage footprint. SoS 3.0.1 and onwards ships with an "as-is" troubleshooting script, dispatch_inspector.py, in <code><font size="2">etc/apps/sos/bin</font></code>.
</p>
<a name="pdfts"></a><h2> <a name="pdfts_i.27m_having_problems_with_the_splunk_pdf_server_app"><span class="mw-headline" id="I.27m_having_problems_with_the_Splunk_PDF_Server_app"> I'm having problems with the Splunk PDF Server app </span></a></h2>
<p>This topic is about troubleshooting the PDF Server for Linux app. This app enabled a Linux-based Splunk instance to generate emailed reports in PDF format. The PDF Report Server App was deprecated in Splunk Enterprise version 6.0. This feature was removed from Splunk Enterprise in version 6.2. 
</p><p>In version 6.2 you cannot generate PDFs from dashboards or forms that were built using advanced XML.
</p><p>Splunk Enterprise 6.2 continues to support integrated PDF generation, as described in Generate PDFs of your reports and dashboards in the Reporting Manual.
</p>
<a name="appviews"></a><h2> <a name="appviews_view_x_in_app_y_is_not_showing_the_expected_results"><span class="mw-headline" id="View_X_in_App_Y_is_not_showing_the_expected_results"> View X in App Y is not showing the expected results </span></a></h2>
<p>You're using an app, and one of its views is not showing you the results you expect. Where to begin troubleshooting? Well, here.
</p>
<h3> <a name="appviews_determine_the_search_string_that_powers_the_view_panel_that_is_not_showing_the_expected_results"><span class="mw-headline" id="Determine_the_search_string_that_powers_the_view_panel_that_is_not_showing_the_expected_results"> Determine the search string that powers the view panel that is not showing the expected results </span></a></h3>
<p>There are many methods to achieve this.
</p>
<ul><li> You can look at the view source by appending "?showsource=1" ("&amp;showsource=1" if other parameters have already been appended) to the view URL in the browser address bar. In our case: https://splunk-diag.splunk.com:8000/en-US/app/search/search_user_activity?showsource=1
</li></ul><h3> <a name="appviews_expand_macros_and_event_types"><span class="mw-headline" id="Expand_macros_and_event_types"> Expand macros and event types </span></a></h3>
<p>Macros and event-types are very handy knowledge objects, but unless you know exactly what they do they tend to obscure the way a given search works. For that reason, I find it easier to expand them manually so that you know *exactly* what your search is doing.
</p><p>In our case, we have to expand <code><font size="2">audit_searchlocal</font></code>  and  <code><font size="2">audit_rexsearch</font></code>. We can do this with btool (<code><font size="2">splunk btool macros list audit_</font></code>) or from the UI.
</p>
<h3> <a name="appviews_run_the_search_manually_from_the_flashtimeline.2c_in_the_relevant_app_context"><span class="mw-headline" id="Run_the_search_manually_from_the_flashtimeline.2C_in_the_relevant_app_context"> Run the search manually from the flashtimeline, in the relevant app context </span></a></h3>
<p>The question we are now going to try to answer is: Can we reproduce this manually, outside of the view it was reported in?
</p><p>On splunk-diag.splunk.com, the answer seems to be "yes":
</p>
<code><font size="2"><br>user&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Median search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;95th Percentile search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Search count<br>splunk-system-user&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.84&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19.00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3491.06&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;484<br></font></code>
<p>This is incorrect. We know that plenty of different users have been running searches on this server over the past 24 hours.
</p>
<h3> <a name="appviews_compare_results_against_source_events"><span class="mw-headline" id="Compare_results_against_source_events"> Compare results against source events </span></a></h3>
<p>The next step is simple: Let's compare the results generated by the search and its multiple evals against the source events. The first thing we notice is that looking at the last command of the search ("chart ... by user") and at the values of the "user" field from the field picker, we expect 11 different rows (as many as there are distinct values for the "user" field).
</p><p>We should see where the "user" field is referenced in the search and possibly modified. This really only happens twice:
</p>
<code><font size="2">- eval user = if(user="n/a", null(), user)<br></font></code>
<code><font size="2">- stats min(_time) as _time first(user) as user max(total_run_time) as total_run_time first(search) as search by search_id<br></font></code>
<p>The first command is not a good suspect as it couldn't possibly result in the squashing of the user field down to "splunk-system-user". The second command is quite interesting though: With "first(user) AS user ... by search_id", we essentially squash the value of "user" for each search (uniquely referenced by the search_id field) to the *most recent* value of the field (that is what first() does: it looks for the first value of the field encountered while searching =&gt; the most recent).
</p>
<h3> <a name="appviews_dig_deeper"><span class="mw-headline" id="Dig_deeper"> Dig deeper </span></a></h3>
<p>In order to drill down to the source of the problem, let's pick *one* example. A good one if possible: A search that we know was run by an actual user. I'm going to go with SID=1338858385.644, which was run by Ed at 6:06pm today.
</p><p>All I need to do is to add the SID as a search term.
</p><p><br>
This search returns one result, with an inadequate value for user as we expect:
</p>
<code><font size="2"><br>user&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Median search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;95th Percentile search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total search time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Search count<br>splunk-system-user 0.83&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.83&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.83&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br></font></code>
<p>And here are the two source events:
</p><p>Event #1:
</p><p><code><font size="2">Audit:[timestamp=06-04-2012 18:06:25.693, user=esastri, action=search, info=granted , search_id='1338858385.644', search='search index="case_85720" host="agent007.corp.faceblock.com_0" source="*transforms.txt" *', autojoin='1', buckets=1, ttl=600, max_count=10000, maxtime=8640000, enable_lookups='1', extra_fields='app,bucketSizeMB,frozenTimePeriodInSecs,host,"host source","host stanza source",index,index_name,is_configured,lastExceedDate,license_size,maxColdDBSizeGB,maxDataSize,maxHotBuckets,maxTotalDBSizeGB,maxTotalDataSizeMB,maxWarmDBCount,maxWarmDBSizeGB,quotaExceededCount,retention_period,section,source,state,version', apiStartTime='ZERO_TIME', apiEndTime='ZERO_TIME', savedsearch_name=""][n/a]</font></code>
</p><p>Event #2:
<code><font size="2">Audit:[timestamp=06-04-2012 18:06:38.636, user=splunk-system-user, action=search, info=completed, search_id='1338858385.644', total_run_time=0.83, event_count=88, result_count=88, available_count=88, scan_count=88, drop_count=0, exec_time=1338858386, api_et=N/A, api_lt=N/A, search_et=N/A, search_lt=N/A, is_realtime=0, savedsearch_name=""][n/a]</font></code>
</p><p><br>
Event #1 is the oldest event and is logged at the time that the search is launched. Note that the user field is correct =&gt; esastri
</p><p>Event #2 is the newest event and is logged at the time that the search completes, hence reporting things such as "total_run_time" or "event_count". Note that the user field is *incorrect* =&gt; splunk-system-user
</p><p>As we discussed earlier, stats first(user) by search_id will pick up the *most recent* value of the user field for a given search id. This is why "splunk-system-user" is picked up.
</p>
<h3> <a name="appviews_conclusion"><span class="mw-headline" id="Conclusion"> Conclusion </span></a></h3>
<p>The bug is: Audit events reporting that a search has finished are *all* logged with "user=splunk-system-user". This seems incorrect and is a deviation from previous behavior.
</p><p>The workaround is: replace "stats first(user) AS user ... by search_id" with "stats last(user) AS user ... by search_id".
</p>
<h1>Common back end scenarios</h1><a name="buckets"></a><h2> <a name="buckets_what_do_i_do_with_buckets.3f"><span class="mw-headline" id="What_do_I_do_with_buckets.3F"> What do I do with buckets? </span></a></h2>
<p>Buckets are portions of Splunk indexes. This article points you to a few resources for troubleshooting problems with buckets.
</p>
<h3> <a name="buckets_might_i_be_having_issues_with_bucket_rotation.3f"><span class="mw-headline" id="Might_I_be_having_issues_with_bucket_rotation.3F"> Might I be having issues with bucket rotation? </span></a></h3>
<p>An unsuitable bucket rotation and retention policy can lead to:
</p>
<ul><li> old events being deleted before they reach frozen buckets,
</li><li> hot and warm buckets filling up, stopping Splunk,
</li><li> old events not being archived correctly and thus still searchable when they shouldn't be, and
</li><li> poor searching or indexing performance.
</li></ul><p>Here's a Community Wiki article about bucket rotation and retention with specific recommendations and examples.
</p>
<h3> <a name="buckets_recover_metadata_for_a_corrupt_splunk_index_directory"><span class="mw-headline" id="Recover_metadata_for_a_corrupt_Splunk_index_directory"> Recover metadata for a corrupt Splunk index directory </span></a></h3>
<p>Contact Splunk Support for direction before using this command. 
</p><p>The <code><font size="2">recover-metadata</font></code> command recovers missing or corrupt metadata associated with any Splunk index directory, sometimes also referred to as a bucket.
</p><p>If your Splunk instance will not start, a possible cause is that one or more of your index buckets is corrupt in some way. Contact Support; they will help you determine if this is indeed the case and if so, which bucket(s) are affected. Then, run this command:
</p><p><code><font size="2">$SPLUNK_HOME/bin/splunk cmd recover-metadata &lt;full path to the exact index directory/bucket&gt;</font></code>
</p><p>Splunk returns a success or failure message. 
</p>
<h3> <a name="buckets_recovering_and_rebuilding_buckets"><span class="mw-headline" id="Recovering_and_rebuilding_buckets"> Recovering and rebuilding buckets </span></a></h3>
<p>The Managing Indexers and Clusters Manual has a thorough explanation of buckets. This section of "How Splunk stores indexes" tells you how to troubleshoot bucket problems, like recovering after a crash and rebuilding buckets. You'll probably want to read from the start of that page, though, to get some background first.
</p>
<a name="ulimiterrors"></a><h2> <a name="ulimiterrors_i_get_errors_about_ulimit_in_splunkd.log"><span class="mw-headline" id="I_get_errors_about_ulimit_in_splunkd.log"> I get errors about ulimit in splunkd.log </span></a></h2>
<p>Are you seeing messages like these in splunkd.log while running Splunk on Linux, possibly accompanied by a Splunk crash?
</p>
<code><font size="2"><br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: virtual address space size: unlimited<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: data segment size: 1879048192 bytes [hard maximum: unlimited]<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: resident memory size: 2147482624 bytes [hard maximum: unlimited]<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: stack size: 33554432 bytes [hard maximum: 2147483646 bytes]<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: core file size: 1073741312 bytes [hard maximum: unlimited]<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: data file size: 2147483646 bytes<br>03-03-2011 21:50:09.027 ERROR ulimit - Splunk may not work due to low file size limit<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: open files: 1024<br>03-03-2011 21:50:09.027 INFO &nbsp;ulimit - Limit: cpu time: unlimited<br>03-03-2011 21:50:09.029 INFO &nbsp;loader - Splunkd starting (build 95063).<br></font></code>
<p>If so, you might need to adjust your server ulimit. Ulimit controls the resources available to a Linux shell and processors the Linux shell has started. A dedicated Splunk server needs higher limits than are provided by default.
</p><p>To check your limits, type:
</p><p><code><font size="2">ulimit -a</font></code>
</p><p>Or restart Splunk and look in splunkd.log for events mentioning ulimit:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*splunkd.log ulimit</font></code><br></div>
<p>You probably want your new values to stay set even after you reboot. To persistently modify the values, edit settings in <code><font size="2">/etc/security/limits.conf</font></code>
</p><p>The critical values are:
</p>
<ul><li> <b>The file size</b> (<code><font size="2">ulimit -f</font></code>). The size of an uncompressed bucket file can be very high.
</li><li> <b>The data segment size</b> (<code><font size="2">ulimit -d</font></code>). With Splunk 4.2+, increase the value to at least 1 GB = 1073741824 bytes.
</li><li> <b>The number of open files</b> (<code><font size="2">ulimit -n</font></code>), sometimes called the number of <b>file descriptors</b>. Increase the value to at least 8192 (depending on your server capacity).
</li><li> <b>The max user processes</b> (<code><font size="2">ulimit -u</font></code>). Increase to match the file descriptors. (it limits the number of http threads)
</li></ul><p>Another value that you might need to modify on an older system (but not on most modern 
systems) is the system-wide file size, <code><font size="2">fs.file-max</font></code>, in <code><font size="2">/etc/sysctl.conf</font></code>.
</p><p>Why must you increase ulimit to run Splunk? Well, you might concurrently need file descriptors for every forwarder socket and every deployment client socket. Each bucket can use 10 to 100 files, every search consumes up to 3, and then consider every file to be indexed and every user connected.
</p>
<a name="authtimeout"></a><h2> <a name="authtimeout_intermittent_authentication_timeouts_on_search_peers"><span class="mw-headline" id="Intermittent_authentication_timeouts_on_search_peers"> Intermittent authentication timeouts on search peers </span></a></h2>
<p>Splunk Web users can experience intermittent timeouts from search peers when there are more concurrent searches attempting to run than the search peers can respond to.
</p><p>A group of search heads can schedule more concurrent searches than some peers are capable of handling with their CPU core count.
</p>
<h3> <a name="authtimeout_symptoms"><span class="mw-headline" id="Symptoms"> Symptoms </span></a></h3>
<p>On the search head, you might see yellow banners in quick succession warning that a peer or peers are 'Down' due to Authentication Failed and/or Replication Status Failed. Typically this can happen a few times a day, but the banners appear and disappear seemingly randomly.
</p><p>On the search head, splunkd.log will have messages like:
</p><p><code><font size="2">WARN DistributedPeerManager - Unable to distribute to peer named xxxx at uri htp://xxxx:8089 because peer has status = "Authentication Failed". </font></code>
</p><p><code><font size="2">WARN DistributedPeerManager - Unable to distribute to peer named xxxx:8089 at uri htp://xxxx:8089 because peer has status = "Down". </font></code>
</p><p><code><font size="2">WARN DistributedPeerManager - Unable to distribute to peer named xxxx at uri htp://xxxx:8089 because replication was unsuccessful. replicationStatus Failed </font></code>
</p><p>The symptoms can appear with or without other Splunk features such as search head pooling and index replication being enabled. The symptoms are more common in environments with two or more search heads.
</p>
<h3> <a name="authtimeout_diagnosis"><span class="mw-headline" id="Diagnosis"> Diagnosis </span></a></h3>
<p>To properly diagnose this issue and proceed with its resolution, you must deploy and run the SoS technology add-on (TA) on all indexers/search peers. In addition, install the SoS app itself on a search head. Once the TA has been enabled and has begun collecting data, the next time the issue occurs, you will have performance data to validate the diagnosis.
</p><p><b>1. Find an auth-token timeout to scope the <i>time</i> the issue occurred</b>.
</p><p>The authentication timeout is 10 seconds, so when the auth-tokens endpoint on the peer takes more than 10 seconds to respond, you'll see an auth or peer status banner on the search head.
</p><p>To find an auth timeout on the peer named in the search head banner:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*splunkd_access* splunk_server="search_peer_name" auth | timechart max(spent) </font></code><br></div>
<p>Or to find an auth timeout on any peer:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*splunkd_access* auth spent &gt; 10000 NOT streams | table splunk_server spent _time </font></code><br></div>
<p><b>2. Examine the load average just before the auth timeout and check for a dramatic increase</b>.
</p><p>Now that you've established the time frame in step 1, examine metrics.log's load average over the time frame to determine whether the load increased significantly just before the timeouts were triggered. Typically the total time frame is about 2 minutes.
</p><p>To find the load average:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*metrics.log* host="search_peer_name" group=thruput | timechart bins=200 max(load_average)</font></code><br></div>
<p><b>3. Examine the CPU and memory usage on the search peers</b>.
</p><p>Use the SoS view for CPU/Memory Usage (<b>SOS &gt; Resource Usage &gt; Splunk CPU/Memory Usage</b>) to review the peak resource usage on the search peer during the time scoped above. Look at the <b>Average CPU Usage</b> panel. If you have too many concurrent searches, you will see that the peer uses more than the available percentage of CPU per core. For example: A healthy 8 core box will show no more than 100% x 8 cores = 800% average CPU usage. In contrast, a box overtaxed with searches typically shows 1000% or more average CPU usage during the time frame where the timeouts appear.
</p><p>For more information about your CPU and memory usage, you can run the <a href="#authtimeout_cpu_and_memory_usage" class="external text">useful search</a> described below.
</p>
<h3> <a name="authtimeout_remedies"><span class="mw-headline" id="Remedies"> Remedies </span></a></h3>
<p>Examine the concurrent search load. There are typically searches that had dubious scheduling choices made and/or are scoped in inefficient ways.
</p><p>Use the SoS Dispatch Inspector view to learn about the dispatched search objects, the app they were triggered from, and their default schedule. Or you can find this information using the <a href="#authtimeout_search_concurrency" class="external text">useful search</a> provided below.
</p><p>Once you've identified your pileup of concurrent searches, get started on this list of things you should do. All of them are good practices.
</p>
<ul><li> Most importantly, spread the searches out to reduce concurrency. For example: Use the advanced scheduling cron to balance the searches so that they don't all run on the same minute every hour. Read about scheduling reports and configuring scheduled report priority in the <i>Reporting Manual</i>.
</li><li> Limit real-time searches. For example, a scheduled real-time alert that looks back over the last 2 minutes that only triggers an email. Unless it triggers a script, this task should be configured as a scheduled search set to run 10 minutes in the past (to address potential source latency) over a 5 minutes window, combined with a cron offset. This offers the same effect without tying down a CPU core across all peers, all the time. Read more about expected performance and known limitations of real-time searches and reports in the <i>Search Manual</i>.
</li><li> Re-scope the search time for actual information needs. For example: Scheduled searches that run every 15 minutes over a 4 hour time frame are a waste of limited resources. Unless you have a very good reason why a search should look back an additional 3 hours and 45 minutes on every search (such as extreme forwarder latency), it's a waste of shared resources. Read more about alerts in the <i>Alerting Manual</i>.
</li><li> Additionally, there's the option to use limits.conf to lower the search concurrency of all the search heads. Note that if you do <i>only</i> this step, you will get a different set of banners (about reaching the max number of concurrent searches) and <b>you will still not be able to run concurrent searches</b>. But if you do some of the other steps, too, you might want to configure the search concurrency like this:
</li></ul><code><font size="2"><br>[search]<br>base_max_searches = 2<br># Defaults to 6<br>max_searches_per_cpu = 1<br># Defaults to 1<br>max_rt_search_multiplier = 1<br># Defaults to 1 in 6.0, in 5.x defaults to 3<br><br>[scheduler]<br>max_searches_perc = 20<br># Defaults to 50<br>auto_summary_perc &nbsp;= 10<br># Defaults to 50<br></font></code>
<ul><li> Add more search peers.
</li><li> Add more cores to the search peers.
</li><li> As a last resort, there's also the option of increasing the distsearch.conf timeouts. This is a workaround, and will slow down search results during peak load times. Increase the timeouts in distsearch.conf on the search head:
</li></ul><code><font size="2"><br>[distributedSearch]<br>statusTimeout = 30<br># Defaults to 10<br>authTokenConnectionTimeout = 30<br># Default is 5<br>authTokenSendTimeout = 60<br># Default is 10<br>authTokenReceiveTimeout = 60<br># Default is 10<br></font></code>
<h3> <a name="authtimeout_useful_searches"><span class="mw-headline" id="Useful_searches"> Useful searches </span></a></h3>
<h4><font size="3"><b><i> <a name="authtimeout_search_concurrency"><span class="mw-headline" id="Search_concurrency"> Search concurrency </span></a></i></b></font></h4>
<p>If you have SoS installed on your search head, you can use this search to examine search concurrency.
</p>
<code><font size="2"><br>`set_sos_index` sourcetype=ps host="search_peer_name"<br>| multikv <br>| `get_splunk_process_type` <br>| search type="searches" <br>| rex field=ARGS "_--user=(?&lt;search_user&gt;.*?)_--" <br>| rex field=ARGS "--id=(?&lt;sid&gt;.*?)_--" <br>| rex field=sid "remote_(?&lt;search_head&gt;[^_]*?)_" <br>| eval is_remote=if(like(sid,"%remote%"),"remote","local") <br>| eval is_scheduled=if(like(sid,"%scheduler_%"),"scheduled","ad-hoc") <br>| eval is_realtime=if(like(sid,"%rt_%"),"real-time","historical") <br>| eval is_subsearch=if(like(sid,"%subsearch_%"),"subsearch","generic") <br>| eval search_type=is_remote.", ".is_scheduled.", ".is_realtime <br>| timechart bins=200 dc(sid) AS "Search count" by is_scheduled <br></font></code>
<h4><font size="3"><b><i> <a name="authtimeout_cpu_and_memory_usage"><span class="mw-headline" id="CPU_and_memory_usage"> CPU and memory usage </span></a></i></b></font></h4>
<p>If you have the SoS App installed on the search head, you can find CPU and memory usage for all search processes at <b>one</b> point based on the intersection of the "ps" run interval and maximum load:
</p>
<code><font size="2"><br>index=sos sourcetype="ps" host="search_peer_name" &nbsp;| head 1<br>| multikv <br>| `get_splunk_process_type` <br>| search type=searches <br>| &nbsp;rex field=ARGS "--id=(?&lt;sid&gt;.*?)_--" <br>| stats dc(sid) AS "Search count" sum(pctCPU) <br>AS "Total&nbsp;%CPU" sum(eval(round(RSZ_KB/1024,2))) AS "Total physical memory used (MB)"<br></font></code>

<a name="troubleshootingeventsindexingdelay"></a><h2> <a name="troubleshootingeventsindexingdelay_troubleshooting_event_indexing_delay"><span class="mw-headline" id="Troubleshooting_event_indexing_delay"> Troubleshooting event indexing delay</span></a></h2>
<h3> <a name="troubleshootingeventsindexingdelay_symptoms"><span class="mw-headline" id="Symptoms"> Symptoms </span></a></h3>
<p>Events collected from a forwarder or from a log file are not yet searchable on Splunk.
Even though the time stamps of the events are within the search time range, a search does not return the events. Later, a search over the same time range returns the events.
</p>
<h3> <a name="troubleshootingeventsindexingdelay_diagnosis"><span class="mw-headline" id="Diagnosis"> Diagnosis </span></a></h3>
<p>Quantify the problem by measuring how long your Splunk deployment is taking to make your data searchable.
</p><p>To measure the delay between the time stamp of the events and the indexing time (the time that the indexer receives and processes the events), use the following method:
</p><p><b>1.</b> Look at the delay in seconds per host. 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=mysource  | eval delay_sec=_indextime-_time | timechart min(delay_sec) avg(delay_sec) max(delay_sec) by host </font></code><br></div>
<p><b>2.</b> Look at the delay in seconds per source for a particular host.
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=mysource host=myhost | eval delay_sec=_indextime-_time | timechart min(delay_sec) avg(delay_sec) max(delay_sec) by source </font></code><br></div>
<p><b>3.</b> Look at the delay per host for the Splunk internal logs. 
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*splunkd.log* | eval delay_sec=_indextime-_time | timechart min(delay_sec) avg(delay_sec) max(delay_sec) by host </font></code><br></div>
<p>Run these searches on <b>realtime - all time</b> mode for a little while to see the events that are being received right now. In addition to real-time, you can run <b>historical</b> searches to compare a day this week to a day from a previous week.
</p><p>Compare the delay between hosts and forwarders.
</p>
<ul><li> Determine the common denominator between them. For example, all of the delayed events might be from the same log file or the same host or source type.
</li></ul><p>Compare the delay from your events with the delay from the internal Splunk logs.
</p>
<ul><li> If all the logs are delayed, <b>including the internal logs</b>, then the delay is a forwarding issue.
</li><li> If some sources are delayed but not others, this indicates a problem with the input.
</li></ul><p>As you implement each fix below, you can measure how well it's working by running these searches again.
</p>
<h3> <a name="troubleshootingeventsindexingdelay_root_causes"><span class="mw-headline" id="Root_causes"> Root causes </span></a></h3>
<p>There are several possible root causes. Some might not be applicable to your situation.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingeventsindexingdelay_possible_thruput_limits"><span class="mw-headline" id="Possible_thruput_limits"> Possible thruput limits </span></a></i></b></font></h4>
<p>Universal and lightweight forwarders have a <b>default thruput limit of 256Kbps</b>. This default can be configured in limits.conf.
The default value is correct for a forwarder with a low profile, indexing up to ~920 Mb/hour.
But in the case of higher indexing volumes, or when the forwarder has to collect the historical logs after the first start, the default might be too low. This could delay the recent events.
</p><p>To check the forwarder default thruput limit, on the command line in the splunk folder type:
</p><p><code><font size="2">cd $SPLUNK_HOME/bin <br>
./splunk cmd btool limits list thruput --debug <br></font></code>
</p><p>Example of result on a universal forwarder. The default limit is 256KBps, and is set up in the app:
</p><p><code><font size="2">
/opt/splunkforwarder/etc/apps/SplunkUniversalForwarder/default/limits.conf [thruput]<br>
/opt/splunkforwarder/etc/apps/SplunkUniversalForwarder/default/limits.conf maxKBps = 256<br></font></code>
</p><p>Example of result on an indexer or heavy forwarder. The default is unlimited:
</p><p><code><font size="2">
/opt/splunk/etc/system/default/limits.conf [thruput]<br>
/opt/splunk/etc/system/default/limits.conf maxKBps = 0<br></font></code>
</p><p>To verify in the forwarder:
When the thruput limit is reached, monitoring pauses and the following events are recorded in splunkd.log:
</p><p><code><font size="2">INFO  TailingProcessor - Could not send data to output queue (parsingQueue), retrying...</font></code>
</p><p>To verify how often the forwarder is hitting this limit, check the forwarder's metrics.log. (Look for this on the forwarder because metrics.log is not forwarded by default on universal and light forwarders.)
</p><p><code><font size="2"> 
cd $SPLUNK_HOME/var/log/splunk/metrics.log<br>
grep "name=thruput" metrics.log<br></font></code>
</p><p>Example: The instantaneous_kbps and average_kbps are always under 256KBps.
</p><p><code><font size="2">
11-19-2013 07:36:01.398 -0600 INFO  Metrics - group=thruput, name=thruput, instantaneous_kbps=251.790673, instantaneous_eps=3.934229, average_kbps=110.691774, total_k_processed=101429722, kb=7808.000000, ev=122
</font></code>
</p>
<h5> <a name="troubleshootingeventsindexingdelay_remedy"><span class="mw-headline" id="Remedy"> Remedy </span></a></h5>
<p>Create a custom limits.conf with a higher limit or no limit.
The configuration can be in system/local, or in an app that will have precedence on the existing limit.
</p><p>Example: Configure in a dedicated app, in /opt/splunkforwarder/etc/apps/Gofaster/local/limits.conf 
</p><p>Double the thruput limit, from 256 to 512 KBps:
</p>
<code><font size="2"><br>[thruput]<br>maxKBps = 512<br></font></code>
<p>Or for unlimited thruput:
<code><font size="2">
<br>[thruput]<br>
maxKBps = 0
</font></code>
</p><p>Notes:
</p>
<ul><li> Unlimited speed can cause higher resource usage on the forwarder. Keep a limit if you need to control the monitoring and network usage.
</li><li> Restart to apply.
</li><li> Verify the result of the configuration with btool.
</li><li> Later, verify in metrics.log that the forwarder is not reaching the new limit constantly.
</li></ul><h4><font size="3"><b><i> <a name="troubleshootingeventsindexingdelay_possible_network_limits"><span class="mw-headline" id="Possible_network_limits"> Possible network limits </span></a></i></b></font></h4>
<p>Once the thruput limit is removed, if the events are still slow, use the metrics method to check if the forwarders are hitting a network limit.
Compare with other forwarders on different networks or different VPN tunnels.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingeventsindexingdelay_monitoring_archived_logs_file"><span class="mw-headline" id="Monitoring_archived_logs_file"> Monitoring archived logs file </span></a></i></b></font></h4>
<p>Compressed files (like .gz and .zip) are handled by the Archive processor, and are serialized. Therefore if you index a large set of compressed files, they will come through the indexer one after the other. The second file will only come through after the first one has been indexed. 
</p><p>Example: splunk ArchiveProcessor starting to read file in splunkd.log
</p><p><code><font size="2">
12-12-2013 00:18:07.388 +0000 INFO  ArchiveProcessor - handling file=/var/log/application/rsyslog.log.1.gz
</font></code>
</p>
<h5> <a name="troubleshootingeventsindexingdelay_remedy_2"><span class="mw-headline" id="Remedy_2"> Remedy </span></a></h5>
<p>An available workaround is to have Splunk to monitor the uncompressed files.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingeventsindexingdelay_possible_timestamp.2ftimezone_issue"><span class="mw-headline" id="Possible_timestamp.2Ftimezone_issue"> Possible timestamp/timezone issue </span></a></i></b></font></h4>
<p>Using a <b>real time-alltime</b> search, if an event is visible immediately in real time, but not otherwise (that is, with an historical search), it might be that the time stamp of the event is in the future. Historical searches (even all-time) show events with timestamps only within the window of the search.
</p><p>Use this search to verify the source type, the time stamp detected (_time), the time of the user on the search head (now), and the time zone applied (date_zone).
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">source=mysource host=myhost | eval delay_sec=_indextime-_time | convert ctime(_indextime) AS indextime | eval now=now() | table _time indextime now date_zone source sourcetype host </font></code><br></div>
<p>Notes:
</p>
<ul><li> The _time is converted to the user profile time zone configured on the search head at search time. 
</li><li> The date_zone is applied at index time on the indexer.
</li></ul><h5> <a name="troubleshootingeventsindexingdelay_remedy_3"><span class="mw-headline" id="Remedy_3"> Remedy </span></a></h5>
<p>Fix the time zone and time stamp extraction. Take a sample of the data and test it with data preview.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingeventsindexingdelay_windows_event_logs_delay"><span class="mw-headline" id="Windows_Event_logs_delay"> Windows Event logs delay </span></a></i></b></font></h4>
<p>If the only events delayed are WinEventLogs, and the forwarder is on a busy domain controller, with a high number of events per second, you might be encountering the Windows collection log performance limit on Splunk 4.* and 5.*.
</p><p>Or if the forwarder was recently started, it might be still collecting the older events first.
</p>
<h5> <a name="troubleshootingeventsindexingdelay_remedy_4"><span class="mw-headline" id="Remedy_4"> Remedy </span></a></h5>
<ul><li> Use the Splunk 6.0 windows forwarder, with WinEventLog collection switched to improved Modular inputs. See "Welcome to Splunk Enterprise 6.0" in the release notes.
</li></ul><ul><li> If possible, collect only recent logs (<code><font size="2">current_only=1</font></code>), or monitor the recent events first (<code><font size="2">start_from=newest</font></code>). See "Monitor Windows event log data" in the <i>Getting Data In Manual</i>.
</li></ul><ul><li> Reduce the volume of events to collect. On splunk 6.0, use the whitelist/blacklist filters in the WinEventLog stanza to exclude particular EventCodes. See inputs.conf.spec.
</li></ul><a name="troubleshootingwmi"></a><h2> <a name="troubleshootingwmi_common_issues_with_splunk_and_wmi"><span class="mw-headline" id="Common_issues_with_Splunk_and_WMI"> Common issues with Splunk and WMI </span></a></h2>
<p>This topic discusses common issues encountered when getting WMI-based data into Splunk. It offers solutions for problems such as the following:
</p>
<ul><li> Splunk can't get data from remote machines.
</li><li> Splunk can't get local data through WMI.
</li><li> Splunk sometimes crashes when getting remote data.
</li><li> Splunk connects to WMI differently depending on product version.
</li></ul><h3> <a name="troubleshootingwmi_splunk_can.27t_get_data_from_remote_machines"><span class="mw-headline" id="Splunk_can.27t_get_data_from_remote_machines">Splunk can't get data from remote machines </span></a></h3>
<p>When Splunk can index events on the local machine, but can't get data from remote machines using WMI, authentication or network connectivity is often the reason. Splunk requires a user account with valid credentials for the Active Directory (AD) domain or forest in which it's installed in order to collect data remotely. It also requires a clear network path to the machine from which it gets data, unblocked by firewalls on either the source or target machines.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingwmi_determine_that_splunk_has_been_installed_as_a_domain_user"><span class="mw-headline" id="Determine_that_Splunk_has_been_installed_as_a_domain_user"> Determine that Splunk has been installed as a domain user </span></a></i></b></font></h4>
<p>The first thing to do is to make sure that Splunk is installed as a domain user. If this requirement isn't met, Splunk won't be able to get data remotely even if the network is functioning.
</p><p><b>1.</b> Open a command prompt.
</p><p><b>2.</b> Run the <code><font size="2"> SC</font></code> command to query the Services Command Manager about the <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services.
</p>
<code><font size="2"><br>C:\&gt; sc qc splunkd<br>[SC] QueryServiceConfig SUCCESS<br><br>SERVICE_NAME: splunkd<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TYPE &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 10 &nbsp;WIN32_OWN_PROCESS<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;START_TYPE &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 2 &nbsp;&nbsp;AUTO_START<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ERROR_CONTROL &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 1 &nbsp;&nbsp;NORMAL<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BINARY_PATH_NAME &nbsp;&nbsp;: "C:\Program Files\Splunk\bin\splunkd.exe" service<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LOAD_ORDER_GROUP &nbsp;&nbsp;:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TAG &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DISPLAY_NAME &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: Splunkd<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DEPENDENCIES &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SERVICE_START_NAME&nbsp;: LocalSystem<br></font></code>
<p>The <b><code><font size="2">SERVICE_START_NAME</font></code></b> field tells you the user that Splunk is configured to run as. If this field shows <code><font size="2">LocalSystem</font></code>, then Splunk is not configured to run as a domain user. Uninstall Splunk, then reinstall it and make sure to specify "Other user" during the setup process.
</p><p><b>Note:</b> You can also determine which user Splunk is configured to run as by using the Services control panel.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingwmi_review_the_splunkd.log_file"><span class="mw-headline" id="Review_the_splunkd.log_file"> Review the splunkd.log file </span></a></i></b></font></h4>
<p>If Splunk is correctly configured as a domain user, the next step is to investigate why Splunk is having problems connecting to WMI providers.
</p><p>Open the <code><font size="2">%SPLUNK_HOME%\var\log\splunk\splunkd.log</font></code> file and search for <b>wmi</b>.  
</p><p>When Splunk encounters an error attempting to connect to a WMI provider, it logs errors in <code><font size="2">splunkd.log</font></code> as follows:
</p><p><code><font size="2">
03-11-2009 10:08:29.296 ERROR ExecProcessor - error from "python E:\Splunk\bin\scripts\splunk-wmi.py" ERROR WMI - Instantiation of IWbemServices::ExecQueryAsync failed (error code 800706be)</font></code>
</p><p><code><font size="2">03-11-2009 10:08:29.296 ERROR ExecProcessor - error from "python E:\Splunk\bin\scripts\splunk-wmi.py" ERROR WMI - IWbemServices::CancelAsyncCall error (WMI Namespace "\\ADLDBS01\root\cimv2", Param "Application", HRESULT error 80041002)
</font></code>
</p><p>The following table shows the most common errors encountered when connecting to WMI providers:
</p>
<table border="1" cellpadding="5" cellspacing="0"><tr bgcolor="#D9EAED"><th bgcolor="#C0C0C0"> Error code
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr><td valign="center" align="left"> 80070005
</td><td valign="center" align="left"> Access is denied. (due to an incorrect login)
</td></tr><tr><td valign="center" align="left"> 80041064
</td><td valign="center" align="left"> User credentials cannot be used for local connections.
</td></tr><tr><td valign="center" align="left"> 800706BA
</td><td valign="center" align="left"> The RPC server is unavailable.
</td></tr><tr><td valign="center" align="left"> 80041003
</td><td valign="center" align="left"> Access Denied. (due to explicit access restrictions)
</td></tr></table><p>If you see lines within the log file that contain <code><font size="2"><b>HRESULT error</b></font></code> then Splunk is unable to complete the WMI operation due to a network connectivity or authentication problem. You can use the <code><font size="2">WBEMTEST</font></code> utility to corroborate what is shown in Splunk's log file.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingwmi_enable_debug_logging"><span class="mw-headline" id="Enable_debug_logging"> Enable debug logging </span></a></i></b></font></h4>
<p>You can get even more detailed information about what is causing the errors by enabling debug logging in Splunk's logging engine. 
</p><p><b>Note:</b> After you have confirmed the cause of the error, be sure to turn debug logging off.
</p><p>To enable debugging for WMI-based inputs, you must set two parameters:
</p><p><b>1.</b> Edit <code><font size="2">log.cfg</font></code> in <code><font size="2">%SPLUNK_HOME\etc</font></code>.  Add the following parameter:
</p>
<code><font size="2"><br>[splunkd]<br>&nbsp;category.ExecProcessor=DEBUG <br></font></code>
<p><b>2.</b> Edit <code><font size="2">log-cmdline.cfg</font></code>, also in <code><font size="2">%SPLUNK_HOME%\etc</font></code>.  Add the following parameter:
</p>
<code><font size="2"><br>&nbsp;category.WMI=DEBUG <br></font></code>
<p><b>Note:</b> You can place this attribute/value pair anywhere in the file, as long as it is on its own line. <code><font size="2">log-cmdline.cfg</font></code> does not use stanzas.
</p><p><b>3.</b> Restart Splunk:
</p>
<code><font size="2"><br>C:\Program Files\Splunk\bin&gt; splunk restart<br></font></code>
<p><b>4.</b> Once Splunk has restarted, let it run for a few minutes until you see debug log events coming into Splunk.
</p><p><b>Note:</b> You can search Splunk's logfiles within Splunk by supplying <code><font size="2">index="_internal"</font></code> as part of your search string.  Review "<a href="#whatsplunklogsaboutitself" class="external text">What Splunk logs about itself</a>" in the Troubleshooting Manual for additional information.
</p><p><b>5.</b> Once Splunk has collected enough debug log data, send a <a href="#contactsplunksupport" class="external text">diag to Splunk Support</a>:
</p>
<code><font size="2"><br>C:\Program Files\Splunk\bin&gt; splunk diag<br></font></code>
<p><b>Important:</b> Once you finish troubleshooting, revert back to the default settings:
</p><p><b>1.</b> In <code><font size="2">log.cfg</font></code>, change the <code><font size="2">category.ExecProcessor</font></code> attribute to its default setting:
</p>
<code><font size="2"><br>[splunkd]<br>category.ExecProcessor=WARN<br></font></code>
<p><b>Note:</b> You can also remove this entry from the file.
</p><p><b>2.</b> In <code><font size="2">log-cmdline.cfg</font></code>, change the <code><font size="2">category.WMI</font></code> attribute to its default setting:
</p>
<code><font size="2"><br>category.WMI=ERROR<br></font></code>
<p><b>Note:</b> Any changes made to <code><font size="2">log.cfg</font></code> are overwritten when you upgrade Splunk. Create a <code><font size="2">log-local.cfg</font></code> in <code><font size="2">%SPLUNK_HOME%\etc</font></code> to avoid this problem.
</p>
<h4><font size="3"><b><i> <a name="troubleshootingwmi_use_the_wbemtest_utility_to_reproduce_the_error_outside_of_splunk"><span class="mw-headline" id="Use_the_WBEMTEST_utility_to_reproduce_the_error_outside_of_Splunk"> Use the WBEMTEST utility to reproduce the error outside of Splunk </span></a></i></b></font></h4>
<p>If you see <code><font size="2">HRESULT error</font></code> entries in the <code><font size="2">splunkd.log</font></code>, use the <code><font size="2">WBEMTEST</font></code> utility to confirm the error outside of Splunk.
</p><p><b>1.</b> Log into the Splunk server as the Splunk user.
</p><p><b>2.</b> Click <b>Start &gt; Run&acirc;&#128;&brvbar;</b>
</p><p><b>3.</b> In the <b>Run</b> dialog, type in <code><font size="2">wbemtest</font></code> and click <b>OK</b>.
</p><p><b>4.</b> In the <b>Windows Management Instrumentation Tester</b> window, click the <b>Connect&acirc;&#128;&brvbar;</b> button.
</p><p>The <b>Connect</b> window appears.
</p><p><b>5.</b> In the <b>Namespace</b> field of the <b>Connect</b> window, type in the namespace of the server that is experiencing errors.
</p><p><b>Note:</b> You must type in the full path of the namespace. For example, if the server you are attempting to connect to is called ADLDBS01, you must type in <code><font size="2">\\ADLDBS01\root\cimv2</font></code> (including the backslashes).
</p><p><b>6.</b> Click <b>Connect</b>.
</p><p><img alt="Wmifaq1.png" src="images/2/23/Wmifaq1.png" width="435" height="482"></p><p><b>Note:</b> You should be able to connect to the server without needing to supply credentials. If you are prompted for credentials, then the Splunk user is not correctly configured to access WMI.
</p><p><b>7.</b> Once you are connected to the server, set your WMI connection mode by selecting one of the radio buttons in <b>Method Invocation Options</b> the lower right corner of the <code><font size="2">WBEMTEST</font></code> window:
</p>
<ul><li> For Splunk 3.4.9 and earlier, choose <b>Asynchronous</b>.
</li><li> For versions of Splunk after 3.4.9, choose <b>Semisynchronous</b>.
</li></ul><p><b>8.</b> Click &acirc;&#128;&#156;Query&acirc;&#128;&brvbar;&acirc;&#128;&#157;
</p><p>The <b>Query</b> window appears.
</p><p><b>9.</b> In the <b>Query</b> window, type in a valid Windows Query Language (WQL) statement, such as the one supplied below, then click <b>Apply</b>.
</p><p>Following is a WQL statement that you can test WMI connections with:
</p><p><code><font size="2">SELECT Category, CategoryString, ComputerName, EventCode, EventIdentifier, EventType, Logfile, Message, RecordNumber, SourceName, TimeGenerated, TimeWritten, Type, User FROM Win32_NTLogEvent WHERE Logfile = "Application&acirc;&#128;&#157;</font></code>
</p><p>The following graphic shows an example of successful results:
<img alt="Wmifaq5.png" src="images/9/95/Wmifaq5.png" width="436" height="277"></p>
<h4><font size="3"><b><i> <a name="troubleshootingwmi_check_windows_firewall"><span class="mw-headline" id="Check_Windows_Firewall"> Check Windows Firewall </span></a></i></b></font></h4>
<p>If Windows Firewall (or any other firewall software) is running on either the source or target machine, Splunk might be blocked from getting data through WMI providers. Make sure that you explicitly allow WMI through on the firewalls on both machines. You can also disable Windows Firewall, but this is not recommended by Splunk or Microsoft.
</p><p>Additional information about connecting through Windows Firewall can be found at "Connecting Through Windows Firewall", http://msdn.microsoft.com/en-us/library/aa389286(VS.85).aspx on MSDN.  If you are trying to extract events from a Windows Vista or Windows Server 2008 computer, review "Connecting to WMI remotely starting with Windows Vista", http://msdn.microsoft.com/en-us/library/aa822854(VS.85).aspx, also on MSDN.
</p>
<h3> <a name="troubleshootingwmi_splunk_is_unable_to_get_local_data_through_wmi"><span class="mw-headline" id="Splunk_is_unable_to_get_local_data_through_WMI">Splunk is unable to get local data through WMI</span></a></h3>
<p>When Splunk is unable to get data from the local machine through WMI providers, this might be because WMI is experiencing issues under load. When this happens, try restarting the Windows Management Instrumentation (<code><font size="2">wmimgmt</font></code>) service from within the Services control panel, or by using the <code><font size="2">sc</font></code> command-line utility.
</p>
<h3> <a name="troubleshootingwmi_splunk_sometimes_crashes_when_collecting_data_over_wmi"><span class="mw-headline" id="Splunk_sometimes_crashes_when_collecting_data_over_WMI">Splunk sometimes crashes when collecting data over WMI </span></a></h3>
<p>WMI can occasionally cause the <code><font size="2">splunk-wmi.exe</font></code> process to crash. Splunk will spawn a new process when this happens (you can tell by the changed process ID).
</p>
<ul><li> While there is no guaranteed fix for this issue, you can reduce the number of crashes by reducing the number of servers you are monitoring through WMI with any given Splunk instance. Limit the number of WMI-based inputs per instance to 80 or fewer.
</li></ul><ul><li> If you monitor the same subset of WMI providers on large numbers of machines, you can run into WMI memory constraints on the monitoring server. This can also cause crashes. Limit the number of WMI-based data inputs per server monitored through WMI. It's best to reduce the total number of WMI connections per instance to 120 or fewer on 32-bit Windows servers, and 240 or fewer on 64-bit Windows servers. 
</li></ul><ul><li> Consider using universal forwarders to get your data. You can either install universal forwarders on a few machines and get data from other machines through WMI, or you can put universal forwarders on all remote machines.
</li></ul><h3> <a name="troubleshootingwmi_splunk_connects_to_wmi_differently_based_on_product_version"><span class="mw-headline" id="Splunk_connects_to_WMI_differently_based_on_product_version">Splunk connects to WMI differently based on product version</span></a></h3>
<p>When Splunk makes requests to WMI, it does so in one of three ways: Synchronous, asynchronous and semisynchronous.
</p><p>Splunk makes what are known as <i>semisynchronous</i> calls to WMI providers. This means that when Splunk makes a call to WMI, it continues running while WMI deals with the request. 
</p><p>Semisynchronous mode offers the best balance of resource usage and security on the computer making the request. It differs from the faster <i>asynchronous</i> mode, but is more secure due to the way that the system handles retrieval of the WMI objects. Both of these modes are faster than <i>synchronous</i> mode, which forces programs making that kind of WMI request to wait until WMI returns the data.
</p><p>When WMI is dealing with a large number of requests, you might notice a slower response because memory usage on the system increases until the retrieved WMI objects are no longer needed by Splunk (after indexing).
</p><p>More information about how WMI calls are made is available at "Calling a Method", http://msdn.microsoft.com/en-us/library/aa384832(VS.85).aspx on MSDN.
</p><p><b>Note:</b> Versions of Splunk prior to 3.4.10 make asynchronous connections to WMI providers.
</p>
<h3> <a name="troubleshootingwmi_manually_verify_that_wmi_is_working"><span class="mw-headline" id="Manually_verify_that_WMI_is_working"> Manually verify that WMI is working </span></a></h3>
<p>To test WMI, you can run the <code><font size="2">splunk-wmi.exe</font></code> command manually with a desired query and/or namespace to see the output that it produces.
</p><p><b>Caution:</b> When running this command, be sure to temporarily change Splunk's data store directory (the location that <code><font size="2">SPLUNK_DB</font></code> points to), so that you do not miss any WMI events. To change Splunk's database store, refer to "Test access to WMI providers" in the Getting Data In Manual.
</p><p>Here is an example of a valid <code><font size="2">splunk-wmi</font></code> statement:
</p><p><code><font size="2">C:\Program Files\Splunk\bin&gt; splunk cmd splunk-wmi.exe -wql "select * FROM Win32_PerfFormattedData_PerfDisk_PhysicalDisk"</font></code>
</p><p>The following output shows a failure to connect to the desired WMI provider:
</p><p><code><font size="2">
$ ./splunk cmd splunk-wmi.exe -wql "select * FROM Win32_PerfFormattedData_PerfDisk_PhysicalDisk_typo"</font></code>
</p><p><code><font size="2">ERROR WMI - Error occurred while trying to retrieve results from a WMI query (error="Specified class is not valid." HRESULT=80041010) (.: select * FROM Win32_PerfFormattedData_PerfDisk_PhysicalDisk_typo)</font></code>
</p><p><code><font size="2">ERROR WMI - Giving up attempt to connect to WMI provider after maximum number of retries at maximum backoff time (.: select * FROM Win32_PerfFormattedData_PerfDisk_PhysicalDisk_typo)
</font></code></p><p>Clean shutdown completed.

</p><p>The following shows a successful connection to a WMI provider:
</p>
<code><font size="2"><br>jrodman@jrodman-PC /cygdrive/c/Program Files/Splunk/bin<br>$ ./splunk cmd splunk-wmi.exe -wql "select * FROM Win32_PerfFormattedData_PerfDisk_PhysicalDisk"<br>20090904144105.000000<br>AvgDiskBytesPerRead=0<br>AvgDiskBytesPerTransfer=0<br>AvgDiskBytesPerWrite=0<br>AvgDiskQueueLength=0<br>AvgDiskReadQueueLength=0<br>AvgDiskWriteQueueLength=0<br>AvgDisksecPerRead=0<br>AvgDisksecPerTransfer=0<br>AvgDisksecPerWrite=0<br>Caption=NULL<br>CurrentDiskQueueLength=0<br>Description=NULL<br>DiskBytesPersec=0<br>$<br>DiskReadsPersec=0<br>DiskTransfersPersec=0<br>DiskWriteBytesPersec=0<br>DiskWritesPersec=0<br>Frequency_Object=NULL<br>Frequency_PerfTime=NULL<br>Frequency_Sys100NS=NULL<br>Name=0 D: C:<br>PercentDiskReadTime=0<br>PercentDiskTime=0<br>PercentDiskWriteTime=0<br>PercentIdleTime=98<br>SplitIOPerSec=0<br>Timestamp_Object=NULL<br>Timestamp_PerfTime=NULL<br>Timestamp_Sys100NS=NULL<br>wmi_type=unspecified<br><br>---splunk-wmi-end-of-event---<br>20090904144105.000000<br>AvgDiskBytesPerRead=0<br>AvgDiskBytesPerTransfer=0<br>AvgDiskBytesPerWrite=0<br>AvgDiskQueueLength=0<br>AvgDiskReadQueueLength=0<br>AvgDiskWriteQueueLength=0<br>AvgDisksecPerRead=0<br>AvgDisksecPerTransfer=0<br>AvgDisksecPerWrite=0<br>Caption=NULL<br>CurrentDiskQueueLength=0<br>Description=NULL<br>DiskBytesPersec=0<br>DiskReadBytesPersec=0<br>DiskReadsPersec=0<br>DiskTransfersPersec=0<br>DiskWriteBytesPersec=0<br>DiskWritesPersec=0<br>Frequency_Object=NULL<br>Frequency_PerfTime=NULL<br>Frequency_Sys100NS=NULL<br>Name=Total<br>PercentDiskReadTime=0<br>PercentDiskTime=0<br>PercentDiskWriteTime=0<br>PercentIdleTime=98<br>SplitIOPerSec=0<br>Timestamp_Object=NULL<br>Timestamp_PerfTime=NULL<br>Timestamp_Sys100NS=NULL<br>wmi_type=unspecified<br><br>---splunk-wmi-end-of-event---<br><br>Clean shutdown completed.<br></font></code>
<h3> <a name="troubleshootingwmi_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>See the Admin Manual for information on getting started for Windows admins.
</p>
<a name="troubleshootingwindowseventlogs"></a><h2> <a name="troubleshootingwindowseventlogs_troubleshooting_windows_event_log_collection"><span class="mw-headline" id="Troubleshooting_Windows_event_log_collection"> Troubleshooting Windows event log collection </span></a></h2>
<p>This topic discusses solutions to problems encountered when attempting to get Windows event log data into Splunk.
</p><p>Problems with collection and indexing of Windows event logs generally fall into two categories:
</p>
<ul><li> <b>Event logs are not collected from the server.</b> This is usually due to either a local configuration problem or, in the case of remote event log collection, a network, permissions, or authentication issue.
</li><li> <b>Event logs are collected from the server, but information within the event log is either missing or incorrect.</b> This is usually due to problems associated with a particular event log channel, or because of the methods used to collect data from those channels.
</li></ul><h3> <a name="troubleshootingwindowseventlogs_troubleshooting_issues_with_event_logs_collected_locally"><span class="mw-headline" id="Troubleshooting_issues_with_event_logs_collected_locally"> Troubleshooting issues with event logs collected locally </span></a></h3>
<p>When you have problems getting data into your local Splunk instance, try these tips to fix the problem:
</p>
<ul><li> Make sure that the desired event log channels are selected in Splunk Web or properly configured in inputs.conf.
</li><li> Make sure to select fewer than 64 event log channels per event log input.
</li><li> Make sure that you are not attempting to index exported event logs that are incompatible with the indexing system (for example, attempting to index event logs exported from a Windows Server 2008 computer on a Windows XP computer will result in missing log data).
</li><li> Make sure that, if you are monitoring non-standard event log channels, that you have the appropriate dynamic linked libraries (DLLs) that are associated with that event log channel. This is particularly important when indexing exported log files from a different computer.
</li></ul><h3> <a name="troubleshootingwindowseventlogs_troubleshooting_issues_with_event_logs_collected_remotely"><span class="mw-headline" id="Troubleshooting_issues_with_event_logs_collected_remotely"> Troubleshooting issues with event logs collected remotely </span></a></h3>
<p>When you experience issues getting event logs from remote Windows servers, try these solutions to fix the problem:
</p>
<ul><li> Make sure that your Splunk user is configured correctly for WMI.
</li><li> Make sure that your Splunk user is valid, and does not have an expired password.
</li><li> Make sure that the Event Log service is running on both the source and target machines.
</li><li> Make sure that your Active Directory (AD) is functioning correctly.
</li><li> Make sure that your computers are configured to allow WMI data between them.
</li><li> Make sure that your event logs are properly configured for remote access.
</li></ul><h3> <a name="troubleshootingwindowseventlogs_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>See the Admin Manual for information on getting started for Windows admins.
</p>
<a name="advancedwindowstroubleshooting"></a><h2> <a name="advancedwindowstroubleshooting_i_need_advanced_help_troubleshooting_splunk_for_windows"><span class="mw-headline" id="I_need_advanced_help_troubleshooting_Splunk_for_Windows"> I need advanced help troubleshooting Splunk for Windows </span></a></h2>
<p>Review this topic if you're having trouble getting data into your Splunk for Windows instance, or if Splunk is having problems starting or running.
</p><p>This topic provides solutions to common issues encountered when working with the Windows version of Splunk. It's divided into several subtopics: 
</p>
<ul><li> Generic issues
</li><li> Issues with WMI
</li><li> Issues with forwarders
</li></ul><h3> <a name="advancedwindowstroubleshooting_general_issues"><span class="mw-headline" id="General_issues">  General issues </span></a></h3>
<p>This section contains solutions to common issues encountered when running Splunk on Windows.
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_splunk_fails_to_start"><span class="mw-headline" id="Splunk_fails_to_start"> Splunk fails to start </span></a></i></b></font></h4>
<p>There are several factors that might prevent Splunk from starting properly. Whether it didn't start automatically, or you are having problems manually starting it, here are some solutions to try:
</p>
<ul><li> <b>Make sure that your system meets the Splunk system requirements.</b> These requirements differ depending on the type of Splunk you're trying to run (full instance versus forwarder).
</li></ul><ul><li> <b>Make sure that the Splunk services are enabled.</b> Go into Control Panel and check that the <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services have their <b>Startup type</b> set to "Automatic."
</li></ul><ul><li> <b>Check file and security permissions.</b> When you install Splunk as a user other than Local System, Splunk does not have full permissions to run on the system by default. Try these solutions to get Splunk back up and running:
<ul><li> Make sure the Splunk user is in the local Administrators group on the machine.
</li><li> Make sure that the Splunk user has Full Control permissions for the entire <code><font size="2">%SPLUNK_HOME%</font></code> directory, and is also the owner of all files and subdirectories in <code><font size="2">%SPLUNK_HOME%</font></code>. You must explicitly define this in the Security properties of the <code><font size="2">%SPLUNK_HOME%</font></code> directory.
</li><li> Be sure to read the "Considerations for deciding how to monitor remote Windows data" for additional information about permissions required to run Splunk as a domain user.
</li></ul></li></ul><ul><li> <b>Make sure Splunk isn't crashing.</b> Check your <code><font size="2">%SPLUNK_HOME%\var\log\splunk</font></code> directory. If Splunk crashes on startup, you'll see one or more dump files located in this directory. If Splunk is crashing, try the solutions listed above first. If those don't work, then uninstall and reinstall the program. If you still encounter problems, contact Splunk Support or visit the Answers page for additional guidance.
</li></ul><p><b>Note:</b> Splunk Support requires an enterprise license in most cases.
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_no_data_is_received"><span class="mw-headline" id="No_data_is_received"> No data is received </span></a></i></b></font></h4>
<p>Splunk for Windows operates similarly to Splunk for other operating systems. If you're not getting data and it's not because of a permissions or network connectivity issue, then there is likely something happening within Splunk, such as an incorrectly configured input. 
</p><p>If you're having trouble collecting Windows event logs, review "<a href="#troubleshootingwindowseventlogs" class="external text">Troubleshooting Windows event logs</a>."
</p><p>Try these solutions to figure out where your data is going:
</p>
<ul><li> <b>Make sure the clocks on all machines in your network are synchronized.</b> If your Active Directory is set up correctly, this should already be done for you. Make sure the <code><font size="2">W32Time</font></code> services on all your machines are running and properly syncing time with the appropriate domain controller.
</li><li> <b>Make sure your inputs are correctly configured.</b> In particular, the performance monitoring, Registry monitoring and Active Directory monitoring inputs must be properly configured if you want them to return data. When collecting performance logs remotely over WMI, for example, use Splunk Web instead of configuration files to create those inputs, as typos in configuration files will prevent Splunk from collecting the data you want.
</li><li> <b>Make sure the type of input you are attempting to configure actually returns data.</b> Some performance counters and Registry keys do not change throughout the course of a Windows session, for example. If there is no change, there is no data to collect.
</li><li> <b>When using the Search app, make sure the time range for your search is correct.</b> If you're searching for events that are outside the time range shown in the Search app, they won't appear there. Adjust the Search app's time range if needed.
</li><li> <b>Make sure that the indexing or parsing pipelines on your indexer or forwarder are not blocked.</b> For example, if Splunk can't send events from a forwarder to an indexer, due to a network issue, it may appear as though Splunk is not indexing the data, when data has not actually arrived. Check <code><font size="2">%SPLUNK_HOME%\var\log\splunk\metrics.log</font></code> for information about the status of Splunk's processing queue. For more information on metrics.log, consult "<a href="#metricslog" class="external text">Work with metrics.log</a>" in this manual.
</li></ul><h3> <a name="advancedwindowstroubleshooting_wmi_issues"><span class="mw-headline" id="WMI_issues"> WMI issues </span></a></h3>
<p>This section contains information about problems encountered when using WMI providers to gather data from remote machines.
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_no_wmi-based_events_come_into_splunk"><span class="mw-headline" id="No_WMI-based_events_come_into_Splunk"> No WMI-based events come into Splunk </span></a></i></b></font></h4>
<p>When Splunk is unable to index WMI-based events, it is likely because of a permissions or security issue. Be sure to review the permissions checklist located in "Monitor WMI-based data" in the Getting Data In Manual. A summary of that checklist follows:
</p>
<ul><li> Splunk must run as a user that is a member of the local Administrators group on the server doing the indexing.
</li><li> The Splunk user must be a member of the domain groups required to access the appropriate WMI resources.
</li><li> The Splunk user must be configured with specific local and domain security policy rights.
</li><li> WMI security must be correctly configured.
</li><li> Windows Firewall must be correctly configured.
</li><li> User Access Control must be considered.
</li></ul><p>You can also see additional information about Splunk's WMI operations by turning on debug logging. To turn on debug logging, follow the instructions in "Troubleshooting WMI Logging" in the Getting Data In Manual.
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_wmi-based_events_come_in.2c_but_sometimes_splunk_crashes"><span class="mw-headline" id="WMI-based_events_come_in.2C_but_sometimes_Splunk_crashes"> WMI-based events come in, but sometimes Splunk crashes </span></a></i></b></font></h4>
<p>WMI can sometimes causes the Splunk WMI process (<code><font size="2">splunk-wmi.exe</font></code>) to crash. If that happens, Splunk will start another WMI process immediately, but you might see crash files in your <code><font size="2">%SPLUNK_HOME%\var\log\splunk</font></code> directory.
</p><p>If Splunk is crashing, try the following solutions:
</p>
<ul><li> <b>Reduce the amount of WMI inputs on each Splunk instance.</b> For best results, limit the number of WMI connections per instance to 120 or fewer on 32-bit Windows systems, or 240 or fewer for 64-bit systems. Note that each server monitored can use more than one WMI connection, depending on the amount of inputs configured for each server.
</li><li> <b>Use a universal forwarder to get data.</b> Splunk recommends that you use a universal forwarder to send data from remote machines to an indexer. Universal forwarders are more scalable and reliable than WMI in nearly all cases, and require far less security management than WMI does.
</li></ul><h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_splunk.27s_wmi_process_runs_slowly"><span class="mw-headline" id="Splunk.27s_WMI_process_runs_slowly"> Splunk's WMI process runs slowly </span></a></i></b></font></h4>
<p>Splunk makes what are known as <i>semisynchronous</i> calls to WMI providers. This means that when Splunk makes a call to WMI, it continues running while WMI deals with the request. 
</p><p>Semisynchronous mode offers the best balance of resource usage and security. It differs from the faster <i>asynchronous</i> mode, but is more secure due to the way that the system handles retrieval of the WMI objects. Both of these modes are faster than <i>synchronous</i> mode, which forces programs making that kind of WMI request to wait until WMI returns the data.
</p><p>When WMI is dealing with a large number of requests, you might notice a slower response because memory usage on the system increases until the retrieved WMI objects are no longer needed by Splunk (after indexing).
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_more_help"><span class="mw-headline" id="More_help"> More help </span></a></i></b></font></h4>
<p>If you are still having issues, read "<a href="#troubleshootingwmi" class="external text">Troubleshooting common issues with Splunk and WMI</a>".
</p>
<h3> <a name="advancedwindowstroubleshooting_forwarder_issues"><span class="mw-headline" id="Forwarder_Issues"> Forwarder Issues </span></a></h3>
<p>This section provides help for users who use Splunk's forwarding and receiving capabilities, including the new universal forwarder included with Version 4.2 and later.
</p>
<h4><font size="3"><b><i> <a name="advancedwindowstroubleshooting_forwarder_doesn.27t_send_any_data"><span class="mw-headline" id="Forwarder_doesn.27t_send_any_data"> Forwarder doesn't send any data </span></a></i></b></font></h4>
<p>If you're using a forwarder to send data to a receiver and the receiver isn't getting any data, there are a number of things you can try to fix the problem:
</p>
<ul><li> <b>Make sure that there is network connectivity between the forwarder and the receiver.</b>
<ul><li> On the machine running the forwarder, open a command prompt and telnet to the IP address and port of the receiver. For example, if your Splunk receiver is configured at IP address 192.168.1.10 port 9997, you would type:
<ul><li> <code><font size="2">&gt; telnet 192.168.1.10 9997</font></code>
</li></ul></li><li> Check the Windows Firewall on both the forwarder and the receiver. Windows Firewall must be configured to allow access in both directions for WMI. Either open ports to allow traffic, or disable Windows Firewall.
</li></ul></li></ul><p><b>Note:</b> On versions of Windows later than Windows XP or Windows Server 2003, the <code><font size="2">telnet</font></code> client might not be installed. While the <code><font size="2">telnet</font></code> client is not required for forwarding, you will not be able to use it to determine basic IP connectivity if it isn't installed. Follow the instructions located at "Install Telnet Client" (http://technet.microsoft.com/en-us/library/cc771275%28WS.10%29.aspx) on MS TechNet to install the <code><font size="2">telnet</font></code> client.
</p>
<ul><li> <b>Make sure the configuration files on your forwarder are properly formatted.</b>
<ul><li> Review your configuration files carefully, and check for spelling and syntax errors.
</li><li> Stanza names must always be bracketed with square brackets (<code><font size="2">[ ]</font></code>). Don't use curly braces or parentheses.
</li><li> The syntax for remote performance monitoring differs significantly from local performance monitoring. Be sure to review "Real-time Windows performance monitoring" in the Getting Data In Manual for specific information.
</li></ul></li></ul><ul><li> <b>If the universal forwarder is running as a user other than Local System, confirm that security and access control are correctly configured.</b>
<ul><li> Ensure that the Splunk user is a local Administrator on the machine.
</li><li> Ensure that the Splunk user is valid (for example, it is neither locked out of the domain nor expired). Check that the user's password is also valid (not expired).
</li><li> Ensure that the Splunk user has access to the desired resource(s).
</li><li> Remember that special permissions are required to access some resources, such as the Security event log, Additionally, changing permissions for these resources sometimes requires special knowledge (such as the Security Description Definition Language).
</li><li> Make sure that Active Directory is functioning correctly, and fix it if it is not.
</li></ul></li></ul><p>Once you have confirmed any or all of these, restart the universal forwarder to ensure it gets a new authentication token from a domain controller.
</p><p><b>Note:</b> When assigning access, it's best practice to use the least permissive security paradigm. This entails denying all access to a resource initially, and only then granting access for specific users as necessary.
</p>
<h3> <a name="advancedwindowstroubleshooting_for_more_information"><span class="mw-headline" id="For_more_information"> For more information </span></a></h3>
<p>See the Admin Manual for information on getting started for Windows admins.
</p><p>Have additional questions or need more help? Be sure to visit Splunk Answers and see what questions and answers the Splunk community has around troubleshooting Splunk on Windows.
</p>
<a name="suselinuxerror"></a><h2> <a name="suselinuxerror_suse_linux_search_error"><span class="mw-headline" id="SuSE_Linux_search_error"> SuSE Linux search error </span></a></h2>
<p>Users running Splunk on a SuSE server may see the error message:
</p><p><code><font size="2">Unable to get a properly formatted response from the server; canceling the current search</font></code> 
</p><p>when executing a search. Alternatively, the dashboard just won't display properly.
</p><p>To resolve this issue, edit the <code><font size="2">/etc/mime.types</font></code> file. Delete (or comment out) these two lines:
</p>
<code><font size="2"><br>text/x-xsl xsl<br>text/x-xslt xslt xsl<br></font></code>
<p>Also change this line:
</p>
<code><font size="2"><br>text/xml xml<br></font></code>
<p>to:
</p>
<code><font size="2"><br>text/xml xml xsl<br></font></code>
<p>With these changes in place, restart Splunk and clear your browser cache. 
</p><p><b>Note:</b> If you are using a proxy, you will need to flush that as well.
</p>
<a name="garbledevents"></a><h2> <a name="garbledevents_garbled_events"><span class="mw-headline" id="Garbled_events"> Garbled events</span></a></h2>
<h3> <a name="garbledevents_symptom"><span class="mw-headline" id="Symptom"> Symptom </span></a></h3>
<p>Events in Splunk look strange or display as foreign characters or files.
</p>
<h3> <a name="garbledevents_explanation"><span class="mw-headline" id="Explanation"> Explanation </span></a></h3>
<p>Many files are human readable that are not in a properly encoded format. Many applications will auto-trim text or special characters including nulls, so it is important to know what is included in the log file, not just what the application displays. 
</p>
<h3> <a name="garbledevents_solution"><span class="mw-headline" id="Solution"> Solution </span></a></h3>
<p>To correct this, set the charset in props.conf for this input to the appropriate character set (using the CHARSET attribute).
</p><p>If you don't know the encoding of your source file, and have access to a *nix machine, you can use the "file" command:
</p>
<code><font size="2"><br>file sample.log<br>sample.log: UTF-8 Unicode English text<br></font></code>
<p>In this example, the encoding is UTF-8. Note, though, that Splunk accepts many other encodings. Find a list of supported character sets, and instructions on specifying a charset, in "Configure character set encoding" in the Getting Data In Manual.
</p>
<a name="binaryfileerror"></a><h2> <a name="binaryfileerror_binary_file_error"><span class="mw-headline" id="Binary_file_error"> Binary file error</span></a></h2>
<table cellpadding="10" cellspacing="0" border="1" width="100%"><tr><td valign="center" align="left"> This page is currently a work in progress; expect frequent near-term updates.
</td></tr></table><h3> <a name="binaryfileerror_symptom"><span class="mw-headline" id="Symptom"> Symptom </span></a></h3>
<p>My file won't process. My (forwarder's) splunkd.log has an error saying my file might be binary.
</p>
<h3> <a name="binaryfileerror_explanation"><span class="mw-headline" id="Explanation"> Explanation </span></a></h3>
<p>Sometimes non-UTF-8 logs are not processed because they are seen as binary in the binary check process.
</p>
<h3> <a name="binaryfileerror_solution"><span class="mw-headline" id="Solution"> Solution </span></a></h3>
<p>Set the charset in props.conf for this input to the appropriate charset. This error shows up in the splunkd.log where the props.conf needs to be specified. So, if you're using a forwarder, the forwarder's splunkd.log is where you'll find the error, and that's also where you need to configure the props.conf.
</p>
<a name="searchheadpoolingperformancedegraded"></a><h2> <a name="searchheadpoolingperformancedegraded_performance_degraded_in_a_search_head_pooling_environment"><span class="mw-headline" id="Performance_degraded_in_a_search_head_pooling_environment"> Performance degraded in a search head pooling environment </span></a></h2>
<p>In a pool environment, you're noticing that searches are taking longer than they used to. How do you figure out where your performance degradation is coming from?
</p><p>Here are a few tests you can run.
</p><p><br>
On the search head, in the pooled location, at the *nix command line,
</p><p><code><font size="2">time find /path/to/pool/dir | wc -l</font></code>
</p><p>measures the time to find the things in .../dir and then count them. Another simple command to try is:
</p><p><code><font size="2">time ls -lR /path/to/pool/dir | wc -l</font></code>,
</p><p>which measures how long it takes to count items in the pool.
</p><p>Anything over ten seconds or so for either of these commands indicates an issue on the shared storage.
</p><p><br>
Run a simple search (for example, <code><font size="2">index=_internal source=*splunkd.log | tail 20</font></code>) with and without SHP enabled. Compare timings.
</p><p>Try some basic commands outside Splunk. Running <code><font size="2">ls</font></code> or <code><font size="2">dir</font></code> should be very fast.
</p><p>If you don't have shell access, other tests you can run include:
</p>
<ul><li> logging in (which uses a shared token)
</li><li> accessing knowledge objects
</li></ul><p><br>
In splunkd.log
<code><font size="2">searchstats</font></code>
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*splunkd_access.log NOT rtsearch spent&gt;29999</font></code><br></div>
<p>any search taking over 30 seconds to return is a slow search.
</p><p><br>
If 
</p>
<ul><li> the only slow things are searches (but not, for example, bundle replication), then your problem might be with your mount point. Run some commands outside of Splunk to validate that your mount point is healthy.
</li><li> accessing knowledge objects takes a long time
</li></ul><p><br>
Search in metrics.log for the load_average:
</p>
<div class="inlineQuery splunk_search_4_3"><code><font size="2">index=_internal source=*metrics.log load_average</font></code><br></div>
<p>look in metrics for 2-5 minutes before and after the duration of the slow-running search
</p><p>If you see this is high, and you have SoS installed, refer to the same period of time and look at the CPU graphs on SoS to make sure you're not seeing a system load.
</p><p>If it's a mount point problem, the box is not going to be challenged.
</p><p>If it's a search load problem, the CPU usage will be high for the duration of the slow search.
</p><p><br></p>
<h3> <a name="searchheadpoolingperformancedegraded_is_it_a_search_load_problem.3f"><span class="mw-headline" id="Is_it_a_search_load_problem.3F"> Is it a search load problem? </span></a></h3>
<p>Start turning off field extractions. Is it still slow?
</p><p>Next turn off real time all time and wildcards in your searches.
</p><p>If you have the Splunk on Splunk app, you can check the app's search load view.
</p><p>Consider search scheduling. Have you scheduled many searches to run at the same time? Move some of your scheduled searches to different minutes past the hour.
</p>
</body><script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

        <script src="js/index.js"></script></html>
