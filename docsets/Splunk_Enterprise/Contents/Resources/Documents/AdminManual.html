<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="http://ogp.me/ns/fb#" charset="utf-8"><head><meta charset="UTF-8"><title></title>
<link rel="stylesheet" href="css/normalize.css">
<link rel="stylesheet" type="text/css" href="css/main.css">
<link rel="stylesheet" href="css/style.css">
<style>
html,body {
margin: 0px;
padding: 10px;
width: 210mm;
max-width: 210mm;
overflow-x: hidden;
}
pre {
	width: 100%;
	overflow-x: hidden;
}
</style>
<script src="js/prefixfree.min.js"></script>
</head><body><h1>Welcome to Splunk Enterprise administration</h1><a name="howtousethismanual"></a><div class="all-questions"><h2> <a name="howtousethismanual_how_to_use_this_manual"><span class="mw-headline" id="How_to_use_this_manual"> How to use this manual</span></a></h2>
<p>This manual provides information about the different ways you can administer Splunk. It also introduces you to some initial administration tasks for Windows and *nix. 
</p><p><b>Note:</b> Unless otherwise stated, tasks and processes in this manual are suitable for both Windows and *nix operating systems. 
</p><p>For a bigger picture overview of the Splunk administration process, including tasks not described in this manual (such as setting up users or data and security configuration), see "<a href="#learnhowtoadministersplunk" class="external text">Splunk Administration: The big picture</a>," in this manual.
</p><p>For a list and simple description of the other manuals available to Splunk users, see "<a href="#whatsinthismanual" class="external text">Other manuals for the Splunk administrator</a>".
</p>

<h3> <a name="howtousethismanual_what_you_can_do_with_the_administration_manual"><span class="mw-headline" id="What_you_can_do_with_the_Administration_Manual">What you can do with the Administration Manual</span></a></h3>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="25%" valign="center" align="left"> <b>Start Splunk and do some initial configuration</b>
</td><td width="75%" valign="center" align="left">All the things you need to do to get started on Splunk, from starting Splunk and installing your license, to binding Splunk to an IP. See: <a href="#startsplunk" class="external text">"What to do first"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Use Splunk Web to configure and administer Splunk</b>
</td><td valign="center" align="left">An overview of Splunk Web and how you can use it to administer Splunk. See <a href="#whatssplunkweb" class="external text">"Use Splunk Web"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Use configuration files to configure and administer Splunk</b>
</td><td valign="center" align="left">A discussion about configuration files: where to find them, how to create and edit them, and some important stuff about file precedences. See <a href="#aboutconfigurationfiles" class="external text">"About configuration files"</a> to get started.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Use the Splunk command line interface (CLI) to configure and administer Splunk</b>
</td><td valign="center" align="left">An overview of how to use the Command Line Interface to configure Splunk. See <a href="#aboutthecli" class="external text">"About the CLI"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Optimize Splunk on Windows</b>
</td><td valign="center" align="left">Some Windows-specific things you should know about working with Splunk, including some tips for optimal deployment and information about working with system images. See <a href="#introductionforwindowsadmins" class="external text">"Introduction for Windows admins"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Learn about Splunk licenses</b>
</td><td valign="center" align="left"><a href="#installyourlicense" class="external text">Install your license</a> then go here to learn everything you need to know about Splunk licenses: <a href="#howsplunklicensingworks" class="external text">"Manage Splunk licenses"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Get familiar with Splunk apps</b>
</td><td valign="center" align="left">An introduction and overview of Splunk Apps and how you might integrate them into your Splunk configuration. See <a href="#whatsanapp" class="external text">"Meet Splunk apps"</a> for more information.
</td></tr><tr valign="top"><td valign="center" align="left"><b>Manage user settings</b>
</td><td valign="center" align="left">The <a href="#aboutusersandroles" class="external text">Manage users</a> chapter shows you how to manage settings for users.
<p>For more information about creating users, see Users and role-based access control in the Securing Splunk Enterprise manual.
</p>
</td></tr></table><a name="learnhowtoadministersplunk"></a><h2> <a name="learnhowtoadministersplunk_splunk_administration:_the_big_picture"><span class="mw-headline" id="Splunk_administration:_The_big_picture"> Splunk administration: The big picture</span></a></h2>
<p>The <a href="#whatsinthismanual" class="external text">Admin Manual</a> that you're reading now provides information about the initial administration tasks as well as information about the different methods you can use to administer Splunk. (For a more specific overview of what you can do with the Admin Manual, see <a href="#howtousethismanual" class="external text">How to use this manual</a>.)
</p><p>Since Splunk administration goes far beyond the initial tasks mentioned in this manual, you will likely find yourself browsing through several manuals in the Splunk documentation set. 
</p><p>Below are a few administration tasks you might want to do after initial configuration and where to go to learn more:
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td valign="center" align="left">Perform backups
</td><td valign="center" align="left"><a href="#backupconfigurations" class="external text">Back up configuration information</a><br>Back up indexed data<br>Set a retirement and archiving policy
</td></tr><tr valign="top"><td valign="center" align="left">Define alerts
</td><td valign="center" align="left">Define alerts
</td></tr><tr valign="top"><td valign="center" align="left">Manage search jobs
</td><td valign="center" align="left">Supervise your jobs with the Jobs page
</td></tr></table><p>For further administration help, you can refer to the manuals described below.
</p>
<h3> <a name="learnhowtoadministersplunk_install_and_upgrade_splunk"><span class="mw-headline" id="Install_and_upgrade_Splunk">Install and upgrade Splunk</span></a></h3>
<p>The Installation Manual describes how to install and upgrade Splunk. For information on specific tasks, start here:
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left"> Understand installation requirements
</td><td width="50%" valign="center" align="left"> Plan your installation
</td></tr><tr valign="top"><td valign="center" align="left">Estimate hardware capacity needs
</td><td valign="center" align="left">Estimate hardware requirements
</td></tr><tr valign="top"><td valign="center" align="left">Install Splunk
</td><td valign="center" align="left">Install Splunk on Windows<br>Install Splunk on Unix, Linux, or MacOS
</td></tr><tr valign="top"><td valign="center" align="left">Upgrade Splunk
</td><td valign="center" align="left">Upgrade from an earlier version
</td></tr></table><h3> <a name="learnhowtoadministersplunk_get_data_into_splunk"><span class="mw-headline" id="Get_data_into_Splunk">Get data into Splunk</span></a></h3>
<p>Getting Data In  is the place to go for information about Splunk data inputs: how to consume data from external sources and how to enhance the value of your data.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left"> Learn how to consume external data
</td><td width="50%" valign="center" align="left"> How to get data into Splunk
</td></tr><tr valign="top"><td valign="center" align="left">Configure file and directory inputs
</td><td valign="center" align="left">Get data from files and directories
</td></tr><tr valign="top"><td valign="center" align="left">Configure network inputs
</td><td valign="center" align="left">Get network events
</td></tr><tr valign="top"><td valign="center" align="left">Configure Windows inputs
</td><td valign="center" align="left">Get Windows data
</td></tr><tr valign="top"><td valign="center" align="left">Configure miscellaneous inputs
</td><td valign="center" align="left">Other ways to get stuff in
</td></tr><tr valign="top"><td valign="center" align="left">Enhance the value of your data
</td><td valign="center" align="left">Configure event processing<br>Configure timestamps<br>Configure indexed field extraction<br>Configure host values<br>Configure source types<br>Manage event segmentation<br>Use lookups and workflow actions
</td></tr><tr valign="top"><td valign="center" align="left">See how your data will look after indexing
</td><td valign="center" align="left">Preview your data
</td></tr><tr valign="top"><td valign="center" align="left">Improve the process
</td><td valign="center" align="left">Improve the data input process
</td></tr></table><h3> <a name="learnhowtoadministersplunk_manage_indexes_and_indexers"><span class="mw-headline" id="Manage_indexes_and_indexers">Manage indexes and indexers</span></a></h3>
<p>Managing Indexers and Clusters  tells you how to configure indexes. It also explains how to manage the components that maintain indexes: indexers and clusters of indexers. 
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left"> Learn about indexing
</td><td width="50%" valign="center" align="left"> Indexing overview
</td></tr><tr valign="top"><td valign="center" align="left">Manage indexes
</td><td valign="center" align="left">Manage indexes
</td></tr><tr valign="top"><td valign="center" align="left">Manage index storage
</td><td valign="center" align="left">Manage index storage
</td></tr><tr valign="top"><td valign="center" align="left">Back up indexes
</td><td valign="center" align="left">Back up indexed data
</td></tr><tr valign="top"><td valign="center" align="left">Archive indexes
</td><td valign="center" align="left">Set a retirement and archiving policy
</td></tr><tr valign="top"><td valign="center" align="left">Learn about clusters and index replication
</td><td valign="center" align="left">About clusters and index replication
</td></tr><tr valign="top"><td valign="center" align="left">Deploy clusters
</td><td valign="center" align="left">Deploy clusters
</td></tr><tr valign="top"><td valign="center" align="left">Configure clusters
</td><td valign="center" align="left">Configure clusters
</td></tr><tr valign="top"><td valign="center" align="left">Manage clusters
</td><td valign="center" align="left">Manage clusters
</td></tr><tr valign="top"><td valign="center" align="left">Learn about cluster architecture
</td><td valign="center" align="left">How clusters work
</td></tr></table><h3> <a name="learnhowtoadministersplunk_scale_splunk"><span class="mw-headline" id="Scale_Splunk">Scale Splunk</span></a></h3>
<p>The  Distributed Deployment Manual describes how to distribute Splunk functionality across multiple components, such as forwarders, indexers, and search heads. Associated manuals cover distributed components in detail:
</p>
<ul><li> The Forwarding Data Manual describes forwarders.
</li><li> The Distributed Search Manual describes search heads.
</li><li> The Updating Splunk Components Manual explains how to use the deployment server and forwarder management to manage your deployment.
</li></ul><table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left">Learn about distributed Splunk
</td><td width="50%" valign="center" align="left"> Distributed Splunk overview
</td></tr><tr valign="top"><td valign="center" align="left">Perform capacity planning for Splunk deployments
</td><td valign="center" align="left">Estimate hardware requirements
</td></tr><tr valign="top"><td valign="center" align="left">Learn how to forward data
</td><td valign="center" align="left">Forward data
</td></tr><tr valign="top"><td valign="center" align="left">Distribute searches across multiple indexers
</td><td valign="center" align="left">Search across multiple indexers
</td></tr><tr valign="top"><td valign="center" align="left">Update the deployment
</td><td valign="center" align="left">Deploy configuration updates across your environment
</td></tr></table><h3> <a name="learnhowtoadministersplunk_secure_splunk"><span class="mw-headline" id="Secure_Splunk">Secure Splunk</span></a></h3>
<p>Securing Splunk tells you how to secure your Splunk deployment. 
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left">Authenticate users and edit roles
</td><td width="50%" valign="center" align="left"> User and role-based access control
</td></tr><tr valign="top"><td valign="center" align="left">Secure Splunk data with SSL
</td><td valign="center" align="left">Secure authentication and encryption
</td></tr><tr valign="top"><td valign="center" align="left">Audit Splunk
</td><td valign="center" align="left">Audit Splunk activity
</td></tr><tr valign="top"><td valign="center" align="left">Use Single Sign-On (SSO) with Splunk
</td><td valign="center" align="left">Configure Single Sign-on
</td></tr><tr valign="top"><td valign="center" align="left">Use Splunk with LDAP
</td><td valign="center" align="left">Set up user authentication with LDAP
</td></tr></table><h3> <a name="learnhowtoadministersplunk_troubleshoot_splunk"><span class="mw-headline" id="Troubleshoot_Splunk">Troubleshoot Splunk</span></a></h3>
<p>The  Troubleshooting Manual provides overall guidance on Splunk troubleshooting. In addition, topics in other manuals provide troubleshooting information on specific issues. 
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Task:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left">Learn about Splunk troubleshooting tools
</td><td width="50%" valign="center" align="left"> First steps
</td></tr><tr valign="top"><td valign="center" align="left">Learn about Splunk log files
</td><td valign="center" align="left">Splunk log files
</td></tr><tr valign="top"><td valign="center" align="left">Work with Splunk support
</td><td valign="center" align="left">Contact Splunk support
</td></tr><tr valign="top"><td valign="center" align="left">Resolve common problems
</td><td valign="center" align="left">Some common scenarios
</td></tr></table><h3> <a name="learnhowtoadministersplunk_references_and_other_information"><span class="mw-headline" id="References_and_other_information">References and other information</span></a></h3>
<p>The Splunk documentation includes several useful references, as well as some other sources of information that might be of use to the Splunk administrator.
</p>
<table cellpadding="5" cellspacing="0" border="1" width="90%"><tr><th bgcolor="#C0C0C0">Reference:
</th><th bgcolor="#C0C0C0">Look here:
</th></tr><tr valign="top"><td width="50%" valign="center" align="left">Configuration file reference
</td><td width="50%" valign="center" align="left"> <a href="#alertactionsconf" class="external text">Configuration file reference</a> in the Admin Manual
</td></tr><tr valign="top"><td valign="center" align="left">REST API reference
</td><td valign="center" align="left">REST API Reference Manual
</td></tr><tr valign="top"><td valign="center" align="left">CLI help
</td><td valign="center" align="left">Available through installed instances of Splunk. For details on how to invoke it, read  <a href="#gethelpwiththecli" class="external text">Get help with the CLI</a> in the Admin Manual.
</td></tr><tr valign="top"><td valign="center" align="left">Release information
</td><td valign="center" align="left">Release Notes
</td></tr><tr valign="top"><td valign="center" align="left">Information on managing Splunk knowledge
</td><td valign="center" align="left">Knowledge Manager Manual
</td></tr></table><a name="whatsinthismanual"></a><h2> <a name="whatsinthismanual_other_manuals_for_the_splunk_administrator"><span class="mw-headline" id="Other_manuals_for_the_Splunk_administrator">Other manuals for the Splunk administrator</span></a></h2>
<p>The Admin manual is one of several books with important information and procedures for the Splunk administrator. But it's just the beginning of what you can do with Splunk.
</p><p>If you need to configure, run, or maintain Splunk as a service for yourself or other users, start with this book. Then go to these other manuals for details on specific areas of Splunk administration:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Manual
</th><th bgcolor="#C0C0C0">What it covers
</th><th bgcolor="#C0C0C0">Key topic areas
</th></tr><tr valign="top"><td valign="center" align="left"><b>Getting Data In</b>
</td><td valign="center" align="left">Specifying data inputs and improving how Splunk handles data
</td><td valign="center" align="left"><b>How to get data into Splunk</b><br><b>Configure event processing</b><br><b>Preview your data</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Managing Indexers and Clusters</b>
</td><td valign="center" align="left">Managing Splunk indexers and clusters of indexers
</td><td valign="center" align="left"><b>About indexing and indexers</b><br><b>Manage indexes</b><br><b>Back up and archive your indexes</b><br><b>About clusters and index replication</b><br><b>Deploy clusters</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Distributed Deployment</b>
</td><td valign="center" align="left">Scaling your deployment to fit the needs of your enterprise.
</td><td valign="center" align="left"><b>Distributed Splunk overview</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Forwarding Data</b>
</td><td valign="center" align="left">Forwarding data into Splunk.
</td><td valign="center" align="left"><b>Forward data</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Distributed Search</b>
</td><td valign="center" align="left">Using search heads to distribute searches across multiple indexers.
</td><td valign="center" align="left"><b>Search across multiple indexers</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Updating Splunk Components</b>
</td><td valign="center" align="left">Using the deployment server and forwarder management to update Splunk components such as forwarders and indexers.
</td><td valign="center" align="left"><b>Deploy updates across your environment</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Securing Splunk</b>
</td><td valign="center" align="left">Data security and user authentication
</td><td valign="center" align="left"><b>User authentication and roles</b><br><b>Encryption and authentication with SSL</b><br><b>Auditing</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Troubleshooting</b>
</td><td valign="center" align="left">Solving problems
</td><td valign="center" align="left"><b>First steps</b><br><b>Splunk log files</b><br><b>Some common scenarios</b>
</td></tr><tr valign="top"><td valign="center" align="left"><b>Installation</b>
</td><td valign="center" align="left">Installing and upgrading Splunk
</td><td valign="center" align="left"><b>System requirements</b><br><b>Step by step installation procedures</b><br><b>Upgrade from an earlier version</b>
</td></tr></table><p>The topic <a href="#learnhowtoadministersplunk" class="external text">"Learn to administer Splunk"</a> provides more detailed guidance on where to go to read about specific admin tasks.
</p>
<h3> <a name="whatsinthismanual_other_books_of_interest_to_the_splunk_administrator"><span class="mw-headline" id="Other_books_of_interest_to_the_Splunk_administrator"> Other books of interest to the Splunk administrator</span></a></h3>
<p>In addition to the manuals that describe the primary administration tasks, you might want to visit other manuals from time to time, depending on the size of your Splunk installation and the scope of your responsibilities. These are other manuals in the Splunk core documentation set:
</p>
<ul><li> <b>Splunk Tutorial</b>. This manual provides an introduction to searching with Splunk.
</li><li> <b>Knowledge Manager</b>. This manual describes how to manage Splunk knowledge objects, such as event types, tags, lookups, field extractions, workflow actions, saved searches, and views.
</li><li> <b>Alerting</b>. This manual describes Splunk's alerting and monitoring functionality.
</li><li> <b>Data Visualizations</b>. This manual describes the range of visualizations that Splunk provides.
</li><li> <b>Search</b>. This manual tells you how to search and how to use the Splunk search language.
</li><li> <b>Search Reference</b>. This reference contains a detailed catalog of the Splunk search commands.
</li><li> <b>Developing Views and Apps for Splunk Web</b>. This manual explains how to develop views and apps using advanced XML. It also contains other developer topics, such as custom scripts and extending Splunk. 
</li><li> <b>REST API Reference</b>. This manual provides information on all publicly accessible REST API endpoints.
</li><li> <b>Release Notes</b>. Look here for information about new features, known issues, and fixed problems.
</li></ul><h3> <a name="whatsinthismanual_the_larger_world_of_splunk_documentation"><span class="mw-headline" id="The_larger_world_of_Splunk_documentation"> The larger world of Splunk documentation </span></a></h3>
<p>For links to the full set of Splunk core documentation, including the manuals listed above, visit: <b>Splunk core documentation</b>.
</p><p>To access all the Splunk documentation, including manuals for apps, go to this page: <b> Welcome to Splunk documentation</b>.
</p>
<h3> <a name="whatsinthismanual_make_a_pdf"><span class="mw-headline" id="Make_a_PDF">Make a PDF</span></a></h3>
<p>If you'd like a PDF version of this manual, click the red <b>Download the Admin Manual as PDF</b> link below the table of contents on the left side of this page. A PDF version of the manual is generated on the fly. You can save it or print it to read later.
</p>
<a name="introductionforwindowsadmins"></a><h2> <a name="introductionforwindowsadmins_introduction_for_windows_admins"><span class="mw-headline" id="Introduction_for_Windows_admins"> Introduction for Windows admins</span></a></h2>
<p>Welcome!
</p><p>Splunk is a powerful, effective tool for Windows administrators to resolve problems that occur on their Windows networks. Its out-of-the-box feature set positions it to be the secret weapon in the Windows administrator's toolbox. The ability to add apps that augment its functionality makes it even more extensible. And it has a growing, thriving community of users.
</p>
<h3> <a name="introductionforwindowsadmins_how_to_use_this_manual_as_a_windows_user"><span class="mw-headline" id="How_to_use_this_manual_as_a_Windows_user"> How to use this manual as a Windows user</span></a></h3>
<p>This manual has topics that will help you experiment with, learn, deploy, and get the most out of Splunk. 
</p><p>Unless otherwise specified, the information in this manual is helpful for both Windows and *nix users. If you are unfamiliar with Windows or *nix operational commands, we strongly recommend you check out <a href="#differencesbetweenunixandwindowsinsplunkoperations" class="external text">Differences between *nix and Windows in Splunk operations</a>.
</p><p>We've also provided some extra information in the chapter "get the most out of Splunk on Windows". This chapter is intended for Windows users to help you make the most of Splunk and includes the following information.
</p><p><b><a href="#deployingsplunkonwindows" class="external text">Deploy Splunk on Windows</a></b> provides some considerations and preparations specific to Windows users. Use this topic when you plan your deployment. 
</p><p><b><a href="#optimizesplunkforpeakperformance" class="external text">Optimize Splunk for peak performance</a></b> describes ways to keep your Splunk on Windows deployment running properly, either during the course of the deployment, or after the deployment is complete.
</p><p><b><a href="#putsplunkontosystemimages" class="external text">Put Splunk onto system images</a></b> helps you make Splunk a part of every Windows system image or installation process. From here you can find tasks for installing Splunk and Splunk forwarders onto your system images.
</p>
<h3> <a name="introductionforwindowsadmins_for_more_information"><span class="mw-headline" id="For_more_information">For more information</span></a></h3>
<p>Here's some additional Windows topics of interest in other Splunk manuals:
</p>
<ul><li> An overview of all of the installed Splunk for Windows services (from the Installation Manual)
</li><li> What Splunk can monitor (from the Getting Data In Manual)
</li><li> Considerations for deciding how to monitor remote Windows data (from the Getting Data In Manual). Read this topic for important information on how to get data from multiple machines remotely. 
</li><li> Consolidate data from multiple machines (from the Forwarding Data Manual)
</li></ul><p>Other useful information:
</p>
<ul><li> Where is my data? (from the Getting Data In Manual)
</li><li> Use Splunk's Command Line Interface (CLI) (from the Getting Data In Manual)
</li><li> Sources, sourcetypes and fields (from the Getting Data In Manual)
</li><li> Fields and field extraction (from the Knowledge Manager Manual)
</li><li> Real-time searches (from the User Manual)
</li><li> Saved searches (from the User Manual)
</li><li> Dashboard creation (from the User Manual)
</li></ul><h3> <a name="introductionforwindowsadmins_if_you_need_help"><span class="mw-headline" id="If_you_need_help"> If you need help </span></a></h3>
<p>If you are looking for in-depth Splunk knowledge, a number of education programs are available.
</p><p>When you get stuck, Splunk has a large free support infrastructure that can help:
</p>
<ul><li> Splunk Answers. 
</li><li> The Splunk Community Wiki. 
</li><li> The Splunk Internet Relay Chat (IRC) channel (EFNet #splunk). (IRC client required)
</li></ul><p>If you still don't have an answer to your question, you can get in touch with Splunk's support team. The Support Contact page tells you how to do that.
</p><p><b>Note:</b> Levels of support above the community level require an Enterprise license. To get one, you'll need to speak with the Sales team.
</p>
<a name="moreaboutsplunkfree"></a><h2> <a name="moreaboutsplunkfree_about_splunk_free"><span class="mw-headline" id="About_Splunk_Free"> About Splunk Free</span></a></h2>
<p>Splunk Free is the totally free version of Splunk. It lets you index up to 500 MB/day and will never expire. 
</p><p>The 500 MB limit is the amount of new data you can add (we call this indexing) per day, but you can keep adding more and more data every day, storing as much as you want. For example, you could add 500 MB of data per day and eventually have 10 TB of data in Splunk. 
</p><p>If you need more than 500 MB/day, you'll need to purchase a license. See <a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a> for more information about licensing.
</p><p>Splunk regulates your license usage by tracking <a href="#aboutlicenseviolations" class="external text">license violations</a>. If you go over 500 MB/day more than 3 times in a 30 day period, Splunk continues to index your data, but disables search functionality until you are back down to 3 or fewer warnings in the 30 day period. 
</p>
<h3> <a name="moreaboutsplunkfree_is_splunk_free_for_you.3f"><span class="mw-headline" id="Is_Splunk_Free_for_you.3F">Is Splunk Free for you?</span></a></h3>
<p>Splunk Free is designed for personal, ad-hoc search and visualization of IT data. You can use Splunk Free for ongoing indexing of small volumes (&lt;500 MB/day) of data. Additionally, you can use it for short-term bulk-loading and analysis of larger data sets--Splunk Free lets you bulk-load much larger data sets up to 3 times within a 30 day period. This can be useful for forensic review of large data sets.  
</p>
<h3> <a name="moreaboutsplunkfree_what_is_included_with_splunk_free.3f"><span class="mw-headline" id="What_is_included_with_Splunk_Free.3F">What is included with Splunk Free?</span></a></h3>
<p>Splunk Free is a single-user product. All of Splunk's features are supported, with the following exceptions:
</p>
<ul><li> You cannot set up clusters or distributed search configurations in Splunk Free.
</li><li> Forwarding in TCP/HTTP formats is not available. This means you can forward data to other Splunk instances, but not to non-Splunk instances.
</li><li> Deployment management capabilities are not available.
</li><li> Alerting/monitoring is not available.
</li><li> Indexer clustering is not available.
</li><li> Report acceleration summaries are not available.
</li><li> While a Splunk Free instance can be used as a forwarder (to a Splunk Enterprise indexer) it can't be the client of a deployment server.
</li><li> There is no authentication or user and role management when using Splunk Free. This means:
<ul><li> There is no login. The command line or browser can access and control all aspects of Splunk Free with no user/password prompt.
</li><li> All accesses are treated as equivalent to the admin user. There is only one role (admin), and it is not configurable. You cannot add more roles or create user accounts. 
</li><li> Searches are run against all public indexes, 'index=*'.
</li><li> Restrictions on search, such as user quotas, maximum per-search time ranges, and search filters, are not supported. 
</li><li> The capability system is disabled. All available capabilities are enabled for all users accessing Splunk Free.
</li></ul></li></ul><h3> <a name="moreaboutsplunkfree_switching_to_free_from_an_enterprise_trial_license"><span class="mw-headline" id="Switching_to_Free_from_an_Enterprise_Trial_license">Switching to Free from an Enterprise Trial license</span></a></h3>
<p>When you first download and install Splunk, you are automatically using an Enterprise Trial license. You can continue to use the Enterprise Trial license until it expires, or switch to the Free license right away, depending on your requirements. 
</p>
<h4><font size="3"><b><i> <a name="moreaboutsplunkfree_what_you_should_know_about_switching_to_free"><span class="mw-headline" id="What_you_should_know_about_switching_to_Free">What you should know about switching to Free</span></a></i></b></font></h4>
<p>Splunk Enterprise Trial gives you access to a number of features that are not available in Splunk Free. When you switch, <b>be aware of the following</b>:
</p>
<ul><li> User accounts or roles that you've created will no longer work. 
</li><li> Anyone connecting to the instance will automatically be logged on as 'admin'. You will no longer see a login screen, though you will see the update check occur. 
</li><li> Any knowledge objects created by any user other than 'admin' (such as event type, transaction, or source type definitions) and not already globally shared will not be available. If you need these knowledge objects to continue to be available after you switch to Splunk Free, you can do one of the following:
<ul><li> Use Splunk Web to promote them to be globally available before you switch using the information in <a href="#managingappobjects" class="external text">this topic</a>.
</li><li> Hand edit the configuration files they are in to promote them as described <a href="#apparchitectureandobjectownership_make_splunk_knowledge_objects_globally_available" class="external text">here</a>.
</li></ul></li><li> Any alerts you have defined will no longer fire/function, although you can still schedule searches to run for dashboards and summary indexing purposes.
<ul><li> You will <b>no longer receive alerts from Splunk</b>.
</li></ul></li><li> Configurations in outputs.conf to forward to third-party applications in TCP or HTTP formats will stop working.
</li></ul><p>When you attempt to make any of the above configurations in Splunk Web while using an Enterprise Trial license, you will be warned about the above limitations in a Free Splunk.
</p>
<h3> <a name="moreaboutsplunkfree_how_do_i_switch_to_splunk_free.3f"><span class="mw-headline" id="How_do_I_switch_to_Splunk_Free.3F">How do I switch to Splunk Free?</span></a></h3>
<p>If you currently have Splunk Enterprise (trial or not), you can either wait for your Enterprise license to expire, or switch to a Free license at any time. To switch to a Free License:
</p><p><b>1.</b> Log in to Splunk Web as a user with admin privileges and navigate to <b>Settings &gt; Licensing</b>.
</p><p><b>2.</b> Click <b>Change license group</b> at the top of the page.
</p><p><img alt="ChangeLicenseGroup60.png" src="images/e/e5/ChangeLicenseGroup60.png" width="452" height="260"></p><p><b>3.</b> Select <b>Free license</b> and click <b>Save</b>.
</p><p><b>4.</b> You are prompted to restart.
</p>
<a name="differencesbetweenunixandwindowsinsplunkoperations"></a><h2> <a name="differencesbetweenunixandwindowsinsplunkoperations_differences_between_.2anix_and_windows_in_splunk_operations"><span class="mw-headline" id="Differences_between_.2Anix_and_Windows_in_Splunk_operations"> Differences between *nix and Windows in Splunk operations</span></a></h2>
<p>This topic clarifies the functional differences that you'll encounter between *nix and Windows operating systems, under the context in which they matter in Splunk operations.  It does not delve into technical comparisons of - or advocacy for - either flavor of OS, but rather explains why you'll see things referenced one way or another on various OS-specific Splunk manual pages.
</p>
<h3> <a name="differencesbetweenunixandwindowsinsplunkoperations_paths"><span class="mw-headline" id="Paths"> Paths </span></a></h3>
<p>A major difference in the way that *nix operating systems handle files and directories is the type of slash used to separate files or directories in the pathname.  *nix systems use the forward slash, ("/").  Windows, on the other hand, uses the backslash ("\").
</p><p>An example of a *nix path:
</p><p><code><font size="2">/opt/splunk/bin/splunkd</font></code>
</p><p>An example of a Windows path:
</p><p><code><font size="2">C:\Program Files\Splunk\bin\splunkd.exe</font></code>
</p>
<h3> <a name="differencesbetweenunixandwindowsinsplunkoperations_environment_variables"><span class="mw-headline" id="Environment_variables"> Environment variables</span></a></h3>
<p>Another area where the operating systems differ is in the representation of environment variables.  Both systems have a way to temporarily store data in one or more environment variables. On *nix systems, this is shown by using the dollar sign ("$") in front of the environment variable name, like so:
</p>
<code><font size="2"><br># SPLUNK_HOME=/opt/splunk; export $SPLUNK_HOME<br></font></code>
<p>On Windows, it's a bit different - to specify an environment variable, you need to use the percent sign ("%").  Depending on the type of environment variable you are using, you may need to place one or two percent signs before the environment name, or on either side of the name.
</p>
<code><font size="2"><br>&gt; set SPLUNK_HOME="C:\Program Files\Splunk"<br>&gt; echo&nbsp;%SPLUNK_HOME%<br>C:\Program Files\Splunk<br>&gt;<br></font></code>
<p>To set the&nbsp;%SPLUNK_HOME% variable in the Windows environment, you can do one of two things:
</p>
<ul><li> Edit <code><font size="2">splunk-launch.conf</font></code> in&nbsp;%SPLUNK_HOME%\etc.
</li></ul><ul><li> Set the variable by accessing the "Environment Variables" window.  Open an Explorer window, and on the left pane, right-click "My Computer", then select "Properties" from the window that appears.  Once the System Properties window appears, select the "Advanced" tab, then click on the "Environment Variables" button that appears along the bottom window of the tab.
</li></ul><a name="howyoucanconfigure"></a><h2> <a name="howyoucanconfigure_ways_you_can_configure_splunk"><span class="mw-headline" id="Ways_you_can_configure_Splunk"> Ways you can configure Splunk </span></a></h2>
<p>Splunk maintains its configuration information in a set of <b>configuration files</b>. You can configure Splunk by using any (or all!) of these methods:
</p>
<ul><li> Use Splunk Web.
</li><li> Use Splunk's Command Line Interface (CLI) commands.
</li><li> Edit Splunk's configuration files directly.
</li><li> Use App setup screens that use the Splunk REST API to update configurations.
</li></ul><p>All of these methods change the contents of the underlying configuration files. You may find different methods handy in different situations. 
</p>
<h3> <a name="howyoucanconfigure_use_splunk_web"><span class="mw-headline" id="Use_Splunk_Web"> Use Splunk Web </span></a></h3>
<p>You can perform most common configuration tasks in Splunk Web. Splunk Web runs by default on port 8000 of the host on which it is installed:
</p>
<ul><li> If you're running Splunk on your local machine, the URL to access Splunk Web is <code><font size="2">http://localhost:8000</font></code>.
</li><li> If you're running Splunk on a remote machine, the URL to access Splunk Web is <code><font size="2">http://&lt;hostname&gt;:8000</font></code>, where <code><font size="2">&lt;hostname&gt;</font></code> is the name of the machine Splunk is running on. 
</li></ul><p>Administration menus can be found under <b>Settings</b> in the Splunk Web menu bar. Most tasks in the Splunk documentation set are described for Splunk Web. For more information about Splunk Web, see <a href="#whatssplunkweb" class="external text">Meet Splunk Web</a>. 
</p>
<h3> <a name="howyoucanconfigure_edit_configuration_files"><span class="mw-headline" id="Edit_configuration_files"> Edit configuration files </span></a></h3>
<p>Most of Splunk's configuration information is stored in .conf files. These files are located under your Splunk installation directory (usually referred to in the documentation as <code><font size="2">$SPLUNK_HOME</font></code>) under <code><font size="2">/etc/system</font></code>. In most cases you can copy these files to a local directory and make changes to these files with your preferred text editor. 
</p><p>Before you begin editing configuration files, read  <a href="#aboutconfigurationfiles" class="external text">"About configuration files"</a>.
</p>
<h3> <a name="howyoucanconfigure_use_splunk_cli"><span class="mw-headline" id="Use_Splunk_CLI"> Use Splunk CLI </span></a></h3>
<p>Many configuration options are available via the CLI. These options are documented in the CLI chapter in this manual. You can also get CLI help reference with the <code><font size="2">help</font></code> command while Splunk is running:
</p>
<code><font size="2"><br>./splunk help<br></font></code>
<p>For more information about the CLI, refer to "About the CLI" in this manual. If you are unfamiliar with CLI commands, or are working in a Windows environment, you should also check out <a href="#differencesbetweenunixandwindowsinsplunkoperations" class="external text">Differences between *nix and Windows in Splunk operations</a>.
</p>
<h3> <a name="howyoucanconfigure_setup_screens_for_an_app"><span class="mw-headline" id="Setup_screens_for_an_app"> Setup screens for an app </span></a></h3>
<p>Developers can create setup screens for an app that allow users to set configurations for that app without editing the configuration files directly. Setup screens make it easier to distribute apps to different environments, or to customize an app for a particular usage. 
</p><p>Setup screens use Splunk's REST API to manage the app's configuration files.
</p><p>For more information about setup screens, refer to "Configure a setup screen for your app" in the Developing Views and Apps for Splunk Web manual.
</p>
<h3> <a name="howyoucanconfigure_managing_a_distributed_environment"><span class="mw-headline" id="Managing_a_distributed_environment"> Managing a distributed environment</span></a></h3>
<p>The Splunk deployment server provides centralized management and configuration for distributed environments. You can use it to deploy sets of configuration files or other content to groups of Splunk instances across the enterprise.
</p><p>For information about managing deployments, refer to the "Updating Splunk Components" manual.
</p>
<h1>Get the most out of Splunk Enterprise on Windows</h1><a name="deployingsplunkonwindows"></a><h2> <a name="deployingsplunkonwindows_deploy_splunk_on_windows"><span class="mw-headline" id="Deploy_Splunk_on_Windows"> Deploy Splunk on Windows</span></a></h2>
<p>You can integrate Splunk into your Windows environment in any number of ways. This topic discusses some of those scenarios and offers guidelines on how to best adapt your Splunk for Windows deployment to your enterprise. 
</p><p>While this topic is geared more toward deploying Splunk in a Windows environment, Splunk itself also has distributed deployment capabilities that you should be aware of, even as you integrate it into your Windows enterprise. The Distributed Deployment Manual has lots of information on spreading Splunk services across a number of computers.
</p><p>When deploying Splunk on Windows on a large scale, you can rely completely on your own deployment utilities (such as System Center Configuration Manager or Tivoli/BigFix) to place both Splunk and its configurations on the machines in your enterprise. Or, you can integrate Splunk into system images and then deploy Splunk configurations and apps using Splunk's deployment server.
</p>
<h3> <a name="deployingsplunkonwindows_concepts"><span class="mw-headline" id="Concepts"> Concepts </span></a></h3>
<p>When you deploy Splunk into your Windows network, it captures data from the machines and stores it centrally. Once the data is there, you can search and create reports and dashboards based on the indexed data. More importantly, for system administrators, Splunk can send alerts to let you know what is happening as the data arrives.
</p><p>In a typical deployment, you dedicate some hardware to Splunk for indexing purposes, and then use a combination of universal forwarders and Windows Management Instrumentation (WMI) to collect data from other machines in the enterprise.
</p>
<h3> <a name="deployingsplunkonwindows_considerations"><span class="mw-headline" id="Considerations"> Considerations  </span></a></h3>
<p>Deploying Splunk in a Windows enterprise requires a number of planning steps. 
</p><p>First, you must inventory your enterprise, beginning at the physical network, and leading up to how the machines on that network are individually configured. This includes, but is not limited to:
</p>
<ul><li> Counting the number of machines in your environment and defining a subset of those which need Splunk installed. Doing this defines the initial framework of your Splunk topology.
</li><li> Calculating your network bandwidth, both in your main site and at any remote or external sites. Doing this determines where you will install your main Splunk instance, and where and how you will use Splunk forwarders. 
</li><li> Assessing the current health of your network, particularly in areas where networks are separated. Making sure your edge routers and switches are functioning properly will allow you to set a baseline for network performance both during and after the deployment.
</li></ul><p>Then, you must answer a number of questions prior to starting the deployment, including:
</p>
<ul><li> <b>What data on your machines needs indexing? What part of this data do you want to search, report, or alert across?</b> This is probably the most important consideration to review. The answers to these questions determine how you address every other consideration. It determines where to install Splunk, and what types of Splunk you use in those installations. It also determines how much computing and network bandwidth Splunk will potentially use.
</li></ul><ul><li> <b>How is the network laid out? How are any external site links configured? What security is present on those links?</b> Fully understanding your network topology helps determine which machines you should install Splunk on, and what types of Splunk (indexers or forwarders) you should install on those machines from a networking standpoint.
</li></ul><p>A site with thin LAN or WAN links makes it necessary to consider how much Splunk data should be transferred between sites. For example, if you have a hub-and-spoke type of network, with a central site connected to branch sites, it might be a better idea to deploy forwarders on machines in the branch sites, which send data to an intermediate forwarder in each branch. Then, the intermediate forwarder would send data back to the central site. This is a less costly move than having all machines in a branch site forward their data to an indexer in the central site.
</p><p>If you have external sites that have file, print or database services, you'll need to account for that traffic as well.
</p>
<ul><li> <b>How is your Active Directory (AD) configured?</b>  How are the operations masters roles on your domain controllers (DCs) defined? Are all domain controllers centrally located, or do you have controllers located in satellite sites? If your AD is distributed, are your bridgehead servers configured properly? Is your Inter-site Topology Generator (ISTG)-role server functioning correctly? If you are running Windows Server 2008 R2, do you have read-only domain controllers (RODCs) in your branch sites? If so, then you have to consider the impact of AD replication traffic as well as Splunk and other network traffic.
</li></ul><ul><li> <b>What other roles are the servers in your network playing?</b> Splunk indexers need resources to run at peak performance, and sharing servers with other resource-intensive applications or services (such as Microsoft Exchange, SQL Server and even Active Directory itself) can potentially lead to problems with Splunk on those machines. For additional information on sharing server resources with Splunk indexers, see "Introduction to capacity planning for Splunk Enterprise" in the Capacity Planning Manual.
</li></ul><ul><li> <b>How will you communicate the deployment to your users?</b> A Splunk installation means the environment is changing. Depending on how Splunk is rolled out, some machines will get new software installed. Users might incorrectly link these new installs to perceived problems or slowness on their individual machine. You should keep your user base informed of any changes to reduce the number of support calls related to the deployment.
</li></ul><h3> <a name="deployingsplunkonwindows_prepare_your_splunk_on_windows_deployment"><span class="mw-headline" id="Prepare_your_Splunk_on_Windows_deployment"> Prepare your Splunk on Windows deployment </span></a></h3>
<p>How you deploy Splunk into your existing environment depends on the needs you have for Splunk, balanced with the available computing resources you have, your physical and network layouts, and your corporate infrastructure. As there is no one specific way to deploy Splunk, there are no step-by-step instructions to follow. There are, however, some general guidelines to observe.
</p><p>For a more successful Splunk deployment:
</p>
<ul><li> <b>Prepare your network.</b> Before integrating Splunk into your environment:
<ul><li> Make sure that your network is functioning properly, and that all switches, routers and cabling are correctly configured. 
</li><li> Replace any broken or failing equipment. 
</li><li> Ensure any virtual LANs (VLANs) are properly set up. 
</li><li> Test network throughput, particularly between sites with thin network links.
</li></ul></li></ul><ul><li> <b>Prepare your Active Directory.</b> While AD is not a requirement to run Splunk, it's a good idea to ensure that it is functioning properly prior to your deployment. This includes but is not limited to:
<ul><li> Identifying all of your domain controllers, and the operations master roles any of them might perform. If you have RODCs at your branch sites, make sure that they have the fastest connections as possible to operations masters DCs. 
</li><li> Ensuring that AD replication is functioning correctly, and that all site links have a DC with a copy of the global catalog.
</li><li> If your forest is divided into multiple sites, make sure your ISTG role server is functioning properly, or that you have assigned at least two bridgehead servers in your site (one primary, one backup).
</li><li> Ensuring that your DNS infrastructure is working properly.
</li></ul></li></ul><p>You might need to place DCs on different subnets on your network, and seize flexible single master operations (FSMO, or operations master) roles as necessary to ensure peak AD operation and replication performance during the deployment.
</p>
<ul><li> <b>Define your Splunk deployment.</b> Once your Windows network is properly prepared, you must now determine where Splunk will go in the network. Consider the following:
<ul><li> Determine the set(s) of data that you want Splunk to index on each machine, and whether or not you need for Splunk to send alerts on any collected data.
</li><li> Dedicate one or more machines in each network segment to handle Splunk indexing, if possible. For additional information on capacity planning for a distributed Splunk deployment, review "Introduction to capacity planning for Splunk Enterprise" in the Capacity Planning Manual.
</li><li> Don't install full Splunk on machines that run resource-intensive services like AD (in particular, DCs that hold FSMO roles), any version of Exchange, SQL Server, or machine virtualization product such as Hyper-V or VMWare. Instead, use a universal forwarder, or connect to those machines using WMI.
</li><li> If you're running Windows Server 2008/2008 R2 Core, remember that you'll have no GUI available to make changes using Splunk Web when you install Splunk on those machines.
</li><li> Arrange your Splunk layout so that it uses minimal network resources, particularly across thin WAN links. Universal forwarders greatly reduce the amount of Splunk-related traffic sent over the wire.
</li></ul></li></ul><ul><li> <b>Communicate your deployment plans to your users.</b> It's important to advise your users about the status of the deployment, throughout the course of it. This will significantly reduce the amount of support calls you receive later.
</li></ul><a name="optimizesplunkforpeakperformance"></a><h2> <a name="optimizesplunkforpeakperformance_optimize_splunk_for_peak_performance"><span class="mw-headline" id="Optimize_Splunk_for_peak_performance"> Optimize Splunk for peak performance</span></a></h2>
<p>Like many services, Splunk on Windows needs proper maintenance in order to run at peak performance. This topic discusses the methods that you can apply to keep your Splunk on Windows deployment running properly, either during the course of the deployment, or after the deployment is complete. 
</p><p>To ensure peak Splunk performance:
</p>
<ul><li> <b>Designate one or more machines solely for Splunk operations.</b> Splunk scales horizontally. This means that more physical computers dedicated to Splunk, rather than more resources in a single computer, translate into better performance. Where possible, split up your indexing and searching activities across a number of machines, and only run main Splunk services on those machines. With the exception of the universal forwarder performance is reduced when you run Splunk on servers that share other services.
</li></ul><ul><li> <b>Dedicate fast disks for your Splunk indexes.</b> The faster the available disks on a system are for Splunk indexing, the faster Splunk will run. Use disks with spindle speeds faster than 10,000 RPM when possible. When dedicating redundant storage for Splunk, use hardware-based RAID 1+0 (also known as RAID 10). It offers the best balance of speed and redundancy. Software-based RAID configurations through the Windows Disk Management utility are not recommended.
</li></ul><ul><li> <b>Don't allow anti-virus programs to scan disks used for Splunk operations.</b> When anti-virus file system drivers scan files for viruses on access, performance is significantly reduced, especially when Splunk internally ages data that has recently been indexed. If you must use anti-virus programs on the servers running Splunk, make sure that <i>all</i> Splunk directories and programs are excluded from on-access file scans.
</li></ul><ul><li> <b>Use multiple indexes, where possible.</b> Distribute the data that in indexed by Splunk into different indexes. Sending all data to the default index can cause I/O bottlenecks on your system. Where appropriate, configure your indexes so that they point to different physical volumes on your systems, when possible. For information on how to configure indexes, read "Configure your indexes" in this manual.
</li></ul><ul><li> <b>Don't store your indexes on the same physical disk or partition as the operating system.</b> The disk that holds your Windows OS directory (<code><font size="2">%WINDIR%</font></code>) or its swap file is not recommended for Splunk data storage. Put your Splunk indexes on other disks on your system. 
</li></ul><p>For more information on how indexes are stored, including information on database bucket types and how Splunk stores and ages them, review "How Splunk stores indexes" in this manual.
</p>
<ul><li> <b>Don't store the hot and warm database buckets of your Splunk indexes on network volumes.</b> Network latency will decrease performance significantly. Reserve fast, local disk for the hot and warm buckets of your Splunk indexes. You can specify network shares such as Distributed File System (DFS) volumes or Network File System (NFS) mounts for the cold and frozen buckets of the index, but note that searches that include data stored in the cold database buckets will be slower.
</li></ul><ul><li> <b>Maintain disk availability, bandwidth and space on your Splunk indexers.</b> Make sure that the disk volumes that hold Splunk's indexes maintain 20% or more free space at all times. Disk performance decreases proportionally to available space because disk seek times increase. This affects how fast Splunk indexes data, and can also determine how quickly search results, reports and alerts are returned. In a default Splunk installation, the drive(s) that contain your indexes must have at least 5000 megabytes (approximately 5 gigabytes) of free disk space, or indexing will pause.
</li></ul><a name="putsplunkontosystemimages"></a><h2> <a name="putsplunkontosystemimages_put_splunk_onto_system_images"><span class="mw-headline" id="Put_Splunk_onto_system_images"> Put Splunk onto system images</span></a></h2>
<p>This topic explains the concepts of making Splunk a part of every Windows system image or installation process. It also guides you through the general process of integration, regardless of the imaging utilities that you use.
</p>
<ul><li> For more specific information about getting Windows data into Splunk, review "About Windows data and Splunk" in the Getting Data In Manual. 
</li><li> For information on distributed Splunk deployments, read  "Distributed overview" in the Distributed Deployment Manual. This overview is essential reading for understanding how to set up Splunk deployments, irrespective of the operating system that you use. You can also read about Splunk's distributed deployment capabilities there.
</li><li> For information about planning larger Splunk deployments, read "Introduction to capacity planning for Splunk Enterprise" in the Capacity Planning Manual and "<a href="#deployingsplunkonwindows" class="external text">Deploying Splunk on Windows</a>" in this manual.
</li></ul><h3> <a name="putsplunkontosystemimages_concepts_for_system_integration_on_windows"><span class="mw-headline" id="Concepts_for_system_integration_on_Windows">Concepts for system integration on Windows</span></a></h3>
<p>The main reason to integrate Splunk into Windows system images is to ensure that Splunk is available immediately when the machine is activated for use in the enterprise. This frees you from having to install and configure Splunk after activation. 
</p><p>In this scenario, when a Windows system is activated, it immediately launches Splunk after booting. Then, depending on the type of Splunk instance installed and the configuration given, Splunk either collects data from the machine and forwards it to an indexer (in many cases), or begins indexing data that is forwarded from other Windows machines.
</p><p>System administrators can also configure Splunk instances to contact a deployment server, which allows for further configuration and update management.
</p><p>In many typical environments, universal forwarders on Windows machines send data to a central indexer or group of indexers, which then allow that data to be searched, reported and alerted on, depending on your specific needs.
</p>
<h3> <a name="putsplunkontosystemimages_considerations_for_system_integration"><span class="mw-headline" id="Considerations_for_system_integration"> Considerations for system integration</span></a></h3>
<p>Integrating Splunk into your Windows system images requires planning. 
</p><p>In most cases, the preferred Splunk component to integrate into a Windows system image is a universal forwarder. The universal forwarder is designed to share resources on computers that perform other roles, and does much of the work that an indexer can, at much less cost. You can also modify the forwarder's configuration using Splunk's deployment server or an enterprise-wide configuration manager with no need to use Splunk Web to make changes.
</p><p>In some situations, you may want to integrate a full instance of Splunk into a system image. Where and when this is more appropriate depends on your specific needs and resource availability.
</p><p>Splunk doesn't recommend that you include a full version of Splunk in an image for a server that performs any other type of role, unless you have specific need for the capability that an indexer has over a forwarder. Installing multiple indexers in an enterprise does not give you additional indexing power or speed, and can lead to undesirable results.
</p><p>Before integrating Splunk into a system image, consider:
</p>
<ul><li> <b>the amount of data you want Splunk to index, and where you want Splunk to send that data, if applicable.</b> This feeds directly into disk space calculations, and should be a top consideration.
</li><li> <b>the type of Splunk instance to install on the image or machine.</b> Universal forwarders have a significant advantage when installing on workstations or servers that perform other duties, but might not be appropriate in some cases.
</li><li> <b>the available system resources on the imaged machine.</b> How much disk space, RAM and CPU resources are available on each imaged system? Will it support a Splunk install?
</li><li> <b>the resource requirements of your network.</b> Splunk needs network resources, whether you're using it to connect to remote machines using WMI to collect data, or you're installing forwarders on each machine and sending that data to an indexer. 
</li><li> <b>the system requirements of other programs installed on the image.</b> If Splunk is sharing resources with another server, it can take available resources from those other programs. Consider whether or not you should install other programs on a workstation or server that is running a full instance of Splunk. A universal forwarder will work better in cases like this, as it is designed to be lightweight.
</li><li> <b>the role that the imaged machine plays in your environment.</b> Will it be a workstation only running productivity applications like Office? Or will it be an operations master domain controller for your Active Directory forest?
</li></ul><h3> <a name="putsplunkontosystemimages_integrate_splunk_into_a_system_image"><span class="mw-headline" id="Integrate_Splunk_into_a_System_Image"> Integrate Splunk into a System Image </span></a></h3>
<p>Once you have determined the answers to the questions in the checklist above, the next step is to integrate Splunk into your system images.  The steps listed are generic, allowing you to use your favorite system imaging or configuration tool to complete the task.
</p><p>Choose one of the following options for system integration:
</p>
<ul><li> <a href="#integrateauniversalforwarderontoasystemimage" class="external text">Integrate a universal forwarder into a system image</a>
</li></ul><ul><li> <a href="#integratefullsplunkontoasystemimage" class="external text">Integrate a full version of Splunk into a system image</a>
</li></ul><a name="integrateauniversalforwarderontoasystemimage"></a><h2> <a name="integrateauniversalforwarderontoasystemimage_integrate_a_universal_forwarder_onto_a_system_image"><span class="mw-headline" id="Integrate_a_universal_forwarder_onto_a_system_image"> Integrate a universal forwarder onto a system image</span></a></h2>
<p>This topic discusses the procedure to integrate a Splunk universal forwarder into a Windows system image. For additional information about integrating Splunk into images, see "Integrate Splunk into system images."
</p><p>To integrate a universal forwarder into a system image:
</p><p><b>1.</b> Using a reference computer, install and configure Windows to your liking, including installing any needed Windows features, patches and other components.
</p><p><b>2.</b> Install and configure any necessary applications, taking into account Splunk's system and hardware capacity requirements.
</p><p><b>3.</b> Install and configure the universal forwarder from the command line, supplying at least the <code><font size="2">LAUNCHSPLUNK=0</font></code> command line flag.
</p><p><b>Important:</b> You must specify the <code><font size="2">LAUNCHSPLUNK=0</font></code> command line flag to prevent Splunk from running after the installation is completed..
</p><p><b>4.</b> Proceed through the graphical portion of the install, selecting the inputs, deployment servers, and/or forwarder destinations you need.
</p><p><b>5.</b> Once you have completed the install, open a command prompt.
</p><p><b>6.</b> From this prompt, edit any additional configuration files that are not configurable in the installer.
</p><p><b>7.</b> Close the command prompt window.
</p><p><b>8.</b> Ensure that the <code><font size="2">splunkd</font></code>  service is set to start automatically by setting its startup type to 'Automatic' in the Services Control Panel.
</p><p><b>9.</b> Prepare the system image for domain participation using a utility such as SYSPREP (for Windows XP and Windows Server 2003/2003 R2) and/or Windows System Image Manager (WSIM) (for Windows Vista, Windows 7, and Windows Server 2008/2008 R2).
</p><p><b>Note:</b> Microsoft recommends using SYSPREP and WSIM as the method to change machine Security Identifiers (SIDs) prior to cloning, as opposed to using third-party tools (such as Ghost Walker or NTSID.)
</p><p><b>10.</b> Once you have configured the system for imaging, reboot the machine and clone it with your favorite imaging utility.
</p><p>The image is now ready for deployment.
</p>
<a name="integratefullsplunkontoasystemimage"></a><h2> <a name="integratefullsplunkontoasystemimage_integrate_full_splunk_onto_a_system_image"><span class="mw-headline" id="Integrate_full_Splunk_onto_a_system_image"> Integrate full Splunk onto a system image</span></a></h2>
<p>This topic discusses the procedure to integrate a full version of Splunk into a Windows system image. For additional information about integrating Splunk into images, see "<a href="#putsplunkontosystemimages" class="external text">Put Splunk onto system images</a>" in this manual.
</p><p>To integrate a full version of Splunk into a system image:
</p><p><b>1.</b> Using a reference computer, install and configure Windows to your liking, including installing any needed Windows features, patches and other components.
</p><p><b>2.</b> Install and configure any necessary applications, taking into account Splunk's system and hardware capacity requirements.
</p><p><b>3.</b> Install and configure Splunk.
</p><p><b>Important:</b> You can install using the GUI installer, but more options are available when installing the package from the command line.
</p><p><b>4.</b> Once you have configured Splunk inputs, open a command prompt.
</p><p><b>5.</b> From this prompt, stop Splunk by changing to the <code><font size="2">%SPLUNK_HOME%\bin</font></code> directory and issuing a <code><font size="2">.\splunk stop</font></code> 
</p><p><b>6.</b> Clean any event data by issuing a <code><font size="2">.\splunk clean eventdata</font></code>.
</p><p><b>7.</b> Close the command prompt window.
</p><p><b>8.</b> Ensure that the <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services are set to start automatically by setting their startup type to 'Automatic' in the Services Control Panel.
</p><p><b>9.</b> Prepare the system image for domain participation using a utility such as SYSPREP (for Windows XP and Windows Server 2003/2003 R2) and/or Windows System Image Manager (WSIM) (for Windows Vista, Windows 7, and Windows Server 2008/2008 R2).
</p><p><b>Note:</b> Microsoft recommends using SYSPREP and WSIM as the method to change machine Security Identifiers (SIDs) prior to cloning, as opposed to using third-party tools (such as Ghost Walker or NTSID.)
</p><p><b>10.</b> Once you have configured the system for imaging, reboot the machine and clone it with your favorite imaging utility.
</p><p>The image is now ready for deployment.
</p>
<h1>Administer Splunk Enterprise with Splunk Web</h1><a name="launchsplunkweb"></a><h2> <a name="launchsplunkweb_launch_splunk_web"><span class="mw-headline" id="Launch_Splunk_Web"> Launch Splunk Web</span></a></h2>
<p>Once Splunk is running, you can launch the Web Interface. To learn more about what you can do with Splunk Web, see  <a href="#whatssplunkweb" class="external text">"Meet Splunk Web"</a>
</p><p>To launch Splunk Web, navigate to:
</p>
<code><font size="2"><br>http://mysplunkhost:&lt;port&gt;<br></font></code>
<p>Using whatever host and port you chose during installation.
</p><p>The first time you log in to Splunk with an Enterprise license, the default login details are: <br><b>Username - <i>admin</i><br></b>
<b>Password - <i>changeme</i><br></b>
</p><p><b>Note:</b> Splunk with a free license does not have access controls, so you will not be prompted for login information. 
</p><p><b>Note:</b> Starting in Splunk version 4.1.4, you cannot access Splunk Free from a remote browser until you have edited <code><font size="2">$SPLUNK_HOME/etc/local/server.conf</font></code> and set <code><font size="2">allowRemoteLogin</font></code> to <code><font size="2">Always</font></code>. If you are running Splunk Enterprise, remote login is disabled by default (set to <code><font size="2">requireSetPassword</font></code>) for the admin user until you change the default password.
</p>
<a name="whatssplunkweb"></a><h2> <a name="whatssplunkweb_meet_splunk_web"><span class="mw-headline" id="Meet_Splunk_Web"> Meet Splunk Web</span></a></h2>
<p><b>Splunk Web</b> is Splunk's browser-based interface. Here's just a few of the things you can do in Splunk Web:
</p>
<ul><li>Configure your data inputs
</li><li>Search data and report and visualize results
</li><li>Investigate problems
</li><li>Manage users natively or via LDAP strategies
</li><li>Troubleshoot Splunk deployments 
</li><li>Manage clusters and peers
</li></ul><p>Refer to the system requirements for a list of supported operating systems and browsers.
</p>
<h3> <a name="whatssplunkweb_splunk_home"><span class="mw-headline" id="Splunk_Home"> Splunk Home </span></a></h3>
<p>The first time you log into Splunk, you'll land in Splunk Home. All of your apps will appear on this page. Splunk Home include the Splunk Enterprise navigation bar, the Apps panel, the Explore Splunk Enterprise panel, and a custom default dashboard (not shown here).
</p><p><img alt="6.2 splunk home.png" src="images/9/9e/6.2_splunk_home.png" width="700" height="207"></p><p><br>
Your account might also be configured to start in another view such as Search or Pivot in the <b>Search &amp; Reporting</b> app.
</p><p>You can return to Splunk Home from any other view by clicking the Splunk logo at the top left in Splunk Web.
</p>
<h4><font size="3"><b><i> <a name="whatssplunkweb_find"><span class="mw-headline" id="Find"> Find</span></a></i></b></font></h4>
<p><img alt="Find.png" src="images/d/dd/Find.png" width="300" height="181"></p><p>The Find field in the upper right corner lets you find all of your existing configurations and objects. For more information, see: Navigating Splunk Enterprise
</p>
<h4><font size="3"><b><i> <a name="whatssplunkweb_apps"><span class="mw-headline" id="Apps"> Apps </span></a></i></b></font></h4>
<p>The Apps panel lists the apps that are installed on your Splunk instance that you have permission to view. Select the app from the list to open it.
</p><p>For an out-of-the-box Splunk Enterprise installation, you see one App in the workspace: Search &amp; Reporting. When you have more than one app, you can drag and drop the apps within the workspace to rearrange them.
</p><p>You can do two actions on this panel:
</p>
<ul><li> Click the gear icon to view and manage the apps that are installed in your Splunk instance.
</li><li> Click the plus icon to browse for more apps to install.
</li></ul><h5> <a name="whatssplunkweb_search_and_reporting"><span class="mw-headline" id="Search_and_reporting"> Search and reporting </span></a></h5>
<p>Search and reporting is a default App that allows you to search across and create reports for your data. For more information check out the following Splunk Enterprise manuals:
</p>
<ul><li>Search Manual
</li><li>Search Tutorial
</li><li>Reporting Manual
</li></ul><h4><font size="3"><b><i> <a name="whatssplunkweb_the_explore_splunk_enterprise_panel"><span class="mw-headline" id="The_Explore_Splunk_Enterprise_panel">The Explore Splunk Enterprise panel</span></a></i></b></font></h4>
<p>The Explore Splunk Enterprise panel helps you get started with Splunk Enterprise. Click on the icons to:
</p>
<ul><li> <b>Add Data</b> 
</li><li> Browse for new <b>Apps</b>
</li><li> Read Splunk Enterprise <b>Documentation</b>
</li><li> Explore <b>Splunk Answers</b>
</li></ul><h4><font size="3"><b><i> <a name="whatssplunkweb_the_home_dashboard"><span class="mw-headline" id="The_home_dashboard">The home dashboard</span></a></i></b></font></h4>
<p>Use the drop down menu to select a dashboard.
</p>
<h3> <a name="whatssplunkweb_meet_the_splunk_settings_menu"><span class="mw-headline" id="Meet_the_Splunk_Settings_menu"> Meet the Splunk Settings menu</span></a></h3>
<p>Splunk Web provides a convenient interface for managing most aspects of Splunk operations. Most of the functions can be accessed by clicking <b>Settings</b> in the menu. From here you can:
</p>
<h4><font size="3"><b><i> <a name="whatssplunkweb_manage_your_data"><span class="mw-headline" id="Manage_your_data">Manage your data</span></a></i></b></font></h4>
<p>Under <b>Settings &gt; Data</b> you can do the following:
</p>
<ul><li><b>Data Inputs</b> Lets you view a list of data types and configure them. To add an input, click the <b>Add data</b> button in the Data Inputs page. For more information about how to add data, see the Getting Data In manual.
</li><li><b>Forwarding and receiving</b> lets you set up your forwarders and receivers. For more information about setting up forwarding and receiving, see the Forwarding Data manual.
</li><li><b>Indexes</b> lets you add, disable, and enable indexes.
</li><li><b>Report acceleration summaries</b> takes you to the searching and reporting app to lets you review your existing report summaries. For more information about creating report summaries, see the Knowledge Manager Manual.
</li></ul><h4><font size="3"><b><i> <a name="whatssplunkweb_manage_users_and_user_authentication"><span class="mw-headline" id="Manage_users_and_user_authentication">Manage users and user authentication</span></a></i></b></font></h4>
<p>By navigating to <b>Settings &gt; Users and Authentication &gt; Access Control</b> you can do the following:
</p>
<ul><li>Create and manage users
</li><li>Define and assign roles 
</li><li>Set up LDAP authentication strategies 
</li></ul><p>For more information about working with users and authentication, see the Securing Splunk manual.
</p>
<h4><font size="3"><b><i> <a name="whatssplunkweb_work_with_apps"><span class="mw-headline" id="Work_with_Apps">Work with Apps</span></a></i></b></font></h4>
<p>To see your installed <b>apps</b>, select <b>Apps</b> in the menu bar.
</p><p>From this page, you can select an app from a list of those you have already installed and are currently available to you. From here you can also access the following menu options:
</p>
<ul><li><b>Find more Apps</b> lets you search for and install additional apps.
</li><li><b>Manage Apps</b> lets you manage your existing apps.
</li></ul><p>You can also access all of your apps in the Home page.
</p><p>For more information about apps, see Developing views and apps for Splunk Web.
</p>
<h4><font size="3"><b><i> <a name="whatssplunkweb_manage_aspects_of_your_system"><span class="mw-headline" id="Manage_aspects_of_your_system">Manage aspects of your system</span></a></i></b></font></h4>
<p>The options under <b>Settings &gt; System</b> let you do the following:
</p>
<ul><li><b>Server settings</b> lets you manage Splunk settings like ports, host name, index paths, email server, and system logging and deployment client information. For more about configuring and managing distributed environments with Splunk Web, see the Updating Splunk Components manual.
</li><li><b>Server controls</b> lets you restart Splunk.
</li><li><b>Licensing</b> lets you manage and renew your Splunk licenses.
</li></ul><a name="splunkdefaultdashboards"></a><h2> <a name="splunkdefaultdashboards_splunk_enterprise_default_dashboards"><span class="mw-headline" id="Splunk_Enterprise_default_dashboards"> Splunk Enterprise default dashboards</span></a></h2>
<p>Splunk Enterprise comes packaged with a set of useful dashboards. They help you to  troubleshoot your system and searches and can also help you come up with ideas about how you might want to design dashboards and views of your own.
</p>
<h3> <a name="splunkdefaultdashboards_activity_dashboards"><span class="mw-headline" id="Activity_dashboards"> Activity dashboards </span></a></h3>
<p>You can find the following dashboards by clicking <b>Activity &gt; System Activity</b> in the user bar near the top of the page.
</p><p><b>Note:</b> These dashboards are visible only to users with Admin role permissions. See "Add and manage users" in <i>Securing Splunk Enterprise</i>. For information about setting up permissions for dashboards, see the <i>Knowledge Manager manual</i>.
</p><p><img alt="ActivityDashboard.png" src="images/f/f5/ActivityDashboard.png" width="700" height="195"></p>
<ul><li><b>Search activity</b> - This dashboard collection provides at-a-glance info about search activity for your Splunk instance. You can find out when searches are running, the amount of load they're putting on the system, which searches are the most popular, which search views and dashboards are getting the most usage, and more. The following dashboards are provided:
<ul><li>Search activity overview 
</li><li>Search details  
</li><li>Search user activity 
</li></ul></li><li> <b>Server activity</b> - This collection of dashboards provides metrics related to splunkd and Splunk Web performance and is handy for troubleshooting. You'll find the numbers of errors reported, lists of the most recent errors, lists of timestamping issues and unhandled exceptions, a chart displaying recent browser usage, and more. The following dashboards are provided:
<ul><li>Internal messages and errors
</li><li>License usage
</li></ul></li><li> <b>Scheduler activity</b> - This collection of dashboards gives you insight into the work of the search scheduler, which ensures that both ad hoc and scheduled searches are run in a timely manner.
<ul><li>Scheduler activity overview
</li><li>Scheduler activity by user or app
</li><li>Scheduler activity by saved search
</li><li>Scheduler errors
</li></ul></li></ul><h3> <a name="splunkdefaultdashboards_the_summary_dashboard"><span class="mw-headline" id="The_Summary_Dashboard">The Summary Dashboard</span></a></h3>
<p>The Summary dashboard is the first thing you see as you enter the Search &amp; Reporting app. It provides a search bar and time range picker which you can use to input and run your initial search. 
</p><p>When you add an input to Splunk, that input gets added relative to the app you're in. Some apps, like the *nix and Windows apps, write input data to a specific index (in the case of *nix and Windows, that is the <b>os</b> index). If you review the summary dashboard and you don't see data that you're certain is in Splunk, be sure that you're looking at the right index. 
</p><p>You may want to add the index that an app uses to the list of default indexes for the role you're using. For more information about roles, refer to this topic about roles in Securing Splunk.For more information about Summary Dashboards, see the Search Tutorial.
</p>
<a name="customizeuserexperience"></a><h2> <a name="customizeuserexperience_customize_splunk_web_banner_messages"><span class="mw-headline" id="Customize_Splunk_Web_banner_messages"> Customize Splunk Web banner messages</span></a></h2>
<table cellpadding="10" cellspacing="0" border="1" width="100%"><tr><td valign="center" align="left"> This page is currently a work in progress; expect frequent near-term updates.
</td></tr></table><p>You can add and edit notifications that display at the top of a user's page when they log in. The notification also appears under the <b>Messages</b> menu in Splunk Web. 
</p><p>You need admin or system user level privileges to add or edit a notification.
</p><p>To change or add a notification:
</p><p>1. Select <b>Settings&gt; User Interface</b>.
</p><p>2. Click <b>New</b> to create a new message, or click <b>Bulletin Messages</b> and select the message you want to edit.
</p><p>3. Edit the existing message text, or give your new message a name and message text.
</p><p>4. Click <b>Save.</b> The message will appear as a yellow banner at the top of the page. It will also appear as a note in the <b>Messages</b> menu.
</p>
<a name="specifyaproxyserver"></a><h2> <a name="specifyaproxyserver_use_splunk_web_with_a_proxy_server"><span class="mw-headline" id="Use_Splunk_Web_with_a_proxy_server"> Use Splunk Web with a proxy server</span></a></h2>
<p>When Splunk Web is located behind a proxy server, users may have trouble with Splunk Web links that access the Splunk website. For example, some Splunk Web pages link directly to the download site for Splunk apps and many "learn more" links will take you to the online documentation. 
</p><p>To resolve this, simply set the <code><font size="2">HTTP_PROXY</font></code> environment variable. For permanent results, you can specify the setting in the <code><font size="2">splunk-launch.conf</font></code> configuration file, located in <code><font size="2">$SPLUNK_HOME/etc/</font></code> on *nix systems and <code><font size="2">%SPLUNK_HOME%\etc\</font></code> on Windows.
</p><p><b>Note:</b> The App Manager is not supported for use with a proxy server, if you use a proxy server with Splunk Web, you must download and update apps manually.
</p><p>In <code><font size="2">splunk-launch.conf</font></code>, add this attribute/value pair:
</p>
<code><font size="2"><br>HTTP_PROXY = &lt;IP address or host name&gt;:&lt;port number&gt; <br></font></code>
<p>For example:
</p>
<code><font size="2"><br>HTTP_PROXY = 10.1.8.11:8787<br></font></code>
<p><b>Important:</b> If your proxy server only handles HTTPS requests, you must use the following attribute/value pair:
</p>
<code><font size="2"><br>HTTPS_PROXY = &lt;IP address or host name&gt;:&lt;port number&gt;<br></font></code>
<p>For example:
</p>
<code><font size="2"><br>HTTPS_PROXY = 10.1.8.11:8888<br></font></code>

<h1>Administer Splunk Enterprise with configuration files</h1><a name="aboutconfigurationfiles"></a><h2> <a name="aboutconfigurationfiles_about_configuration_files"><span class="mw-headline" id="About_configuration_files"> About configuration files</span></a></h2>
<p>Splunk's configuration information is stored in <b>configuration files</b>. These files are  identified by their <code><font size="2">.conf</font></code> extension and hold the information for different aspects of your Splunk configurations, including:
</p>
<ul><li>System settings
</li><li>Authentication and authorization information
</li><li>Index mappings and setting
</li><li>Deployment and cluster configurations
</li><li>Knowledge objects and saved searches
</li></ul><p>For a full list of configuration files and an overview of the area each file covers,
see <a href="#listofconfigurationfiles" class="external text">List of configuration files</a> in this manual.
</p><p>Most of your configuration files come packaged with your Splunk installation and can be found in <code><font size="2">"$SPLUNK_HOME/etc/system/default/</font></code>. 
</p>
<h3> <a name="aboutconfigurationfiles_using_splunk_web_to_manage_configuration_files"><span class="mw-headline" id="Using_Splunk_Web_to_manage_configuration_files">Using Splunk Web to manage configuration files</span></a></h3>
<p>When you change your configuration in Splunk Web, that change is written to a copy of the configuration file for that setting. Splunk makes a copy of this configuration file (if it does not already exist), writes the change to that copy, and adds it to a directory under <code><font size="2">$SPLUNK_HOME/etc/...</font></code>. The actual directory that the new file is added to depends on a number of factors, which are discussed in <a href="#configurationfiledirectories" class="external text">Configuration file directories</a>. The most common directory is <code><font size="2">$SPLUNK_HOME/etc/local</font></code> which will will use as our example below.
</p><p>If you add a new index in Splunk Web, Splunk typically does the following:
</p><p>1. Checks for a copy of the file. 
</p><p>2. If there is no copy, it creates a new copy of <code><font size="2">indexes.conf</font></code> and adds it to a directory, such as <code><font size="2">$SPLUNK_HOME/etc/local</font></code>. 
</p><p>3. Splunk writes the change to the copy of <code><font size="2">inputs.conf</font></code>.
</p><p>4. The default file is not changed, and is left alone in <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>.
</p>
<h3> <a name="aboutconfigurationfiles_editing_the_configuration_file_directly"><span class="mw-headline" id="Editing_the_configuration_file_directly">Editing the configuration file directly</span></a></h3>
<p>While you can do a lot of configuration from Splunk Web, you can also choose to edit the configuration files directly for any setting. For some more advanced customizations that Splunk Web does not support, there is no Splunk Web access to that property and you must edit the configuration files directly. 
</p><p><b>Note:</b> Editing configuration files requires more frequent restarts than making your changes in Splunk Web. For more information about restarting Splunk after editing a configuration file, see <a href="#configurationfilechangesthatrequirerestart" class="external text">"When to restart Splunk after a configuration file change."</a>
</p><p>Editing .conf files directly requires care. Never edit a default configuration file directly. Instead, create a copy and place it in a different directory, the same way Splunk Web does. 
</p><p>It's important to keep the default file intact and in its original location. We'll talk more about this in <a href="#howtoeditaconfigurationfile" class="external text">"How to edit a configuration file."</a>
</p><p>Before changing any configuration files:
</p>
<ul><li> Learn about how the default configuration files work, and where to put the copies that you edit. See <a href="#configurationfiledirectories" class="external text">"Configuration file directories."</a>
</li><li> Learn about the structure of the stanzas that comprise configuration files and how the attributes you want to edit are set up. See <a href="#configurationfilestructureandsyntax" class="external text">"Configuration file structure."</a>
</li><li> Learn how different copies of the same configuration files in different directories are layered and combined (so that you know the best place to put your copies). See <a href="#wheretofindtheconfigurationfiles" class="external text">"Configuration file precedence."</a>
</li></ul><p>Once you are familiar with the configuration file content and directory structure, and understand how to leverage Splunk's configuration file precedence, check out <a href="#howtoeditaconfigurationfile" class="external text">"How to edit a configuration file"</a> to learn how to safely modify your files.
</p>
<a name="configurationfiledirectories"></a><h2> <a name="configurationfiledirectories_configuration_file_directories"><span class="mw-headline" id="Configuration_file_directories"> Configuration file directories</span></a></h2>
<p>A single Splunk instance typically has multiple versions of configuration files across several of directories. You can have configuration files with the same names in your default, local, and app directories. This creates a layering effect that allows Splunk to determine configuration priorities based on factors such as the current user and the current app. 
</p><p>To learn more about how configurations are prioritized by Splunk, see <a href="#wheretofindtheconfigurationfiles" class="external text">"Configuration file precedence"</a>. 
</p><p><b>Note:</b> The most accurate list of settings available for a given configuration file is in the <code><font size="2">.spec</font></code> file for that configuration file. You can find the latest version of the <code><font size="2">.spec</font></code> and <code><font size="2">.example</font></code> files in the <a href="#alertactionsconf" class="external text">"Configuration file reference"</a>, or in <code><font size="2">$SPLUNK_HOME/etc/system/README</font></code>.
</p>
<h3> <a name="configurationfiledirectories_about_the_default_files"><span class="mw-headline" id="About_the_default_files">About the default files</span></a></h3>
<p><i>"all these worlds are yours, except /default - attempt no editing there"</i>
</p><p>-- duckfez, 2010
</p><p>The following is the configuration directory structure in <code><font size="2">$SPLUNK_HOME/etc</font></code>:
</p><p><code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>
</p><p>This contains the pre-configured configuration files. You should never modify the files in this directory. Instead, you should edit a copy of the file in your local or app directory:
</p>
<ul><li>The default file can also be useful should your need to roll back any changes you make to your files. 
</li><li>Splunk overwrites the default files each time you upgrade Splunk.
</li><li>Splunk always looks at the default directory last, so any attributes or stanzas that you change in one of the other configuration directories takes precedence over the default version.
</li></ul><h3> <a name="configurationfiledirectories_where_you_can_place_.28or_find.29_your_modified_configuration_files"><span class="mw-headline" id="Where_you_can_place_.28or_find.29_your_modified_configuration_files">Where you can place (or find) your modified configuration files</span></a></h3>
<p>You can layer several versions of a configuration files, with different attribute values used by Splunk according to the layering scheme described in <a href="#wheretofindtheconfigurationfiles" class="external text">"Configuration file precedence"</a>. 
</p><p>Never edit files in their default directories. Instead, create and/or edit your files in one of the configuration directories, such as <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>. These directories are not overwritten during upgrades. 
</p><p>For most deployments you can use the <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code> directory to make configuration changes. However, in certain situations you may want to work with the files in other directories. The following is the configuration directory structure in <code><font size="2">$SPLUNK_HOME/etc</font></code>:
</p>
<ul><li> <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>
<ul><li> Local changes on a site-wide basis go here; for example, settings you want to make available to all apps. If the configuration file you're looking for doesn't already exist in this directory, create it and give it write permissions.
</li></ul></li><li> <code><font size="2">$SPLUNK_HOME/etc/slave-apps/[_cluster|&lt;app_name&gt;]/[local|default]</font></code>
<ul><li> <b>For cluster peer nodes only.</b> 
</li><li> The subdirectories under <code><font size="2">$SPLUNK_HOME/etc/slave-apps</font></code> contain configuration files that are common across all peer nodes. 
</li><li> <b>Do not</b> change the content of these subdirectories on the cluster peer itself. Instead, use the cluster master to distribute any new or modified files to them. 
</li><li> The <code><font size="2">_cluster</font></code> directory contains configuration files that are not part of real apps but that still need to be identical across all peers. A typical example is the <code><font size="2">indexes.conf</font></code> file.
</li><li> For more information, see "Update common peer configurations" in the Managing Indexers and Clusters manual.
</li></ul></li><li> <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/[local|default]</font></code>
<ul><li> If you're in an app when a configuration change is made, the setting goes into a configuration file in the app's <code><font size="2">/local</font></code> directory. For example, edits for search-time settings in the default Splunk search app go here: <code><font size="2">$SPLUNK_HOME/etc/apps/search/local/</font></code>. 
</li><li> If you want to edit a configuration file so that the change only applies to a certain app, copy the file to the app's <code><font size="2">/local</font></code> directory (with write permissions) and make your changes there. 
</li></ul></li><li> <code><font size="2">$SPLUNK_HOME/etc/users</font></code> 
<ul><li> User-specific configuration changes go here.
</li></ul></li><li> <code><font size="2">$SPLUNK_HOME/etc/system/README</font></code>
<ul><li> This directory contains supporting reference documentation. For most configuration files, there are two reference files: <code><font size="2">.spec</font></code> and <code><font size="2">.example</font></code>; for example, <code><font size="2">inputs.conf.spec</font></code> and <code><font size="2">inputs.conf.example</font></code>. The <code><font size="2">.spec</font></code> file specifies the syntax, including a list of available attributes and variables. The <code><font size="2">.example</font></code> file contains examples of real-world usage.
</li></ul></li></ul><a name="configurationfilestructureandsyntax"></a><h2> <a name="configurationfilestructureandsyntax_configuration_file_structure"><span class="mw-headline" id="Configuration_file_structure"> Configuration file structure</span></a></h2>
<p>Before you edit configuration files, you should familiarize yourself with the structure of the files.
</p>
<h3> <a name="configurationfilestructureandsyntax_stanzas"><span class="mw-headline" id="Stanzas">Stanzas</span></a></h3>
<p>Configuration files consist of one or more <b>stanzas</b>, or sections. Each stanza begins with a stanza header in square brackets. This header identifies the settings held within that stanza. Each setting is an attribute value pair that specifies particular configuration settings.
</p><p>For example, <code><font size="2">inputs.conf</font></code> provides an <code><font size="2">[SSL]</font></code> that includes settings for the server certificate and password (among other things):
</p>
<div class="samplecode"><code><font size="2">[SSL]<br>serverCert = &lt;pathname&gt;<br>password = &lt;password&gt;<br></font></code></div>
<p>Depending on the stanza type, some of the attributes might be required, while others could be optional.
</p>
<h3> <a name="configurationfilestructureandsyntax_setting_up_a_new_stanza"><span class="mw-headline" id="Setting_up_a_new_stanza">Setting up a new stanza</span></a></h3>
<p>When you edit a configuration file, you might be changing the default stanza, like above, or you might need to add a brand-new stanza.
</p><p>Here's the basic pattern:
</p>
<div class="samplecode"><code><font size="2"><br>[stanza1_header]<br>&lt;attribute1&gt; = &lt;val1&gt;<br># comment <br>&lt;attribute2&gt; = &lt;val2&gt;<br>...<br><br>[stanza2_header]<br>&lt;attribute1&gt; = &lt;val1&gt;<br>&lt;attribute2&gt; = &lt;val2&gt;<br>...<br></font></code></div>
<p><b>Important:</b> Attributes are case-sensitive. For example, <code><font size="2">sourcetype = my_app</font></code> is <b>not</b> the same as <code><font size="2">SOURCETYPE = my_app</font></code>. One will work; the other won't.
</p>
<h3> <a name="configurationfilestructureandsyntax_stanza_scope"><span class="mw-headline" id="Stanza_scope">Stanza scope</span></a></h3>
<p>Configuration files frequently have stanzas with varying scopes, with the  more specific stanzas taking precedence. For example, consider this example of an  <code><font size="2">outputs.conf</font></code> configuration file, used to configure <b>forwarders</b>:
</p>
<div class="samplecode"><code><font size="2"><br>[tcpout]<br>indexAndForward=true<br>compressed=true<br><br>[tcpout:my_indexersA]<br>autoLB=true<br>compressed=false<br>server=mysplunk_indexer1:9997, mysplunk_indexer2:9997<br><br>[tcpout:my_indexersB]<br>autoLB=true<br>server=mysplunk_indexer3:9997, mysplunk_indexer4:9997<br><br></font></code></div>
<p>Note that this example file has two levels of stanzas: 
</p>
<ul><li> The global <code><font size="2">[tcpout]</font></code>, with settings that affect all tcp forwarding.
</li><li> Two  <code><font size="2">[tcpout:&lt;target_list&gt;]</font></code> stanzas, whose settings affect only the indexers defined in each target group.
</li></ul><p>The setting for <code><font size="2">compressed</font></code> in <code><font size="2">[tcpout:my_indexersA]</font></code> overrides that attribute's setting in <code><font size="2">[tcpout]</font></code>, <i>for the indexers in the my_indexersA target group only.</i>
</p><p>For more information on forwarders and <code><font size="2">outputs.conf</font></code>, see "Configure forwarders with outputs.conf".
</p>
<a name="wheretofindtheconfigurationfiles"></a><h2> <a name="wheretofindtheconfigurationfiles_configuration_file_precedence"><span class="mw-headline" id="Configuration_file_precedence"> Configuration file precedence </span></a></h2>
<p>For more information about configuration files, read "About configuration files".  
</p><p>Splunk uses <b>configuration files</b> to determine nearly every aspect of its behavior. A single Splunk instance can  have many copies of the same configuration file. These file copies are usually layered in directories that affect either the users, an <b>app</b>, or the system as a whole. 
</p><p>When editing configuration files, it is important to understand how Splunk evaluates these files and which ones take precedence.
</p><p>When incorporating changes, Splunk does the following to your configuration files:
</p>
<ul><li>Splunk merges the settings from all copies of the file, using a location-based prioritization scheme. 
</li></ul><ul><li>When different copies have conflicting attribute values (that is, when they set the same attribute to different values), Splunk uses the value from the file with the highest priority. 
</li></ul><ul><li>Splunk determines the priority of configuration files by their location in its directory structure, according to whether the file is located in a system, app, or user directory, in that order. To determine priority among the collection of apps directories, Splunk uses ASCII sort order. Files in an apps directory named "A" have a higher priority than files in an apps directory named "B", and so on.
</li></ul><p><b>Note:</b> Besides resolving configuration settings amongst multiple copies of a file, Splunk sometimes needs to resolve settings within a single file. For information on how Splunk determines precedence within a single <code><font size="2">props.conf</font></code> file, see "Attribute precedence within a single props.conf file".
</p>
<h3> <a name="wheretofindtheconfigurationfiles_about_configuration_file_context"><span class="mw-headline" id="About_configuration_file_context">About configuration file context</span></a></h3>
<p>How precedence is determined depends upon the context of the file.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_app_or_user_context_versus_global_context"><span class="mw-headline" id="App_or_user_context_versus_global_context"> App or user context versus global context</span></a></i></b></font></h4>
<p>To determine priority among copies of a configuration file, Splunk first determines the directory scheme. 
</p><p>Splunk uses two main schemes of directory precedence. 
</p>
<ul><li> App or user: Some activities, like searching, take place in an app/user context. The app/user context is vital to search-time processing, where certain knowledge objects or actions might be valid only for specific users in specific apps.
</li></ul><ul><li> Global: Activities like indexing take place in a global context. They are independent of any app or user. For example, configuration files that determine monitoring behavior occur outside of the app/user context and are global in nature.
</li></ul><h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_cluster_peer_configuration_context"><span class="mw-headline" id="Cluster_peer_configuration_context"> Cluster peer configuration context </span></a></i></b></font></h4>
<p>There's also an expanded precedence order for cluster peer node global configurations. This is because some configuration files, like <code><font size="2">indexes.conf</font></code>, must be identical across peer nodes. 
</p><p>To keep them consistent, files are managed from the cluster master, which distributes them to the peer nodes so that all peer nodes contain the same versions of the files. These files have the highest precedence in a cluster peer's configuration, which is explained in the
next section.
</p><p>For more information about how configurations are distributed across peer nodes, see "Update common peer configurations" in the Managing Indexers and Clusters manual.
</p>
<h3> <a name="wheretofindtheconfigurationfiles_how_splunk_determines_precedence_order"><span class="mw-headline" id="How_Splunk_determines_precedence_order"> How Splunk determines precedence order </span></a></h3>
<p>This subsection provides a conceptual understanding of precedence order and context. For ordered listings by directory name, see "Summary of directory order precedence", later in this topic.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_precedence_order_within_global_context:"><span class="mw-headline" id="Precedence_order_within_global_context:">Precedence order within global context:</span></a></i></b></font></h4>
<p>When the context is global (that is, where there's no app/user context), directory priority descends in this order:
</p><p><b>1.</b> System local directory  -- highest priority<br><b>2.</b> App local directories <br><b>3.</b> App default directories <br><b>4.</b> System default directory -- lowest priority
</p><p>When consuming a global configuration, such as <code><font size="2">inputs.conf</font></code>, Splunk first uses the attributes from any copy of the file in <code><font size="2">system/local</font></code>. Then it looks for any copies of the file located in the app directories, adding any attributes found in them, but ignoring attributes already discovered in system/local. As a last resort, for any attributes not explicitly assigned at either the system or app level, it assigns default values from the file in the <code><font size="2">system/default</font></code> directory.  
</p><p><b>Note:</b> As the next section describes, cluster peer nodes have an expanded order of precedence.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_precedence_for_cluster_peer_nodes"><span class="mw-headline" id="Precedence_for_cluster_peer_nodes">Precedence for cluster peer nodes</span></a></i></b></font></h4>
<p>For cluster peer nodes, the global context considers some additional peer-specific ("slave-app") directories. These directories contain apps and configurations that are identical across all peer nodes. Here is the expanded precedence order for cluster peers:
</p><p><b>1.</b> Slave-app local directories <b>(cluster peers only)</b> -- highest priority<br><b>2.</b> System local directory<br><b>3.</b> App local directories <br><b>4.</b> Slave-app default directories <b>(cluster peers only)</b> <br><b>5.</b> App default directories <br><b>6.</b> System default directory -- lowest priority
</p><p>With cluster peers, custom settings common to all the peers (those in the slave-app local directories) have the highest precedence.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_precedence_order_within_app_or_user_context"><span class="mw-headline" id="Precedence_order_within_app_or_user_context">Precedence order within app or user context</span></a></i></b></font></h4>
<p>When there's an app/user context, directory priority descends from user to app to system:
</p><p><b>1.</b> User directories for current user -- highest priority <br><b>2.</b> App directories for currently running app (local, followed by default) <br><b>3.</b>  App directories for all other apps (local, followed by default) -- for exported settings only <br><b>4.</b> System directories (local, followed by default) -- lowest priority
</p><p>An attribute in <code><font size="2">savedsearches.conf</font></code>, for example, might be set at all three levels: the user, the app, and the system. Splunk will always use the value of the user-level attribute, if any, in preference to a value for that same attribute set at the app or system level.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_how_app_directory_names_affect_precedence"><span class="mw-headline" id="How_app_directory_names_affect_precedence">How app directory names affect precedence </span></a></i></b></font></h4>
<p><b>Note:</b> For most practical purposes, the information in this subsection probably won't matter, but it might prove useful if you need to force a certain order of evaluation or for troubleshooting. 
</p><p>To determine priority among the collection of apps directories, Splunk uses ASCII sort order. Files in an apps directory named "A" have a higher priority than files in an apps directory named "B", and so on. Also, all apps starting with an uppercase letter have precedence over any apps starting with a lowercase letter, due to ASCII sort order. ("A" has precedence over "Z", but "Z" has precedence over "a", for example.) 
</p><p>In addition, numbered directories have a higher priority than alphabetical directories and are evaluated in lexicographic, not numerical, order. For example, in descending order of precedence:
</p>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/apps/myapp1<br>$SPLUNK_HOME/etc/apps/myapp10<br>$SPLUNK_HOME/etc/apps/myapp2<br>$SPLUNK_HOME/etc/apps/myapp20<br>...<br>$SPLUNK_HOME/etc/apps/myappApple<br>$SPLUNK_HOME/etc/apps/myappBanana<br>$SPLUNK_HOME/etc/apps/myappZabaglione<br>...<br>$SPLUNK_HOME/etc/apps/myappapple<br>$SPLUNK_HOME/etc/apps/myappbanana<br>$SPLUNK_HOME/etc/apps/myappzabaglione<br>...<br></font></code>
</div>
<p><b>Note:</b> When determining precedence <b>in the app/user context,</b> directories for the currently running app take priority over those for all other apps, independent of how they're named. Furthermore, other apps are only examined for exported settings.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_summary_of_directory_precedence"><span class="mw-headline" id="Summary_of_directory_precedence">Summary of directory precedence </span></a></i></b></font></h4>
<p>Putting this all together, the order of directory priority, from highest to lowest, goes like this:
</p>
<h5> <a name="wheretofindtheconfigurationfiles_global_context:"><span class="mw-headline" id="Global_context:">Global context:</span></a></h5>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/system/local/*<br><br>$SPLUNK_HOME/etc/apps/A/local/* ... $SPLUNK_HOME/etc/apps/z/local/*<br><br>$SPLUNK_HOME/etc/apps/A/default/* ... $SPLUNK_HOME/etc/apps/z/default/*<br><br>$SPLUNK_HOME/etc/system/default/*<br><br></font></code>
</div>
<h5> <a name="wheretofindtheconfigurationfiles_global_context_-_cluster_peer_nodes_only:"><span class="mw-headline" id="Global_context_-_cluster_peer_nodes_only:">Global context - cluster peer nodes only:</span></a></h5>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/slave-apps/A/local/* ... $SPLUNK_HOME/etc/slave-apps/z/local/*<br><br>$SPLUNK_HOME/etc/system/local/*<br><br>$SPLUNK_HOME/etc/apps/A/local/* ... $SPLUNK_HOME/etc/apps/z/local/*<br><br>$SPLUNK_HOME/etc/slave-apps/A/default/* ... $SPLUNK_HOME/etc/slave-apps/z/default/*<br><br>$SPLUNK_HOME/etc/apps/A/default/* ... $SPLUNK_HOME/etc/apps/z/default/*<br><br>$SPLUNK_HOME/etc/system/default/*<br><br></font></code>
</div>
<p><b>Important:</b> Within the <code><font size="2">slave-apps/[local|default]</font></code> directories, the special <code><font size="2">_cluster</font></code> subdirectory has a higher precedence than any app subdirectories starting with a lowercase letter (for example, <code><font size="2">anApp</font></code>). However, it has a <i>lower</i> precedence than any apps starting with an uppercase letter (for example, <code><font size="2">AnApp</font></code>). This is due to the location of the underscore ("_") character in the ASCII sort order.
</p>
<h5> <a name="wheretofindtheconfigurationfiles_app.2fuser_context:"><span class="mw-headline" id="App.2Fuser_context:">App/user context:</span></a></h5>
<div class="samplecode">
<code><font size="2"><br>$SPLUNK_HOME/etc/users/*<br><br>$SPLUNK_HOME/etc/apps/Current_running_app/local/*<br><br>$SPLUNK_HOME/etc/apps/Current_running_app/default/*<br><br>$SPLUNK_HOME/etc/apps/A/local/*, $SPLUNK_HOME/etc/apps/A/default/*, ... $SPLUNK_HOME/etc/apps/z/local/*, $SPLUNK_HOME/etc/apps/z/default/* (but see note below)<br><br>$SPLUNK_HOME/etc/system/local/*<br><br>$SPLUNK_HOME/etc/system/default/*<br></font></code>
</div>
<p><b>Important:</b> In the app/user context, all configuration files for the currently running app take priority over files from all other apps. This is true for the app's local <i>and</i> default directories. So, if the current context is app C, Splunk evaluates both <code><font size="2">$SPLUNK_HOME/etc/apps/C/local/*</font></code> and <code><font size="2">$SPLUNK_HOME/etc/apps/C/default/*</font></code> before evaluating the local or default directories for any other apps.  Furthermore, Splunk only looks at configuration data for other apps if that data has been exported globally through the app's <code><font size="2">default.meta</font></code> file, as described in this topic on setting app permissions. 
</p><p>Also, note that <code><font size="2">/etc/users/</font></code> is evaluated only when the particular user logs in or performs a search.
</p>
<h3> <a name="wheretofindtheconfigurationfiles_example_of_how_attribute_precedence_works"><span class="mw-headline" id="Example_of_how_attribute_precedence_works">Example of how attribute precedence works </span></a></h3>
<p>This example of attribute precedence uses <code><font size="2">props.conf</font></code>. The <code><font size="2">props.conf</font></code> file is unusual, because its context can be either global or app/user, depending on when Splunk is evaluating it. Splunk evaluates <code><font size="2">props.conf</font></code> at both index time (global) and search time (apps/user).
</p><p>Assume <code><font size="2">$SPLUNK_HOME/etc/system/local/props.conf</font></code> contains this stanza:
</p>
<div class="samplecode">
<code><font size="2"><br>[source::/opt/Locke/Logs/error*]<br>sourcetype = fatal-error<br></font></code>
</div>
<p>and <code><font size="2">$SPLUNK_HOME/etc/apps/t2rss/local/props.conf</font></code> contains another version of the same stanza:
</p>
<div class="samplecode">
<code><font size="2"><br>[source::/opt/Locke/Logs/error*]<br>sourcetype = t2rss-error<br>SHOULD_LINEMERGE = True<br>BREAK_ONLY_BEFORE_DATE = True<br></font></code>
</div>
<p>The line merging attribute assignments in <code><font size="2">t2rss</font></code> always apply, as they only occur in that version of the file.  However, there's a conflict with the <code><font size="2">sourcetype</font></code> attribute. In the <code><font size="2">/system/local</font></code> version, the <code><font size="2">sourcetype</font></code> has a value of "fatal-error". In the <code><font size="2">/apps/t2rss/local</font></code> version, it has a value of "t2rss-error". 
</p><p>Since this is a <code><font size="2">sourcetype</font></code> assignment, which gets applied at index time, Splunk uses the global context for determining directory precedence. In the global context, Splunk gives highest priority to attribute assignments in <code><font size="2">system/local</font></code>. Thus, the <code><font size="2">sourcetype</font></code> attribute gets assigned a value of "fatal-error". 
</p><p>The final, internally merged version of the file looks like this:
</p>
<div class="samplecode">
<code><font size="2"><br>[source::/opt/Locke/Logs/error*]<br>sourcetype = fatal-error<br>SHOULD_LINEMERGE = True<br>BREAK_ONLY_BEFORE_DATE = True<br></font></code>
</div>
<h3> <a name="wheretofindtheconfigurationfiles_list_of_configuration_files_and_their_context"><span class="mw-headline" id="List_of_configuration_files_and_their_context">List of configuration files and their context</span></a></h3>
<p>As mentioned, Splunk decides how to evaluate a configuration file based on the context that the file operates within, global or app/user. Generally speaking, files that  affect data input, indexing, or deployment activities are global; files that affect search activities usually have a app/user context. 
</p><p>The <code><font size="2">props.conf</font></code> and <code><font size="2">transforms.conf</font></code> files can be evaluated in either a app/user or a global context, depending on whether Splunk is using them at index or search time.
</p>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_global_configuration_files"><span class="mw-headline" id="Global_configuration_files">Global configuration files</span></a></i></b></font></h4>
<code><font size="2"><br>admon.conf<br>authentication.conf<br>authorize.conf<br>crawl.conf<br>deploymentclient.conf<br>distsearch.conf<br>indexes.conf<br>inputs.conf<br>outputs.conf<br>pdf_server.conf<br>procmonfilters.conf<br>props.conf -- global and app/user context<br>pubsub.conf<br>regmonfilters.conf<br>report_server.conf<br>restmap.conf<br>searchbnf.conf<br>segmenters.conf<br>server.conf<br>serverclass.conf<br>serverclass.seed.xml.conf<br>source-classifier.conf<br>sourcetypes.conf<br>sysmon.conf<br>tenants.conf<br>transforms.conf &nbsp;-- global and app/user context<br>user-seed.conf -- special case: Must be located in /system/default<br>web.conf<br>wmi.conf<br></font></code>
<h4><font size="3"><b><i> <a name="wheretofindtheconfigurationfiles_app.2fuser_configuration_files"><span class="mw-headline" id="App.2Fuser_configuration_files"> App/user configuration files</span></a></i></b></font></h4>
<code><font size="2"><br>alert_actions.conf<br>app.conf<br>audit.conf<br>commands.conf<br>eventdiscoverer.conf<br>event_renderers.conf<br>eventtypes.conf<br>fields.conf<br>limits.conf<br>literals.conf<br>macros.conf<br>multikv.conf<br>props.conf -- global and app/user context<br>savedsearches.conf<br>tags.conf<br>times.conf<br>transactiontypes.conf<br>transforms.conf &nbsp;-- global and app/user context<br>user-prefs.conf<br>workflow_actions.conf<br></font></code>
<h3> <a name="wheretofindtheconfigurationfiles_troubleshooting_configuration_precedence_and_other_issues"><span class="mw-headline" id="Troubleshooting_configuration_precedence_and_other_issues"> Troubleshooting configuration precedence and other issues</span></a></h3>
<p>Splunk's configuration file system supports many overlapping configuration files in many different locations. The price of this level of flexibility is that figuring out which value for which configuration option is being used in your Splunk installation can sometimes be quite complex. If you're looking for some tips on figuring out what configuration setting is being used in a given situation, read "Use btool to troubleshoot configurations" in the Troubleshooting Manual.
</p>
<a name="attributeprecedencewithinafile"></a><h2> <a name="attributeprecedencewithinafile_attribute_precedence_within_a_single_props.conf_file"><span class="mw-headline" id="Attribute_precedence_within_a_single_props.conf_file"> Attribute precedence within a single props.conf file </span></a></h2>
<p>In addition to understanding <a href="#wheretofindtheconfigurationfiles" class="external text">how attribute precedence works across files</a>, you also sometimes need to consider attribute priority within a single <a href="#propsconf" class="external text">props.conf</a> file.
</p>
<h3> <a name="attributeprecedencewithinafile_precedence_within_sets_of_stanzas_affecting_the_same_target"><span class="mw-headline" id="Precedence_within_sets_of_stanzas_affecting_the_same_target"> Precedence within sets of stanzas affecting the same target </span></a></h3>
<p>When two or more <b>stanzas</b> specify a behavior that affects the same item, items are evaluated by the stanzas' ASCII order. For example, assume you specify in <code><font size="2">props.conf</font></code> the following stanzas:
</p>
<code><font size="2"><br>[source::.../bar/baz]<br>attr = val1<br><br>[source::.../bar/*]<br>attr = val2<br></font></code>
<p>The second stanza's value for <code><font size="2">attr</font></code> will  be used, because its path is higher in the ASCII order and takes precedence.
</p>
<h3> <a name="attributeprecedencewithinafile_overriding_default_attribute_priority_in_props.conf"><span class="mw-headline" id="Overriding_default_attribute_priority_in_props.conf">Overriding default attribute priority in props.conf</span></a></h3>
<p>There's a way to override the default ASCII priority in <code><font size="2">props.conf</font></code>. Use the <code><font size="2">priority</font></code> key to specify a higher or lower priority for a given stanza. 
</p><p>For example, suppose we have a source:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;source::az<br></font></code>
<p>and the following patterns:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...a...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = a<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...z...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = z<br></font></code>
<p>In this case, the default behavior is that the settings provided by the pattern "source::...a..." take precedence over those provided by "source::...z...". Thus, sourcetype will have the value "a".
</p><p>To override this default ASCII ordering, use the <code><font size="2">priority</font></code> key:
</p>
<code><font size="2"><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...a...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = a<br>&nbsp;&nbsp;&nbsp;&nbsp;priority = 5<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...z...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = z<br>&nbsp;&nbsp;&nbsp;&nbsp;priority = 10<br></font></code>
<p>Assigning a higher priority to the second stanza causes <code><font size="2">sourcetype</font></code> to have the value "z".
</p><p>There's another attribute precedence issue to consider. By default, stanzas that match a string literally ("literal-matching stanzas") take precedence over regex pattern-matching stanzas. This is due to the default values of their <code><font size="2">priority</font></code> keys:
</p>
<ul><li> 0 is the default for pattern-matching stanzas
</li><li> 100 is the default for literal-matching stanzas 
</li></ul><p>So, literal-matching stanzas will always take precedence over pattern-matching stanzas, unless you change that behavior by explicitly setting their <code><font size="2">priority</font></code> keys.
</p><p>You can use the <code><font size="2">priority</font></code> key to resolve collisions between patterns of the same type, such as <code><font size="2">sourcetype</font></code> patterns or <code><font size="2">host</font></code> patterns. The <code><font size="2">priority</font></code> key does not, however, affect precedence across spec types. For example, <code><font size="2">source</font></code> patterns take priority over <code><font size="2">host</font></code> and <code><font size="2">sourcetype</font></code> patterns, regardless of priority key values.
</p>
<h3> <a name="attributeprecedencewithinafile_precedence_for_events_with_multiple_attribute_assignments"><span class="mw-headline" id="Precedence_for_events_with_multiple_attribute_assignments"> Precedence for events with multiple attribute assignments </span></a></h3>
<p>The <code><font size="2">props.conf</font></code> file sets attributes for processing individual events by host, source, or sourcetype (and sometimes event type).  So it's possible for one event to have the same attribute set differently for the <b>default fields</b>: host, source or sourcetype.  The precedence order is:
</p>
<ul><li> source
</li><li> host 
</li><li> sourcetype
</li></ul><p>You might want to override the default <code><font size="2">props.conf</font></code> settings. For example, assume you are tailing <code><font size="2">mylogfile.xml</font></code>, which by default is labeled <code><font size="2">sourcetype = xml_file</font></code>. This configuration will re-index the entire file whenever it changes, even if you manually specify another sourcetype, because the property is set by source. To override this, add the explicit configuration by source:
</p>
<code><font size="2"><br>[source::/var/log/mylogfile.xml]<br>CHECK_METHOD = endpoint_md5<br></font></code>

<a name="howtoeditaconfigurationfile"></a><h2> <a name="howtoeditaconfigurationfile_how_to_copy_and_edit_a_configuration_file"><span class="mw-headline" id="How_to_copy_and_edit_a_configuration_file"> How to copy and edit a configuration file</span></a></h2>
<p>Before you edit a configuration file, make sure you are familiar with the following:
</p>
<ul><li> To learn about the default configuration files, where they live, and where to put the ones you edit, see <a href="#configurationfiledirectories" class="external text">"Configuration file directories."</a>
</li><li> To learn about file structure and how the attributes you want to edit are set up, see <a href="#configurationfilestructureandsyntax" class="external text">"Configuration file structure."</a>
</li><li> To learn how configuration files across multiple directories are layers and combines, see <a href="#wheretofindtheconfigurationfiles" class="external text">"Configuration file precedence."</a>
</li></ul><h3> <a name="howtoeditaconfigurationfile_copy_the_default_file"><span class="mw-headline" id="Copy_the_default_file">Copy the default file</span></a></h3>
<p>When you decide to customize an attribute in a file, you create a copy of the file in your preferred directory, such as <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>. <b>Do not</b> copy the entire contents of the default configuration file, only copy the attribute(s) you want to customize. This is to ensure that any Splunk updates to default values are properly distributed.
</p><p><b>Note:</b> Make sure that when you copy your file to the local directory, you give it "write" permissions.
</p><p>For example, suppose that you want to use a self-signed SSL certificate, and you need to redirect the path to the new certificate in <code><font size="2">server.conf</font></code>. You copy the &lt;SSL&gt; stanza into your new copy of server.conf as well as a few other stanzas, and edit only the SSL stanza, leaving the other stanzas at their default value. 
</p><p>In a later release, Splunk changes one of the default port settings in server.conf, which is one of the stanzas you copied but did not edit. 
</p><p>The next time you upgrade Splunk, the new version of server.conf with the attribute values overwrites the version of server.conf in the default directory. However, because Splunk gives precedence to the configuration settings of the version in the local directory, the new port written to the default directory is not applied.
</p><p><b>Note:</b> Some configuration files do not have default versions. These configuration files still have <code><font size="2">.spec</font></code> and <code><font size="2">.example</font></code> files you use for creating copies.
</p>
<h3> <a name="howtoeditaconfigurationfile_clearing_attributes"><span class="mw-headline" id="Clearing_attributes"> Clearing attributes </span></a></h3>
<p>You can clear any attribute by setting it to null. For example:
</p>
<code><font size="2"><br>forwardedindex.0.whitelist = <br></font></code>
<p>This overrides any previous value that the attribute held, including any value set in its default file, causing the system to consider the value entirely unset.
</p>
<h3> <a name="howtoeditaconfigurationfile_using_comments"><span class="mw-headline" id="Using_comments">Using comments</span></a></h3>
<p>You can insert comments in configuration files. To do so, use the # sign:
</p>
<code><font size="2"><br># This stanza forwards some log files.<br>[monitor:///var/log]<br></font></code>
<p><b>Important:</b> Start the comment at the left margin. Do not put the comment on the same line as the stanza or attribute:
</p>
<code><font size="2"><br>[monitor:///var/log] &nbsp;&nbsp;&nbsp;# This is a really bad place to put your comment.<br></font></code>
<p>For an attribute, such as 
</p>
<code><font size="2"><br>a_setting = 5 &nbsp;#5 is the best number<br></font></code>
<p>This sets the a_setting attribute to the value "5  #5 is the best number", which may cause unexpected results.
</p>
<h3> <a name="howtoeditaconfigurationfile_creating_and_editing_configuration_files_on_non-utf-8_operating_systems"><span class="mw-headline" id="Creating_and_editing_configuration_files_on_non-UTF-8_operating_systems"> Creating and editing configuration files on non-UTF-8 operating systems </span></a></h3>
<p>Splunk expects configuration files to be in ASCII/UTF-8. If you are editing or creating a configuration file on an operating system that is non-UTF-8, you must ensure that the editor you are using is configured to save in ASCII/UTF-8.
</p>
<a name="configurationfilechangesthatrequirerestart"></a><h2> <a name="configurationfilechangesthatrequirerestart_when_to_restart_splunk_after_a_configuration_file_change"><span class="mw-headline" id="When_to_restart_Splunk_after_a_configuration_file_change"> When to restart Splunk after a configuration file change</span></a></h2>
<table cellpadding="10" cellspacing="0" border="1" width="100%"><tr><td valign="center" align="left"> This page is currently a work in progress; expect frequent near-term updates.
</td></tr></table><p>When you make changes to Splunk via the configuration files, you might need to restart Splunk to see those changes picked up by the system. 
</p><p><b>Note:</b> Changes made in Splunk Web are less likely to require restarts. This is because Splunk Web automatically updates the underlying configuration file(s) and notifies the running Splunk instance (splunkd) of the changes.
</p><p>This topic provides guidelines to help you determine whether to restart after a change. Whether a change requires a restart depends on a number of factors, and this topic does not provide a definitive authority. Always check the configuration file or its reference topic to see whether a particular change requires a restart.  For a full list of configuration files and an overview of the area each file covers,
see <a href="#listofconfigurationfiles" class="external text">List of configuration files</a> in this manual.
</p>
<h3> <a name="configurationfilechangesthatrequirerestart_when_to_restart_forwarders"><span class="mw-headline" id="When_to_restart_forwarders"> When to restart forwarders </span></a></h3>
<p>If you make a configuration file change to a heavy forwarder, you must restart the forwarder, but you do not need to restart the receiving indexer. If the changes are part of a deployed app already configured to restart after changes, then the forwarder restarts automatically.
</p>
<h3> <a name="configurationfilechangesthatrequirerestart_when_to_restart_splunk_web"><span class="mw-headline" id="When_to_restart_Splunk_Web"> When to restart Splunk Web</span></a></h3>
<p>You must restart Splunk Web to enable or disable SSL for Splunk Web access.
</p>
<h3> <a name="configurationfilechangesthatrequirerestart_when_to_restart_splunkd"><span class="mw-headline" id="When_to_restart_Splunkd"> When to restart Splunkd</span></a></h3>
<p>As a general rule, anything that modifies:
</p>
<ul><li> Settings and properties that affect indexers and indexing behavior 
</li><li> Settings and properties that affect users and roles.
</li><li> Settings and properties that affect Splunk's core configuration.
</li></ul><h5> <a name="configurationfilechangesthatrequirerestart_index_changes"><span class="mw-headline" id="Index_changes">Index changes</span></a></h5>
<p><b>Note:</b> When settings which affect indexing are made through the UI and CLI they do not require restarts and take place immediately.
</p>
<ul><li> Index time field extractions
</li><li> Time stamp properties
</li></ul><h5> <a name="configurationfilechangesthatrequirerestart_user_and_role_changes"><span class="mw-headline" id="User_and_role_changes">User and role changes</span></a></h5>
<p>Any user and role changes made in configuration files require a restart, including:
</p>
<ul><li> LDAP configurations (If you make these changes in Splunk Web you can reload the changes without restarting.)
</li><li> Password changes
</li><li> Changes to role capabilities
</li><li> Splunk Native authentication changes, such as user-to-role mappings.
</li></ul><h5> <a name="configurationfilechangesthatrequirerestart_system_changes"><span class="mw-headline" id="System_changes">System changes</span></a></h5>
<p>Things which affect the system settings or server state require restart.
</p>
<ul><li> Licensing changes
</li><li> Web server configuration updates
</li><li> Changes to general indexer settings (minimum free disk space, default server name, etc.)
</li><li> Changes to General Settings (eg., port settings)
</li><li> Changing a forwarder's output settings
</li><li> Changing the timezone in the OS of a splunk server (Splunk retrieves its local timezone from the underlying OS at startup)
</li><li> Creating a pool of search heads
</li><li> Installing some apps may require a restart. Consult the documentation for each app you are installing.
</li><li> Props and transforms that do not hit the following endpoints:
<ul><li> /configs/conf-props/_reload
</li><li> /configs/conf-transforms/_reload
</li><li> /admin/transforms-reload
</li></ul></li></ul><h4><font size="3"><b><i> <a name="configurationfilechangesthatrequirerestart_splunk_changes_that_do_not_require_a_restart"><span class="mw-headline" id="Splunk_changes_that_do_not_require_a_restart"> Splunk changes that do not require a restart</span></a></i></b></font></h4>
<p>Settings which apply to search-time processing take effect immediately and do not require a restart. This is because searches run in a separate process that reloads configurations. For example, lookup tables, tags and event types are re-read for each search.
</p><p>This includes (but is not limited to) changes to: 
</p>
<ul><li>Lookup tables
</li><li>Field extractions
</li><li>Knowledge objects
</li><li>Tags
</li><li>Event types
</li><li>Props and transforms that hit the following endpoints:
<ul><li> /configs/conf-props/_reload
</li><li> /configs/conf-transforms/_reload
</li><li> /admin/transforms-reload
</li></ul></li></ul><p>Files that contain search-time operations include (but are not limited to):
</p>
<ul><li><code><font size="2">macros.conf</font></code> 
</li><li><code><font size="2">props.conf</font></code> Changes to search-time field extractions are re-read at search time
</li><li><code><font size="2">transforms.conf</font></code>
</li><li><code><font size="2">savedsearches.conf</font></code> (If a change creates an endpoint you must restart.)
</li></ul><p><br></p>
<h3> <a name="configurationfilechangesthatrequirerestart_learn_more:"><span class="mw-headline" id="Learn_More:">Learn More:</span></a></h3>
<ul><li> When and how to restart clusters
</li><li> Update peer configurations
</li></ul><a name="listofconfigurationfiles"></a><h2> <a name="listofconfigurationfiles_list_of_configuration_files"><span class="mw-headline" id="List_of_configuration_files"> List of configuration files</span></a></h2>
<p>The following is a list of the available spec and example files associated with each conf file. Some conf files do not have spec or example files; contact Support before editing a conf file that does not have an accompanying spec or example file. 
</p><p><b>Important:</b> Do not edit the default copy of any conf file in <code><font size="2">$SPLUNK_HOME/etc/system/default/</font></code>. Make a copy of the file in  <code><font size="2">$SPLUNK_HOME/etc/system/local/</font></code> or <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local</font></code> and edit that copy.  
</p>
<table cellpadding="6"><tr><td valign="center" align="left">File</td><td valign="center" align="left">Purpose
</td></tr><tr><td valign="center" align="left">alert_actions.conf</td><td valign="center" align="left">Create an alert.
</td></tr><tr><td valign="center" align="left">app.conf</td><td valign="center" align="left">Configure your custom app.
</td></tr><tr><td valign="center" align="left">audit.conf</td><td valign="center" align="left">Configure auditing and event hashing. This feature is not available for this release.
</td></tr><tr><td valign="center" align="left">authentication.conf</td><td valign="center" align="left">Toggle between Splunk's built-in authentication or LDAP, and configure LDAP.
</td></tr><tr><td valign="center" align="left">authorize.conf</td><td valign="center" align="left">Configure roles, including granular access controls.
</td></tr><tr><td valign="center" align="left">commands.conf</td><td valign="center" align="left">Connect search commands to any custom search script.
</td></tr><tr><td valign="center" align="left">crawl.conf</td><td valign="center" align="left">Configure crawl to find new data sources.
</td></tr><tr><td valign="center" align="left">default.meta.conf</td><td valign="center" align="left">A template file for use in creating app-specific default.meta files.
</td></tr><tr><td valign="center" align="left">deploymentclient.conf</td><td valign="center" align="left">Specify behavior for clients of the deployment server.
</td></tr><tr><td valign="center" align="left">distsearch.conf</td><td valign="center" align="left">Specify behavior for distributed search.
</td></tr><tr><td valign="center" align="left">eventdiscoverer.conf</td><td valign="center" align="left">Set terms to ignore for typelearner (event discovery).
</td></tr><tr><td valign="center" align="left">event_renderers.conf</td><td valign="center" align="left">Configure event-rendering properties.
</td></tr><tr><td valign="center" align="left">eventtypes.conf</td><td valign="center" align="left">Create event type definitions.
</td></tr><tr><td valign="center" align="left">fields.conf</td><td valign="center" align="left">Create multivalue fields and add search capability for indexed fields.
</td></tr><tr><td valign="center" align="left">indexes.conf</td><td valign="center" align="left">Manage and configure index settings.
</td></tr><tr><td valign="center" align="left">inputs.conf</td><td valign="center" align="left">Set up data inputs.
</td></tr><tr><td valign="center" align="left"><a href="#instancecfgconf" class="external text">instance.cfg.conf</a></td><td valign="center" align="left">Designate and manage settings for specific instances of Splunk. This can be handy, for example, when identifying forwarders for internal searches.
</td></tr><tr><td valign="center" align="left">limits.conf</td><td valign="center" align="left">Set various limits (such as maximum result size or concurrent real-time searches) for search commands.
</td></tr><tr><td valign="center" align="left">literals.conf</td><td valign="center" align="left">Customize the text, such as search error strings, displayed in Splunk Web.
</td></tr><tr><td valign="center" align="left">macros.conf</td><td valign="center" align="left">Create and use search macros.
</td></tr><tr><td valign="center" align="left">multikv.conf</td><td valign="center" align="left">Configure extraction rules for table-like events (ps, netstat, ls).
</td></tr><tr><td valign="center" align="left">outputs.conf</td><td valign="center" align="left">Set up forwarding behavior.
</td></tr><tr><td valign="center" align="left">pdf_server.conf</td><td valign="center" align="left">Configure the Splunk PDF Server. The PDF Server app was deprecated in Splunk Enterprise 6.0. The feature was removed in Splunk Enterprise 6.2.
</td></tr><tr><td valign="center" align="left">procmon-filters.conf</td><td valign="center" align="left">Monitor Windows process data.
</td></tr><tr><td valign="center" align="left">props.conf</td><td valign="center" align="left">Set indexing property configurations, including timezone offset, custom source type rules, and pattern collision priorities.  Also, map transforms to event properties.
</td></tr><tr><td valign="center" align="left">pubsub.conf</td><td valign="center" align="left">Define a custom client of the deployment server.
</td></tr><tr><td valign="center" align="left">restmap.conf</td><td valign="center" align="left">Create custom REST endpoints.
</td></tr><tr><td valign="center" align="left">savedsearches.conf</td><td valign="center" align="left">Define  ordinary reports, scheduled reports, and alerts.
</td></tr><tr><td valign="center" align="left">searchbnf.conf</td><td valign="center" align="left">Configure the search assistant.
</td></tr><tr><td valign="center" align="left">segmenters.conf</td><td valign="center" align="left">Configure segmentation.
</td></tr><tr><td valign="center" align="left">server.conf</td><td valign="center" align="left">Enable SSL for Splunk's back-end (communications between Splunkd and Splunk Web) and specify certification locations.
</td></tr><tr><td valign="center" align="left">serverclass.conf</td><td valign="center" align="left">Define deployment server classes for use with deployment server.
</td></tr><tr><td valign="center" align="left">serverclass.seed.xml.conf</td><td valign="center" align="left">Configure how to seed a deployment client with apps at start-up time.
</td></tr><tr><td valign="center" align="left">source-classifier.conf</td><td valign="center" align="left">Terms to ignore (such as sensitive data) when creating a source type.
</td></tr><tr><td valign="center" align="left">sourcetypes.conf</td><td valign="center" align="left">Machine-generated file that stores source type learning rules.
</td></tr><tr><td valign="center" align="left">tags.conf</td><td valign="center" align="left">Configure tags for fields.
</td></tr><tr><td valign="center" align="left">tenants.conf</td><td valign="center" align="left">Configure deployments in multi-tenant environments (deprecated).
</td></tr><tr><td valign="center" align="left">times.conf</td><td valign="center" align="left">Define custom time ranges for use in the Search app.
</td></tr><tr><td valign="center" align="left">transactiontypes.conf</td><td valign="center" align="left">Add additional transaction types for transaction search.
</td></tr><tr><td valign="center" align="left">transforms.conf</td><td valign="center" align="left">Configure regex transformations to perform on data inputs. Use in tandem with props.conf.
</td></tr><tr><td valign="center" align="left">user-seed.conf</td><td valign="center" align="left">Set a default user and password.
</td></tr><tr><td valign="center" align="left"><a href="#viewstatesconf" class="external text">viewstates.conf</a></td><td valign="center" align="left">Use this file to set up IU views (such as charts) in Splunk.
</td></tr><tr><td valign="center" align="left">web.conf</td><td valign="center" align="left">Configure Splunk Web, enable HTTPS.
</td></tr><tr><td valign="center" align="left">wmi.conf</td><td valign="center" align="left">Set up Windows management instrumentation (WMI) inputs.
</td></tr><tr><td valign="center" align="left">workflow_actions.conf</td><td valign="center" align="left">Configure workflow actions.
</td></tr></table><a name="configurationparametersandthedatapipeline"></a><h2> <a name="configurationparametersandthedatapipeline_configuration_parameters_and_the_data_pipeline"><span class="mw-headline" id="Configuration_parameters_and_the_data_pipeline"> Configuration parameters and the data pipeline</span></a></h2>
<p>Data goes through several phases as it transitions from raw input to searchable events. This process is called the <b>data pipeline</b> and consists of four phases:
</p>
<ul><li> <b>Input</b> 
</li><li> <b>Parsing</b>
</li><li> <b>Indexing</b> 
</li><li> <b>Search</b> 
</li></ul><p>Each phase of the data pipeline relies on different configuration file parameters. Knowing which phase uses a particular parameter allows you to identify where in your Splunk deployment topology you need to set the parameter. 
</p>
<h3> <a name="configurationparametersandthedatapipeline_what_the_data_pipeline_looks_like"><span class="mw-headline" id="What_the_data_pipeline_looks_like">What the data pipeline looks like</span></a></h3>
<p>This diagram outlines the data pipeline:
</p><p><img alt="Datapipeline1 60.png" src="images/5/5e/Datapipeline1_60.png" width="700" height="924"></p><p>The Distributed Deployment manual describes the data pipeline in detail, in "How data moves through Splunk: the data pipeline". 
</p>
<h3> <a name="configurationparametersandthedatapipeline_how_splunk_enterprise_components_correlate_to_phases_of_the_pipeline"><span class="mw-headline" id="How_Splunk_Enterprise_components_correlate_to_phases_of_the_pipeline"> How Splunk Enterprise components correlate to phases of the pipeline </span></a></h3>
<p>One or more Splunk Enterprise components can perform each of the pipeline phases. For example, a universal forwarder, a heavy forwarder, or an indexer can perform the input phase.  
</p><p>Data only goes through each phase once, so each configuration belongs on only one component, specifically, the first component in the deployment that handles that phase.  For example, say you have data entering the system through a set of universal forwarders, which forward the data to an intermediate heavy forwarder, which then forwards the data onwards to an indexer. In that case, the input phase for that data occurs on the universal forwarders, and the parsing phase occurs on the heavy forwarder.  
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Data pipeline phase
</th><th bgcolor="#C0C0C0"> Components that can perform this role
</th></tr><tr><td valign="center" align="left"> <b>Input</b>
</td><td valign="center" align="left"> <b>indexer</b><br><b>universal forwarder</b><br><b>heavy forwarder</b>
</td></tr><tr><td valign="center" align="left"> <b>Parsing</b>
</td><td valign="center" align="left"> indexer<br>heavy forwarder<br>light/universal forwarder (in conjunction with the <code><font size="2">INDEXED_EXTRACTIONS</font></code> attribute only)
</td></tr><tr><td valign="center" align="left"> <b>Indexing</b>
</td><td valign="center" align="left"> indexer
</td></tr><tr><td valign="center" align="left"> <b>Search</b>
</td><td valign="center" align="left"> indexer<br><b>search head</b>
</td></tr></table><p>Where to set a configuration parameter depends on the components in your specific deployment.  For example, you set parsing parameters on the indexers in most cases.  But if you have heavy forwarders feeding data to the indexers, you instead set parsing parameters on the heavy forwarders. Similarly, you set search parameters on the search heads, if any. But if you aren't deploying dedicated search heads, you set the search parameters on the indexers.
</p><p>For more information, see "Components and roles" in the Distributed Deployment Manual.
</p>
<h3> <a name="configurationparametersandthedatapipeline_how_configuration_parameters_correlate_to_phases_of_the_pipeline"><span class="mw-headline" id="How_configuration_parameters_correlate_to_phases_of_the_pipeline">How configuration parameters correlate to phases of the pipeline</span></a></h3>
<p>This is a non-exhaustive list of configuration parameters and the pipeline phases that use them. By combining this information with an understanding of which Splunk component in your particular deployment performs each phase, you can determine where to configure each setting. 
</p><p>For example, if you are using universal forwarders to consume inputs, you need to configure  <code><font size="2">inputs.conf</font></code> parameters on the forwarders. If, however, your indexer is directly consuming network inputs, you need to configure those network-related <code><font size="2">inputs.conf</font></code> parameters on the indexer.
</p>
<h4><font size="3"><b><i> <a name="configurationparametersandthedatapipeline_input_phase"><span class="mw-headline" id="Input_phase">Input phase</span></a></i></b></font></h4>
<ul><li> inputs.conf
</li><li> props.conf
<ul><li> CHARSET
</li><li> NO_BINARY_CHECK
</li><li> CHECK_METHOD
</li><li> sourcetype
</li></ul></li><li> wmi.conf
</li><li> regmon-filters.conf
</li></ul><h4><font size="3"><b><i> <a name="configurationparametersandthedatapipeline_parsing_phase"><span class="mw-headline" id="Parsing_phase">Parsing phase</span></a></i></b></font></h4>
<ul><li> props.conf
<ul><li> LINE_BREAKER, SHOULD_LINEMERGE, BREAK_ONLY_BEFORE_DATE, and all other line merging settings
</li><li> TZ, DATETIME_CONFIG, TIME_FORMAT, TIME_PREFIX, and all other time extraction settings and rules
</li><li> TRANSFORMS* which includes per-event queue filtering, per-event index assignment, per-event routing.  Applied in the order defined
</li><li> SEDCMD*
</li><li> MORE_THAN*, LESS_THAN*
</li><li> INDEXED_EXTRACTIONS
</li></ul></li><li> transforms.conf
<ul><li> stanzas referenced by a TRANSFORMS* clause in props.conf
</li><li> LOOKAHEAD, DEST_KEY, WRITE_META, DEFAULT_VALUE, REPEAT_MATCH
</li></ul></li><li> datetime.xml
</li></ul><h4><font size="3"><b><i> <a name="configurationparametersandthedatapipeline_indexing_phase"><span class="mw-headline" id="Indexing_phase">Indexing phase</span></a></i></b></font></h4>
<ul><li> props.conf
<ul><li> SEGMENTATION*
</li></ul></li><li> indexes.conf
</li><li> segmenters.conf
</li></ul><h4><font size="3"><b><i> <a name="configurationparametersandthedatapipeline_search_phase"><span class="mw-headline" id="Search_phase">Search phase</span></a></i></b></font></h4>
<ul><li> props.conf
<ul><li> EXTRACT*
</li><li> REPORT*
</li><li> LOOKUP*
</li><li> KV_MODE
</li><li> FIELDALIAS*
</li><li> rename
</li></ul></li><li> transforms.conf
<ul><li> stanzas referenced by a REPORT* clause in props.conf
</li><li> filename, external_cmd, and all other lookup-related settings
</li><li> FIELDS, DELIMS
</li><li> MV_ADD
</li></ul></li><li> lookup files in the lookups folders
</li><li> search and lookup scripts in the bin folders
</li><li> search commands and lookup scripts
</li><li> savedsearches.conf
</li><li> eventtypes.conf
</li><li> tags.conf
</li><li> commands.conf
</li><li> alert_actions.conf
</li><li> macros.conf
</li><li> fields.conf
</li><li> transactiontypes.conf
</li><li> multikv.conf
</li></ul><h4><font size="3"><b><i> <a name="configurationparametersandthedatapipeline_other_configuration_settings"><span class="mw-headline" id="Other_configuration_settings">Other configuration settings</span></a></i></b></font></h4>
<p>There are some settings that don't work well in a distributed Splunk environment. These tend to be exceptional and include:
</p>
<ul><li> props.conf
<ul><li> CHECK_FOR_HEADER, LEARN_MODEL, maxDist. These are created in the parsing phase, but they require generated configurations to be moved to the search phase configuration location.
</li></ul></li></ul><a name="backupconfigurations"></a><h2> <a name="backupconfigurations_back_up_configuration_information"><span class="mw-headline" id="Back_up_configuration_information"> Back up configuration information</span></a></h2>
<p>All Splunk's configuration information is contained in <b>configuration files</b>. To back up the set of configuration files, make an archive or copy of <code><font size="2">$SPLUNK_HOME/etc/</font></code>.  This directory, along with its subdirectories, contains all the default and custom settings for your Splunk install, and all apps, including  saved searches, user accounts, tags, custom source type names, and other configuration information. 
</p><p>Copy this directory to a new Splunk instance to restore. You don't have to stop Splunk to do this.  
</p><p>For more information about configuration files, read <a href="#aboutconfigurationfiles" class="external text">"About configuration files"</a>. 
</p>
<h3> <a name="backupconfigurations_back_up_the_cluster_master_node"><span class="mw-headline" id="Back_up_the_cluster_master_node"> Back up the cluster master node</span></a></h3>
<p>If you're using <b>index replication</b>, you can back up the master node's static configuration.  This is of particular use when configuring a stand-by master that can take over if the primary master fails. For details, see "Configure the master" in the Managing Indexers and Clusters manual.
</p>
<h1>Administer Splunk Enterprise with the command line interface (CLI)</h1><a name="aboutthecli"></a><h2> <a name="aboutthecli_about_the_cli"><span class="mw-headline" id="About_the_CLI"> About the CLI</span></a></h2>
<p>You can use the Splunk Enterprise command line interface (CLI) to monitor, configure, and execute searches on your Splunk server.  The CLI help exists in the product and is accessible through a terminal or shell interface. This topic discusses how to access this information and what commands are available. 
</p>
<h3> <a name="aboutthecli_how_to_access_the_cli"><span class="mw-headline" id="How_to_access_the_CLI"> How to access the CLI </span></a></h3>
<p>The CLI is located in <code><font size="2">$SPLUNK_HOME/bin/splunk</font></code>.
</p><p>To access Splunk CLI, you need either:
</p>
<ul><li> Shell access to a Splunk server, or
</li><li> Permission to access the correct port on a remote Splunk server.
</li></ul><p>If you have administrator or root privileges, you can simplify CLI access by adding the top level directory of your Splunk installation, <code><font size="2">$SPLUNK_HOME/bin</font></code> , to your shell path.   
</p><p>This example works for Linux/BSD/Solaris users who installed Splunk in the default location:
</p>
<code><font size="2"><br># export SPLUNK_HOME=/opt/splunk<br># export PATH=$SPLUNK_HOME/bin:$PATH<br></font></code>
<p>This example works for Mac users who installed splunk in the default location:
</p>
<code><font size="2"><br># export SPLUNK_HOME=/Applications/Splunk<br># export PATH=$SPLUNK_HOME/bin:$PATH<br></font></code>
<p>Now you can invoke CLI commands using:
</p>
<div class="samplecode">
<p>./splunk &lt;command&gt;
</p>
</div>
<p><br>
To set the <code><font size="2">$SPLUNK_HOME</font></code> environment variable while working in a CLI session:
</p>
<ul><li> In *nix: <code><font size="2">source /opt/splunk/bin/setSplunkEnv</font></code>
</li></ul><ul><li> In Windows: <code><font size="2">splunk.exe envvars &gt; setSplunkEnv.bat &amp; setSplunkEnv.bat</font></code>
</li></ul><h3> <a name="aboutthecli_cli_help_documentation"><span class="mw-headline" id="CLI_help_documentation"> CLI help documentation </span></a></h3>
<p>If you have administrator privileges, you can use the CLI not only to search but also to configure and monitor your Splunk server (or servers). The CLI commands used for configuring and monitoring Splunk are not search commands. Search commands are arguments to the <code><font size="2">search</font></code> and <code><font size="2">dispatch</font></code> CLI commands. Some commands require you to authenticate with a username and password or specify a target Splunk server. 
</p><p>You can look up help information for the CLI using:
</p>
<div class="samplecode">
<p>./splunk help
</p>
</div>
<p>For more information about how to access help for specific CLI commands or tasks, see <a href="#gethelpwiththecli" class="external text">"Get help with the CLI"</a> and <a href="#cliadmincommands" class="external text">"Administrative CLI commands"</a> in this manual.
</p>
<h3> <a name="aboutthecli_note_for_mac_users"><span class="mw-headline" id="Note_for_Mac_users"> Note for Mac users </span></a></h3>
<p>Mac OS X requires you to have superuser level access to run any command that accesses system files or directories.  Run CLI commands using <b>sudo</b> or "su -" for a new shell as root. The recommended method is to use sudo. (By default the user "root" is not enabled but any administrator user can use sudo.)
</p>
<h3> <a name="aboutthecli_working_with_the_cli_on_windows"><span class="mw-headline" id="Working_with_the_CLI_on_Windows">Working with the CLI on Windows</span></a></h3>
<p>To access and use CLI commands on Windows, run <code><font size="2">cmd.exe</font></code> as administrator first. Also, If you're using Windows, Splunk does not require the "./" to run CLI commands.
</p>
<h3> <a name="aboutthecli_answers"><span class="mw-headline" id="Answers">Answers</span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has around using the CLI.
</p>
<a name="gethelpwiththecli"></a><h2> <a name="gethelpwiththecli_get_help_with_the_cli"><span class="mw-headline" id="Get_help_with_the_CLI"> Get help with the CLI</span></a></h2>
<p>This topic discusses how to access Splunk's built-in CLI help reference, which contains information about the CLI commands and how to use them. This topic also briefly discusses the universal parameters, which are parameters that you can use with any CLI command. 
</p>
<h3> <a name="gethelpwiththecli_access_cli_help_reference"><span class="mw-headline" id="Access_CLI_help_reference"> Access CLI help reference </span></a></h3>
<p>If you need to find a CLI command or syntax for a CLI command, use Splunk's built-in CLI help reference.
</p><p>To start, you can access the default help information with the <code><font size="2">help</font></code> command:
</p>
<div class="samplecode">
<p>./splunk help
</p>
</div>
<p>This will return a list of objects to help you access more specific CLI help topics, such as administrative commands, clustering, forwarding, licensing, searching, etc.
</p>
<h3> <a name="gethelpwiththecli_universal_parameters"><span class="mw-headline" id="Universal_parameters"> Universal parameters </span></a></h3>
<p>Some commands require that you authenticate with a username and password, or specify a target host or app. For these commands you can include one of the universal parameters: <code><font size="2">auth</font></code>, <code><font size="2">app</font></code>, or <code><font size="2">uri</font></code>. 
</p>
<code><font size="2"><br>./splunk [command] [object] [-parameter &lt;value&gt; | &lt;value&gt;]... [-app] [-owner] [-uri] [-auth]<br></font></code>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Parameter
</th><th bgcolor="#C0C0C0">Description
</th></tr><tr><td valign="center" align="left"> app
</td><td valign="center" align="left"> Specify the App or namespace to run the command; for search, defaults to the Search App.
</td></tr><tr><td valign="center" align="left"> auth
</td><td valign="center" align="left"> Specify login credentials to execute commands that require you to be logged in.
</td></tr><tr><td valign="center" align="left"> owner
</td><td valign="center" align="left"> Specify the owner/user context associated with an object; if not specified, defaults to the currently logged in user.
</td></tr><tr><td valign="center" align="left"> uri
</td><td valign="center" align="left"> Excute a command on any specified (remote) Splunk server.
</td></tr></table><h4><font size="3"><b><i> <a name="gethelpwiththecli_app"><span class="mw-headline" id="app"> app </span></a></i></b></font></h4>
<p>In the CLI, <code><font size="2">app</font></code> is an object for many commands, such as <code><font size="2">create app</font></code> or <code><font size="2">enable app</font></code>. But, it is also a parameter that you can add to a CLI command if you want to run that command on a specific app. 
</p><p><b>Syntax:</b>
</p>
<code><font size="2"><br>./splunk command object [-parameter value]... -app appname <br></font></code>
<p>For example, when you run a search in the CLI, it defaults to the Search app. If want to run the search in another app:
</p>
<div class="samplecode">
<p>./splunk search "eventype=error | stats count by source" -deatach f -preview t -app unix 
</p>
</div>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_auth"><span class="mw-headline" id="auth"> auth </span></a></i></b></font></h4>
<p>If a CLI command requires authentication, Splunk will prompt you to supply the username and password. You can also use the <code><font size="2">-auth</font></code> flag to pass this information inline with the command. The <code><font size="2">auth</font></code> parameter is also useful if you need to run a command that requires different permissions to execute than the currently logged-in user has.
</p><p><b>Note:</b> <code><font size="2">auth</font></code> must be the last parameter specified in a CLI command argument.
</p><p><b>Syntax:</b>
</p>
<code><font size="2"><br>./splunk command object [-parameter value]... -auth username:password <br></font></code>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_uri"><span class="mw-headline" id="uri"> uri </span></a></i></b></font></h4>
<p>If you want to run a command on a remote Splunk server, use the <code><font size="2">-uri</font></code> flag to specify the target host. 
</p><p><b>Syntax:</b>
</p>
<code><font size="2"><br>./splunk command object [-parameter value]... -uri specified-server<br></font></code>
<p>Specify the target Splunk server with the following format:
</p>
<code><font size="2"><br>[http|https]://name_of_server:management_port<br></font></code>
<p>You can specify an IP address for the <code><font size="2">name_of_server</font></code>. Both IPv4 and IPv6 formats are supported; for example, the <code><font size="2">specified-server</font></code> may read as: 127.0.0.1:80 or "[2001:db8::1]:80". By default, splunkd listens on IPv4 only. To enable IPv6 support, refer to the instructions in <a href="#configuresplunkforipv6" class="external text">"Configure Splunk for IPv6"</a>.
</p><p><br><b>Example:</b>
The following example returns search results from the remote "splunkserver" on port 8089.
</p>
<div class="samplecode">
<p>./splunk search "host=fflanda error 404 *.gif" -auth admin -uri https://splunkserver:8089
</p>
</div>
<p>For more information about the CLI commands you can run on a remote server, see <a href="#accessandusetheclionaremoteserver" class="external text">the next topic</a> in this chapter.
</p>
<h3> <a name="gethelpwiththecli_useful_help_topics"><span class="mw-headline" id="Useful_help_topics">Useful help topics</span></a></h3>
<p>When you run the default Splunk CLI help, you will see these objects listed.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_administrative_cli_commands"><span class="mw-headline" id="Administrative_CLI_commands">Administrative CLI commands</span></a></i></b></font></h4>
<p>You can use the CLI for administrative functions such as adding or editing inputs, updating configuration settings, and searching. If you want to see the list of administrative CLI commands type in:
</p>
<div class="samplecode">
<p>./splunk help commands
</p>
</div>
<p>These commands are discussed in more detail in <a href="#cliadmincommands" class="external text">"Administrative CLI commands"</a>, the next topic in this manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_clustering"><span class="mw-headline" id="CLI_help_for_clustering">CLI help for clustering</span></a></i></b></font></h4>
<p>Index replication, which is also referred to as clustering, is a Splunk feature that consists of clusters of indexers configured to replicate data to achieve several goals: data availability, data fidelity, disaster tolerance, and improved search performance. 
</p><p>You can use the CLI to view and edit clustering configurations on the cluster master or cluster peer. For the list of commands and parameters related to clustering, type in:
</p>
<div class="samplecode">
<p>./splunk help clustering
</p>
</div>
<p>For more information, read "About clusters and index replication" and "Configure the cluster with the CLI" in the Managing Indexers and Clusters Manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_splunk_controls"><span class="mw-headline" id="CLI_help_for_Splunk_controls">CLI help for Splunk controls</span></a></i></b></font></h4>
<p>Use the CLI to start, stop, and restart Splunk server (<code><font size="2">splunkd</font></code>) and web (<code><font size="2">splunkweb</font></code>) processes or check to see if the process is running. For the list of controls, type in:
</p>
<div class="samplecode">
<p>./splunk help controls
</p>
</div>
<p>For more information, read <a href="#startsplunk" class="external text">"Start and stop Splunk"</a> in the Admin Manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_data_management"><span class="mw-headline" id="CLI_help_for_data_management">CLI help for data management</span></a></i></b></font></h4>
<p>When you add data to Splunk, Splunk processes it and stores it in an <b>index</b>. By default, data you feed to Splunk is stored in the <b>main</b> index, but you can use the CLI to create and specify other indexes for Splunk to use for different data inputs. To see the list of objects and commands to manage indexes and datastores, type in:
</p>
<div class="samplecode">
<p>./splunk help datastore <br>
./splunk help index
</p>
</div>
<p>For more information, read "About managing indexes", "Set up multiple indexes", and  "Remove indexes and data from Splunk" in the Managing indexers and Clusters Manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_distributed_search_deployments"><span class="mw-headline" id="CLI_help_for_distributed_search_deployments">CLI help for distributed search deployments</span></a></i></b></font></h4>
<p>Use the CLI to view and manage your distributed search configurations. For the list of objects and commands, type in:
</p>
<div class="samplecode">
<p>./splunk help distributed
</p>
</div>
<p>For more information about how distributed Splunk works and how to configure distributed search, read the "Distributed Splunk overview",   "Components and roles (of a distributed environment)", "About distributed search", and "Configure distributed search" in the Distributed Deployment Manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_forwarding_and_receiving"><span class="mw-headline" id="CLI_help_for_forwarding_and_receiving">CLI help for forwarding and receiving</span></a></i></b></font></h4>
<p>Splunk deployments can include dozens or hundreds of forwarders forwarding data to one or more receivers. Use the CLI to view and manage your data forwarding configuration. For the list of forwarding objects and commands, type in:
</p>
<div class="samplecode">
<p>./splunk help forwarding
</p>
</div>
<p>For more information, read  "Components and roles" and "About forwarding and receiving" in the Distributed Deployment Manual.
</p>
<h4><font size="3"><b><i> <a name="gethelpwiththecli_cli_help_for_search_and_real-time_search"><span class="mw-headline" id="CLI_help_for_search_and_real-time_search">CLI help for search and real-time search</span></a></i></b></font></h4>
<p>You can also use the CLI to run both historical and real-time searches. Access the help page about Splunk search and real-time search with:
</p>
<div class="samplecode">
<p>./splunk help search <br>
./splunk help rtsearch
</p>
</div>
<p>Also, use objects <code><font size="2">search-commands</font></code>, <code><font size="2">search-fields</font></code>, and <code><font size="2">search-modifiers</font></code> to access the respective help descriptions and syntax:
</p>
<div class="samplecode">
<p>./splunk help search-commands <br>
./splunk help search-fields <br>
./splunk help search-modifiers
</p>
</div>
<p><b>Note:</b> The Splunk CLI interprets spaces as breaks. Use dashes between multiple words for topic names that are more than one word.
</p><p>To learn more about searching your data with the CLI, refer to "About CLI searches"  and "Syntax for CLI searches" in the Search Reference Manual and "Real-time searches and reports in the CLI" in the Search Manual.
</p>
<a name="cliadmincommands"></a><h2> <a name="cliadmincommands_administrative_cli_commands"><span class="mw-headline" id="Administrative_CLI_commands"> Administrative CLI commands</span></a></h2>
<p>This topic discusses the administrative CLI commands, which are the commands used to manage or configure your Splunk server and distributed deployment. 
</p><p>For information about accessing the CLI and what is covered in the CLI help, see the previous topic, <a href="#gethelpwiththecli" class="external text">"Get help with the CLI"</a>. If you're looking for details about how to run searches from the CLI, refer to "About CLI searches" in the Search Reference Manual.
</p><p>Your Splunk role configuration dictates what actions (commands) you can execute. Most actions require you to be a Splunk admin. Read more about setting up and managing Splunk users and roles in the <a href="#aboutusersandroles" class="external text">"About users and roles"</a> topic in the Admin Manual. 
</p>
<h3> <a name="cliadmincommands_splunk_cli_command_syntax"><span class="mw-headline" id="Splunk_CLI_command_syntax">Splunk CLI command syntax</span></a></h3>
<p>The general syntax for a CLI command is this:
</p>
<code><font size="2"><br>./splunk &lt;command&gt; [&lt;object&gt;] [[-&lt;parameter&gt;] &lt;value&gt;]...<br></font></code>
<p>Note the following:
</p>
<ul><li> Some commands don't require an object or parameters.
</li><li> Some commands have a default parameter that can be specified by its value alone.
</li></ul><h3> <a name="cliadmincommands_commands_and_objects"><span class="mw-headline" id="Commands_and_objects">Commands and objects</span></a></h3>
<p>A <b>command</b> is an action that you can perform. An <b>object</b> is something you perform an action on. 
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr><th bgcolor="#C0C0C0">Command
</th><th bgcolor="#C0C0C0">Objects
</th></tr><tr><td width="20%" valign="center" align="left"> add
</td><td valign="center" align="left"> exec, forward-server, index, licenser-pools, licenses, monitor, oneshot, saved-search, search-server, tcp, udp, user
</td></tr><tr><td valign="center" align="left"> apply
</td><td valign="center" align="left"> cluster-bundle
</td></tr><tr><td valign="center" align="left"> anonymize
</td><td valign="center" align="left"> source
</td></tr><tr><td valign="center" align="left"> clean
</td><td valign="center" align="left"> all, eventdata, globaldata, userdata, inputdata
</td></tr><tr><td valign="center" align="left"> diag
</td><td valign="center" align="left"> NONE
</td></tr><tr><td valign="center" align="left"> disable
</td><td valign="center" align="left"> app, boot-start, deploy-client, deploy-server, dist-search, index, listen, local-index, perfmon, webserver, web-ssl, wmi
</td></tr><tr><td valign="center" align="left"> display
</td><td valign="center" align="left"> app, boot-start, deploy-client, deploy-server, dist-search, index, jobs, listen, local-index
</td></tr><tr><td valign="center" align="left"> edit
</td><td valign="center" align="left"> app, cluster-config, exec, index, licenser-localslave, licenser-groups, monitor, saved-search, search-server, tcp, udp, user
</td></tr><tr><td valign="center" align="left"> enable
</td><td valign="center" align="left"> app, boot-start, deploy-client, deploy-server, dist-search, index, listen, local-index, perfmon, webserver, web-ssl, wmi
</td></tr><tr><td valign="center" align="left"> export
</td><td valign="center" align="left"> eventdata, userdata
</td></tr><tr><td valign="center" align="left"> import
</td><td valign="center" align="left"> userdata
</td></tr><tr><td valign="center" align="left"> install
</td><td valign="center" align="left"> app
</td></tr><tr><td valign="center" align="left"> find
</td><td valign="center" align="left"> logs
</td></tr><tr><td valign="center" align="left"> help
</td><td valign="center" align="left"> NONE
</td></tr><tr><td valign="center" align="left"> list
</td><td valign="center" align="left"> cluster-config, cluster-generation, cluster-peers, cluster-buckets, deploy-clients, exec, forward-server, index, licenser-groups, licenser-localslave, licenser-messages, licenser-pools, licenser-slaves, licenser-stacks, licenses, jobs, master-info, monitor, peer-info, peer-buckets, perfmon, saved-search, search-server, tcp, udp, user, wmi
</td></tr><tr><td valign="center" align="left"> login,logout
</td><td valign="center" align="left"> NONE
</td></tr><tr><td valign="center" align="left"> package
</td><td valign="center" align="left"> app
</td></tr><tr><td valign="center" align="left"> reload
</td><td valign="center" align="left"> ad, auth, deploy-server, index, monitor, registry, script, tcp, udp, perfmon, wmi
</td></tr><tr><td valign="center" align="left"> remove
</td><td valign="center" align="left"> app, exec, forward-server, index, jobs, licenser-pools, licenses, monitor, saved-search, search-server, tcp, udp, user
</td></tr><tr><td valign="center" align="left"> rolling-restart
</td><td valign="center" align="left"> cluster-peers
</td></tr><tr><td valign="center" align="left"> rtsearch
</td><td valign="center" align="left"> app, batch, detach, earliest_time, header, index_earliest, index_latest, max_time, maxout, output, preview, rt_id, timeout, wrap
</td></tr><tr><td valign="center" align="left"> search
</td><td valign="center" align="left"> app, batch, detach, earliest_time, header, id, index_earliest, index_latest, latest_time, max_time, maxout, output, preview, timeout, wrap
</td></tr><tr><td valign="center" align="left"> set
</td><td valign="center" align="left"> datastore-dir, deploy-poll, default-hostname, default-index, minfreemb, servername, server-type, splunkd-port, web-port
</td></tr><tr><td valign="center" align="left"> show
</td><td valign="center" align="left"> config, cluster-bundle-status, datastore-dir, deploy-poll, default-hostname, default-index, jobs, minfreemb, servername, splunkd-port, web-port
</td></tr><tr><td valign="center" align="left"> spool
</td><td valign="center" align="left"> NONE
</td></tr><tr><td valign="center" align="left"> start,stop,restart
</td><td valign="center" align="left"> splunkd, splunkweb
</td></tr><tr><td valign="center" align="left"> status
</td><td valign="center" align="left"> splunkd, splunkweb
</td></tr><tr><td valign="center" align="left"> validate
</td><td valign="center" align="left"> index
</td></tr><tr><td valign="center" align="left"> version
</td><td valign="center" align="left"> NONE
</td></tr></table><h3> <a name="cliadmincommands_exporting_search_results_with_the_cli"><span class="mw-headline" id="Exporting_search_results_with_the_CLI">Exporting search results with the CLI</span></a></h3>
<p>You can use the CLI to export large numbers of search results. For information about how to export search results with the CLI, as well as information about the other export methods offered by Splunk Enterprise, see "Export search results" in the <i>Search Manual</i>.
</p>
<h3> <a name="cliadmincommands_troubleshooting_with_the_cli"><span class="mw-headline" id="Troubleshooting_with_the_CLI">Troubleshooting with the CLI </span></a></h3>
<p>Splunk's CLI also includes tools that help with troubleshooting Splunk issues. These tools are invoked using the Splunk CLI command <code><font size="2">cmd</font></code>:
</p>
<div class="samplecode">
<p>./splunk cmd &lt;tool&gt;
</p>
</div>
<p>For the list of CLI utilities, see "Command line tools for use with Support" in the Troubleshooting Manual.
</p>
<a name="accessandusetheclionaremoteserver"></a><h2> <a name="accessandusetheclionaremoteserver_use_the_cli_to_administer_a_remote_splunk_server"><span class="mw-headline" id="Use_the_CLI_to_administer_a_remote_Splunk_server"> Use the CLI to administer a remote Splunk server </span></a></h2>
<p>You can use the <code><font size="2">uri</font></code> parameter with any CLI command to send that command to another Splunk server and view the results on your local server. 
</p><p>This topic discusses:
</p>
<ul><li> Syntax for using the <code><font size="2">uri</font></code> parameter.
</li><li> CLI commands that you cannot use remotely.
</li></ul><p><b>Note:</b> Starting in 4.1.4, remote CLI access is disabled by default for the admin user until you have changed its default password. 
</p>
<h3> <a name="accessandusetheclionaremoteserver_enable_remote_access"><span class="mw-headline" id="Enable_remote_access"> Enable remote access </span></a></h3>
<p>If you are running Splunk Free (no login credentials), remote access is disabled by default until you've edited <code><font size="2">$SPLUNK_HOME/etc/system/local/server.conf</font></code> and set the value:
</p>
<code><font size="2"><br>allowRemoteLogin=always<br></font></code>
<p><b>Note:</b> The <code><font size="2">add oneshot</font></code> command works on local servers but cannot be used remotely.
</p><p>For more information about editing configuration files, refer to <a href="#aboutconfigurationfiles" class="external text">"About configuration files"</a> in this manual.
</p>
<h3> <a name="accessandusetheclionaremoteserver_send_cli_commands_to_a_remote_server"><span class="mw-headline" id="Send_CLI_commands_to_a_remote_server"> Send CLI commands to a remote server </span></a></h3>
<p>The general syntax for using the <code><font size="2">uri</font></code> parameter with any CLI command is:
</p>
<code><font size="2"><br>./splunk command object [-parameter &lt;value&gt;]... -uri &lt;specified-server&gt;<br></font></code>
<p>The <code><font size="2">uri</font></code> value, <code><font size="2">specified-server</font></code> is formatted as:
</p>
<code><font size="2"><br>[http|https]://name_of_server:management_port<br></font></code>
<p>Also, the <code><font size="2">name_of_server</font></code> can be the fully-resolved domain name or the IP address of the remote Splunk server.
</p><p><b>Important:</b> This <code><font size="2">uri</font></code> value is the <code><font size="2">mgmtHostPort</font></code> value that you defined in the remote Splunk server's <code><font size="2">web.conf</font></code>. For more information, see the <a href="#webconf" class="external text">web.conf reference</a> in this manual.
</p><p>For more general information about the CLI, see <a href="#aboutthecli" class="external text">"About the CLI"</a> and <a href="#gethelpwiththecli" class="external text">"Get help with the CLI"</a> in this manual.
</p>
<h4><font size="3"><b><i> <a name="accessandusetheclionaremoteserver_search_a_remote_server"><span class="mw-headline" id="Search_a_remote_server"> Search a remote server </span></a></i></b></font></h4>
<p>The following example returns search results from the remote "splunkserver". 
</p>
<code><font size="2"><br>./splunk search "host=fflanda error 404 *.gif" -uri https://splunkserver:8089<br></font></code>
<p>For details on syntax for searching using the CLI, refer to "About CLI searches" in the Search Reference Manual.
</p>
<h4><font size="3"><b><i> <a name="accessandusetheclionaremoteserver_view_apps_installed_on_a_remote_server"><span class="mw-headline" id="View_apps_installed_on_a_remote_server"> View apps installed on a remote server </span></a></i></b></font></h4>
<p>The following example returns the list of apps that are installed on the remote "splunkserver".
</p>
<code><font size="2"><br>./splunk display app -uri https://splunkserver:8089<br></font></code>
<h3> <a name="accessandusetheclionaremoteserver_change_your_default_uri_value"><span class="mw-headline" id="Change_your_default_URI_value"> Change your default URI value </span></a></h3>
<p>You can set a default URI value using the SPLUNK_URI environment variable. If you change this value to be the URI of the remote server, you do not need to include the <code><font size="2">uri</font></code> parameter each time you want to access that remote server.
</p><p>To change the value of SPLUNK_URI, type either:
</p>
<code><font size="2"><br>$ export SPLUNK_URI=[http|https]://name_of_server:management_port &nbsp;&nbsp;&nbsp;&nbsp;# For Unix shells<br>C:\&gt; set SPLUNK_URI=[http|https]://name_of_server:management_port &nbsp;&nbsp;&nbsp;&nbsp;# For Windows shell<br></font></code>
<p>For the examples above, you can change your SPLUNK_URI value by typing:
</p>
<code><font size="2"><br>$ export SPLUNK_URI=https://splunkserver:8089<br></font></code>
<h3> <a name="accessandusetheclionaremoteserver_cli_commands_you_cannot_run_remotely"><span class="mw-headline" id="CLI_commands_you_cannot_run_remotely"> CLI commands you cannot run remotely </span></a></h3>
<p>With the exception of commands that control the server, you can run all CLI commands remotely. These server control commands include:
</p>
<ul><li> Start, stop, restart
</li><li> Status, version
</li></ul><p>You can view all CLI commands by accessing the CLI help reference. For more information, see <a href="#gethelpwiththecli" class="external text">"Get help with the CLI"</a> in this manual.
</p>
<a name="customizethecliloginbanner"></a><h2> <a name="customizethecliloginbanner_customize_the_cli_login_banner"><span class="mw-headline" id="Customize_the_CLI_login_banner"> Customize the CLI login banner</span></a></h2>
<p>If you provide CLI access to data, you may need to customize your login banner to notify your users of monitoring, their legal obligations, and penalties for misuse. You can also add additional security (in the form of basic authentication) for your CLI logins.
</p><p>To create a custom login banner and add basic authentication, add the following stanzas to your local server.conf file:
</p>
<div class="samplecode"><code><font size="2">[httpServer]<br>cliLoginBanner = &lt;string&gt;<br>allowBasicAuth = true|false<br>basicAuthRealm = &lt;string&gt;</font></code></div>
<ul><li> For <code><font size="2">cliLoginBanner = &lt;string&gt;</font></code>
</li></ul><p>Create a message that you want your user to see in the Splunk CLI, such as access policy information, before they are prompted for authentication credentials. The default value is no message.
</p><p>To create a multi-line banner, place the lines in a comma separated list, putting each line in double-quotes. For example:
</p>
<div class="samplecode"><code><font size="2">cliLoginBanner="Line 1","Line 2","Line 3"</font></code></div>
<p>To include a double quote within the banner text, use two quotes in a row. For example:
</p>
<div class="samplecode"><code><font size="2">cliLoginBanner="This is a line that ""contains quote characters""!"</font></code></div>
<ul><li> For <code><font size="2">allowBasicAuth = true|false</font></code>:
</li></ul><p>Set this value to <code><font size="2">true</font></code> if you want to require clients to make authenticated requests to the Splunk server using "HTTP Basic" authentication in addition to Splunk's existing (<code><font size="2">authtoken</font></code>) authentication. This is useful for allowing programmatic access to REST endpoints and for allowing access to the REST API from a web browser. It is not required for the UI or CLI. The default value is <code><font size="2">true</font></code>.
</p>
<ul><li> For <code><font size="2">basicAuthRealm = &lt;string&gt;</font></code>:
</li></ul><p>If you have enabled <code><font size="2">allowBasicAuth</font></code>, use this attribute to add a text string that can be presented in a Web browser when credentials are prompted. You can display a short message that describes the server and/or access policy. The text: "/splunk" displays by default.
</p>
<h1>Start Splunk Enterprise and perform initial tasks</h1><a name="startsplunk"></a><h2> <a name="startsplunk_start_and_stop_splunk_enterprise"><span class="mw-headline" id="Start_and_stop_Splunk_Enterprise"> Start and stop Splunk Enterprise</span></a></h2>
<p>This topic provides brief instructions for starting and stopping Splunk Enterprise.
</p>
<h3> <a name="startsplunk_start_splunk_enterprise_on_windows"><span class="mw-headline" id="Start_Splunk_Enterprise_on_Windows"> Start Splunk Enterprise on Windows</span></a></h3>
<p>On Windows, Splunk Enterprise installs by default into <code><font size="2">C:\Program Files\Splunk</font></code>. Many examples in the Splunk documentation use <code><font size="2">$SPLUNK_HOME</font></code> to indicate the Splunk installation directory. You can replace the string <code><font size="2">$SPLUNK_HOME</font></code> (and the Windows variant <code><font size="2">%SPLUNK_HOME%</font></code>) with <code><font size="2">C:\Program Files\Splunk</font></code> if you installed Splunk Enterprise into the default directory. 
</p><p>Splunk Enterprise installs with two services, <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code>. In normal operation, only <code><font size="2">splunkd</font></code> runs, handling all Splunk Enterprise operations, including the Splunk Web interface. To change this, you must put Splunk Enterprise in legacy mode. Read "<a href="#startsplunk_start_splunk_enterprise_on_windows_in_legacy_mode" class="external text">Start Splunk Enterprise on Windows in legacy mode</a>."
</p><p>You can start and stop Splunk on Windows in one of the following ways:
</p><p><b>1.</b> Start and stop Splunk Enterprise processes via the Windows Services control panel (accessible from <code><font size="2">Start -&gt; Control Panel -&gt; Administrative Tools -&gt; Services</font></code>)
</p>
<ul><li> Server daemon and Web interface: <code><font size="2">splunkd</font></code>
</li><li> Web interface (in legacy mode only): <code><font size="2">splunkweb</font></code>. In normal operation, this service starts, then immediately quits when it receives a start request.
</li></ul><p><b>2.</b> Start and stop Splunk Enterprise services from a command prompt by using the <code><font size="2">NET START &lt;service&gt;</font></code> or <code><font size="2">NET STOP &lt;service&gt;</font></code> commands:
</p>
<ul><li> Server daemon and Web interface: <code><font size="2">splunkd</font></code>
</li><li> Web interface (in legacy mode only): <code><font size="2">splunkweb</font></code>. In normal operation, this service starts, then immediately quits when it receives a start request.
</li></ul><p><b>3.</b> Start, stop, or restart both processes at once by going to <code><font size="2">%SPLUNK_HOME%\bin</font></code> and typing 
</p><p><code><font size="2">&gt; splunk [start|stop|restart]</font></code>
</p>
<h4><font size="3"><b><i> <a name="startsplunk_start_splunk_enterprise_on_windows_in_legacy_mode"><span class="mw-headline" id="Start_Splunk_Enterprise_on_Windows_in_legacy_mode"> Start Splunk Enterprise on Windows in legacy mode </span></a></i></b></font></h4>
<p>If you want run Splunk Enterprise in legacy mode, where <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> both run, you must change a configuration parameter.
</p><p><b>Important: Do not run Splunk Web in legacy mode permanently.</b> Use legacy mode to temporarily work around issues introduced by the new integration of the user interface with the main splunkd service. Once you correct the issues, return Splunk Web to normal mode as soon as possible.
</p><p>To put Splunk Enterprise in legacy mode:
</p><p><b>1.</b> From a command prompt, go to <code><font size="2">%SPLUNK_HOME%\etc\system\default</font></code>.
</p><p><b>2.</b> Make a copy of <code><font size="2">web.conf</font></code> and place it into <code><font size="2">%SPLUNK_HOME%\etc\system\local</font></code>.
</p><p><b>3.</b> Edit <code><font size="2">web.conf</font></code> in <code><font size="2">%SPLUNK_HOME%\etc\system\local</font></code>.
</p><p><b>4.</b> In <code><font size="2">web.conf</font></code>, set the <code><font size="2">appserverPorts</font></code> and <code><font size="2">httpport</font></code> attributes as follows:
</p>
<div class="samplecode"><code><font size="2"><br>[settings]<br>appServerPorts = 0<br>httpport = 8000<br></font></code></div>
<p><b>5.</b> Save the file and close it.
</p><p><b>6.</b> Restart Splunk Enterprise. The <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services start and remain running.
</p><p><b>7.</b> Log into Splunk Enterprise by browsing to <code><font size="2">http://&lt;server name&gt;:&lt;httpport&gt;</font></code> and entering your credentials.
</p><p>To restore normal Splunk Enterprise operations: edit <code><font size="2">%SPLUNK_HOME%\etc\system\local\web.conf</font></code> and remove the <code><font size="2">appServerPorts</font></code> and <code><font size="2">httpport</font></code> attributes.
</p>
<h3> <a name="startsplunk_start_splunk_enterprise_on_unix"><span class="mw-headline" id="Start_Splunk_Enterprise_on_UNIX"> Start Splunk Enterprise on UNIX</span></a></h3>
<p>Splunk Enterprise installs with one process on *nix, <code><font size="2">splunkd</font></code>. In normal operation, only <code><font size="2">splunkd</font></code> runs, handling all Splunk Enterprise operations, including the Splunk Web interface. To change this, you must put Splunk Enterprise in legacy mode. See "<a href="#startsplunk_start_splunk_enterprise_on_unix_in_legacy_mode" class="external text">Start Splunk Enterprise on Unix in legacy mode</a>."
</p>
<h4><font size="3"><b><i> <a name="startsplunk_start_splunk_enterprise"><span class="mw-headline" id="Start_Splunk_Enterprise">Start Splunk Enterprise</span></a></i></b></font></h4>
<p>From a shell prompt on the Splunk Enterprise server host, run this command:
</p><p><code><font size="2"># splunk start</font></code>
</p><p><b>Note</b>: If you have configured Splunk Enterprise to start at boot time, you should start it using the service command.  This ensures that the user configured in the init.d script starts the software.
</p><p><code><font size="2"># service splunk start</font></code>
</p><p>This starts <code><font size="2">splunkd</font></code> (indexer and the Splunk Web interface).
</p><p>To start them individually, type:
</p><p><code><font size="2"># splunk start splunkd</font></code>
</p><p>or
</p><p>(in legacy mode only) <code><font size="2"># splunk start splunkweb</font></code>
</p><p><b>Note:</b> If either the <code><font size="2">startwebserver</font></code> attribute is disabled, or the <code><font size="2">appServerPorts</font></code> attribute is set to anything other than 0 in <code><font size="2">web.conf</font></code>, then manually starting <code><font size="2">splunkweb</font></code> does not do anything. The <code><font size="2">splunkweb</font></code> process will not start in either case. See <a href="#startsplunk_start_splunk_enterprise_on_unix_in_legacy_mode" class="external text">Start Splunk Enterprise on Unix in legacy mode</a>."
</p><p>To restart Splunk Enterprise (<code><font size="2">splunkd</font></code> or <code><font size="2">splunkweb</font></code>) type:
</p><p><code><font size="2"># splunk restart</font></code>
</p><p><code><font size="2"># splunk restart splunkd</font></code>
</p><p>(in legacy mode only) <code><font size="2"># splunk restart splunkweb</font></code>
</p>
<h4><font size="3"><b><i> <a name="startsplunk_start_splunk_enterprise_on_unix_in_legacy_mode"><span class="mw-headline" id="Start_Splunk_Enterprise_on_Unix_in_legacy_mode"> Start Splunk Enterprise on Unix in legacy mode </span></a></i></b></font></h4>
<p>If you want run Splunk Enterprise in such a way that <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> both run, you must put Splunk Enterprise into legacy mode.
</p><p>To put Splunk Enterprise in legacy mode:
</p><p><b>1.</b> From a shell prompt, go to <code><font size="2">$SPLUNK_HOME/etc/system/default</font></code>.
</p><p><b>2.</b> Make a copy of <code><font size="2">web.conf</font></code> and place it into <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>.
</p><p><b>3.</b> Edit <code><font size="2">web.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code>.
</p><p><b>4.</b> In <code><font size="2">web.conf</font></code>, set the <code><font size="2">appserverPorts</font></code> and <code><font size="2">httpport</font></code> attributes as follows:
</p>
<div class="samplecode"><code><font size="2"><br>[settings]<br>appServerPorts = 0<br>httpport = 8000<br></font></code></div>
<p><b>5.</b> Save the file and close it.
</p><p><b>6.</b> Restart Splunk Enterprise (see "Start Splunk Enterprise on Unix"). The <code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code> services start and remain running.
</p><p><b>7.</b> Log into Splunk Enterprise by browsing to <code><font size="2">http://&lt;server name&gt;:&lt;httpport&gt;</font></code> and entering your credentials.
</p><p>To restore normal Splunk Enterprise operations: edit <code><font size="2">%SPLUNK_HOME%\etc\system\local\web.conf</font></code> and remove the <code><font size="2">appServerPorts</font></code> and <code><font size="2">httpport</font></code> attributes.
</p>
<h4><font size="3"><b><i> <a name="startsplunk_stop_splunk_enterprise"><span class="mw-headline" id="Stop_Splunk_Enterprise"> Stop Splunk Enterprise </span></a></i></b></font></h4>
<p>To shut down Splunk Enterprise, run this command:
</p><p><code><font size="2"># splunk stop</font></code>
</p><p>To stop <code><font size="2">splunkd</font></code> and Splunk Web individually, type:
</p><p><code><font size="2"># splunk stop splunkd</font></code>
</p><p>or
</p><p>(in legacy mode only) <code><font size="2"># splunk stop splunkweb</font></code>
</p>
<h4><font size="3"><b><i> <a name="startsplunk_check_if_splunk_is_running"><span class="mw-headline" id="Check_if_Splunk_is_running"> Check if Splunk is running </span></a></i></b></font></h4>
<p>To check if Splunk Enterprise is running, type this command at the shell prompt on the server host:
</p><p><code><font size="2"># splunk status</font></code>
</p><p>You should see this output:
</p>
<div class="samplecode"><code><font size="2"><br>splunkd is running (PID: 3162).<br>splunk helpers are running (PIDs: 3164).<br></font></code></div>
<p>If Splunk Enterprise runs in legacy mode, you will see an additional line in the output:
</p>
<div class="samplecode"><code><font size="2"><br>splunkweb is running (PID: 3216).<br></font></code></div>
<p><b>Note:</b> On Unix systems, you must be logged in as the user who runs Splunk Enterprise to run the <code><font size="2">splunk status</font></code> command. Other users cannot read the necessary files to report status correctly.  
</p><p>You can also use <code><font size="2">ps</font></code> to check for running Splunk Enterprise processes:
</p><p><code><font size="2"># ps aux | grep splunk | grep -v grep</font></code>
</p><p>Solaris users should use the <code><font size="2">-ef</font></code> arguments to <code><font size="2">ps</font></code> instead of <code><font size="2">aux</font></code>:
</p><p><code><font size="2"># ps -ef | grep splunk | grep -v grep</font></code>
</p>
<h3> <a name="startsplunk_restart_splunk_enterprise_from_splunk_web"><span class="mw-headline" id="Restart_Splunk_Enterprise_from_Splunk_Web"> Restart Splunk Enterprise from Splunk Web </span></a></h3>
<p>You can also restart Splunk from Splunk Web:
</p><p><b>1.</b> Navigate to <b>System &gt; Server controls</b>.  
</p><p><b>2.</b> Click <b>Restart Splunk</b>.
</p><p>This will restart the <code><font size="2">splunkd</font></code> and (in legacy mode only) the <code><font size="2">splunkweb</font></code> processes.
</p>
<a name="configuresplunktostartatboottime"></a><h2> <a name="configuresplunktostartatboottime_configure_splunk_to_start_at_boot_time"><span class="mw-headline" id="Configure_Splunk_to_start_at_boot_time"> Configure Splunk to start at boot time</span></a></h2>
<p>On Windows, Splunk starts by default at machine startup. To disable this, see "Disable boot-start on Windows" at the end of this topic.
</p><p>On *nix platforms, you must configure Splunk to start at boot time.
</p>
<h3> <a name="configuresplunktostartatboottime_enable_boot-start_on_.2anix_platforms"><span class="mw-headline" id="Enable_boot-start_on_.2Anix_platforms"> Enable boot-start on *nix platforms</span></a></h3>
<p>Splunk provides a utility that updates your system boot configuration so that Splunk starts when the system boots up. This utility creates a suitable <code><font size="2">init</font></code> script (or makes a similar configuration change, depending on your OS).  
</p><p>As root, run:
</p>
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk enable boot-start<br></font></code>
<p>If you don't start Splunk as root, you can pass in the <code><font size="2">-user</font></code>  parameter to specify which user to start Splunk as.  For example, if Splunk runs as the  user bob, then as root you would run:
</p>
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk enable boot-start -user bob<br></font></code>
<p>If you want to stop Splunk from running at system startup time, run:
</p>
<code><font size="2"><br>$SPLUNK_HOME/bin/splunk disable boot-start<br></font></code>
<p>More information is available in <code><font size="2">$SPLUNK_HOME/etc/init.d/README</font></code> and if you type <code><font size="2">help boot-start</font></code> from the command line.
</p>
<h4><font size="3"><b><i> <a name="configuresplunktostartatboottime_note_for_mac_users"><span class="mw-headline" id="Note_for_Mac_users">Note for Mac users</span></a></i></b></font></h4>
<p>Splunk automatically creates a script and configuration file in the directory: <code><font size="2">/System/Library/StartupItems</font></code>.  This script is run at system start, and automatically stops Splunk at system shutdown.
</p><p><b>Note:</b> If you are using a Mac OS, you <b>must</b> have root level permissions (or use <b>sudo</b>). You need administrator access to use <b>sudo</b>.
</p><p><b>Example:</b>
</p><p>Enable Splunk to start at system start up on Mac OS using:
</p><p>just the CLI:
</p>
<code><font size="2"><br>./splunk enable boot-start<br></font></code>
<p>the CLI with <b>sudo</b>:
</p>
<code><font size="2"><br>sudo ./splunk enable boot-start<br></font></code>
<h3> <a name="configuresplunktostartatboottime_disable_boot-start_on_windows"><span class="mw-headline" id="Disable_boot-start_on_Windows">Disable boot-start on Windows</span></a></h3>
<p>By default, Splunk starts automatically when you start your Windows machine. You can configure the Splunk processes (<code><font size="2">splunkd</font></code> and <code><font size="2">splunkweb</font></code>) to start manually from the Windows Services control panel.
</p>
<a name="installyourlicense"></a><h2> <a name="installyourlicense_install_your_license"><span class="mw-headline" id="Install_your_license"> Install your license</span></a></h2>
<p>The first time you download Splunk, you are asked to register. 
</p><p>Your registration authorizes you to receive a temporary (60 day) Enterprise trial license, which allows a maximum indexing volume of 500 MB/day. This license is included with your download. 
</p><p>The Enterprise license enables the following features:
</p>
<ul><li> Multiple user accounts and access controls.
</li><li> Distributed search and data routing.
</li><li> Deployment management.
</li></ul><p>For more information about Splunk licensing, read <a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a> in this manual.
</p>
<h3> <a name="installyourlicense_where_is_your_new_license.3f"><span class="mw-headline" id="Where_is_your_new_license.3F">Where is your new license? </span></a></h3>
<p>When you request a new license, you should receive the license in an email from Splunk. You can also access that new license in your <code><font size="2">splunk.com</font></code> My Orders page. 
</p><p>To install and update your licenses via Splunk Web, navigate to <b>Settings &gt; Licensing</b> and follow <a href="#installalicense" class="external text">these instructions</a>.
</p>
<a name="changedefaultvalues"></a><h2> <a name="changedefaultvalues_change_default_values"><span class="mw-headline" id="Change_default_values"> Change default values</span></a></h2>
<p>Before you begin configuring Splunk Enterprise for your environment, check through the following default settings to see if there's anything you'd like to change.
</p>
<h3> <a name="changedefaultvalues_set_or_change_environment_variables"><span class="mw-headline" id="Set_or_change_environment_variables"> Set or change environment variables </span></a></h3>
<p>You can change how Splunk Enterprise starts by setting environment variables on your operating system.
</p><p>On *nix, use the <code><font size="2">setenv</font></code> or <code><font size="2">export</font></code> commands to set a particular variable. For example:
</p>
<div class="samplecode"><code><font size="2"># export SPLUNK_HOME = /opt/splunk02/splunk</font></code></div>
<p>If you want to set the environment permanently, edit the appropriate shell initialization file and add entries for the variables you want Splunk Enterprise to use when it starts up.
</p><p>On Windows, use the <code><font size="2">set</font></code> environment variable in either a command prompt or PowerShell window:
</p>
<div class="samplecode"><code><font size="2">C:\&gt; set SPLUNK_HOME = "C:\Program Files\Splunk"</font></code></div>
<p>If you want to set the environment permanently, use the "Environment Variables" window to add the entry to the "User variables" list.
</p><p><img alt="Windows EnvironmentVariablesList.png" src="images/c/c3/Windows_EnvironmentVariablesList.png" width="384" height="424"></p><p>There are several environment variables that are available:
</p>
<table cellpadding="6"><tr><th bgcolor="#C0C0C0"> Environment variable
</th><th bgcolor="#C0C0C0"> Purpose
</th></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_HOME</font></code>
</td><td valign="center" align="left"> The fully-qualified path to the Splunk Enterprise installation directory.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_DB</font></code>
</td><td valign="center" align="left"> The fully-qualified path to the directory that contains the Splunk Enterprise index directories.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_BINDIP</font></code>
</td><td valign="center" align="left"> The IP address on the system that Splunk Enterprise should bind to on startup to accept connections. Useful for when a host has more than one live IP address.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_IGNORE_SELINUX</font></code>
</td><td valign="center" align="left"> Tells Splunk Enterprise to attempt to start when running in Linux host with SELinux enabled. By default, Splunk Enterprise quits immediately when it detects that SELinux is active. This variable defeats that check and can be used in scenarios where you have configured SELinux to allow Splunk Enterprise to work.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_OS_USER</font></code>
</td><td valign="center" align="left"> Tells Splunk Enterprise to assume the credentials of the user you specify, regardless of what user you started it as. For example, if you specify the user 'splunk' on your system and start Splunk Enterprise as root, it adopts the privileges of the 'splunk' user and any files written by those processes will be owned by the 'splunk' user.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_SERVER_NAME</font></code>
</td><td valign="center" align="left"> The name of the splunkd service (on Windows) or process (on *nix). Do not set this variable unless you know what you are doing.
</td></tr><tr><td valign="center" align="left"> <code><font size="2">SPLUNK_WEB_NAME</font></code>
</td><td valign="center" align="left"> The name of the splunkweb service (on Windows) or process (on *nix). Do not set this variable unless you know what you are doing.
</td></tr></table><p>You can also edit these environment variables for each instance by editing <code><font size="2">splunk-launch.conf</font></code> (and, in some cases, <code><font size="2">web.conf</font></code>. This is handy when you run more than one Splunk instance on a host. See "<a href="#splunk-launchconf" class="external text">splunk-launch.conf</a>".
</p>
<h3> <a name="changedefaultvalues_changing_the_admin_default_password"><span class="mw-headline" id="Changing_the_admin_default_password"> Changing the admin default password </span></a></h3>
<p>Splunk with an Enterprise license has a default administration account and password, admin/changeme.  Splunk recommends strongly that you change the default.  You can do this via Splunk's CLI or Splunk Web. 
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_web"><span class="mw-headline" id="Use_Splunk_Web"> Use Splunk Web </span></a></i></b></font></h4>
<p>To change the admin default password:
</p><p><b>1.</b> Log into Splunk Web as the admin user.
</p><p><b>2.</b> Click <b>Manager</b> in the top-right of the interface.
</p><p><b>3.</b> Click <b>Access controls</b> in the Users and Authentication section of the screen.
</p><p><b>4.</b> Click <b>Users</b>.
</p><p><b>5.</b> Click the <b>admin</b> user.
</p><p><b>6.</b> Update the password, and click <b>Save</b>.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_cli"><span class="mw-headline" id="Use_Splunk_CLI"> Use Splunk CLI </span></a></i></b></font></h4>
<p>The Splunk CLI command is:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user<br></font></code>
</div>
<p><b>Important:</b> You must authenticate with the existing password before you can change it. Log into Splunk via the CLI or use the <code><font size="2">-auth</font></code> parameter.  For example, this command changes the admin password from <i>changeme</i> to <i>foo</i>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user admin -password foo -role admin -auth admin:changeme<br></font></code>
</div>
<p><b>Note:</b> On *nix operating systems, the shell interprets some special characters as command directives. You must either escape these characters by preceding them with <code><font size="2">\</font></code> individually, or enclose the password in single quotes (<code><font size="2">'</font></code>). For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user admin -password 'fflanda$' -role admin -auth admin:changeme<br></font></code>
</div>
<p>or
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user admin -password fflanda\$ -role admin -auth admin:changeme<br></font></code>
</div>
<p>On Windows, use the caret (<code><font size="2">^</font></code>) to escape reserved shell characters, or enclose the password in double-quotes (<code><font size="2">"</font></code>). For example:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user admin -password "fflanda&gt;" -role admin -auth admin:changeme<br></font></code>
</div>
<p>or
</p>
<div class="samplecode">
<code><font size="2"><br>splunk edit user admin -password fflanda^&gt; -role admin -auth admin:changeme<br></font></code>
</div>
<p><b>Note:</b> You can also reset all of your passwords across servers at once. See "Deploy secure passwords across multiple servers for the procedure.
</p>
<h3> <a name="changedefaultvalues_change_network_ports"><span class="mw-headline" id="Change_network_ports"> Change network ports </span></a></h3>
<p>Splunk configures two ports at installation time:
</p>
<ul><li> <b>The HTTP/HTTPS port.</b> This port provides the socket for Splunk Web. It defaults to 8000.
</li><li> <b>The management port.</b> This port is used to communicate with the <code><font size="2">splunkd</font></code> daemon.  Splunk Web talks to <code><font size="2">splunkd</font></code> on this port, as does the command line interface and any distributed connections from other servers. This port defaults to 8089.
</li></ul><p><b>Important:</b> During installation, you might have set these ports to values other than the defaults.
</p><p><b>Note:</b> Splunk instances <b>receiving</b> data from <b>forwarders</b> must be configured with an additional port, the receiver port. They use this port to listen for incoming data from forwarders. This configuration does not occur during installation. The default receiver port is 9997. For more information, see "Enable a receiver" in the Forwarding Data Manual.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_web_2"><span class="mw-headline" id="Use_Splunk_Web_2"> Use Splunk Web </span></a></i></b></font></h4>
<p>To change the ports from their installation settings:
</p><p><b>1.</b> Log into Splunk Web as the admin user.
</p><p><b>2.</b> Click <b>Manager</b> in the top-right of the interface.
</p><p><b>3.</b> Click the <b>System settings</b> link in the System section of the screen.
</p><p><b>4.</b> Click <b>General settings</b>.
</p><p><b>5.</b> Change the value for either <b>Management port</b> or <b>Web port</b>, and click <b>Save</b>.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_cli_2"><span class="mw-headline" id="Use_Splunk_CLI_2"> Use Splunk CLI </span></a></i></b></font></h4>
<p>To change the port settings via the Splunk CLI, use the CLI command <code><font size="2">set</font></code>. For example, this command sets the Splunk Web port to 9000:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set &nbsp;web-port 9000<br></font></code>
</div>
<p>This command sets the splunkd port to 9089:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set &nbsp;splunkd-port 9089<br></font></code>
</div>
<h3> <a name="changedefaultvalues_change_the_default_splunk_server_name"><span class="mw-headline" id="Change_the_default_Splunk_server_name"> Change the default Splunk server name </span></a></h3>
<p>The Splunk server name setting controls both the name displayed within Splunk Web and the name sent to other Splunk Servers in a distributed setting. 
</p><p>The default name is taken from either the DNS or IP address of the Splunk Server host.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_web_3"><span class="mw-headline" id="Use_Splunk_Web_3"> Use Splunk Web </span></a></i></b></font></h4>
<p>To change the Splunk server name:
</p><p><b>1.</b> Log into Splunk Web as the admin user.
</p><p><b>2.</b> Click <b>Manager</b> in the top-right of the interface.
</p><p><b>3.</b> Click the <b>System settings</b> link in the System section of the screen.
</p><p><b>4.</b> Click <b>General settings</b>.
</p><p><b>5.</b> Change the value for <b>Splunk server name</b>, and click <b>Save</b>.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_cli_3"><span class="mw-headline" id="Use_Splunk_CLI_3"> Use Splunk CLI </span></a></i></b></font></h4>
<p>To change the server name via the CLI, use the <code><font size="2">set servername</font></code> command. For  example, this command sets the server name to <i>foo</i>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set servername foo<br></font></code>
</div>
<h3> <a name="changedefaultvalues_changing_the_datastore_location"><span class="mw-headline" id="Changing_the_datastore_location"> Changing the datastore location </span></a></h3>
<p>The datastore is the top-level directory where the Splunk Server stores all indexed data. 
</p><p><b>Note:</b> If you change this directory, the server does not migrate old datastore files. Instead, it starts over again at the new location.
</p><p>To migrate your data to another directory follow the instructions in "Move an index".
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_web_4"><span class="mw-headline" id="Use_Splunk_Web_4"> Use Splunk Web </span></a></i></b></font></h4>
<p>To change the datastore location:
</p><p><b>1.</b> Log into Splunk Web as the admin user.
</p><p><b>2.</b> Click <b>Manager</b> in the top-right of the interface.
</p><p><b>3.</b> Click the <b>System settings</b> link in the System section of the screen.
</p><p><b>4.</b> Click <b>General settings</b>.
</p><p><b>5.</b> Change the path in <b>Path to indexes</b>, and click <b>Save</b>.
</p><p><b>6.</b> Use the CLI to restart Splunk. Navigate to <code><font size="2">$SPLUNK_HOME/bin/</font></code> (*nix) or <code><font size="2">%SPLUNK_HOME%\bin</font></code> (Windows) and run this command:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk restart<br></font></code>
</div>
<p><b>Important:</b> Do not use the restart function inside Manager. This will not have the intended effect of causing the index directory to change. You <i>must</i> restart from the CLI.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_cli_4"><span class="mw-headline" id="Use_Splunk_CLI_4"> Use Splunk CLI </span></a></i></b></font></h4>
<p>To change the datastore directory via the CLI, use the <code><font size="2">set datastore-dir</font></code> command. For example, this command sets the datastore directory to <code><font size="2">/var/splunk/</font></code>:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set datastore-dir /var/splunk/<br></font></code>
</div>
<h3> <a name="changedefaultvalues_set_minimum_free_disk_space"><span class="mw-headline" id="Set_minimum_free_disk_space"> Set minimum free disk space </span></a></h3>
<p>The minimum free disk space setting controls how low disk space in the datastore location can fall before Splunk stops indexing. 
</p><p>Splunk resumes indexing when more space becomes available. 
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_web_5"><span class="mw-headline" id="Use_Splunk_Web_5"> Use Splunk Web </span></a></i></b></font></h4>
<p>To set minimum free disk space:
</p><p><b>1.</b> Log into Splunk Web as the admin user.
</p><p><b>2.</b> Click <b>Manager</b> in the top-right of the interface.
</p><p><b>3.</b> Click the <b>System settings</b> link in the System section of the screen.
</p><p><b>4.</b> Click <b>General settings</b>.
</p><p><b>5.</b> Change the value for <b>Pause indexing if free disk space falls below</b>, and click <b>Save</b>.
</p>
<h4><font size="3"><b><i> <a name="changedefaultvalues_use_splunk_cli_5"><span class="mw-headline" id="Use_Splunk_CLI_5"> Use Splunk CLI </span></a></i></b></font></h4>
<p>To change the minimum free space value via the CLI, use the <code><font size="2">set minfreemb</font></code> command. For example, this command sets the minimum free space to 2000 MB:
</p>
<div class="samplecode">
<code><font size="2"><br>splunk set minfreemb 2000<br></font></code>
</div>
<h3> <a name="changedefaultvalues_other_default_settings"><span class="mw-headline" id="Other_default_settings"> Other default settings </span></a></h3>
<p>The Splunk Web Manager General Settings screen has a few other default settings that you might want to change. Explore it, to see the range of options.
</p>
<a name="bindsplunktoanip"></a><h2> <a name="bindsplunktoanip_bind_splunk_to_an_ip"><span class="mw-headline" id="Bind_Splunk_to_an_IP"> Bind Splunk to an IP</span></a></h2>
<p>You can force Splunk to bind its ports to a specified IP address.  By default, Splunk will bind to the IP address 0.0.0.0, meaning all available IP addresses.
</p><p>Changing Splunk's bind IP only applies to the Splunk daemon (splunkd), which listens on:
</p>
<ul><li> TCP port 8089 (by default)
</li><li> any port that has been configured as for:
<ul><li> SplunkTCP inputs
</li><li> TCP or UDP inputs
</li></ul></li></ul><p>To bind the Splunk Web process (splunkweb) to a specific IP, use the <code><font size="2">server.socket_host</font></code> setting in web.conf.
</p>
<h3> <a name="bindsplunktoanip_temporarily"><span class="mw-headline" id="Temporarily">Temporarily</span></a></h3>
<p>To make this a temporary change, set the environment variable <code><font size="2">SPLUNK_BINDIP=&lt;ipaddress&gt;</font></code> before starting Splunk.
</p>
<h3> <a name="bindsplunktoanip_permanently"><span class="mw-headline" id="Permanently">Permanently</span></a></h3>
<p>If you want this to be a permanent change in your working environment, modify <code><font size="2">$SPLUNK_HOME/etc/splunk-launch.conf</font></code> to include the <code><font size="2">SPLUNK_BINDIP</font></code> attribute and <code><font size="2">&lt;ipaddress&gt;</font></code> value. For example, to bind Splunk ports to 127.0.0.1 (for local loopback only), <code><font size="2">splunk-launch.conf</font></code> should read:
</p>
<code><font size="2"><br># Modify the following line to suit the location of your Splunk install.<br># If unset, Splunk will use the parent of the directory this configuration<br># file was found in<br>#<br># SPLUNK_HOME=/opt/splunk<br>SPLUNK_BINDIP=127.0.0.1<br></font></code>
<p><b>Important:</b> The <code><font size="2">mgmtHostPort</font></code> attribute in <code><font size="2">web.conf</font></code> has a default value of <code><font size="2">127.0.0.1:8089</font></code>. Therefore, if you change <code><font size="2">SPLUNK_BINDIP</font></code> to any value besides <code><font size="2">127.0.0.1</font></code>, you must also change <code><font size="2">mgmtHostPort</font></code> to use the same IP address. For example, if you make this change in <code><font size="2">splunk-launch.conf</font></code>:
</p>
<code><font size="2"><br>SPLUNK_BINDIP=10.10.10.1<br></font></code>
<p>you must also make this change in <code><font size="2">web.conf</font></code> (assuming the management port is <code><font size="2">8089</font></code>):
</p>
<code><font size="2"><br>mgmtHostPort=10.10.10.1:8089<br></font></code>
<p>See web.conf for more information on the <code><font size="2">mgmtHostPort</font></code> attribute.
</p>
<h3> <a name="bindsplunktoanip_ipv6_considerations"><span class="mw-headline" id="IPv6_considerations">IPv6 considerations</span></a></h3>
<p>Starting in version 4.3, the <code><font size="2">web.conf mgmtHostPort</font></code> setting has been extended to allow it to take IPv6 addresses if they are enclosed in square brackets. Therefore, if you configure splunkd to only listen on IPv6 (via the setting in <code><font size="2">server.conf</font></code> described in <a href="#configuresplunkforipv6" class="external text">"Configure Splunk for IPv6"</a> in this manual), you must change this from <code><font size="2">127.0.0.1:8089</font></code> to <code><font size="2">[::1]:8089</font></code>.
</p>
<a name="configuresplunkforipv6"></a><h2> <a name="configuresplunkforipv6_configure_splunk_for_ipv6"><span class="mw-headline" id="Configure_Splunk_for_IPv6"> Configure Splunk for IPv6</span></a></h2>
<p>This topic discusses Splunk's support for IPv6 and how to configure it. Before following the procedures in this topic, you may want to review:
</p>
<ul><li> <a href="#aboutconfigurationfiles" class="external text">"About configuration files"</a> in this manual to learn about how Splunk's configuration files work
</li><li> "Get data from TCP and UDP ports" in the Getting Data In manual
</li><li> <a href="#serverconf" class="external text">"server.conf"</a> in this manual to see the reference of options available in the <code><font size="2">server.conf</font></code> configuration file
</li><li> <a href="#inputsconf" class="external text">"inputs.conf"</a> in this manual to see the reference of options available in the <code><font size="2">inputs.conf</font></code> configuration file
</li></ul><p>Starting in version 4.3, Splunk supports IPv6. Users can connect to Splunk Web, use the CLI, and forward data over IPv6 networks. 
</p>
<h3> <a name="configuresplunkforipv6_ipv6_platform_support"><span class="mw-headline" id="IPv6_platform_support">IPv6 platform support</span></a></h3>
<p>All Splunk-supported OS platforms (as described in "Supported OSes" in the Installation Manual) are supported for use with IPv6 configurations except for the following:
</p>
<ul><li> HPUX PA-RISC
</li><li> Solaris 8, and 9
</li><li> AIX
</li></ul><h3> <a name="configuresplunkforipv6_configure_splunk_to_listen_on_an_ipv6_network"><span class="mw-headline" id="Configure_Splunk_to_listen_on_an_IPv6_network">Configure Splunk to listen on an IPv6 network</span></a></h3>
<p>You have a few options when configuring Splunk to listen over IPv6. You can configure Splunk to: 
</p>
<ul><li> connect to IPv6 addresses only and ignore all IPv4 results from DNS
</li><li> connect to both IPv4 and IPv6 addresses and
<ul><li> try the IPv6 address first
</li><li> try the IPv4 address first
</li></ul></li><li> connect to IPv4 addresses only and ignore all IPv6 results from DNS
</li></ul><p>To configure how Splunk listens on IPv6:
Edit a copy of <code><font size="2">server.conf</font></code> in <code><font size="2">$SPLUNK_HOME/etc/system/local</font></code> to add the following:
</p><p><code><font size="2">listenOnIPv6=[yes|no|only]</font></code>
</p>
<ul><li> <code><font size="2">yes</font></code> means that splunkd will listen for connections from both IPv6 and IPv4.
</li><li> <code><font size="2">no</font></code> means that splunkd will listen on IPv4 only,  <b>this is the default setting</b>.
</li><li> <code><font size="2">only</font></code> means that Splunk will listen for incoming connections on IPv6 only.
</li></ul><p><code><font size="2">connectUsingIpVersion=[4-first|6-first|4-only|6-only|auto]</font></code>
</p>
<ul><li> <code><font size="2">4-first</font></code> means splunkd will try to connect to the IPv4 address first and if that fails, try IPv6.
</li><li> <code><font size="2">6-first</font></code> is the reverse of <code><font size="2">4-first</font></code>. This is the policy most IPv6-enabled client apps like web browsers take, but can be less robust in the early stages of IPv6 deployment. 
</li><li> <code><font size="2">4-only</font></code> means that splunkd will ignore any IPv6 results from DNS.
</li><li> <code><font size="2">6-only</font></code> means that splunkd will Ignore any IPv4 results from DNS.
</li><li> <code><font size="2">auto</font></code> means that splunkd picks a reasonable policy based on the setting of <code><font size="2">listenOnIPv6</font></code>. <b>This is the default value</b>. 
<ul><li> If splunkd is listening only on IPv4, this behaves as though you specified <code><font size="2">4-only</font></code>.
</li><li> If splunkd is listening only on IPv6, this behaves as though you specified <code><font size="2">6-only</font></code>.
</li><li> If splunkd is listening on both, this behaves as though you specified <code><font size="2">6-first</font></code>. 
</li></ul></li></ul><p><b>Important:</b>  These settings only affect DNS lookups. For example, a setting of <code><font size="2">connectUsingIpVersion = 6-first</font></code> will not prevent a stanza with an explicit IPv4 address (like "server=10.1.2.3:9001") from working.
</p>
<h3> <a name="configuresplunkforipv6_if_you_have_just_a_few_inputs_and_don.27t_want_to_enable_ipv6_for_your_entire_deployment"><span class="mw-headline" id="If_you_have_just_a_few_inputs_and_don.27t_want_to_enable_IPv6_for_your_entire_deployment">If you have just a few inputs and don't want to enable IPv6 for your entire deployment</span></a></h3>
<p>If you've just got a few data sources coming over IPv6 but don't want to enable it for your entire Splunk deployment, you can add the <code><font size="2">listenOnIPv6</font></code> setting described above to any <code><font size="2">[udp], [tcp], [tcp-ssl], [splunktcp]</font></code>, or <code><font size="2"> [splunktcp-ssl] </font></code>stanza in <code><font size="2">inputs.conf</font></code>. This overrides the setting of the same name in <code><font size="2">server.conf</font></code> for that particular input.
</p>
<h3> <a name="configuresplunkforipv6_forwarding_data_over_ipv6"><span class="mw-headline" id="Forwarding_data_over_IPv6">Forwarding data over IPv6</span></a></h3>
<p>Your Splunk <b>forwarders</b> can forward over IPv6; the following are supported in <code><font size="2">outputs.conf</font></code>:
</p>
<ul><li> The <code><font size="2">server</font></code> setting in <code><font size="2">[tcpout]</font></code> stanzas can include IPv6 addresses in the standard <code><font size="2">[host]:port</font></code> format.
</li><li> The <code><font size="2">[tcpout-server]</font></code> stanza can take an IPv6 address in the standard <code><font size="2">[host]:port</font></code> format.
</li><li> The <code><font size="2">server</font></code> setting  in  <code><font size="2">[syslog]</font></code> stanzas can include IPv6 addresses in the standard <code><font size="2">[host]:port</font></code> format.
</li></ul><h3> <a name="configuresplunkforipv6_distributed_search_configuration_for_ipv6"><span class="mw-headline" id="Distributed_search_configuration_for_IPv6">Distributed search configuration for IPv6</span></a></h3>
<p>Your Splunk <b>distributed search</b> deployment can use IPv6; the following are supported in <code><font size="2">distsearch.conf</font></code>:
</p>
<ul><li> The <code><font size="2">servers</font></code> setting can include IPv6 addresses in the standard <code><font size="2">[host]:port</font></code> format
</li><li> However, <code><font size="2">heartbeatMcastAddr</font></code> has not been updated to support IPv6 addresses; this setting is deprecated in Splunk 4.3 and will be removed from the product in a future release.
</li></ul><h3> <a name="configuresplunkforipv6_access_to_splunk_web_over_ipv6"><span class="mw-headline" id="Access_to_Splunk_Web_over_IPv6">Access to Splunk Web over IPv6</span></a></h3>
<p>If your network policy allows or requires IPv6 connections from web browsers, you can configure the splunkweb service to behave differently than splunkd. Starting in 4.3, <code><font size="2">web.conf</font></code> supports a <code><font size="2">listenOnIPv6</font></code> setting. This setting behaves exactly like the one in <code><font size="2">server.conf</font></code> described above, but applies only to Splunk Web.
</p><p>The existing <code><font size="2">web.conf mgmtHostPort</font></code> setting has been extended to allow it to take IPv6 addresses if they are enclosed in square brackets. Therefore, if you configure splunkd to only listen on IPv6 (via the setting in <code><font size="2">server.conf</font></code> described above), you must change this from <code><font size="2">127.0.0.1:8089</font></code> to <code><font size="2">[::1]:8089</font></code>.
</p>
<h3> <a name="configuresplunkforipv6_the_splunk_cli_and_ipv6"><span class="mw-headline" id="The_Splunk_CLI_and_IPv6">The Splunk CLI and IPv6</span></a></h3>
<p>The Splunk CLI can communicate to splunkd over IPv6. This works if you have set <code><font size="2">mgmtHostPort</font></code> in <code><font size="2">web.conf</font></code>, defined the <code><font size="2">$SPLUNK_URI</font></code> environment variable, or use the <code><font size="2">-uri</font></code> command line option. When using the <code><font size="2">-uri</font></code> option, be sure to enclose IPv6 IP address in brackets and the entire address and port in quotes, for example: <code><font size="2">-uri "[2001:db8::1]:80"</font></code>. 
</p>
<h3> <a name="configuresplunkforipv6_ipv6_and_sso"><span class="mw-headline" id="IPv6_and_SSO"> IPv6 and SSO </span></a></h3>
<p>If you are using IPv6 with SSO, you do not use the square bracket notation for the <code><font size="2">trustedIP</font></code> property, as shown in the example below. This applies to both <code><font size="2">web.conf</font></code> and <code><font size="2">server.conf</font></code>.
</p><p>In the following <code><font size="2">web.conf</font></code> example, the <code><font size="2">mgmtHostPort</font></code> attribute uses the square bracket notation, but the <code><font size="2">trustedIP</font></code> attribute does not:
</p>
<code><font size="2"><br>[settings]<br>mgmtHostPort = [::1]:8089<br>startwebserver = 1<br>listenOnIPv6=yes<br>trustedIP=2620:70:8000:c205:250:56ff:fe92:1c7,::1,2620:70:8000:c205::129<br>SSOMode = strict<br>remoteUser = X-Remote-User<br>tools.proxy.on = true <br></font></code>
<p>For more information on SSO, see "Configure Single Sign-on" in the Securing Splunk Enterprise manual.
</p>
<a name="configuresplunksecurity"></a><h2> <a name="configuresplunksecurity_secure_your_configuration"><span class="mw-headline" id="Secure_your_configuration"> Secure your configuration</span></a></h2>
<p>If you haven't already, this is a good time to make sure that Splunk and your data are secure. Taking the proper steps to secure Splunk reduces the attack surface and mitigates the risk and impact of most vulnerabilities. 
</p><p>Some key actions you should take after installation: 
</p>
<ul><li> Set up users and roles. You can configure users using Splunks native authentication and/or use LDAP to manage users. See About user authentication
</li><li> Set up certificate authentication (SSL). Splunk ships with a set of default certificates that should be replaced for secure authentication. We provide guidelines and further instructions for adding SSL encryption and authentication and Configure secure authentication.
</li></ul><p>The Securing Splunk Enterprise manual provides more information about ways you can secure Splunk. Including a checklist for hardening your configuration. See
Securing Splunk Enterprise for more information.
</p>
<h1>Configure Splunk licenses</h1><a name="howsplunklicensingworks"></a><h2> <a name="howsplunklicensingworks_how_splunk_enterprise_licensing_works"><span class="mw-headline" id="How_Splunk_Enterprise_licensing_works"> How Splunk Enterprise licensing works</span></a></h2>
<p>Splunk Enterprise takes in data from sources you designate and processes it so that you can analyze it. We call this process indexing. For information about the exact indexing process, refer to "What Splunk Enterprise does with your data" in the Getting Data In Manual. 
</p><p>Splunk Enterprise licenses specify how much data you can index per calendar day (from midnight to midnight by the clock on the <b>license master</b>). 
</p><p>Any host in your Splunk Enterprise infrastructure that performs indexing must be licensed to do so. You can either run a standalone indexer with a license installed locally, or you can configure one of your Splunk Enterprise instances as a <b>license master</b> and set up a <b>license pool</b> from which other indexers, configured as <b>license slaves</b>, can draw.  
</p><p>In addition to indexing volume, access to some Splunk Enterprise features requires an Enterprise license. For more information about different types of licenses, read "<a href="#typesofsplunklicenses" class="external text">Types of Splunk licenses</a>" in this manual. 
</p><p>For information about upgrading an existing license, see "Migrate to the new Splunk licenser" in the Installation Manual.
</p>
<h3> <a name="howsplunklicensingworks_about_the_connection_between_the_license_master_and_license_slaves"><span class="mw-headline" id="About_the_connection_between_the_license_master_and_license_slaves">About the connection between the license master and license slaves</span></a></h3>
<p>When a license master instance is configured, and license slaves are added to it, the license slaves communicate their usage to the license master every minute. If the license master is unreachable for any reason, the license slave starts a 72 hour timer. If the license slave cannot reach the license master for 72 hours, search is blocked on the license slave (although indexing continues). Users will not be able to search data in the indexes on the license slave until that slave can reach the license master again.
</p>
<h3> <a name="howsplunklicensingworks_splunk_enterprise_license_lifecycle.28s.29"><span class="mw-headline" id="Splunk_Enterprise_license_lifecycle.28s.29">Splunk Enterprise license lifecycle(s)</span></a></h3>
<p>When you first install a downloaded copy of Splunk Enterprise, that instance uses a 60 day Trial Enterprise license. This license allows you to try out all of the Enterprise features in Splunk Enterprise for 60 days, and to index up to 500 MB of data per day. 
</p><p>Once the 60 day trial expires (and if you have not purchased and installed an <a href="#typesofsplunklicenses_enterprise_license" class="external text">Enterprise license</a>), you are given the option to switch to Splunk Free. Splunk Free includes <a href="#moreaboutsplunkfree_what_is_included" class="external text">a subset of the features of Splunk Enterprise</a> and is intended for use in standalone deployments and for short-term forensic investigations. It allows you to index up to 500 MB of data a day indefinitely. 
</p><p><b>Important:</b> Splunk Free does not include authentication or scheduled searches/alerting. This means that any user accessing your installation (via Splunk Web or the CLI) will not have to provide credentials. Additionally, scheduled saved searches/alerts will no longer fire. 
</p><p>If you want to continue using Splunk Enterprise features after the 60 day Trial expires, you must purchase an Enterprise license. Contact a Splunk sales rep to learn more. 
</p><p>Once you have purchased and downloaded an Enterprise license, you can install it on your instance and access Splunk Enterprise features. Read "<a href="#typesofsplunklicenses" class="external text">Types of Splunk licenses</a>" in this manual for information about Enterprise features.
</p>
<a name="typesofsplunklicenses"></a><h2> <a name="typesofsplunklicenses_types_of_splunk_software_licenses"><span class="mw-headline" id="Types_of_Splunk_software_licenses"> Types of Splunk software licenses</span></a></h2>
<p>Each Splunk instance requires a license. Splunk licenses specify how much data a given Splunk instance can index and what features you have access to. This topic discusses the various license types and options. 
</p><p>In general, there are four types of licenses:
</p>
<ul><li> The Enterprise license enables all enterprise features, such as authentication and distributed search.
</li><li> The Free license allows you a limited indexing volume and disables authentication, but is perpetual.
</li><li> The Forwarder license allows you to forward, but not index, data and enables authentication.
</li><li> The Beta license, typically enables enterprise features, but is restricted to Splunk Beta releases.
</li></ul><p>Also discussed in this topic are some special licensing considerations if your deployment includes distributed search or index replication. 
</p><p>For information about upgrading an existing license, see "Migrate to the new Splunk licenser" in the Installation Manual.
</p>
<h3> <a name="typesofsplunklicenses_enterprise_license"><span class="mw-headline" id="Enterprise_license">Enterprise license</span></a></h3>
<p>Splunk Enterprise is the standard Splunk license. It allows you to use all Splunk Enterprise features, including authentication, distributed search, deployment management, scheduling of alerts, and role-based access controls. Enterprise licenses are available for purchase and can be any indexing volume. Contact Splunk Sales for more information. 
</p><p>The following are additional types of Enterprise licenses, which include all the same features:
</p>
<h4><font size="3"><b><i> <a name="typesofsplunklicenses_enterprise_trial_license"><span class="mw-headline" id="Enterprise_trial_license">Enterprise trial license</span></a></i></b></font></h4>
<p>When you download Splunk for the first time, you are asked to register. Your registration authorizes you to receive an Enterprise <b>trial</b> license, which allows a maximum indexing volume of 500 MB/day. The Enterprise trial license expires 60 days after you start using Splunk. If you are running with a Enterprise trial license and your license expires, Splunk requires you to switch to a <a href="#moreaboutsplunkfree" class="external text">Splunk Free license</a>. 
</p><p>Once you have installed Splunk, you can choose to run Splunk with the Enterprise trial license until it expires, purchase an Enterprise license, or <a href="#moreaboutsplunkfree_switching_to_free_from_an_enterprise_trial_license" class="external text">switch to the Free license</a>, which is included.
</p><p><b>Note:</b> The Enterprise trial license is also sometimes referred to as "download-trial."
</p>
<h4><font size="3"><b><i> <a name="typesofsplunklicenses_sales_trial_license"><span class="mw-headline" id="Sales_trial_license">Sales trial license</span></a></i></b></font></h4>
<p>If you are working with Splunk Sales, you can request trial Enterprise licenses of varying size and duration. The Enterprise trial license expires 60 days after you start using Splunk. If you are preparing a pilot for a large deployment and have requirements for a longer duration or higher indexing volumes during your trial, contact Splunk Sales or your sales rep directly with your request.
</p>
<h3> <a name="typesofsplunklicenses_free_license"><span class="mw-headline" id="Free_license">Free license</span></a></h3>
<p>The Free license includes 500 MB/day of indexing volume, is free (as in beer), and has no expiration date. 
</p><p>The following features that are available with the Enterprise license are <b>disabled in Splunk Free:</b>
</p>
<ul><li> Multiple user accounts and role-based access controls
</li><li> Distributed search
</li><li> Forwarding in TCP/HTTP formats (you can forward data to other Splunk instances, but not to non-Splunk instances)
</li><li> Deployment management (including for clients)
</li><li> Alerting/monitoring
</li><li> Authentication and user management, including native authentication, LDAP, and scripted authentication.
<ul><li> There is no login.  The command line or browser can access and control all aspects of Splunk with no user/password prompt.
</li><li> You cannot add more roles or create user accounts. 
</li><li> Searches are run against all public indexes, 'index=*' and restrictions on search such as user quotas, maximum per-search time ranges, search filters are not supported. 
</li><li> The capability system is disabled, all capabilities are enabled for all users accessing Splunk.
</li></ul></li></ul><p><br><a href="#moreaboutsplunkfree" class="external text">Learn more about the free version of Splunk</a> in this manual.
</p>
<h3> <a name="typesofsplunklicenses_forwarder_license"><span class="mw-headline" id="Forwarder_license"> Forwarder license </span></a></h3>
<p>This license allows forwarding (but not indexing) of unlimited data, and also enables security on the instance so that users must supply username and password to access it. (The free license can also be used to forward an unlimited amount of data, but has no security.)
</p><p>Forwarder licenses are included with Splunk; you do not have to purchase them separately.
</p><p>Splunk offers several forwarder options:
</p>
<ul><li> The universal forwarder has the license enabled/applied automatically; no additional steps are required post-installation.
</li><li> The light forwarder uses the same license, but you must manually enable it by changing to the Forwarder license group.
</li><li> The heavy forwarder  must also be manually converted to the Forwarder license group. If any indexing is to be performed, the instance should instead be given access to an Enterprise license <b>stack</b>. Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><h3> <a name="typesofsplunklicenses_beta_license"><span class="mw-headline" id="Beta_license"> Beta license </span></a></h3>
<p>Splunk's Beta releases require a different license that is not compatible with other Splunk releases. Also, if you are evaluating a Beta release of Splunk, it will not run with a Free or Enterprise license. Beta licenses typically enable Enterprise features, they are just restricted to Beta releases. If you are evaluating a Beta version of Splunk, it will come with its own license.
</p>
<h3> <a name="typesofsplunklicenses_licenses_for_search_heads_.28for_distributed_search.29"><span class="mw-headline" id="Licenses_for_search_heads_.28for_distributed_search.29"> Licenses for search heads (for distributed search) </span></a></h3>
<p>A <b>search head</b> is a Splunk instance that distributes searches to other Splunk indexers. Although search heads don't usually index any data locally, you will still want to use a license to restrict access to them. 
</p><p>There is no special type of license specifically for search heads, that is to say, there is no "Search head license".  However, <b>you must have an Enterprise license to configure a search head</b>, and how you arrange for licensing for the search head depends on the version of Splunk:
</p>
<ul><li> In the past, <b>for versions prior to 4.2,</b> Splunk suggested using a separate forwarder license on each search head. This was simply because forwarder licenses do not allow indexing, but require authentication for access to the search head.
</li><li> Now, <b>for versions after 4.2,</b> Splunk recommends that, instead of assigning a separate license to each peer, you add the search heads to an Enterprise <b>license pool</b> even if they are not expected to index any data.  Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" and "<a href="#createalicensepool" class="external text">Create or edit a license pool</a>."
</li></ul><p><b>Note:</b> If your existing search head has a pre-4.2 forwarder license installed, the forwarder license will not be read after you upgrade.
</p>
<h3> <a name="typesofsplunklicenses_licenses_for_indexer_cluster_nodes_.28for_index_replication.29"><span class="mw-headline" id="Licenses_for_indexer_cluster_nodes_.28for_index_replication.29"> Licenses for indexer cluster nodes (for index replication) </span></a></h3>
<p>As with any Splunk deployment, your licensing requirements are driven by the volume of data your indexers process. Contact your Splunk sales representative to purchase additional license volume.
</p><p>There are just a few license issues that are specific to index replication:
</p>
<ul><li> All cluster nodes, including masters, peers, and search heads, need to be in an Enterprise license pool, even if they're not expected to index any data.
</li><li> Cluster nodes must share the same licensing configuration.
</li><li> Only incoming data counts against the license; replicated data does not.
</li><li> You cannot use index replication with a free license. 
</li></ul><p>Read more about "System requirements and other deployment considerations" in the <i>Managing Indexers and Clusters</i> manual.
</p>
<a name="groups,stacks,pools,andotherterminology"></a><h2> <a name="groups,stacks,pools,andotherterminology_groups.2c_stacks.2c_pools.2c_and_other_terminology"><span class="mw-headline" id="Groups.2C_stacks.2C_pools.2C_and_other_terminology"> Groups, stacks, pools, and other terminology</span></a></h2>
<p>The licensing functionality of Splunk has changed a lot starting in version 4.2. You can aggregate compatible licenses into stacks of available license volume and define pools of indexers to use license volume from a given stack.
</p><p><b>Splunk Free users:</b> This functionality is only relevant for Enterprise licenses. If you're running a standalone instance of Splunk Free, groups, pools, and stacks are not needed. 
</p><p><img alt="License terms diagram" src="images/3/3e/License_terms.png" width="757" height="347"></p>
<h3> <a name="groups,stacks,pools,andotherterminology_pools"><span class="mw-headline" id="Pools">Pools</span></a></h3>
<p>Starting in version 4.2, you can define a <b>pool</b> of license volume from a given license <b>license stack</b> and specify other indexing Splunk instances as members of that pool for the purposes of volume usage and tracking. 
</p><p>A license pool is made up of a single <b>license master</b> and zero or more <b>license slave</b> instances of Splunk configured to use licensing volume from a set license or <b>license stack</b>.
</p>
<h3> <a name="groups,stacks,pools,andotherterminology_stacks"><span class="mw-headline" id="Stacks">Stacks</span></a></h3>
<p>Starting in version 4.2, certain types of Splunk licenses can be aggregated together, or <b>stacked</b> so that the available license volume is the sum of the volumes of the individual licenses. 
</p><p>This means you can increase your indexing volume capacity over time as you need to without having to swap out licenses. Instead, you simply purchase additional capacity and add it to the appropriate stack.
</p>
<ul><li> <a href="#typesofsplunklicenses_enterprise_license" class="external text">Enterprise licenses and sales trial licenses</a> can be stacked together, and with each other. 
</li><li> The Enterprise *trial* license that is included with the standard Splunk download package cannot be included in a stack. The Enterprise trial license is designed for standalone use and is its own group. Until you install an Enterprise or sales trial license, you will not be able to create a stack or define a pool for other indexers to use.
</li><li> The <a href="#moreaboutsplunkfree" class="external text">Splunk Free license</a> cannot be stacked with other licenses, including Splunk Free licenses.
</li><li> The <a href="#typesofsplunklicenses_forwarder_license" class="external text">forwarder license</a> cannot be stacked with other licenses, including forwarder licenses.
</li></ul><h3> <a name="groups,stacks,pools,andotherterminology_groups"><span class="mw-headline" id="Groups">Groups</span></a></h3>
<p>A <b>license group</b> contains one or more stacks. A stack can be a member of only one group, and only one group can be "active" in your Splunk installation at a time. Specifically this means that a given license master can only administer pools of licenses of one group type at a time. The groups are:
</p>
<ul><li> Enterprise/sales trial group -- This group allows stacking of purchased Enterprise licenses, and sales trial licenses (which are Enterprise licenses with a set expiry date, NOT the same thing as the downloaded Enterprise trial). 
</li><li> Enterprise trial group -- This is the default group when you first install a new Splunk instance. You cannot combine multiple Enterprise trial licenses into a stack and create pools from it. <b>If you switch to a different group, you will not be able to switch back to the Enterprise trial group.</b>
</li><li> Free group -- This group exists to accommodate Splunk Free installations. When an Enterprise trial license expires after 60 days, that Splunk instance is converted to the Free group. You cannot combine multiple Splunk Free licenses into a stack and create pools from it.
</li><li> Forwarder group -- This group exists for the purposes of configuring Splunk as a universal forwarder or light forwarder. These types of forwarders do not perform any indexing, and therefore aren't really managed via the Licensing pages in Manager, but do belong to a license group. If you change the license group of a Splunk instance to the Forwarder group, it assumes that Splunk instance is configured as a forwarder and will not be indexing any data. See <b>forwarders</b> and <a href="#typesofsplunklicenses_forwarder_license" class="external text">"Forwarder licenses"</a> for more information.
</li></ul><h3> <a name="groups,stacks,pools,andotherterminology_license_slaves"><span class="mw-headline" id="License_slaves">License slaves</span></a></h3>
<p>A license slave is a member of one or more license pools. A license slave's access to license volume is controlled by its license master.
</p>
<h3> <a name="groups,stacks,pools,andotherterminology_license_master"><span class="mw-headline" id="License_master">License master</span></a></h3>
<p>A license master controls one or more license slaves. From the license master, you can define pools, add licensing capacity, and manage license slaves.
</p>
<a name="installalicense"></a><h2> <a name="installalicense_install_a_license"><span class="mw-headline" id="Install_a_license"> Install a license</span></a></h2>
<p>This topic discusses installing new licenses. You can install multiple licenses on a Splunk <b>license master</b>. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><p>For information about upgrading an existing license, see "Migrate to the new Splunk licenser" in the Installation Manual.
</p>
<h3> <a name="installalicense_add_a_new_license"><span class="mw-headline" id="Add_a_new_license">Add a new license</span></a></h3>
<p>To add a new license:
</p><p><b>1.</b> Navigate to <b>Settings &gt; Licensing</b>. 
</p><p><b>2.</b> Click <b>Add license</b>.
</p><p><img alt="AddNewLiceBubb.png" src="images/c/c3/AddNewLiceBubb.png" width="400" height="234"></p><p><b>3.</b> Either click <b>Choose file</b> and browse for your license file and select it, or click <b>copy &amp; paste the license XML directly...</b> and paste the text of your license file into the provided field. 
</p><p><b>4.</b> Click <b>Install</b>. If this is the first Enterprise license that you are installing, you must restart Splunk. Your license is installed.
</p>
<a name="configurealicensemaster"></a><h2> <a name="configurealicensemaster_configure_a_license_master"><span class="mw-headline" id="Configure_a_license_master"> Configure a license master</span></a></h2>
<p>This topic discusses configuring a Splunk instance as a <b>license master</b>. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><h3> <a name="configurealicensemaster_kinds_of_license_masters"><span class="mw-headline" id="Kinds_of_license_masters">Kinds of license masters</span></a></h3>
<p>There are two basic styles of license master:
</p>
<ul><li> <b>Standalone</b> license master
<ul><li> If you have a single Splunk indexer and want to manage its licenses, you can run it as its own license master, install one or more Enterprise licenses on it and it will manage itself as a license slave.
</li><li> When you first download and install Splunk Enterprise, it includes a 500 MB 60-day Enterprise Trial license. This instance is automatically configured as a standalone license master, and you cannot create a pool or define any <b>license slaves</b> for this type of license. If you want to create <a href="#groups,stacks,pools,andotherterminology" class="external text">one or more stacks or pools</a> and assign multiple indexers to them, you must purchase and install an <a href="#typesofsplunklicenses_enterprise_license" class="external text">Enterprise license</a>. To install a license, follow the instructions in "<a href="#installalicense" class="external text">Install a license</a>" in this manual. 
</li></ul></li><li> <b>Central</b> license master
<ul><li> If you have more than one indexer and want to manage their access to purchased license capacity from a central location, configure a central license master and add the indexers to it as <b>license slaves</b>. 
</li><li> If the license master is also an indexer, it will be its own license master as well, but Splunk recommends that if you have a <b>search head</b>, you designate it as the license master.
</li><li> If you have a large environment with multiple search heads, you might want to have some or all search heads that are not the license master distribute searches to the license master, for two reasons:
<ul><li> You can run searches against the license logs.
</li><li> If an unusual condition occurs on the search head (for example you have a time-limited license and it will expire in 5 days), this condition will be visible on the search head when running a search, as part of the info messages attached to search results.
</li></ul></li></ul></li></ul><h3> <a name="configurealicensemaster_configure_a_central_license_master"><span class="mw-headline" id="Configure_a_central_license_master">Configure a central license master</span></a></h3>
<p>By default, a standalone instance of Splunk is its own license master. To configure a central license master, <a href="#installalicense" class="external text">install one or more Enterprise licenses</a>. 
</p><p>Once an Enterprise license is installed, you can <a href="#createalicensepool" class="external text">create one or more stacks and pools</a> to access the installed license, and <a href="#manageyourlicenses" class="external text">manage them</a> from the license master.
</p>
<a name="configurealicenseslave"></a><h2> <a name="configurealicenseslave_configure_a_license_slave"><span class="mw-headline" id="Configure_a_license_slave"> Configure a license slave</span></a></h2>
<p>This topic discusses configuring a Splunk indexer as a <b>license slave</b>. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li><li> Read<a href="#configurealicensemaster" class="external text">Configure a license master</a>" in this manual for instructions on setting up a license master. 
</li><li> Read "<a href="#licenserclicommands" class="external text">Manage licenses from the CLI</a>" in this manual for help with performing these tasks from the command line.
</li></ul><p><b>1.</b> On the indexer you want to configure as a license slave, log into Splunk Web and navigate to <b>Settings &gt; Licensing</b>.
</p><p><b>2.</b> Click <b>Change to Slave</b>. 
</p><p><img alt="ChangeSlavBubb.png" src="images/9/90/ChangeSlavBubb.png" width="424" height="126"></p><p><b>3.</b> Switch the radio button from <b>Designate this Splunk instance, &lt;<i>this indexer</i>&gt;, as the master license server</b> to <b>Designate a different Splunk instance as the master license server</b>.
</p><p><b>4.</b> Specify the license master to which this license slave should report. You must provide either an IP address or a hostname and the <b>Splunk management port</b>, which is 8089 by default. 
</p><p><b>Note:</b> The IP address can be specified in IPv4 or IPv6 format. For detailed information on IPv6 support, read "<a href="#configuresplunkforipv6" class="external text">Configure Splunk for IPv6</a>" in this manual.
</p><p><b>5.</b> Click <b>Save</b>. If this instance does not already have an Enterprise license installed, you must restart Splunk. This indexer is now configured as a license slave. 
</p><p>To switch back, navigate to <b>Settings &gt; Licensing</b> and click <b>Switch to local master</b>. If this instance does not already have an Enterprise license installed, you must restart Splunk for this change to take effect.
</p>
<a name="createalicensepool"></a><h2> <a name="createalicensepool_create_or_edit_a_license_pool"><span class="mw-headline" id="Create_or_edit_a_license_pool"> Create or edit a license pool</span></a></h2>
<p>This topic discusses creating a license pool from one or more installed licenses, as well as editing an existing license pool. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li><li> Read "<a href="#installalicense" class="external text">Install a license</a>" to learn more about installing licenses. 
</li><li> Read "<a href="#licenserclicommands" class="external text">Manage licenses from the CLI</a>" in this manual for help with performing these tasks from the command line.
</li></ul><p>When you first download and install Splunk, it includes a 500 MB 60 day Enterprise Trial license. This instance of Splunk is automatically configured as a stand-alone <b>license master</b>, and you cannot create a pool or define any <b>license slaves</b> for this type of license. If you want to create <a href="#groups,stacks,pools,andotherterminology" class="external text">one or more stacks or pools</a> and assign multiple indexers to them, you must purchase and install an <a href="#typesofsplunklicenses_enterprise_license" class="external text">Enterprise license</a>. 
</p><p>In the following example of <b>Settings &gt; Licensing</b>, a 100 MB Enterprise license has just been installed onto a brand new Splunk installation:
</p><p><img alt="License installed Bubb.png" src="images/b/b0/License_installed_Bubb.png" width="681" height="729"></p><p>When you install an Enterprise license onto a brand new Splunk server, Splunk automatically creates an Enterprise <b>license stack</b> called Splunk Enterprise Stack from it and defines a default <b>license pool</b> for it called <code><font size="2">auto_generated_pool_enterprise</font></code>. 
</p><p><b>The default configuration for this default pool adds any license slave that connects to this license master to the pool.</b> You can edit the pool to change this configuration, to add more indexers to it, or create a new license pool from this stack. 
</p>
<h3> <a name="createalicensepool_to_edit_an_existing_license_pool"><span class="mw-headline" id="To_edit_an_existing_license_pool">To edit an existing license pool</span></a></h3>
<p><b>1.</b> Next to the license pool you want to edit, click <b>Edit</b>. The Edit license pool page is displayed.
</p><p><b>2.</b> If desired, change the allocation or alter <a href="#addanindexertoalicensepool_how_indexers_access_license_pools" class="external text">how indexers are permitted to access this pool</a>. You can also change the description, but not the name of the pool.
</p><p><b>3.</b> Click <b>Submit</b>.
</p>
<h3> <a name="createalicensepool_to_create_a_new_license_pool"><span class="mw-headline" id="To_create_a_new_license_pool">To create a new license pool</span></a></h3>
<p><b>Important:</b> Before you can create a new license pool from the default Enterprise stack, you must make some indexing volume available by either editing the <code><font size="2">auto_generated_pool_enterprise</font></code> pool and reducing its allocation, or deleting the pool entirely. Click <b>Delete</b> next to the pool's name to delete it. 
</p><p><b>1.</b> Click <img alt="License add pool.png" src="images/f/f8/License_add_pool.png" width="85" height="29"> toward the bottom of the page. The Create new license pool page is displayed. 
</p><p><b>2.</b> Specify a name and optionally, a description for the pool. 
</p><p><b>3.</b> Set the allocation for this pool. The allocation is how much of the overall stack's licensing volume is available for use by the indexers who belong to this pool. The allocation can be a specific value, or the entire amount of indexing volume available in the stack, as long as it is not allocated to any other pool. 
</p><p><b>4.</b> Specify how indexers are to access this pool. The options are: 
</p>
<ul><li> Any indexer in your environment that is configured as license slave can connect to this license pool and use the license allocation within it.
</li><li> Only indexers that you specify can connect to this pool and use the license allocation within it.  
</li></ul><p><b>5.</b> To allow a specific indexer to draw from the pool, click the plus sign next to the name of the indexer in the list of Available indexers to move it into the list of Associated indexers.
</p>
<a name="addanindexertoalicensepool"></a><h2> <a name="addanindexertoalicensepool_add_an_indexer_to_a_license_pool"><span class="mw-headline" id="Add_an_indexer_to_a_license_pool"> Add an indexer to a license pool</span></a></h2>
<p>This topic discusses adding indexers to existing <b>license pools</b>. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><h3> <a name="addanindexertoalicensepool_how_indexers_access_license_pools"><span class="mw-headline" id="How_indexers_access_license_pools">How indexers access license pools</span></a></h3>
<p>Access to a license pool's <b>stack</b> is controlled by that pool's <b>license master</b>. A pool can be configured to allow access only to specific indexers, or to all indexers who connect to it by specifying the URI and management port of the license master. 
</p>
<h4><font size="3"><b><i> <a name="addanindexertoalicensepool_add_a_specific_indexer"><span class="mw-headline" id="Add_a_specific_indexer">Add a specific indexer</span></a></i></b></font></h4>
<p>Follow these two basic steps to give a specific indexer access to a given license pool's stack:
</p><p><b>1.</b> Configure the indexer to be a license slave and give it the license master's URI and management port. To do this, follow the instructions in "<a href="#configurealicenseslave" class="external text">Configure a license slave</a>" in this manual.
</p><p><b>2.</b> Configure the pool on the license manager to accept access from that indexer. To do this, follow the instructions in "<a href="#createalicensepool_to_edit_an_existing_license_pool" class="external text">Create or edit a license pool</a>" to edit a license pool, choose the radio button option to only allow access to <b>Specific indexers</b> and then click the plus sign next to the names of the indexer in the list of Available indexers to move them into the list of Associated indexers.
</p>
<h4><font size="3"><b><i> <a name="addanindexertoalicensepool_add_any_indexer_that_connects"><span class="mw-headline" id="Add_any_indexer_that_connects">Add any indexer that connects</span></a></i></b></font></h4>
<p>Follow these steps to give all indexers who connect to this license master access to a given license pool's stack:
</p><p><b>1.</b> Configure the indexer to be a license slave and give it the license master's URI and management port. To do this, follow the instructions in "<a href="#configurealicenseslave" class="external text">Configure a license slave</a>" in this manual.
</p><p><b>2.</b> Configure the pool on the license master to accept access from any indexer. To do this, follow the instructions in "<a href="#createalicensepool_to_edit_an_existing_license_pool" class="external text">Create or edit a license pool</a>" to edit a license pool, and choose the radio button option to allow access from <b>Any indexer that connects</b>.
</p>
<a name="licenserclicommands"></a><h2> <a name="licenserclicommands_manage_licenses_from_the_cli"><span class="mw-headline" id="Manage_licenses_from_the_CLI"> Manage licenses from the CLI</span></a></h2>
<p>This topic describes using the Splunk CLI to monitor and manage your Splunk licenses. Before you continue, review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><p>This topic only covers what CLI commands you can use to interact with Splunk's licenser-related objects. Some of these commands also have required and optional arguments that you can specify for each object. For the complete syntax and usage examples, refer to Splunk's CLI help.
</p>
<ul><li> Read "<a href="#aboutthecli" class="external text">About the CLI</a>" in this manual for an introduction to using the Splunk command line interface.
</li></ul><p>For information on managing licenses through Splunk's REST API, refer to "Licenses" in the REST API Reference Manual.
</p>
<h3> <a name="licenserclicommands_cli_licenser_commands_and_objects"><span class="mw-headline" id="CLI_licenser_commands_and_objects"> CLI licenser commands and objects </span></a></h3>
<p>Using the Splunk CLI, you can add, edit, list, and remove licenses and licenser-related objects. The available commands are:
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Command
</th><th bgcolor="#C0C0C0"> Object(s)
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr><td valign="center" align="left"> add
</td><td valign="center" align="left"> licenses, licenser-pools
</td><td valign="center" align="left"> Add a license or a pool of licenses to a license stack. This command is only available if you have an Enterprise license.
</td></tr><tr><td valign="center" align="left"> edit
</td><td valign="center" align="left"> licenser-localslave, licenser-pools
</td><td valign="center" align="left"> Edit the attributes of a local licenser-slave node or a pool of licenses within a license stack. This command is only available if you have an Enterprise license.
</td></tr><tr><td valign="center" align="left"> list
</td><td valign="center" align="left"> licenser-groups, licenser-localslave, licenser-messages, licenser-pools, licenser-slaves, licenser-stacks, licenses
</td><td valign="center" align="left"> Depending on the licenser-related object specified, lists either the attributes of that object or members of that object.
</td></tr><tr><td valign="center" align="left"> remove
</td><td valign="center" align="left"> licenser-pools, licenses
</td><td valign="center" align="left"> Remove licenses or license pools from a license stack.
</td></tr></table><p>License-related objects are: 
</p>
<table cellpadding="5" cellspacing="0" border="1" width="100%"><tr><th bgcolor="#C0C0C0"> Object
</th><th bgcolor="#C0C0C0"> Description
</th></tr><tr><td valign="center" align="left"> licenser-groups
</td><td valign="center" align="left"> the different license groups you can switch to.
</td></tr><tr><td valign="center" align="left"> licenser-localslave
</td><td valign="center" align="left"> a local indexer's configuration.
</td></tr><tr><td valign="center" align="left"> licenser-messages
</td><td valign="center" align="left"> the alerts or warnings about the state of your licenses.
</td></tr><tr><td valign="center" align="left"> licenser-pools
</td><td valign="center" align="left"> a pool, or virtual license. A stack can be divided into various pools, with multiple slaves sharing the quota of each pool.
</td></tr><tr><td valign="center" align="left"> licenser-slaves
</td><td valign="center" align="left"> all the slaves that have contacted the master.
</td></tr><tr><td valign="center" align="left"> licenser-stacks
</td><td valign="center" align="left"> this object represents a stack of licenses. A stack contains licenses of the same type and are cumulative.
</td></tr><tr><td valign="center" align="left"> licenses
</td><td valign="center" align="left"> all licenses for this Splunk instance.
</td></tr></table><h3> <a name="licenserclicommands_common_licenser-related_tasks"><span class="mw-headline" id="Common_licenser-related_tasks"> Common licenser-related tasks </span></a></h3>
<p>The following are examples of common licenser-related tasks. 
</p>
<h4><font size="3"><b><i> <a name="licenserclicommands_managing_licenses"><span class="mw-headline" id="Managing_licenses"> Managing licenses </span></a></i></b></font></h4>
<p>To add a new license to the license stack, specify the path to the license file:
</p>
<code><font size="2"><br>./splunk add licenses /opt/splunk/etc/licenses/enterprise/enterprise.lic <br></font></code>
<p>To list all the licenses in a license stack:
</p>
<code><font size="2"><br>./splunk list licenses<br></font></code>
<p>List also displays the properties of each license, including the features it enables (features), the license group and stack it belongs to (group_id, stack_id), the indexing quote it allows (quota), and the license key that is unique for each license (license_hash).
</p><p>If a license expires, you can remove it from the license stack. To remove a license from the license stack, specify the license's hash:
</p>
<code><font size="2"><br>./splunk remove licenses BM+S8VetLnQEb1F+5Gwx9rR4M4Y91AkIE=781882C56833F36D<br></font></code>
<h4><font size="3"><b><i> <a name="licenserclicommands_managing_license_pools"><span class="mw-headline" id="Managing_license_pools"> Managing license pools </span></a></i></b></font></h4>
<p>You can create a license pool from one or more licenses in a license stack (if you have an <a href="#typesofsplunklicenses_enterprise_license" class="external text">Enterprise license</a>). Basically, a license stack can be carved up into multiple licenser pools. Each pool can have more than one license slave sharing the quota of the pool.
</p><p>To see all the license pools in all the license stacks:
</p>
<code><font size="2"><br>./splunk list licenser-pools<br></font></code>
<p>To add a license pool to the stack, you need to: name the pool, specify the stack that you want to add it to, and specify the indexing volume allocated to that pool:
</p>
<code><font size="2"><br>./splunk add licenser-pools pool01 -quota 10mb -slaves guid1,guid2 -stack_id enterprise<br></font></code>
<p>You can also specify a description for the pool and the slaves that are members of the pool (these are optional).
</p><p>You can edit the license pool's description, indexing quota, and slaves:
</p>
<code><font size="2"><br>./splunk edit licenser-pools pool01 -description "Test" -quota 15mb -slaves guid3,guid4 -append_slaves true<br></font></code>
<p>This basically adds a description for the pool, "Test", changes the quota from 10mb to 15mb, adds slaves guid3 and guid4 to the pool (instead of overwriting or replace guid1 and guid2).
</p><p>To remove a license pool from a stack, specify the name:
</p>
<code><font size="2"><br>./splunk remove pool01<br></font></code>
<h4><font size="3"><b><i> <a name="licenserclicommands_managing_license_slaves"><span class="mw-headline" id="Managing_license_slaves"> Managing license slaves </span></a></i></b></font></h4>
<p>A license slave is a member of one or more license pools. The license slaves access to license volume is controlled by its license master. 
</p><p>To list all the license slaves that have contacted the license master:
</p>
<code><font size="2"><br>./splunk list licenser-slaves<br></font></code>
<p>To list all the properties of the local license slave:
</p>
<code><font size="2"><br>./splunk list licenser-localslave<br></font></code>
<p>To add a license slave, edit the attributes of that local license slave node (specify the uri of the splunkd license master instance or 'self'):
</p>
<code><font size="2"><br>./splunk edit licenser-localslave -master_uri 'https://master:port'<br></font></code>
<h4><font size="3"><b><i> <a name="licenserclicommands_monitoring_license_status"><span class="mw-headline" id="Monitoring_license_status"> Monitoring license status </span></a></i></b></font></h4>
<p>You can use the list command to view messages (alerts or warnings) about the state of your licenses.
</p>
<code><font size="2"><br>./splunk list licenser-messages<br></font></code>

<h1>Manage Splunk licenses</h1><a name="manageyourlicenses"></a><h2> <a name="manageyourlicenses_manage_your_licenses"><span class="mw-headline" id="Manage_your_licenses"> Manage your licenses</span></a></h2>
<p>This topic discusses managing Splunk licenses. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li><li> Read "<a href="#licenserclicommands" class="external text">Manage licenses from the CLI</a>" in this manual for help with performing some of these tasks from the command line.
</li></ul><p>For information about upgrading an existing license, see "Migrate to the new Splunk licenser" in the Installation Manual.
</p>
<h3> <a name="manageyourlicenses_delete_a_license"><span class="mw-headline" id="Delete_a_license">Delete a license</span></a></h3>
<p>If a license expires, you can delete it. To delete one or more licenses:
</p><p><b>1.</b> On the license master, navigate to <b>System &gt; Licensing</b>. 
</p><p><img alt="License delete license.png" src="images/0/0d/License_delete_license.png" width="408" height="94"></p><p><b>2.</b> Click <b>Delete</b> next to the license you want to delete.
</p><p><b>3.</b> Click <b>Delete</b> again to confirm. 
</p><p><b>Note:</b> You cannot delete the last license in a list of licenses on a license master.
</p>
<h3> <a name="manageyourlicenses_view_license_usage"><span class="mw-headline" id="View_license_usage">View license usage</span></a></h3>
<p>You can monitor license usage across your deployment with the License Usage Report View. Access the view in <b>System &gt; Licensing</b> then clicking <b>Usage Report</b>. Read more about the <a href="#aboutsplunkslicenseusagereportview" class="external text">License Usage Report View</a> in the next chapter.
</p>
<a name="aboutlicenseviolations"></a><h2> <a name="aboutlicenseviolations_about_license_violations"><span class="mw-headline" id="About_license_violations"> About license violations</span></a></h2>
<p>This topic discusses license violations, how they come about, and how to resolve them. Before you proceed, you may want to review these topics:
</p>
<ul><li> Read "<a href="#howsplunklicensingworks" class="external text">How Splunk licensing works</a>" in this manual for an introduction to Splunk licensing.
</li><li> Read "<a href="#groups,stacks,pools,andotherterminology" class="external text">Groups, stacks, pools, and other terminology</a>" in this manual for more information about Splunk license terms.
</li></ul><h3> <a name="aboutlicenseviolations_what_are_license_violations_and_warnings.3f"><span class="mw-headline" id="What_are_license_violations_and_warnings.3F">What are license violations and warnings?</span></a></h3>
<p>Warnings and violations occur when you exceed the maximum indexing volume allowed for your license.
</p><p>If you exceed your licensed daily volume on any one calendar day, you will get a violation <i>warning</i>. The message persists for 14 days. <b>If you have 5 or more warnings on an Enterprise license or 3 warnings on a Free license in a rolling 30-day period, you are in <i>violation</i> of your license, and search will be disabled for the offending pool(s). Other pools will remain searchable and be unaffected, as long as the total license usage from all pools does not exceed the total license quota for the license master.</b>
</p><p>Search capabilities return when you have fewer than 5 (Enterprise) or 3 (Free) warnings in the previous 30 days, or when you apply a temporary reset license (available for Enterprise only). To obtain a reset license, contact your sales rep. See the Installation Manual for instructions on how to apply it.
</p><p><b>Note:</b> Summary indexing volume is not counted against your license, although in the event of a license violation, summary indexing will halt like any other non-internal search behavior.
</p><p><b>If you get a license warning, you have until midnight (going by the time on the license master) to resolve it before it counts against the total number of warnings within the rolling 30 day period.</b>
</p><p>During a license violation period:
</p>
<ul><li> <b>Splunk does not stop indexing your data.</b> Splunk only blocks search while you exceed your license.
</li><li> Searches to the <code><font size="2">_internal</font></code> index are not disabled. This means that you can still access the Indexing Status dashboard or run searches against <code><font size="2">_internal</font></code> to diagnose the licensing problem.
</li></ul><h3> <a name="aboutlicenseviolations_what_license_warnings_look_like"><span class="mw-headline" id="What_license_warnings_look_like">What license warnings look like</span></a></h3>
<p>If indexers in a pool exceed the license volume allocated to that pool, you will see a yellow warning banner across the top of Splunk Web: 
</p><p><img alt="License violation warning.png" src="images/5/51/License_violation_warning.png" width="667" height="29"></p><p>Clicking on the link in the banner takes you to <b>Settings &gt; Licensing</b>, where the warning shows up under the <b>Alerts</b> section of the page.  Click on a warning to get more information about it.
</p><p>A similar banner is shown on license slaves when a violation has occurred.
</p><p>Here are some of the conditions that will generate a licensing alert: 
</p>
<ul><li> When a slave becomes an orphan, there will be an alert (transient and fixable before midnight)
</li><li> When a pool has maxed out, there will be an alert (transient and fixable before midnight)
</li><li> When a stack has maxed out, there will be an alert (transient and fixable before midnight)
</li><li> When a warning is given to one or more slaves, there will be an alert (will stay as long as the warning is still valid within that last 30-day period)
</li></ul><h4><font size="3"><b><i> <a name="aboutlicenseviolations_about_the_connection_between_the_license_master_and_license_slaves"><span class="mw-headline" id="About_the_connection_between_the_license_master_and_license_slaves">About the connection between the license master and license slaves</span></a></i></b></font></h4>
<p>When you configure a license master instance and add license slaves to it, the license slaves communicate their usage to the license master every minute. If the license master is down or unreachable for any reason, the license slave starts a 72 hour timer. If the license slave cannot reach the license master for 72 hours, search is blocked on the license slave (although indexing continues). Users will not be able to search data in the indexes on the license slave until that slave can reach the license master again. 
</p><p>To find out if a license slave has been unable to reach the license master, look for an event that contains <code><font size="2">failed to transfer rows</font></code> in splunkd.log or search for it in the _internal index.
</p>
<h3> <a name="aboutlicenseviolations_how_to_avoid_license_violations"><span class="mw-headline" id="How_to_avoid_license_violations">How to avoid license violations</span></a></h3>
<p>To avoid license violations, monitor your license usage and ensure you have sufficient license volume to support it. If you do not have sufficient license volume, you need to either increase your license or decrease your indexing volume.
</p><p>The distributed management console contains alerts that you can enable, including one that monitors license usage. See "<a href="#platformalerts" class="external text">Platform alerts</a>" in this manual.
</p><p>Use the <b>License Usage</b> report to see details about and troubleshoot index volume in your deployment. Read about the <a href="#aboutsplunkslicenseusagereportview" class="external text">license usage report view</a> in the next chapter.
</p>
<h4><font size="3"><b><i> <a name="aboutlicenseviolations_correcting_license_warnings"><span class="mw-headline" id="Correcting_license_warnings">Correcting license warnings</span></a></i></b></font></h4>
<p>If Splunk is telling you to correct your license warning before midnight, you've probably already exceeded your quota for the day. This is called a "soft warning." The daily license quota will reset at midnight (at which point the soft warning will become a "hard warning"). You have until then to fix your situation and ensure that you won't go over quota tomorrow, too.
</p><p>Once you've already indexed data, there is no way to un-index data that will give you "wiggle room" back on your license. You need to get additional license room in one of these two ways:
</p>
<ul><li> Purchase a bigger license.
</li><li> Rearrange license pools if you have a pool with extra license room.
</li></ul><p>If you cannot do either of these, prevent a warning tomorrow by using less of your license. Take a look at <a href="#aboutsplunkslicenseusagereportview" class="external text">the License Usage Report View</a> to learn which data sources are contributing the most to your quota.
</p><p>Once you identify a data culprit, decide whether or not you need all the data it is emitting. If not, read "route and filter data" in the Forwarding Data Manual.
</p>
<h3> <a name="aboutlicenseviolations_answers"><span class="mw-headline" id="Answers">Answers</span></a></h3>
<p>Have questions? Visit Splunk Answers and see what questions and answers the Splunk community has around license violations.
</p>
<a name="swapthelicensemaster"></a><h2> <a name="swapthelicensemaster_swap_the_license_master"><span class="mw-headline" id="Swap_the_license_master"> Swap the license master</span></a></h2>
<p>So you've configured a license pool. What if you want to turn one of your license slaves into your pool's license master?
</p><p>This topic spells out the steps to do that. Big picture, first you promote a slave to master. Then you demote the old master to a slave. Details follow.
</p><p><br><b>1.</b> Remove the new license master from the licensing pool and set it up as a master.
</p>
<ul><li> Log into license slave (which will become new master).
</li><li> Navigate to <b>Settings &gt; Licensing</b>.
</li><li> Follow the prompts to <a href="#configurealicensemaster" class="external text">configure it as a new license master</a>.
</li><li> Restart Splunk.
</li></ul><p><b>2.</b> On the new license master, <a href="#installalicense" class="external text">add the license keys</a>. Check that the license keys match up to the old license master.
</p><p><b>3.</b> Make the other license slaves in the pool point to new license master.
</p>
<ul><li> On each of the slaves, navigate to <b>Settings &gt; Licensing</b>.
</li><li> Change the master license server URI to refer to the new license master and click <b>Save</b>.
</li><li> Restart Splunk on the license slave whose entry you just updated.
</li></ul><p><b>4.</b> Check that one of the license slaves is connected to the new license master.
</p><p><b>5.</b> Demote the old license master to a slave: 
</p>
<ul><li> On the old license master, navigate to <b>Settings &gt; Licensing &gt; Change to slave</b>.
</li><li> Ignore the restart prompt.
</li><li> On the "Change to slave" screen, point the new slave to the new license master ("Designate a different Splunk instance as the master license server").
</li></ul><p><b>6.</b> On the new license slave, stop Splunk and delete the old license file(s) under the /opt/splunk/etc/licenses/enterprise/ folder.
(Otherwise you'll have duplicate licenses and will get errors and/or warnings.)
</p><p><b>7.</b> On the new license slave, start Splunk and confirm that it connects to the new license master.
</p>
<h1>License Usage Report View</h1><a name="aboutsplunkslicenseusagereportview"></a><h2> <a name="aboutsplunkslicenseusagereportview_about_the_splunk_enterprise_license_usage_report_view"><span class="mw-headline" id="About_the_Splunk_Enterprise_license_usage_report_view"> About the Splunk Enterprise license usage report view </span></a></h2>
<h3> <a name="aboutsplunkslicenseusagereportview_introduction_to_the_license_usage_report_view"><span class="mw-headline" id="Introduction_to_the_license_usage_report_view"> Introduction to the license usage report view </span></a></h3>
<p>The license usage report view (LURV) is Splunk's new consolidated resource for questions related to your license capacity and indexed volume. It provides a fast and easy approach to determine the consumption of your Splunk license.  Directly from the Splunk Licensing page, get immediate insight into your daily Splunk indexing volume, as well as any license warnings. In addition, get a comprehensive view of the last 30 days of your Splunk usage with multiple reporting options.
</p><p>LURV displays detailed license usage information for your license pool. The dashboard is logically divided into two parts: one displays information about today's license usage, and any warning information in the current rolling window; the other shows historic license usage during the past 30 days.
</p><p>For every panel in LURV, you can click "Open in search" at the bottom left of the panel. This lets you interact with the search.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_access_the_license_usage_report_view"><span class="mw-headline" id="Access_the_license_usage_report_view"> Access the license usage report view </span></a></i></b></font></h4>
<p>Find LURV in <b>Settings &gt; Licensing &gt; Usage report</b>. 
</p><p><img alt="AccessLURV.png" src="images/2/2c/AccessLURV.png" width="400" height="230"></p><p>Access LURV on your deployment's license master. (If your deployment is only one instance, your instance is its own license master.)
</p>
<h3> <a name="aboutsplunkslicenseusagereportview_today_tab"><span class="mw-headline" id="Today_tab"> Today tab </span></a></h3>
<p>When you first arrive at LURV, you'll see five panels under the "Today" tab. These panels show the status of license usage and the warnings for the day that hasn't yet finished. The licenser's day ends at midnight in whichever time zone the license master is set to.
</p><p>All the panels in the "Today" tab query the Splunk REST API.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_today.27s_license_usage_panel"><span class="mw-headline" id="Today.27s_license_usage_panel"> Today's license usage panel </span></a></i></b></font></h4>
<p>This panel gauges license usage for today, as well as the total daily license quota across all pools.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_today.27s_license_usage_per_pool_panel"><span class="mw-headline" id="Today.27s_license_usage_per_pool_panel"> Today's license usage per pool panel </span></a></i></b></font></h4>
<p>This panel shows the license usage for each pool as well as the daily license quota for each pool.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_today.27s_percentage_of_daily_license_quota_used_per_pool_panel"><span class="mw-headline" id="Today.27s_percentage_of_daily_license_quota_used_per_pool_panel"> Today's percentage of daily license quota used per pool panel </span></a></i></b></font></h4>
<p>This panel shows what percentage of the daily license quota has been indexed by each pool. The percentage is displayed on a logarithmic scale.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_pool_usage_warnings_panel"><span class="mw-headline" id="Pool_usage_warnings_panel"> Pool usage warnings panel </span></a></i></b></font></h4>
<p>This panel shows the warnings, both soft and hard, that each pool has received in the past 30 days (or since the last license reset key was applied). Read "<a href="#aboutlicenseviolations" class="external text">About license violations</a>" in this manual to learn more about soft and hard warnings, and license violations.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_slave_usage_warnings_panel"><span class="mw-headline" id="Slave_usage_warnings_panel"> Slave usage warnings panel </span></a></i></b></font></h4>
<p>For each license slave, this panel shows: the number of warnings, pool membership, and whether the slave is in violation.
</p>
<h3> <a name="aboutsplunkslicenseusagereportview_previous_30_days_tab"><span class="mw-headline" id="Previous_30_Days_tab"> Previous 30 Days tab </span></a></h3>
<p>Clicking on the "Previous 30 Days" tab reveals five more panels and several drop-down options.
</p><p>All visualizations in these panels limit the number of host, source, source type, index, pool (any field you split by) that are plotted. If you have more than 10 distinct values for any of these fields, the values after the 10th are labeled "Other." We've set the maximum number of values plotted to 10 using <code><font size="2">timechart</font></code>. We hope this gives you enough information most of the time without making the visualizations difficult to read.
</p><p>These panels all use data collected from <code><font size="2">license_usage.log</font></code>, <code><font size="2">type=RolloverSummary</font></code> (daily totals). If your <b>license master</b> is down at its local midnight, it will not generate a RolloverSummary event for that day, and you will not see that day's data in these panels.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_split-by:_no_split.2c_indexer.2c_pool"><span class="mw-headline" id="Split-by:_no_split.2C_indexer.2C_pool"> Split-by: no split, indexer, pool </span></a></i></b></font></h4>
<p>These three split-by options are self-explanatory. Read about <a href="#addanindexertoalicensepool" class="external text">adding an indexer to a license pool</a> and about <a href="#groups,stacks,pools,andotherterminology" class="external text">license pools</a> in previous chapters in this manual.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_split-by:_source.2c_source_type.2c_host.2c_index"><span class="mw-headline" id="Split-by:_source.2C_source_type.2C_host.2C_index"> Split-by: source, source type, host, index </span></a></i></b></font></h4>
<p>There are two things you should understand about these four split-by fields: report acceleration and squashing.
</p>
<h5> <a name="aboutsplunkslicenseusagereportview_report_acceleration"><span class="mw-headline" id="Report_acceleration"> Report acceleration </span></a></h5>
<p>Splitting by source, source type, and host uses <code><font size="2">license_usage.log type=Usage</font></code>, which provides real-time usage statistics at one-minute intervals. We recommend accelerating the report that powers these split-by options <b>on your license master</b>. (Without acceleration, the search can be very slow, since it searches through 30 days worth of data that gets generated at a rate of one event per minute -- that's a lot of events!) 
</p><p>Acceleration for this report is disabled by default. To accelerate the report, click the link that shows up in the info message when you select one of these split-by values. You can also find the workflow for accelerating in <b>Settings &gt; Searches and reports &gt; License usage data cube</b>. Read "Accelerate reports" in the Reporting Manual.
</p><p>Note that report acceleration can take up to 10 minutes to start after you select it for the first time. Then Splunk will take some amount time to build the acceleration summary -- typically a few to tens of minutes, depending on the amount of data it's summarizing. Only after the acceleration is finished building will you see faster performance for these split-by options.
</p><p>But after the first acceleration run, subsequent reports will build on what's already there, keeping the report up-to-date (and the reporting fast). You should only have a long wait the very first time you turn on report acceleration.
</p><p><b>Important:</b> Enable report acceleration only on your license master.
</p><p>Configure how frequently the acceleration runs in <a href="#savedsearchesconf" class="external text">savedsearches.conf</a>, with <code><font size="2">auto_summarize</font></code>. The default is every 10 minutes. Keep it frequent, to keep the workload small and steady. We put in a cron for every 10 minutes at the 3 minute mark. This is configurable in <code><font size="2">auto_summarize.cron_schedule</font></code>.
</p>
<h5> <a name="aboutsplunkslicenseusagereportview_squashing"><span class="mw-headline" id="Squashing"> Squashing </span></a></h5>
<p>Every indexer periodically reports to license manager stats of the data indexed: broken down by source, source type, host, and index.  If the number of distinct (source, source type, host, index) tuples grows over the <code><font size="2">squash_threshold</font></code>, Splunk squashes the {host, source} values and only reports a breakdown by {sourcetype, index}.  This is to prevent explosions in memory and license_usage.log lines.
</p><p>Because of squashing on the other fields, only the split-by source type and index will guarantee full reporting (every byte). Split by source and host do not guarantee full reporting necessarily, if those two fields represent many distinct values. Splunk reports the entire quantity indexed, but not the names. So you lose granularity (that is, you don't know who consumed that amount), but you still know what the amount consumed is.
</p><p>Squashing is configurable (with care!) in <a href="#serverconf" class="external text">server.conf</a>, in the <code><font size="2">[license]</font></code> stanza, with the <code><font size="2">squash_threshold</font></code> setting. You can increase the value, but doing so can use a lot of memory, so consult a Splunk Support engineer before changing it.
</p><p>LURV will always tell you (with a warning message in the UI) if squashing has occurred.
</p><p>If you find that you need the granular information, you can get it from metrics.log instead, using <code><font size="2">per_host_thruput</font></code>.
</p>
<h4><font size="3"><b><i> <a name="aboutsplunkslicenseusagereportview_top_5_by_average_daily_volume"><span class="mw-headline" id="Top_5_by_average_daily_volume"> Top 5 by average daily volume </span></a></i></b></font></h4>
<p>The "Top 5" panel shows both average and maximum daily usage of the top five values for whatever split by field you've picked from the Split By menu.
</p><p>Note that this selects the top five average (not peak) values. So, for example, say you have more than five source types. Source type F is normally much smaller than the others but has a brief peak. Source type F's <b>max</b> daily usage is very high, but its <b>average</b> usage might still be low (since it has all those days of very low usage to bring down its average). Since this panel selects the top five <b>average</b> values, source type F might still not show up in this view.
</p>
<h3> <a name="aboutsplunkslicenseusagereportview_use_lurv"><span class="mw-headline" id="Use_LURV"> Use LURV </span></a></h3>
<p>Read the next topic for a tip about <a href="#licenseusagereportviewexamples" class="external text">configuring an alert based on a LURV panel</a>.
</p>
<a name="licenseusagereportviewexamples"></a><h2> <a name="licenseusagereportviewexamples_use_the_license_usage_report_view"><span class="mw-headline" id="Use_the_license_usage_report_view"> Use the license usage report view </span></a></h2>
<p>This topic is about using the license usage report view (LURV). To learn about the view, read the previous topic, "<a href="#aboutsplunkslicenseusagereportview" class="external text">About Splunk's license usage report view</a>."
</p>
<h3> <a name="licenseusagereportviewexamples_set_up_an_alert"><span class="mw-headline" id="Set_up_an_alert"> Set up an alert </span></a></h3>
<p>You can turn any of the LURV panels into an alert. For example, say you want to set up an alert for when license usage reaches 80% of the quota.
</p><p>Start at the <b>Today's percentage of daily license usage quota used</b> panel. Click "Open in search" at the bottom left of a panel. Append
</p><p><code><font size="2"> | where '% used' &gt; 80 </font></code>
</p><p>then select <b>Save as &gt; Alert</b> and follow the alerting wizard.
</p><p>Splunk Enterprise comes with some alerts preconfigured that you can enable. See "<a href="#platformalerts" class="external text">Platform alerts</a>" in this manual.
</p>
<h3> <a name="licenseusagereportviewexamples_troubleshoot_lurv:_no_results_in_30_days_panel"><span class="mw-headline" id="Troubleshoot_LURV:_no_results_in_30_days_panel"> Troubleshoot LURV: no results in 30 days panel </span></a></h3>
<p>A lack of results in the panels of the "Last 30 days" view of the License Usage Report View indicates that the license master instance on which this page is viewed is unable to find events from its own <code><font size="2">$SPLUNK_HOME/var/log/splunk/license_usage.log</font></code> file when searching.
</p><p>This typically has one of two causes:
</p>
<ul><li> The <b>license master</b> is configured to forward its events to the indexers (read more about this best practice in the Distributed Search Manual) but it has not been configured to be a search head. This is easily remedied by adding all indexers to whom the license master is forwarding events as search peers.
</li><li> The license master is not reading (and therefore, indexing) events from its own <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code> directory. This can happen if the <code><font size="2">[monitor://$SPLUNK_HOME/var/log/splunk]</font></code> default data input is disabled for some reason.
</li></ul><p>You might also have a gap in your data if your license master is down at midnight.
</p>
<h1>Monitor Splunk Enterprise with the distributed management console</h1><a name="configurethemonitoringconsole"></a><h2> <a name="configurethemonitoringconsole_configure_the_distributed_management_console"><span class="mw-headline" id="Configure_the_distributed_management_console"> Configure the distributed management console</span></a></h2>
<h3> <a name="configurethemonitoringconsole_what_is_the_distributed_management_console.3f"><span class="mw-headline" id="What_is_the_distributed_management_console.3F"> What is the distributed management console? </span></a></h3>
<p>The distributed management console lets you view detailed performance information about your Splunk Enterprise deployment. The topics in this chapter describe the available dashboards and alerts.
</p><p>The available dashboards provide insight into your deployment's indexing performance, search performance, operating system resource usage, Splunk Enterprise app key value store performance, and license usage.
</p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_find_the_distributed_management_console"><span class="mw-headline" id="Find_the_distributed_management_console"> Find the distributed management console </span></a></i></b></font></h4>
<p>From anywhere in Splunk Web, click <b>Settings</b>, and then click the <b>Distributed Management Console</b> icon on the left.
</p><p><img alt="FindDMC.png" src="images/8/82/FindDMC.png" width="600" height="386"></p><p>The distributed management console (DMC) is visible only to admin users.
</p><p>You can leave DMC in standalone mode on your Splunk Enterprise instance, which means that you can navigate to the DMC on your individual instance in your deployment and see that particular instance's performance. Or you can go through the configuration steps for distributed mode. Distributed mode lets you log into one instance and view performance information for every instance in the deployment. Going through the configuration steps (even in standalone mode) also lets you access the default platform alerts.
</p>
<h3> <a name="configurethemonitoringconsole_which_instance_should_host_the_console.3f"><span class="mw-headline" id="Which_instance_should_host_the_console.3F"> Which instance should host the console? </span></a></h3>
<p>After you configured the DMC in distributed mode, you can navigate to it on only one instance in your deployment and view the console information for your entire deployment.
</p><p>You have several options for where to host the distributed management console. The instance you choose must be provisioned as a search head. See "Reference hardware" in the <i>Capacity Planning Manual</i>. For security and some performance reasons, only Splunk Enterprise administrators should have access to this instance.
</p><p><b>Important:</b> Except for the case of a standalone, non-distributed Splunk Enterprise deployment, the instance hosting the DMC should not be used as a production search head and should not run any searches unrelated to its function as the DMC. This table describes the recommended locations for the DMC, based on deployment type:
</p>
<table cellpadding="5" cellspacing="0" border="1"><tr bgcolor="#D9EAED"><th bgcolor="#C0C0C0"> Distributed
</th><th bgcolor="#C0C0C0"> Indexer clustering
</th><th bgcolor="#C0C0C0"> Search head clustering
</th><th bgcolor="#C0C0C0"> DMC options
</th></tr><tr><td valign="center" align="left"> No
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> The standalone instance.
</td></tr><tr><td valign="center" align="left"> Yes
</td><td valign="center" align="left"> No
</td><td valign="center" align="left"> No
</td><td valign="center" align="left"> The license master or a deployment server servicing a small number (&lt;50) of clients. Use of the instance should be limited to DMC and these specific functions. If neither a license master nor a deployment server is available, run the DMC on a dedicated search head not used for other purposes.
</td></tr><tr><td valign="center" align="left"> Yes
</td><td valign="center" align="left"> Single cluster
</td><td valign="center" align="left"> Not relevant
</td><td valign="center" align="left"> The master node. If preferred, you can instead run the DMC on a dedicated search head not used for other purposes.
</td></tr><tr><td valign="center" align="left"> Yes
</td><td valign="center" align="left"> Multiple clusters
</td><td valign="center" align="left"> Not relevant
</td><td valign="center" align="left"> A search head that is configured as a search head node across all the clusters. This search head must be limited only to DMC use.
</td></tr><tr><td valign="center" align="left"> Yes
</td><td valign="center" align="left"> No
</td><td valign="center" align="left"> Yes
</td><td valign="center" align="left"> The search head cluster deployer. If preferred, you can instead run the DMC on a dedicated search head not used for other purposes.
</td></tr></table><h4><font size="3"><b><i> <a name="configurethemonitoringconsole_in_a_deployment_with_a_single_indexer_cluster:_on_the_master_node"><span class="mw-headline" id="In_a_deployment_with_a_single_indexer_cluster:_On_the_master_node"> In a deployment with a single indexer cluster: On the master node</span></a></i></b></font></h4>
<p>In an indexer cluster, host the DMC on the master node. See "System requirements" in the <i>Managing Indexes and Clusters Manual</i>.
</p><p>As an alternative, you can host the DMC on a search head node in the cluster. If you do so, however, you cannot use the search head to run any non-DMC searches. 
</p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_in_a_deployment_with_multiple_indexer_clusters:_on_a_search_head_node"><span class="mw-headline" id="In_a_deployment_with_multiple_indexer_clusters:_On_a_search_head_node"> In a deployment with multiple indexer clusters: On a search head node </span></a></i></b></font></h4>
<p>If your deployment has multiple indexer clusters, host the DMC on a search head configured as a search head node on each of the clusters. Do not use this search head to run any non-DMC searches.
</p><p>The main steps to accomplish this are:
</p><p><b>1.</b> Configure a single search head as a node on each of the indexer clusters. See Search across multiple indexer clusters" in the <i>Managing Indexes and Clusters Manual</i>. This is your DMC instance.
</p><p><b>2.</b> Configure each master node, as well as all search head nodes in the clusters, as search peers of the DMC instance. See <a href="#configurethemonitoringconsole_add_instances_as_search_peers" class="external text">Add instances as search peers.</a>" 
</p><p><b>Caution:</b> Do not configure the cluster peer nodes (indexers) as search peers to the DMC node. As nodes in the indexer clusters, they are already known to all search head nodes in their cluster, including the DMC node.
</p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_in_a_non-indexer-cluster_environment.2c_option_1:_on_license_master"><span class="mw-headline" id="In_a_non-indexer-cluster_environment.2C_option_1:_On_license_master"> In a non-indexer-cluster environment, option 1: On license master </span></a></i></b></font></h4>
<p>You can configure the monitoring console on your license master if the following are true:
</p>
<ul><li> Your license master can handle the search workload, that is, meets or exceeds the search head reference hardware requirements. See "Reference hardware" in the <i>Capacity Planning Manual</i>.
</li><li> Only Splunk Enterprise admins can access your dedicated license master.
</li></ul><h4><font size="3"><b><i> <a name="configurethemonitoringconsole_in_a_non-indexer-cluster_environment.2c_option_2:_on_a_new_instance"><span class="mw-headline" id="In_a_non-indexer-cluster_environment.2C_option_2:_On_a_new_instance"> In a non-indexer-cluster environment, option 2: On a new instance </span></a></i></b></font></h4>
<p>Another option is to provision a new instance, configure it as a search head of search heads and a search head of indexers, and configure the DMC in distributed mode there.
</p><p><img alt="DMCarch.png" src="images/d/da/DMCarch.png" width="650" height="329"></p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_in_a_search_head_cluster_environment"><span class="mw-headline" id="In_a_search_head_cluster_environment"> In a search head cluster environment </span></a></i></b></font></h4>
<p>Use a deployer or dedicated license master for hosting the DMC. The DMC cannot be on a search head cluster member. See "System requirements and other deployment considerations for search head clusters" in the <i>Distributed Search Manual</i>.
</p><p>The distributed management console is not supported in a search head pooled environment.
</p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_the_dmc_and_deployment_server"><span class="mw-headline" id="The_DMC_and_deployment_server"> The DMC and deployment server </span></a></i></b></font></h4>
<p>In most cases, you cannot host the distributed DMC on a deployment server. The exception is if the deployment server handles only a small number of deployment clients, no more than 50. The DMC and deployment server functionalities can interfere with each other at larger client counts. See "Deployment server provisioning" in the <i>Updating Splunk Enterprise Instances</i> manual.
</p>
<h3> <a name="configurethemonitoringconsole_configure_your_dmc_to_monitor_a_deployment"><span class="mw-headline" id="Configure_your_DMC_to_monitor_a_deployment"> Configure your DMC to monitor a deployment </span></a></h3>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_prerequisites"><span class="mw-headline" id="Prerequisites"> Prerequisites </span></a></i></b></font></h4>
<ul><li> Have a functional Splunk Enterprise deployment. See "Distributed Splunk Enterprise overview" in the <i>Distributed Deployment Manual</i>. Any instance that you want to monitor must be running Splunk Enterprise 6.1 or higher.
</li><li> Check whether your deployment is healthy, that is, that all peers are up.
</li><li> Make sure that each instance in the deployment (each search head, license master, and so on) has a unique server.conf <code><font size="2">serverName</font></code> value and inputs.conf <code><font size="2">host</font></code> value.
</li><li> Forward internal logs (both <code><font size="2">$SPLUNK_HOME/var/log/splunk</font></code> and <code><font size="2">$SPLUNK_HOME/var/log/introspection</font></code>) to indexers from all other instance types. See "Forward search head data" in the <i>Distributed Search Manual</i>. Without this step, many dashboards will lack data. These other instance types include:
<ul><li> Search heads.
</li><li> License masters.
</li><li> Cluster masters.
</li><li> Deployment servers.
</li></ul></li><li> The user setting up the Distributed Management Console needs the "admin_all_objects" capability.
</li></ul><h4><font size="3"><b><i> <a name="configurethemonitoringconsole_add_instances_as_search_peers"><span class="mw-headline" id="Add_instances_as_search_peers"> Add instances as search peers </span></a></i></b></font></h4>
<p><b>1.</b> Log into the <a href="#configurethemonitoringconsole_which_instance_should_host_the_console.3f" class="external text">instance</a> on which you want to configure the distributed management console.
</p><p><b>2.</b> In Splunk Web, select <b>Settings &gt; Distributed search &gt; Search peers</b>.
</p><p><b>3.</b> Add each search head, deployment server, license master, and standalone indexer as a distributed search peer to the instance hosting the distributed management console. You do not need to add clustered indexers, but you must add clustered search heads.
</p>
<h4><font size="3"><b><i> <a name="configurethemonitoringconsole_set_up_dmc_in_distributed_mode"><span class="mw-headline" id="Set_up_DMC_in_distributed_mode"> Set up DMC in distributed mode </span></a></i></b></font></h4>
<p><b>1.</b> Log into the <a href="#configurethemonitoringconsole_which_instance_should_host_the_console.3f" class="external text">instance</a> on which you want to configure the distributed management console. The instance by default is in standalone mode, unconfigured.
</p><p><b>2.</b> In Splunk Web, select <b>Distributed management console &gt; Setup</b>.
</p><p><b>3.</b> Turn on <b>distributed mode</b> at the top left.
</p><p><b>4.</b> Check that:
</p>
<ul><li> The columns labeled <b>instance</b> and <b>machine</b> are populated correctly and populated with values that are unique within a column. <b>Note:</b> If your deployment has nodes running Splunk Enterprise 6.1.x (instead of 6.2.0+), their <b>instance (host)</b> and <b>machine</b> values will not be populated.
<ul><li> To find the value of <b>machine</b>, typically you can log into the 6.1.x instance and run <code><font size="2">hostname</font></code> on *nix or Windows. Here <b>machine</b> represents the FQDN of the machine.
</li><li> To find the value of <b>instance (host)</b>, use btool: <code><font size="2">splunk cmd btool inputs list default</font></code>.
</li><li> When you know these values, in the <b>Setup</b> page, click <b>Edit &gt; Edit instance</b>. A popup presents you with two fields to fill in: <b>Instance (host) name</b> and <b>Machine name</b>.
</li></ul></li><li> The server roles are correct, with the primary or major roles. For example, a search head that is also a license master should have both roles marked. If not, click <b>Edit</b> to correct.
</li><li> A cluster master is identified if you are using indexer clustering. If not, click <b>Edit</b> to correct.
</li></ul><p><b>Caution:</b> Make sure anything marked an indexer is really an indexer.
</p><p><b>5.</b> (Optional) Set custom groups. Custom groups are tags that map directly to distributed search groups. You don't need to add groups the first time you go through DMC setup (or ever). You might find groups useful, for example, if you have multisite indexer clustering (each group can consist of the indexers in one location) or an indexer cluster plus standalone peers. Custom groups are allowed to overlap. That is, one indexer can belong to multiple groups. See distributed search groups in the <i>Distributed Search Manual</i>.
</p><p><b>6.</b> Click <b>Save</b>.
</p><p><b>7.</b> (Optional) Set up <a href="#platformalerts" class="external text">platform alerts</a>.
</p><p>If you add another node to your deployment later, return to <b>Setup</b> and check that the items in step 4 are accurate.
</p>
<h3> <a name="configurethemonitoringconsole_configure_on_a_single_instance"><span class="mw-headline" id="Configure_on_a_single_instance"> Configure on a single instance </span></a></h3>
<p>On a single Splunk Enterprise instance operating by itself, you must configure standalone mode before you can use <a href="#platformalerts" class="external text">platform alerts</a>.
</p><p>To configure:
</p><p><b>1.</b> Navigate to the <b>Setup</b> page in DMC.
</p><p><b>2.</b> Check that search head, license master, and indexer are listed under <b>Server Roles</b>, and nothing else. If not, click <b>Edit</b>.
</p><p><b>3.</b> Click <b>Apply Changes</b> to complete setup.
</p>
<a name="returnthemonitoringconsoletodefaultsettings"></a><h2> <a name="returnthemonitoringconsoletodefaultsettings_return_the_dmc_to_default_settings"><span class="mw-headline" id="Return_the_DMC_to_default_settings"> Return the DMC to default settings</span></a></h2>
<p>To return an instance's Distributed Management Console to the default (original) settings, use the following procedure:
</p><p>1. Delete the <code><font size="2">$SPLUNK_HOME/etc/apps/splunk_management_console/local directory.</font></code>
</p><p>2. Delete the <code><font size="2">$SPLUNK_HOME/etc/apps/splunk_management_console/lookups</font></code> directory.
</p><p>3. In <code><font size="2">$SPLUNK_HOME/etc/system/local/distsearch.conf</font></code>, delete any stanzas that reference distributed search groups created by the DMC. The names of these groups are usually prefaced with <code><font size="2">dmc_group_*</font></code>. 
</p><p>The following is an example of how these stanzas usually appear in <code><font size="2">distsearch.conf</font></code>:
</p>
<div class="samplecode"><code><font size="2">[distributedSearch:dmc_group_cluster_master]<br>servers = localhost:localhost<br><br>[distributedSearch:dmc_group_deployment_server]<br><br>[distributedSearch:dmc_group_kv_store]<br><br>[distributedSearch:dmc_group_search_head]<br>servers = localhost:localhost,undiag02.sv.splunk.com:8089<br><br>[distributedSearch:dmc_group_license_master]<br>servers = localhost:localhost<br><br>[distributedSearch:dmc_customgroup_primary_search-head]<br>servers = undiag02.sv.splunk.com:8089<br><br>[distributedSearch:dmc_group_indexer]<br>default = true<br>servers = 10.159.4.67:8089,10.159.4.70:8089,10.159.4.71:8089,10.159.4.73:8089</font></code></div>
<p>4. Once you have deleted all stanzas, save your changes.
</p><p>5. Restart Splunk Enterprise.
</p>
<a name="platformalerts"></a><h2> <a name="platformalerts_platform_alerts"><span class="mw-headline" id="Platform_alerts"> Platform alerts</span></a></h2>
<h3> <a name="platformalerts_what_are_platform_alerts.3f"><span class="mw-headline" id="What_are_platform_alerts.3F"> What are platform alerts? </span></a></h3>
<p><b>Platform alerts</b> are saved searches included in the distributed management console (DMC). Platform alerts notify Splunk Enterprise administrators of conditions that might compromise their Splunk Enterprise environment. The included platform alerts get their data from REST endpoints.
</p><p>Platform alerts are disabled by default.
</p>
<h3> <a name="platformalerts_enable_platform_alerts"><span class="mw-headline" id="Enable_platform_alerts"> Enable platform alerts </span></a></h3>
<p><b>Prerequisites</b>
</p>
<ul><li> Configure your distributed management console. From the DMC, click <b>Setup</b>. See "<a href="#configurethemonitoringconsole" class="external text">Configure the distributed management console</a>."
</li><li> Set up alerting notifications for alerts on your deployment.
</li></ul><p><br><b>1.</b> From the DMC <b>Overview</b> page, click <b>Alerts &gt; Enable or Disable</b>.
</p><p><img alt="Enable platform alerts.png" src="images/d/d6/Enable_platform_alerts.png" width="300" height="123"></p><p><b>2.</b> Click the <b>Enabled</b> check box next to the alert or alerts that you want to enable.
</p><p>After an alert has triggered, you can view the alert and its results by going to <b>Overview &gt; Alerts &gt; Managed triggered alerts</b>. 
</p><p>See <a href="#platformalerts_configure_platform_alerts" class="external text">Configure platform alerts</a>, next, for alert actions that you can configure, such as email notifications.
</p>
<h3> <a name="platformalerts_configure_platform_alerts"><span class="mw-headline" id="Configure_platform_alerts"> Configure platform alerts </span></a></h3>
<p>From the DMC, navigate to <b>Overview &gt; Alerts &gt; Enable or Disable</b>. Find the alert you want to configure and click <b>edit</b>. You can view the default settings and change parameters such as:
</p>
<ul><li> alert schedule
</li><li> suppression time
</li><li> alert actions (such as emails)
</li></ul><p>See "Set up alert actions," and see about all alerting options in the <i>Alerting Manual</i>.
</p><p>You can also view the complete list of default parameters for platform alerts in <code><font size="2">$SPLUNK_HOME/etc/apps/splunk_management_console/default/savedsearches.conf</font></code>. If you choose to edit configuration files directly, put the new configurations in a local directory instead of the default.
</p>
<h3> <a name="platformalerts_which_alerts_are_included.3f"><span class="mw-headline" id="Which_alerts_are_included.3F"> Which alerts are included? </span></a></h3>
<p>To start monitoring your deployment with platform alerts, you must enable the individual alerts that you want. See "Enable platform alerts."
</p>
<table cellpadding="6"><tr><th bgcolor="#C0C0C0"> Alert&nbsp;name
</th><th bgcolor="#C0C0C0"> Description
</th><th bgcolor="#C0C0C0"> For more information
</th></tr><tr><td valign="center" align="left"> Abnormal state of indexer processor
</td><td valign="center" align="left"> Fires when one or more of your indexers reports an abnormal state. This abnormal state can be either throttled or stopped.
</td><td valign="center" align="left"> For details on which indexer is in which abnormal state, and to begin investigating causes, see the  DMC <b>Indexing Performance: Deployment</b> dashboard's <b>Indexing Performance by Instance</b> panel. See "<a href="#indexingdeployment" class="external text">Indexing performance: deployment</a>" for information about the dashboard, and "How indexing works."
</td></tr><tr><td valign="center" align="left"> Critical system physical memory usage
</td><td valign="center" align="left"> Fires when one or more instances exceeds 90% memory usage. On most Linux distributions, this alert can trigger if the OS is engaged in buffers and filesystem cacheing activities. The OS releases this memory if other processes need it, so it does not always indicate a serious problem.
</td><td valign="center" align="left"> For details on instance memory usage, navigate to the DMC <b>Resource Usage: Deployment</b> dashboard, and see "<a href="#resourceusagedeployment" class="external text">Resource usage: deployment</a>" in this manual.
</td></tr><tr><td valign="center" align="left"> Near-critical disk usage
</td><td valign="center" align="left"> Fires when you have used 80% of your disk capacity.
</td><td valign="center" align="left"> For more information about your disk usage, navigate to the three DMC <b>Resource Usage</b> dashboards and read the corresponding topics in this manual.
</td></tr><tr><td valign="center" align="left"> Saturated event-processing queues
</td><td valign="center" align="left"> Fires when one or more of your indexer queues reports a fill percentage, averaged over the last 15 minutes, of 90% or more. This alert can inform you of potential indexing latency.
</td><td valign="center" align="left"> For more details about your indexer queues, navigate to the two DMC <b>Indexing Performance</b> dashboards and read the corresponding topics in this manual.
</td></tr><tr><td valign="center" align="left"> Search peer not responding
</td><td valign="center" align="left"> Fires when any of your search peers (indexers) is unreachable.
</td><td valign="center" align="left"> For the status of all your instances, see the DMC <b>Instances</b> view.
</td></tr><tr><td valign="center" align="left"> Total license usage near daily quota
</td><td valign="center" align="left"> Fires when you have used 90% of your total daily license quota.
</td><td valign="center" align="left"> For more information about your license usage, click <b>Licensing</b> in the DMC.
</td></tr></table><h3> <a name="platformalerts_about_search_artifacts"><span class="mw-headline" id="About_search_artifacts"> About search artifacts </span></a></h3>
<p>In savedsearches.conf, the <code><font size="2">dispatch.ttl</font></code> setting dictates that the searches from platform alerts keep search artifacts for four hours. 
</p><p>But if an alert is triggered, its search artifact stays for seven days. This means that the link sent in an email to inspect the search results of a triggered alert expires in seven days (by default).
</p>
<a name="indexinginstance"></a><h2> <a name="indexinginstance_indexing_performance:_instance"><span class="mw-headline" id="Indexing_performance:_Instance"> Indexing performance: Instance</span></a></h2>
<h3> <a name="indexinginstance_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about indexing performance on one instance of the potentially several in your deployment.
</p>
<h3> <a name="indexinginstance_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>The snapshot panel called <b>Splunk Enterprise Data Pipeline</b> exposes decaying averages for queue sizes. The averages use data over the previous 15 minutes. This panel, along with the historical panel <b>Median Fill Ratio of Data Processing Queues</b>, helps you narrow down sources of indexing latency to a specific queue. Data starts at parsing and travels through the data pipeline to indexing at the end.
</p><p>The <b>Aggregate CPU Seconds Spent per Indexer Processor Activity</b> panel lets you "Split index service by subtask." The several index services are subtasks related to preparing for and cleaning up after indexing. For more information about the meaning of subtask categories, see the metrics.log topic&Acirc;&nbsp;in the <i>Troubleshooting Manual</i>.
</p>
<h3> <a name="indexinginstance_what_to_look_out_for_in_this_view"><span class="mw-headline" id="What_to_look_out_for_in_this_view"> What to look out for in this view </span></a></h3>
<p>The <b>Splunk Enterprise Data Pipeline</b> panel, along with the historical panel <b>Median Fill Ratio of Data Processing Queues</b>, helps you narrow down sources of indexing latency to a specific queue. Data starts at parsing and travels through the data pipeline to indexing at the end. Here is an example of the panel in an instance with unhealthy queues:
</p><p><img alt="Cloggedpipeline.png" src="images/6/63/Cloggedpipeline.png" width="800" height="230"></p><p>In this example, although the parsing and aggregator queues have very high fill ratios, the problem is likely to be with processes in the typing queue. The typing queue is the first one that slows down, and data is backing up into the other two queues while waiting to get into the typing queue.
</p>
<h3> <a name="indexinginstance_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The snapshot panels get data from Splunk REST endpoints for introspection. If snapshot panels lack data, check the system requirements for platform instrumentation.
</p><p>The historical panels for this view get data from metrics.log.
</p>
<a name="indexingdeployment"></a><h2> <a name="indexingdeployment_indexing_performance:_deployment"><span class="mw-headline" id="Indexing_performance:_Deployment"> Indexing performance: Deployment</span></a></h2>
<h3> <a name="indexingdeployment_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about indexing performance across your Splunk Enterprise deployment.
</p>
<h3> <a name="indexingdeployment_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>In the <b>Overview of Indexing Performance</b> panel, total indexing rate is summed over all indexers.
</p><p>In the <b>Instances by Estimated Indexing Rate</b> panel, the indexing rate is estimated because it uses metrics.log, which takes only the top ten results for each type by default. See "About metrics.log" in the Troubleshooting Manual.
</p>
<h3> <a name="indexingdeployment_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The snapshot panels get data from Splunk REST endpoints for introspection. If snapshot panels lack data, check the system requirements for platform instrumentation.
</p><p>The historical panels for this view get data from metrics.log.
</p>
<a name="searchactivityinstance"></a><h2> <a name="searchactivityinstance_search_activity:_instance"><span class="mw-headline" id="Search_activity:_Instance"> Search activity: Instance</span></a></h2>
<h3> <a name="searchactivityinstance_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search activity.
</p>
<h3> <a name="searchactivityinstance_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>In the <b>Median Resource Usage of Searches</b> panel, note that:
</p>
<ul><li> Resource usage is aggregated over all searches.
</li><li> Memory usage represents physical memory.
</li><li> In this chart, CPU usage is expressed in percentage of one core, not as system-wide CPU usage. As a result, you are likely to see values &gt;100% here. This is not the case for other examples of CPU usage in the distributed management console.
</li></ul><p>In the <b>Aggregate Search Runtime</b> panel, note that:
</p>
<ul><li> For each time bin in the chart, the DMC adds up the runtime of all searches that were running during that time range. Thus, you might see, for example, 1000 seconds of search in 5 minutes. This means that multiple searches were running over the course of those 5 minutes.
</li><li> For the modes historical batch and RT indexed, historical batch can be dispatched only by certain facilities within Splunk Enterprise (the scheduler, for example). RT indexed means indexed real-time.
</li></ul><p>In the <b>Top 10 Memory-Consuming Searches</b> panel, SID means search ID. If you are looking for information about a saved search, audit.log matches the name of your saved search (savedsearch_name) with its search ID (search_id), user, and time. With the search_id, you can look up that search elsewhere, like in the Splunk search logs (see "What Splunk logs about itself").
</p>
<h3> <a name="searchactivityinstance_what_to_look_for_in_this_view"><span class="mw-headline" id="What_to_look_for_in_this_view"> What to look for in this view </span></a></h3>
<p>Consider your search concurrency and resource usage compared to your system limits.
</p><p>For information, see:
</p>
<ul><li> "Write better searches" in the <i>Search Manual</i>.
</li><li> "About search" in the <i>Search Manual</i>.
</li><li> "Configure the priority of scheduled reports" in the <i>Reporting Manual</i>.
</li><li> "Overview of summary-based search and pivot acceleration" in the <i>Knowledge Manager Manual</i>.
</li></ul><h3> <a name="searchactivityinstance_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>In the <b>Search Activity</b> panel, the snapshots are taken every ten seconds by default. So if no searches are currently running, or if the searches you run are very short lived, the snapshots panel is blank and says "no results found."
</p><p>The historical panels get data from introspection logs. If a panel is blank or missing information from non-indexers, check:
</p>
<ul><li> that you are <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="searchactivitydeploymentwide"></a><h2> <a name="searchactivitydeploymentwide_search_activity:_deployment"><span class="mw-headline" id="Search_activity:_Deployment"> Search activity: Deployment</span></a></h2>
<h3> <a name="searchactivitydeploymentwide_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search activity across your Splunk Enterprise deployment.
</p>
<h3> <a name="searchactivitydeploymentwide_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>The memory and CPU usage shown here are for searches only. See the resource usage dashboards for all Splunk Enterprise resource usage.
</p><p>In the <b>Instances by Median CPU Usage</b> panel, CPU can be greater than 100% because of multiple cores.
</p><p>In the <b>Instances by Median Memory Usage</b> panel, memory is physical.
</p><p>For the modes historical batch and RT indexed: historical batch can be dispatched only by certain facilities within Splunk Enterprise (the scheduler, for example). RT indexed means indexed real-time.
</p>
<h3> <a name="searchactivitydeploymentwide_what_to_look_for_in_this_view"><span class="mw-headline" id="What_to_look_for_in_this_view"> What to look for in this view </span></a></h3>
<p>Look for things that are close to exceeding their limits on your machines.
</p><p>For more information, see:
</p>
<ul><li> "Write better searches" in the <i>Search Manual</i>.
</li><li> "About search" in the <i>Search Manual</i>.
</li><li> "Configure the priority of scheduled reports" in the <i>Reporting Manual</i>.
</li><li> "Accommodate many simultaneous searches" in the <i>Capacity Planning Manual</i>.
</li></ul><h3> <a name="searchactivitydeploymentwide_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from introspection logs. If a panel is blank or missing information from non-indexers, check:
</p>
<ul><li> that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="searchusagestatistics"></a><h2> <a name="searchusagestatistics_search_usage_statistics:_instance"><span class="mw-headline" id="Search_usage_statistics:_Instance"> Search usage statistics: Instance </span></a></h2>
<h3> <a name="searchusagestatistics_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search usage statistics.
</p>
<h3> <a name="searchusagestatistics_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>In the <b>Long-Running Searches</b> panel:
</p>
<ul><li> A start time of ZERO_TIME means that search will go up to the epoch.
</li><li> An end time of ZERO_TIME means that the search searches up until the moment that the search is fired.
</li><li> If both start and end time are listed as ZERO_TIME, that indicates an all-time search.
</li></ul><p>In the <b>Common Search Commands</b> panel, runtimes are in seconds.
</p>
<h3> <a name="searchusagestatistics_what_to_look_out_for_in_this_view"><span class="mw-headline" id="What_to_look_out_for_in_this_view"> What to look out for in this view </span></a></h3>
<p>It's good practice to look at your long-running searches. You might find a search that you can optimize.
</p><p>For more information, see "Write better searches" in the <i>Search Manual</i>.
</p>
<h3> <a name="searchusagestatistics_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels in this view get their data from audit.log. If a panel is blank or missing information from non-indexers, check that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>.
</p><p>The <b>Long-Running Searches</b> panel also uses information from a REST endpoint.
</p>
<a name="resourceusageinstance"></a><h2> <a name="resourceusageinstance_resource_usage:_instance"><span class="mw-headline" id="Resource_usage:_Instance"> Resource usage: Instance</span></a></h2>
<h3> <a name="resourceusageinstance_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search activity.
</p>
<h3> <a name="resourceusageinstance_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>In the two "process class" panels, process class can be splunkd server, search, Splunk Web, index service, scripted input, KVStore, or other.
</p><p>Process class means an aggregate of processes within one class. For more information about
</p>
<ul><li> splunkd, read "Splunk Enterprise architecture and processes" in the <i>Installation Manual</i>.
</li><li> search, read "About search" and "Write better searches" in the <i>Search Manual</i>.
</li><li> splunkweb, read "Splunk Enterprise architecture and processes" in the <i>Installation Manual</i>. 
</li><li> scripted input, read "Get data from APIs and other remote data interfaces through scripted inputs" in the <i>Getting Data In Manual</i>.
</li><li> KVStore, see the "<a href="#kvstoreinstance" class="external text">KV store: Instance</a>" view in DMC. 
</li></ul><p>Index service consists of housekeeping tasks related to indexing. They run at the end of the indexing pipeline but are asynchronous. These processes run on their own, not through splunkd.
</p><p>The <b>Disk Usage</b> and <b>Median Disk Usage</b> panels list only the partitions that Splunk Enterprise uses.
</p>
<h3> <a name="resourceusageinstance_what_to_look_out_for_in_this_view"><span class="mw-headline" id="What_to_look_out_for_in_this_view"> What to look out for in this view </span></a></h3>
<p>If a process class that is using a lot of resources turns out to be search, go to the <b>Search activity: Instance</b> dashboard.
</p><p>For long-running processes, a worrying thing to look out for is memory usage continuously increasing over time.
</p>
<h3> <a name="resourceusageinstance_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from introspection logs. If a panel is blank or missing information from non-indexers, check:
</p>
<ul><li> that you are <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="resourceusagemachine"></a><h2> <a name="resourceusagemachine_resource_usage:_machine"><span class="mw-headline" id="Resource_usage:_Machine"> Resource usage: Machine</span></a></h2>
<h3> <a name="resourceusagemachine_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search activity.
</p>
<h3> <a name="resourceusagemachine_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>This view can be useful for operational post mortems, as well as for capacity planning. See the <i>Capacity Planning Manual</i> for more information.
</p><p>About physical memory usage in this view: on Linux, the OS starts using free physical memory to cache filesystem resources. But memory for this is loosely bound, and the OS frees it up if a higher priority process needs it. The DMC reporting cannot discern how much memory is loosely locked up in this way.
</p><p>In the <b>Median CPU Usage</b> panel, 100% means the entire system, however many cores the system has. This is in contrast to the <b>Search Activity</b> dashboards, where 100% means one core.
</p><p>The disk space in this view refers only to partitions with Splunk on them.
</p>
<h3> <a name="resourceusagemachine_what_to_look_out_for_in_this_view"><span class="mw-headline" id="What_to_look_out_for_in_this_view"> What to look out for in this view </span></a></h3>
<p>For long-running processes, a worrying thing to look out for is memory usage continuously increasing over time.
</p>
<h3> <a name="resourceusagemachine_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from introspection logs. If a panel is blank or missing information from non-indexers, check:
</p>
<ul><li> that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="resourceusagedeployment"></a><h2> <a name="resourceusagedeployment_resource_usage:_deployment"><span class="mw-headline" id="Resource_usage:_Deployment"> Resource usage: Deployment</span></a></h2>
<h3> <a name="resourceusagedeployment_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>Several panels about search activity.
</p>
<h3> <a name="resourceusagedeployment_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>About physical memory usage in this view: on Linux, the OS starts using free physical memory to cache filesystem resources. But memory for this is loosely bound, and the OS frees it up if a higher priority process needs it. The DMC reporting cannot discern how much memory is loosely locked up in this way.
</p><p>The <b>Deployment-Wide Median Disk Usage</b> panel takes into account all partitions in use by each Splunk Enterprise instance.
</p>
<h3> <a name="resourceusagedeployment_what_to_look_out_for_in_this_view"><span class="mw-headline" id="What_to_look_out_for_in_this_view"> What to look out for in this view </span></a></h3>
<p>The common theme in this view is that instances are grouped by value ranges. One interesting thing to look for is outliers: instances that are not like the others. Another thing to look for is patterns that appear over time.
</p>
<h3> <a name="resourceusagedeployment_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from introspection logs. If a panel is blank or missing information from non-indexers, check:
</p>
<ul><li> that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="kvstoreinstance"></a><h2> <a name="kvstoreinstance_kv_store:_instance"><span class="mw-headline" id="KV_store:_Instance"> KV store: Instance</span></a></h2>
<h3> <a name="kvstoreinstance_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>The instance level KV store view in the distributed management console (DMC) shows performance information about a single Splunk Enterprise instance running the app key-value store. If you have <a href="#configurethemonitoringconsole" class="external text">configured the DMC</a> with your distributed deployment, you can select which instance in your deployment to view.
</p>
<h4><font size="3"><b><i> <a name="kvstoreinstance_performance_metrics"><span class="mw-headline" id="Performance_metrics">Performance metrics</span></a></i></b></font></h4>
<p>Collection metrics come from the KVStoreCollectionStats component in the _introspection index, which is a historical record of the data at the <code><font size="2">/services/server/introspection/kvstore/collectionstats</font></code> REST endpoint. The metrics are:
</p>
<ul><li> Application. The application the collection belongs to.
</li><li> Collection. The name of the collection in KV store.
</li><li> Number of objects. The count of data objects stored in collection.
</li><li> Accelerations. The count of accelerations set up on the collection. <b>Note:</b> These are traditional database-style indexes used for performance and search acceleration.
</li><li> Accelerations size. The size in MBs of the indexes set up on the collection.
</li><li> Collection size. The size in MBs of all data stored in the collection.
</li></ul><p>Snapshots are collected through REST endpoints, which deliver the most recent information from the pertinent introspection components. The KV store instance snapshots use the endpoint <code><font size="2">/services/server/introspection/kvstore/serverstatus</font></code>.
</p>
<ul><li> Lock percentage. The percentage of KV store uptime that the system has held either global read or write locks. A high lock percentage has impacts across the board. It can starve replication or even make application calls slow, time out, or fail.
</li><li> Page fault percentage. The percentage of KV store operations that resulted in a page fault. A percentage close to 1 indicates poor system performance and is a leading indicator of continued sluggishness as KV store is forced to fallback on disk I/O rather than access data store efficiently in memory.
</li><li> Memory usage. The amount of resident, mapped, and virtual memory in use by KV store. Virtual memory usage is typically twice that of mapped memory for KV store. Virtual memory usage in excess of 3X mapped might indicate a memory leak.
</li><li> Network traffic. Total MBs in and out of KV store network traffic.
</li><li> Flush percentage. Percentage of a minute it takes KV store to flush all writes to disk. Closer to 1 indicates difficulty writing to disk or consistent large write operations. Some OSes can flush data faster than 60 seconds. In that case, this number can be small even if there is a writing bottleneck.
</li><li> Operations. Count of operations issued to KV store. Includes commands, updates, queries, deletes, getmores, and inserts. The introspection process issues a command to deliver KV store stats so the commands counter is typically higher than most other operations.
</li><li> Current connections. Count of connections open on KV store.
</li><li> Total queues. Total operations queued waiting for the lock.
</li><li> Total asserts. Total number of asserts raised by KV store. A non-negative number can indicate a need to check KV store logs.
</li></ul><h5> <a name="kvstoreinstance_historical"><span class="mw-headline" id="Historical">Historical</span></a></h5>
<p>Many of the statistics in this section are present in the <b>Snapshots</b> section. The <b>Historical</b> view presents trend information for the metrics across a set span of time. These stats are collected in <b>KVStoreServerStats</b>. By default the <b>Historical</b> panels show information for the past 4 hours. Any gaps in the graphs in this section typically indicate a point at which KV store or Splunk Enterprise was unreachable.
</p>
<ul><li> Memory usage - see above. 
</li><li> Replication lag. The amount of time between the last operation recorded in the Primary OpLog and the last operation applied to a secondary node. Replication lag in excess of the primary opLog window could result in data not being properly replicated across all nodes of the replication set. In standalone instances without replication this panel does not return any results. <b>Note:</b> Replication lag is collected in the <b>KVStoreReplicaSetStats</b> component in the _introspection index.
</li><li> Operation count (average by minute) - see above. This panel shows individual operation types (for example, commands, updates, and deletes) or for all operations.
</li><li> Asserts - see above. This panel allows for filtering based on type of assert - message, regular, rollovers, user, warning.
</li><li> Lock percentage. Percentage of KV store uptime that the system has held global, read, or write locks. Filter this panel by type of lock held:
<ul><li> Read. Lock held for read operations.
</li><li> Write. Lock held for write operations. KV store locking is "writer greedy," so write locks can make up the majority of the total locks on a collection.
</li><li> Global. Lock held by the global system. KV store implements collection-level locks, reducing the need for aggressive use of the global lock.
</li></ul></li><li> Page faults as a percentage of total operations - see above.
</li><li> Network traffic - see above. Added to this panel are requests made to the KV store.
</li><li> Queues over time. The number of queues, broken down by:
<ul><li> Read. Count of read operations waiting for a read lock to open.
</li><li> Write. Count of write operations waiting for a write lock to open.
</li><li> Total.
</li></ul></li><li> Connections over time.
</li><li> Percent of each minute spent flushing to disk - see above.
</li><li> Slowest operations. The ten slowest operations logged by KV store in the selected time frame. If profiling is off for all collections, this could have no results even if you have very slow operations running. Enable profiling on a per collection basis in collections.conf.
</li></ul><h4><font size="3"><b><i> <a name="kvstoreinstance_where_does_this_view_get_its_data_from.3f"><span class="mw-headline" id="Where_does_this_view_get_its_data_from.3F"> Where does this view get its data from? </span></a></i></b></font></h4>
<p>KV store collects data in the _introspection index. 
</p><p>These statistics are broken into the following components:
</p>
<ul><li> KVStoreServerStats. Information about how the KV store process is performing as a whole. Polled every 27 seconds.
</li><li> KVStoreCollectionStats. Information about collections within the KV store. Polled every 10 minutes.
</li><li> KVStoreReplicaSetStats. Information about replication data across KV store Instances. Polled every 60 seconds.
</li><li> KVProfilingStats. Information about slow operations. Polled every 5 seconds. Only available when profiling is enabled. <b>Note:</b> Enable profile only on development systems or for troubleshooting issues with KV store performance beyond what is available in the default panels. Profiling can negatively affect system performance and so should not be enabled in production environments.
</li></ul><p>In addition, KV store produces entries in a number of internal logs collected by Splunk Enterprise.
</p>
<h3> <a name="kvstoreinstance_interpret_results_in_this_view"><span class="mw-headline" id="Interpret_results_in_this_view"> Interpret results in this view </span></a></h3>
<p>For information on performance indicators and red flags, see "<a href="#kvstoredeployment" class="external text">KV store: Deployment</a>" in this manual.
</p>
<h3> <a name="kvstoreinstance_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from introspection logs. Any gaps in time in the graphs in this section typically indicate a point at which KV store or Splunk Enterprise was unreachable. If a panel is completely blank or missing data from specific Splunk Enterprise instances, check:
</p>
<ul><li> that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your introspection logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="kvstoredeployment"></a><h2> <a name="kvstoredeployment_kv_store:_deployment"><span class="mw-headline" id="KV_store:_Deployment"> KV store: Deployment</span></a></h2>
<h3> <a name="kvstoredeployment_what_does_this_view_show.3f"><span class="mw-headline" id="What_does_this_view_show.3F"> What does this view show? </span></a></h3>
<p>The <b>KV store: Deployment</b> view in the distributed management console (DMC) provides information aggregated across all KV stores in your Splunk Enterprise deployment. For an instance to be included in this view, it must be set with the server role of <b>KV store</b>. Do this in the DMC <b>Setup</b> page.
</p><p>This view and the <b>KV store: Instance</b> view track much of the same information. The difference is that this deployment view collects statistics from KV stores and displays the instances grouped by values of those different metrics.
</p><p>For definitions and context on the individual dashboards and metrics, see "<a href="#kvstoreinstance" class="external text">KV store: instance</a>" in this chapter.
</p>
<h4><font size="3"><b><i> <a name="kvstoredeployment_performance_metrics"><span class="mw-headline" id="Performance_Metrics">Performance Metrics</span></a></i></b></font></h4>
<h5> <a name="kvstoredeployment_deployment_snapshots"><span class="mw-headline" id="Deployment_Snapshots">Deployment Snapshots</span></a></h5>
<p><b>Deployment Snapshot Statistics</b> access the <code><font size="2">/services/server/introspection/kvstore/serverstatus</font></code> REST endpoint. For each KV store instance in the deployment, the <b>Deployment Snapshots</b> provide the following information:
</p>
<ul><li> Instance. The Splunk Enterprise instance name.
</li><li> Memory usage
</li><li> Total queued
</li><li> Current connections
</li><li> Page faults per operation
</li><li> Lock (%)
</li><li> Last flush (ms)
</li><li> Network traffic (MB)
</li><li> Up time (in hours). Amount of time the current instance has been running without restart
</li><li> Replication role. The role the instance plays in the replication set. If the instance is not part of a replication set, it returns "N/A."
</li></ul><h4><font size="3"><b><i> <a name="kvstoredeployment_where_does_this_view_get_its_data_from.3f"><span class="mw-headline" id="Where_does_this_view_get_its_data_from.3F"> Where does this view get its data from? </span></a></i></b></font></h4>
<p>KV store collects data in the _introspection index. 
</p><p>These statistics are broken into the following components:
</p>
<ul><li> KVStoreServerStats. Information about how the KV store process is performing as a whole. Polled every 27 seconds.
</li><li> KVStoreCollectionStats. Information about collections within the KV store. Polled every 10 minutes.
</li><li> KVStoreReplicaSetStats. Information about replication data across KV store Instances. Polled every 60 seconds.
</li><li> KVProfilingStats. Information about slow operations. Polled every 5 seconds. Only available when profiling is enabled. <b>Note:</b> Enable profile only on development systems or for troubleshooting issues with KV store performance beyond what is available in the default panels. Profiling can negatively affect system performance and so should not be enabled in production environments.
</li></ul><p>In addition, KV store produces entries in a number of internal logs collected by Splunk Enterprise.
</p>
<h3> <a name="kvstoredeployment_interpret_this_view"><span class="mw-headline" id="Interpret_this_view"> Interpret this view </span></a></h3>
<table cellpadding="6"><tr><th bgcolor="#C0C0C0"> Panel
</th><th bgcolor="#C0C0C0"> Critical
</th><th bgcolor="#C0C0C0"> Warning
</th><th bgcolor="#C0C0C0"> Normal
</th><th bgcolor="#C0C0C0"> Interpretation
</th></tr><tr><td valign="center" align="left"> Page faults per operation
</td><td valign="center" align="left"> 1.3+
<p>Reads require heavy disk I/O, which could indicate a need for more RAM.
</p>
</td><td valign="center" align="left"> 0.7&ndash;1.3
<p>Reads regularly require disk I/O.
</p>
</td><td valign="center" align="left"> 0&ndash;0.7
<p>Reads rarely require disk I/O.
</p>
</td><td valign="center" align="left"> Measures how often read requests are not satisfied by what Splunk Enterprise has in memory, requiring Splunk Enterprise to contact the disk.
</td></tr><tr><td valign="center" align="left"> Lock percentage
</td><td valign="center" align="left"> 50%+
</td><td valign="center" align="left"> 30%&ndash;50%
</td><td valign="center" align="left"> 0&ndash;30%
</td><td valign="center" align="left"> High lock percentage can starve replication and/or cause application calls to be slow, time out, or fail. High lock percentage typically means that heavy write activity is occurring on the node.
</td></tr><tr><td valign="center" align="left"> Network traffic
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> Network traffic should be commensurate with system use and application expectations. No default thresholds apply.
</td></tr><tr><td valign="center" align="left"> Replication latency
</td><td valign="center" align="left"> &gt;30 seconds
</td><td valign="center" align="left"> 10&ndash;30 seconds
</td><td valign="center" align="left"> 0&ndash;10 seconds
</td><td valign="center" align="left"> Replication needs are system dependent. Generally, replica set members should not fall significantly behind the KV captain. Replication latency over 30 seconds can indicate a mounting replication problem.
</td></tr><tr><td valign="center" align="left"> Primary operations log window
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> N/A
</td><td valign="center" align="left"> Provided for reference. This is the amount of data, in terms of time, a system saved in the operations log for restoration.
</td></tr><tr><td valign="center" align="left"> Flushing rate
</td><td valign="center" align="left"> 50%&ndash;100%
</td><td valign="center" align="left"> 10%&ndash;50%
</td><td valign="center" align="left"> 0&ndash;10%
</td><td valign="center" align="left"> A high flush rate indicates heavy write operations or sluggish system performance.
</td></tr></table><h3> <a name="kvstoredeployment_troubleshoot_this_view"><span class="mw-headline" id="Troubleshoot_this_view"> Troubleshoot this view </span></a></h3>
<p>The historical panels get data from the _introspection and _internal indexes. Gaps in time in these panels indicate a point at which KV store or Splunk Enterprise was unreachable. If a panel is completely blank or missing data from specific Splunk Enterprise instances, check: 
</p>
<ul><li> that you're <a href="#configurethemonitoringconsole_configure_your_distributed_management_console" class="external text">forwarding your logs to your indexers</a>, and
</li><li> the system requirements for platform instrumentation.
</li></ul><a name="dmclicensing"></a><h2> <a name="dmclicensing_licensing"><span class="mw-headline" id="Licensing"> Licensing </span></a></h2>
<p>The <b>Licensing</b> view in the distributed management console (DMC) presents the same information as the <a href="#aboutsplunkslicenseusagereportview" class="external text">license usage report view</a>. The advantage to accessing this view through the DMC instead of through your license master is that if your deployment has multiple license masters, in the DMC view you can select which license master's information to view.
</p><p>For details about the information in this view, see "<a href="#aboutsplunkslicenseusagereportview" class="external text">About the Splunk Enterprise license usage report view</a>" in this manual.
</p>
<h1>Administer the app key value store</h1><a name="aboutkvstore"></a><h2> <a name="aboutkvstore_about_the_app_key_value_store"><span class="mw-headline" id="About_the_app_key_value_store"> About the app key value store</span></a></h2>
<p>The app key value store (or KV store) provides a way to save and retrieve data within your Splunk apps, thereby letting you manage and maintain the state of the application.
</p><p>Here are some ways that Splunk apps might use the KV Store:
</p>
<ul><li> Tracking workflow in an incident-review system that moves an issue from one user to another.
</li><li> Keeping a list of environment assets provided by users.
</li><li> Controlling a job queue.
</li><li> Managing a UI session by storing the user or application state as the user interacts with the app.
</li><li> Storing user metadata.
</li><li> Caching results from search queries by Splunk or an external data store.
</li><li> Storing checkpoint data for modular inputs.
</li></ul><p>For information on using the KV store, see app key value store documentation for Splunk app developers.
</p>
<h3> <a name="aboutkvstore_how_kv_store_works_with_your_deployment"><span class="mw-headline" id="How_KV_store_works_with_your_deployment"> How KV store works with your deployment </span></a></h3>
<p>The KV store stores your data as key-value pairs in collections. Here are the main concepts:
</p>
<ul><li> <b>Collections</b> are the containers for your data, similar to a database table. Collections exist within the context of a given app.
</li><li> <b>Records</b> contain each entry of your data, similar to a row in a database table.
</li><li> <b>Fields</b> correspond to key names, similar to the columns in a database table. Fields contain the values of your data as a JSON file. Although it is not required, you can enforce data types (number, boolean, time, and string) for field values.
</li><li> <b>_key</b> is a reserved field that contains the unique ID for each record. If you don't explicitly specify the _key value, the app auto-generates one.
</li><li> <b>_user</b> is a reserved field that contains the user ID for each record. This field cannot be overridden.
</li><li> <b>Accelerations</b> improve search performance by making searches that contain accelerated fields return faster. Accelerations store a small portion of the collection's data set in an easy-to-traverse form.
</li></ul><p>The KV store files reside on search heads.
</p><p>In a search head cluster, if any node receives a write, the KV store delegates the write to the <b>KV store captain</b>. The KV store keeps the reads local, however.
</p>
<h3> <a name="aboutkvstore_system_requirements"><span class="mw-headline" id="System_requirements"> System requirements </span></a></h3>
<p>KV store is available and supported on all Splunk Enterprise 64-bit builds. It is not available on 32-bit Splunk Enterprise builds. KV store is also not available on universal forwarders. See the Splunk Enterprise system requirements.
</p><p>KV store uses port 8191 by default. You can change the port number in server.conf's <code><font size="2">[kvstore]</font></code> stanza. For information about other ports that Splunk Enterprise uses, see "System requirements and other deployment considerations for search head clusters" in the <i>Distributed Search Manual</i>.
</p><p>For information about other configurations that you can change in KV store, see the "KV store configuration" section in <a href="#serverconf" class="external text">server.conf.spec</a>.
</p>
<h4><font size="3"><b><i> <a name="aboutkvstore_about_splunk_fips"><span class="mw-headline" id="About_Splunk_FIPS"> About Splunk FIPS </span></a></i></b></font></h4>
<p>To use FIPS with KV store, see the "KV store configuration" section in <a href="#serverconf" class="external text">server.conf.spec</a>.
</p><p>If Splunk FIPS is not enabled, those settings will be ignored.
</p><p>If you enable FIPS but do not provide the required settings (<code><font size="2">caCertPath</font></code>, <code><font size="2">sslKeysPath</font></code>, and <code><font size="2">sslKeysPassword</font></code>), KV store does not run. Look for error messages in splunkd.log and on the console that executes <code><font size="2">splunk start</font></code>.
</p>
<h3> <a name="aboutkvstore_determine_whether_your_apps_use_kv_store"><span class="mw-headline" id="Determine_whether_your_apps_use_KV_store"> Determine whether your apps use KV store </span></a></h3>
<p>KV store is enabled by default on Splunk Enterprise 6.2+.
</p><p>Apps that use the KV store typically have collections.conf defined in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app name&gt;/default</font></code>. In addition, transforms.conf will have references to the collections with external_type = kvstore
</p>
<h3> <a name="aboutkvstore_use_the_kv_store"><span class="mw-headline" id="Use_the_KV_store"> Use the KV store</span></a></h3>
<p>To use the KV store:
</p><p><b>1.</b> Create a collection and optionally define a list of fields with data types using configuration files or the REST API.
</p><p><b>2.</b> Perform create-read-update-delete (CRUD) operations using search lookup commands and the Splunk REST API.
</p><p><b>3.</b> Manage collections using the REST API.
</p>
<h3> <a name="aboutkvstore_monitor_its_effect_on_your_splunk_enterprise_deployment"><span class="mw-headline" id="Monitor_its_effect_on_your_Splunk_Enterprise_deployment"> Monitor its effect on your Splunk Enterprise deployment </span></a></h3>
<p>You can monitor your KV store performance through two views in the distributed management console. One view provides insight across your entire deployment (see "<a href="#kvstoredeployment" class="external text">KV store: Deployment</a>" in this manual). The other view gives you information about KV store operations on each search head (see "<a href="#kvstoreinstance" class="external text">KV store: Instance</a>").
</p>
<h3> <a name="aboutkvstore_back_up_kv_store_data"><span class="mw-headline" id="Back_up_KV_store_data"> Back up KV store data</span></a></h3>
<p>Back up and restore your KV store data using the standard backup and restore tools and procedures used by your organization. To back up KV store data, back up all files in the path that is specified in the <code><font size="2">dbPath</font></code> parameter of the <code><font size="2">[kvstore]</font></code> stanza in the <code><font size="2">server.conf</font></code> file. 
</p><p>For general information about backup strategies in Splunk Enterprise, see "Choose your backup strategy" in the <i>Managing Indexers and Clusters of Indexers</i> manual.
</p>
<h1>Meet Splunk apps </h1><a name="whatsanapp"></a><h2> <a name="whatsanapp_apps_and_add-ons"><span class="mw-headline" id="Apps_and_add-ons">Apps and add-ons</span></a></h2>
<p>Users often ask for definitions of app and add-on in an effort to determine what differentiates them from each other. There are no definitive criteria that universally distinguish an app from an add-on. Both are packaged sets of configuration that you install on your instance of Splunk Enterprise, and both make it easier to integrate with, or ingest data from, other technologies or vendors. 
</p>
<ul><li> <b>Apps</b> generally offer extensive user interfaces that enable you to work with your data, and they often make use of one or more add-ons to ingest different types of data. 
</li><li> <b>Add-ons</b> generally enable Splunk Enterprise, or a Splunk app, to ingest or map a particular type of data. 
</li></ul><p>To an Admin user, the difference should matter very little as both apps and add-ons function as tools to help you get data into Splunk Enterprise, then efficiently use it. To an app developer, the difference matters more: see dev.splunk.com for guidance on developing an app.
</p>
<h3> <a name="whatsanapp_app"><span class="mw-headline" id="App">App</span></a></h3>
<p>An <b>app</b> is an application that runs on Splunk Enterprise. Out of the box, Splunk Enterprise includes one basic, default app that enables you to work with your data: the <a href="#thedefaultapps" class="external text">Search and Reporting app</a>. To address use cases beyond the basic, you can install many other apps, some free, some paid, on your instance of Splunk Enterprise. Examples include Splunk App for Microsoft Exchange, Splunk App for Enterprise Security, and Splunk DB Connect. An app may make use of one or more add-ons to facilitate how it collects or maps particular types of data. 
</p><p><br></p>
<h3> <a name="whatsanapp_add-on"><span class="mw-headline" id="Add-on">Add-on</span></a></h3>
<p>An <b>add-on</b> runs on Splunk Enterprise to provide specific capabilities to apps, such as getting data in, mapping data, or providing <b>saved searches</b> and macros. Examples include Splunk Add-on for Checkpoint OPSEC LEA, Splunk Add-on for Box, and Splunk Add-on for McAfee.
</p><p><br></p>
<h3> <a name="whatsanapp_app_and_add-on_support_and_certification"><span class="mw-headline" id="App_and_add-on_support_and_certification">App and add-on support and certification</span></a></h3>
<p>Anyone can develop an app or add-on for Splunk software. Splunk and members of our community create apps and add-ons and share them with other users of Splunk software via Splunkbase, the online app marketplace. Splunk <i>does not</i> support all apps and add-ons on Splunkbase. Labels in Splunkbase indicate who supports each app or add-on.
</p>
<ul><li> The Splunk Support team accepts cases and responds to issues only for the apps and add-ons which display a <b>Splunk Supported</b> label on Splunkbase. 
</li><li> Some developers support their own apps and add-ons. These apps and add-ons display a <b>Developer Supported</b> label on Splunkbase.
</li><li> The Splunk developer community supports apps and add-ons which display a <b>Community Supported</b> label on Splunkbase.
</li></ul><p><img alt="Splunk supported.png" src="images/c/c0/Splunk_supported.png" width="248" height="108"><img alt="Developer supported.png" src="/images/8/8e/Developer_supported.png" width="224" height="110"><img alt="Community supported.png" src="/images/7/7c/Community_supported.png" width="223" height="107"></p><p>Further, app developers can obtain Splunk Certification for their app or add-on. This means that Splunk has examined an app or add-on and found that it conforms to best practices for Splunk development. Certification does not, however, mean that Splunk supports an app or add-on. For example, an add-on created by a community developer that is published on Splunkbase and certified by Splunk is not supported by Splunk. Look for a <b>Splunk Supported</b> label on Splunkbase to determine that Splunk supports an app or add-on.
</p>
<a name="thedefaultapps"></a><h2> <a name="thedefaultapps_search_and_reporting_app"><span class="mw-headline" id="Search_and_Reporting_app"> Search and Reporting app</span></a></h2>
<p>The first time you install and log into Splunk, you land in Splunk Home. The Home page displays Click on Apps in the apps that have been pre-installed for you. 
</p><p><img alt="SearchReportingApp.png" src="images/4/46/SearchReportingApp.png" width="625" height="308"></p><p>By default, Splunk provides the Search and Reporting app. This interface provides the core functionality of Splunk and is designed for general-purpose use. This app displays at the top of your Home Page when you first log in and provides a search field so that you can immediately starting using it.
</p><p>Once in the Search and Reporting app (by running a search or clicking on the app in the Home page) you can use the menu bar options to select the following:
</p>
<ul><li> <b>Search:</b> Search your indexes. See the "Using Splunk Search" in the Search Tutorial for more information.
</li><li> <b>Pivot:</b> Use data models quickly design and generate tables, charts, and  visualizations for your data. See the Pivot Manual for more information.
</li><li> <b>Reports:</b> Turn your searches into reports. "Saving and sharing reports" in the Search Tutorial for more information.
</li><li> <b>Alerts:</b> Set up alerts for your Splunk searches and reports. See the Alerting Manual for more information
</li><li> <b>Dashboards:</b> Leverage predefined dashboards or create your own. See Dashboards and Visualizations manual.
</li></ul><a name="configuresplunktoopeninanapp"></a><h2> <a name="configuresplunktoopeninanapp_configure_splunk_web_to_open_in_an_app"><span class="mw-headline" id="Configure_Splunk_Web_to_open_in_an_app"> Configure Splunk Web to open in an app</span></a></h2>
<p>You can configure Splunk Web so that it opens in a specific app of your choosing instead of Splunk Home. You can make Splunk Web open to open in a specific app for all users, or match apps to specific users.
</p>
<h3> <a name="configuresplunktoopeninanapp_bypass_splunk_home_for_a_single_user"><span class="mw-headline" id="Bypass_Splunk_Home_for_a_single_user">Bypass Splunk Home for a single user</span></a></h3>
<p>You can configure Splunk Web so that when a user logs in, they go straight to an app of your choosing, rather than Splunk Home. 
</p><p>To make the Search app  the default landing app for a user:
</p><p><b>1.</b> Create a file called <code><font size="2">user-prefs.conf</font></code> in the user's local directory:
</p>
<code><font size="2"> &nbsp;etc/users/&lt;user&gt;/user-prefs/local/user-prefs.conf</font></code>
<ul><li> For the <code><font size="2">admin</font></code> user the file would be in: 
</li></ul><code><font size="2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etc/users/admin/user-prefs/local/user-prefs.conf</font></code>
<ul><li> For the <code><font size="2">test</font></code> user, it would be in:
</li></ul><code><font size="2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etc/users/test/user-prefs/local/user-prefs.conf</font></code>
<p><b>2.</b> Put the following line in the <code><font size="2">user-prefs.conf</font></code> file:
</p>
<code><font size="2"> &nbsp;default_namespace = search </font></code>
<h3> <a name="configuresplunktoopeninanapp_bypass_splunk_home_for_all_users"><span class="mw-headline" id="Bypass_Splunk_Home_for_all_users">Bypass Splunk Home for all users</span></a></h3>
<p>You can specify a default app for all users to land in when they log in. For example, if you want the Search app to be the global default, edit <code><font size="2">$SPLUNK_HOME/etc/apps/user-prefs/local/user-prefs.conf</font></code> and specify:
</p>
<code><font size="2"> &nbsp;[general_default]<br>&nbsp;&nbsp;default_namespace = search </font></code>
<p><b>Note:</b> Users who do not have permission to access the Search app will see an error.
</p>
<a name="wheretogetmoreapps"></a><h2> <a name="wheretogetmoreapps_where_to_get_more_apps_and_add-ons"><span class="mw-headline" id="Where_to_get_more_apps_and_add-ons"> Where to get more apps and add-ons</span></a></h2>
<p>You can find new apps and add-ons on Splunkbase: <b>https://splunkbase.splunk.com/</b>. 
</p><p>You can access Splunkbase from the Splunk Enterprise User bar, and download and install apps directly within Splunk Enterprise. Click the <b>Apps</b> menu and select <b>Find more Apps</b>.
</p><p>When you log into Splunk Web, you see Splunk Home by default. You can always get back to Splunk Home by clicking on the Splunk logo at the top (left-hand side) of the page.  
</p>
<h3> <a name="wheretogetmoreapps_if_you_are_connected_to_the_internet"><span class="mw-headline" id="If_you_are_connected_to_the_internet"> If you are connected to the internet </span></a></h3>
<p>If your Splunk Enterprise server or your client machine are connected to the internet, you can download apps and add-ons directly from Splunk Home:
</p><p><b>1.</b> In the User bar, click on the <b>Apps</b> menu and select <b>Find More Apps</b>. 
</p><p><img alt="AppMenu.png" src="images/c/c7/AppMenu.png" width="327" height="61"></p><p><b>2.</b> In the list of apps and add-ons, pick the app or add-on you want and select <b>Download App</b>.
</p><p><img alt="FindMoreApps.png" src="images/1/18/FindMoreApps.png" width="600" height="465"></p><p><b>3.</b> You will be prompted to log in with your splunk.com username and password (note that this is not your Splunk Enterprise username and password).
</p><p><b>4.</b> Your selected item is installed. If it has a Web GUI component (most add-ons contain only knowledge objects like event type definitions and don't have any GUI context), you can navigate to it from Splunk Home.
</p><p><b>Important:</b> If Splunk Web is located behind a proxy server, you might have trouble accessing Splunkbase. To solve this problem, you need to set the <code><font size="2">HTTP_PROXY</font></code> environment variable, as described in "Specify a proxy server".
</p>
<h3> <a name="wheretogetmoreapps_if_you_are_not_connected_to_the_internet"><span class="mw-headline" id="If_you_are_not_connected_to_the_internet"> If you are not connected to the internet </span></a></h3>
<p>If your Splunk Enterprise server and client do not have internet connectivity, you must download apps from Splunkbase and copy them over to your server:
</p><p><b>1.</b> From a computer connected to the internet, browse Splunkbase for the app or add-on you want. 
</p><p><b>2.</b> Download the app or add-on.
</p><p><b>3.</b> Once downloaded, copy it to your Splunk Enterprise server.
</p><p><b>4.</b> Put it in your <code><font size="2">$SPLUNK_HOME/etc/apps</font></code> directory.
</p><p><b>5.</b> Untar and ungzip your app or add-on, using a tool like <code><font size="2">tar -xvf</font></code> (on *nix) or WinZip (on Windows). Note that Splunk apps and add-ons are packaged with a .SPL extension although they are just tarred and gzipped. You may need to force your tool to recognize this extension.
</p><p><b>6.</b> You may need to restart Splunk Enterprise, depending on the contents of the app or add-on.
</p><p><b>7.</b> Your app or add-on is now installed and will be available from Splunk Home (if it has a web UI component).
</p>
<a name="apparchitectureandobjectownership"></a><h2> <a name="apparchitectureandobjectownership_app_architecture_and_object_ownership"><span class="mw-headline" id="App_architecture_and_object_ownership"> App architecture and object ownership</span></a></h2>
<p>Apps are commonly built from Splunk <b>knowledge objects</b>. Splunk knowledge objects include saved searches, event types, tags -- data types that enrich your Splunk deployment and make it easier to find what you need. 
</p><p><b>Note:</b> Occasionally you may save objects to add-ons as well, though this is not common. Apps and add-ons are both stored in the apps directory. On the rare instance that you would need to save objects to an add-on, you would manage the add-on the same as described for apps in this topic.
</p><p>Any user logged into Splunk Web can create and save knowledge objects to the user's directory under the app the user is "in" (assuming sufficient permissions).  This is the default behavior -- whenever a user saves an object, it goes into the user's directory in the currently running app. 
The user directory is located at <code><font size="2">$SPLUNK_HOME/etc/users/&lt;user_name&gt;/&lt;app_name&gt;/local</font></code>. Once the user has saved the object in that app, it is available only to that user when they are in that app unless they do one of the following:
</p>
<ul><li> Promote the object so that it is available to all users who have access
</li><li> Restrict the object to specific roles or users (still within the app context)
</li><li> Mark the object as globally available to all apps, add-ons and users (unless you've explicitly restricted it by role/user)
</li></ul><p><b>Note</b>: Users must have write permissions for an app or add-on before they can promote objects to that level.
</p>
<h4><font size="3"><b><i> <a name="apparchitectureandobjectownership_promote_and_share_splunk_knowledge"><span class="mw-headline" id="Promote_and_share_Splunk_knowledge">Promote and share Splunk knowledge</span></a></i></b></font></h4>
<p>Users can share their Splunk knowledge objects with other users through the Permissions dialog.  This means users who have read permissions in an app or add-on can see the shared objects and use them. For example, if a user shares a saved search, other users can see that saved search, but only within the app in which the search was created. So if you create a saved search in the app "Fflanda" and  share it, other users of Fflanda can see your saved search if they have read permission for Fflanda.
</p><p>Users with write permission can promote their objects to the app level.  This means the objects are copied from their user directory to the app's directory -- from:
</p><p><code><font size="2">$SPLUNK_HOME/etc/users/&lt;user_name&gt;/&lt;app_name&gt;/local/</font></code>
</p><p>to:
</p><p><code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/</font></code>
</p><p>Users can do this only if they have write permission in the app.
</p>
<h4><font size="3"><b><i> <a name="apparchitectureandobjectownership_make_splunk_knowledge_objects_globally_available"><span class="mw-headline" id="Make_Splunk_knowledge_objects_globally_available">Make Splunk knowledge objects globally available</span></a></i></b></font></h4>
<p>Finally, upon promotion, users can decide if they want their object to be available globally, meaning all apps are able to see it. Again, the user must have permission to write to the original app. It's easiest to do this in Splunk Web, but you can also do it later by moving the relevant object into the desired directory. 
</p><p>To make globally available an object "A" (defined in "B.conf") that belongs to user "C" in app "D": 
</p><p><b>1.</b> Move the stanza defining the object A from <code><font size="2">$SPLUNK_HOME/etc/users/C/D/B.conf</font></code> into <code><font size="2">$SPLUNK_HOME/etc/apps/D/local/B.conf</font></code>.
</p><p><b>2.</b> Add a setting, <code><font size="2">export = system</font></code>, to the object A's stanza in the app's <code><font size="2">local.meta</font></code> file. If the stanza for that object doesn't already exist, you can just add one. 
</p><p>For example, to promote an event type called "rhallen" created by a user named "fflanda" in the *Nix app so that it is globally available:
</p><p><b>1.</b> Move the [rhallen] stanza from <code><font size="2">$SPLUNK_HOME/etc/users/fflanda/unix/local/eventtypes.conf</font></code> to <code><font size="2">$SPLUNK_HOME/etc/apps/unix/local/eventtypes.conf</font></code>.
</p><p><b>2.</b> Add the following stanza:
</p>
<code><font size="2"><br>[eventtypes/rhallen]<br>export = system<br></font></code>
<p>to <code><font size="2">$SPLUNK_HOME/etc/apps/unix/metadata/local.meta</font></code>. 
</p><p><b>Note:</b> Adding the <code><font size="2">export = system</font></code> setting to <code><font size="2">local.meta</font></code> isn't necessary when you're sharing event types from the Search app, because it exports all of its events globally by default.
</p>
<h4><font size="3"><b><i> <a name="apparchitectureandobjectownership_what_objects_does_this_apply_to.3f"><span class="mw-headline" id="What_objects_does_this_apply_to.3F">What objects does this apply to?</span></a></i></b></font></h4>
<p>The knowledge objects discussed here are limited to those that are subject to access control. These objects are also known as app-level objects and can be viewed by selecting <b>Apps &gt; Manage Apps</b> from the User menu bar.  This page is available to all users to manage any objects they have created and shared. These objects include:
</p>
<ul><li> Saved searches and Reports
</li><li> Event types 
</li><li> Views and dashboards
</li><li> Field extractions 
</li></ul><p>There are also system-level objects available only to users with admin privileges (or read/write permissions on the specific objects). These objects include:
</p>
<ul><li> Users 
</li><li> Roles 
</li><li> Auth 
</li><li> Distributed search 
</li><li> Inputs 
</li><li> Outputs 
</li><li> Deployment 
</li><li> License 
</li><li> Server settings (for example: host name, port, etc)
</li></ul><p><b>Important:</b> If you add an input, Splunk adds that input to the copy of <code><font size="2">inputs.conf</font></code> that belongs to the app you're currently in. This means that if you navigated to your app directly from Search, your input will be added to <code><font size="2">$SPLUNK_HOME/etc/apps/search/local/inputs.conf</font></code>, which might not be the behavior you desire.
</p>
<h3> <a name="apparchitectureandobjectownership_app_configuration_and_knowledge_precedence"><span class="mw-headline" id="App_configuration_and_knowledge_precedence">App configuration and knowledge precedence</span></a></h3>
<p>When you add knowledge to Splunk, it's added in the context of the app you're in when you add it. When Splunk is evaluating configurations and knowledge, it evaluates them in a specific order of precedence, so that you can control what knowledge definitions and configurations are used in what context. Refer to About configuration files for more information about Splunk configuration files and the order of precedence.
</p>
<a name="managingappobjects"></a><h2> <a name="managingappobjects_manage_app_and_add-on_objects"><span class="mw-headline" id="Manage_app_and_add-on_objects"> Manage app and add-on objects</span></a></h2>
<p>When an <b>app</b> or <b>add-on</b> is created by a Splunk user, a collection of objects is created that make up the app or add-on. These objects can include <b>views</b>, commands, navigation items, <b>event types</b>, <b>saved searches</b>, <b>reports</b>, and more. Each of these objects have permissions associated with them to determine who can view or alter them. By default, the admin user has <b>permissions</b> to alter all the objects in the Splunk system.
</p><p>Refer to these topics for more information:
</p>
<ul><li> For an overview of apps and add-ons, refer to <a href="#whatsanapp" class="external text">"What are apps and add-ons?"</a> in this manual.
</li><li> For more information about app and add-on permissions, refer to <a href="#apparchitectureandobjectownership" class="external text">"App architecture and object ownership"</a> in this manual.
</li><li> To learn more about how to create your own apps and add-ons, refer to the Developing Views and Apps for Splunk Web manual.
</li></ul><h3> <a name="managingappobjects_view_and_manage_app_or_add-on_objects_in_splunk_web"><span class="mw-headline" id="View_and_manage_app_or_add-on_objects_in_Splunk_Web">View and manage app or add-on objects in Splunk Web</span></a></h3>
<p>You can use Splunk Web to view the objects in your Splunk deployment in the following ways:
</p>
<ul><li> To see all the objects for all the apps/add-ons on your system at once: <b>Settings &gt; All configurations</b>.
</li><li> To see all the saved searches and report objects: <b>Settings &gt; Searches and reports</b>.
</li><li> To see all the event types: <b>Settings &gt; Event types</b>.
</li><li> To see all the field extractions: <b>Settings &gt; Fields</b>.
</li></ul><p>You can:
</p>
<ul><li> View and manipulate the objects on any page with the <b>sorting arrows</b> <img alt="Arrows.jpg" src="images/d/db/Arrows.jpg" width="15" height="20"></li><li> Filter the view to see only the objects from a given app or add-on, owned by a particular user, or those that contain a certain string, with the <b>App context bar</b>.
</li></ul><p>Use the Search field on the App context bar to search for strings in fields. By default, Splunk searches for the string in all available fields. To search within a particular field, specify that field. Wildcards are supported.
</p><p><b>Note:</b> For information about the individual search commands on the Search command page, refer to the <b>Search Reference Manual</b>.
</p>
<h3> <a name="managingappobjects_update_an_app_or_add-on_in_the_cli"><span class="mw-headline" id="Update_an_app_or_add-on_in_the_CLI">Update an app or add-on in the CLI</span></a></h3>
<p>To update an existing app on your Splunk instance using the CLI:
</p>
<code><font size="2"><br>./splunk install app &lt;app_package_filename&gt; -update 1 -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
<p>Splunk updates the app or add-on based on the information found in the installation package.
</p>
<h3> <a name="managingappobjects_disable_an_app_or_add-on_using_the_cli"><span class="mw-headline" id="Disable_an_app_or_add-on_using_the_CLI">Disable an app or add-on using the CLI</span></a></h3>
<p>To disable an app via the CLI:
</p>
<code><font size="2"><br>./splunk disable app [app_name] -auth &lt;username&gt;:&lt;password&gt;<br></font></code>
<p><b>Note:</b> If you are running Splunk Free, you do not have to provide a username and password.
</p>
<h3> <a name="managingappobjects_uninstall_an_app_or_add-on"><span class="mw-headline" id="Uninstall_an_app_or_add-on">Uninstall an app or add-on</span></a></h3>
<p>To remove an installed app from a Splunk installation:
</p><p><b>1.</b> (Optional) Remove the app or add-on's indexed data. Typically, Splunk does not access indexed data from a deleted app or add-on. However, you can use Splunk's CLI clean command to remove indexed data from an app before deleting the app. See Remove data from indexes with the CLI command.
</p><p><b>2.</b> Delete the app and its directory. This should be located in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;appname&gt;</font></code>. You can run the following command in the CLI:
</p>
<div class="samplecode">./splunk remove app [appname] -auth &lt;username&gt;:&lt;password&gt;</div>
<p><b>3.</b> You may need to remove user-specific directories created for your app or add-on by deleting the files (if any) found here: <code><font size="2">$SPLUNK_HOME/splunk/etc/users/*/&lt;appname&gt;</font></code>
</p><p><b>4.</b> Restart Splunk.
</p>
<a name="managingappconfigurationsandproperties"></a><h2> <a name="managingappconfigurationsandproperties_managing_app_and_add-on_configurations_and_properties"><span class="mw-headline" id="Managing_app_and_add-on_configurations_and_properties"> Managing app and add-on configurations and properties</span></a></h2>
<p>You can manage the configurations and properties for apps installed in your Splunk Enterprise instance from the Apps menu. Click on <b>Apps</b> in the User bar to select one of your installed apps or manage an app. From the Manage Apps page, you can do the following:
</p>
<ul><li> Edit permissions for an app or add-on
</li><li> Enable or disable an app or add-on
</li><li> Perform actions, such as launch the app, edit the properties, and view app objects
</li></ul><p><br></p>
<h3> <a name="managingappconfigurationsandproperties_edit_app_and_add-on_properties"><span class="mw-headline" id="Edit_app_and_add-on_properties"> Edit app  and add-on properties </span></a></h3>
<p>The edits you make to configuration and properties depend on whether you are the owner of the app or a user.
</p><p>Select <b>Apps &gt; Manage Apps</b> then click <b>Edit properties</b> for the app or add-on you want to edit. You can make the following edits for apps installed in this Splunk Enterprise instance. 
</p><p><img alt="AppEditProperties.png" src="images/b/b3/AppEditProperties.png" width="600" height="298"></p>
<ul><li> <b>Name:</b> Change the display name of the app or add-on in Splunk Web. 
</li></ul><ul><li> <b>Update checking:</b> By default, update checking is enabled. You can override the default and disable update checking. See <a href="#managingappconfigurationsandproperties_checking_for_app_updates" class="external text">Checking for app an add-on updates</a> below for details.
</li></ul><ul><li> <b>Visible:</b> Apps with views should be visible. Add-ons, which often do not have a view, should disable the visible property. 
</li></ul><ul><li> <b>Upload asset:</b> Use this field to select a local file asset files, such as an HTML, JavaScript, or CSS file that can be accessed by the app or add-on. You can only upload one file at a time from this panel.
</li></ul><p>Refer to Apps and add-ons: An Introduction for details on the configuration and properties of apps and add-ons.
</p>
<h3> <a name="managingappconfigurationsandproperties_checking_for_updates"><span class="mw-headline" id="Checking_for_updates"> Checking for updates</span></a></h3>
<p>You can configure Splunk Enterprise whether to check Splunkbase for updates to an app or add-on. By default, checking for updates is enabled. You can disable checking for updates for an app by editing this property from <b>Settings &gt; Apps &gt; Edit properties</b>.
</p><p>However, if this property is not available in Splunk Web, you can also manually edit the apps <code><font size="2">app.conf</font></code> file to disable checking for updates. Create or edit the following stanza in <code><font size="2">$SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/app.conf</font></code> to disable checking for updates:
</p>
<div class="samplecode">
<code><font size="2"><br>[package] <br>check_for_updates = 0 <br></font></code></div>
<p><b>Note</b>: Edit the local version of <code><font size="2">app.conf</font></code>, not the default version. This avoids overriding your setting with the next update of the app.
</p>
<h1>Meet Hunk</h1><a name="aboutsplunkanalyticsforhadoop"></a><h2> <a name="aboutsplunkanalyticsforhadoop_meet_hunk"><span class="mw-headline" id="Meet_Hunk">Meet Hunk</span></a></h2>
<p>Hunk lets you configure remote HDFS datastores as virtual indexes so that Splunk can natively report on data residing in Hadoop. Once your virtual index is properly configured, you can report and visualize data residing in remote Hadoop datastores. The following links point to topics in the Hunk User Manual.
</p>
<h3> <a name="aboutsplunkanalyticsforhadoop_hunk_manual"><span class="mw-headline" id="Hunk_Manual">Hunk Manual</span></a></h3>
<p>Introduction
</p>
<ul><li> Meet Hunk
</li><li> What's new for Hunk 6.2
</li><li> FAQ
</li><li> Learn more and get help
</li></ul><p>Hunk concepts
</p>
<ul><li> About virtual indexes
</li><li> About external results providers
</li><li> About streaming resource libraries
</li><li> How Splunk returns reports on Hadoop data
</li><li> About pass-through authentication
</li></ul><p>Install Hunk
</p>
<ul><li> About installing and configuring Hunk
</li><li> System and software requirements
</li><li> Download and install Splunk
</li><li> Upgrade Hunk
</li><li> Start Splunk
</li><li> License Hunk
</li><li> Use Hunk and Splunk together
</li><li> Uninstall Hunk
</li><li> Get Hunk with the Hunk Amazon Machine Image
</li></ul><p>Manage Hunk using the configuration files
</p>
<ul><li> Set up your Splunk search head instance
</li><li> Set up a provider and virtual index in the configuration file
</li><li> Set up a streaming library
</li><li> Add a sourcetype
</li><li> Manage Hive data
</li><li> Configure Hive preprocessor
</li><li> Configure Hive preprocessor for Parquet
</li><li> Configure Hunk to run reports as a different user
</li><li> Configure report acceleration
</li><li> Configure pass-through authentication
</li><li> Configure Kerberos authentication
</li></ul><p>Manage Hunk in the user interface
</p>
<ul><li> About the Hunk user interface
</li><li> Add or edit an HDFS provider
</li><li> Add or edit a virtual index
</li><li> Set up pass-through authentication
</li></ul><p>Search virtual indexes
</p>
<ul><li> Use search commands on a virtual index
</li><li> Work with report acceleration
</li></ul><p>Reference
</p>
<ul><li> Troubleshoot Hunk
</li><li> Performance best practices
</li><li> Provider Configuration Variables
</li><li> Required configuration variable for YARN
</li></ul><p>REST API reference
</p>
<ul><li> Providers
</li><li> Indexes
</li></ul><p>Release Notes
</p>
<ul><li> Known issues
</li></ul><h3> <a name="aboutsplunkanalyticsforhadoop_tutorial"><span class="mw-headline" id="Tutorial">Tutorial</span></a></h3>
<ul><li> Welcome to the Hunk tutorial
</li><li> Step 1: Set up a Hadoop Virtual Machine instance
</li><li> Step 2: Set up your data
</li><li> Step 3: Set up an HDFS directory for Hunk access
</li><li> Step 4: Install and license Hunk
</li><li> Step 5: Configure an HDFS provider
</li><li> Step 6: Set up a Virtual Index
</li><li> Step 7: Try a simple data search
</li><li> Step 8: Save a report
</li><li> Learn more
</li></ul><h1>Manage users</h1><a name="aboutusersandroles"></a><h2> <a name="aboutusersandroles_about_users_and_roles"><span class="mw-headline" id="About_users_and_roles"> About users and roles</span></a></h2>
<p>If you're running Splunk Enterprise, you can create users with passwords and assign them to <b>roles</b> you have created. Splunk Free does not support user authentication. 
</p><p>Splunk comes with a single default user, the <b>admin</b> user. The default password for the admin user is <b>changeme</b>. As the password implies, you should change this password immediately upon installing Splunk. 
</p>
<h3> <a name="aboutusersandroles_create_users"><span class="mw-headline" id="Create_users">Create users</span></a></h3>
<p>Splunk ships with support for three types of authentication systems, which are described in the Security Manual: 
</p>
<ul><li> <b>Splunk's own built-in system.</b> See "About user authentication with Splunk's built-in system" for more information.
</li></ul><ul><li> <b>LDAP.</b> Splunk supports authentication with its internal authentication services or your existing LDAP server. See "Set up user authentication with LDAP" for more information.
</li></ul><ul><li> <b>Scripted authentication API.</b> Use scripted authentication to tie Splunk's authentication into an external authentication system, such as RADIUS or PAM. See "Set up user authentication with external systems" for more information.
</li></ul><h3> <a name="aboutusersandroles_about_roles"><span class="mw-headline" id="About_roles"> About roles </span></a></h3>
<p>Users are assigned to roles. A role contains a set of  <b>capabilities</b>. These specify what actions are available to roles. For example, capabilities determine whether someone with a particular role is allowed to add inputs or edit saved searches. The various capabilities are listed in "About defining roles with capabilities" in the Securing Splunk Enterprise manual.
</p><p>By default, Splunk comes with the following roles predefined:
</p>
<ul><li> admin -- this role has the most capabilities assigned to it.
</li><li> power -- this role can edit all shared objects (saved searches, etc) and alerts, tag events, and other similar tasks.
</li><li> user -- this role can create and edit its own saved searches, run searches, edit its own preferences, create and edit event types, and other similar tasks.
</li></ul><p>For detailed information on roles and how to assign users to roles, see the chapter "Users and role-based access control" in the Security Manual.
</p>
<h3> <a name="aboutusersandroles_find_existing_users_and_roles"><span class="mw-headline" id="Find_existing_users_and_roles"> Find existing users and roles </span></a></h3>
<p>To locate an existing user or role in Splunk Web, use the Search bar at the top of the Users or Roles page in the Access Controls section by selecting <b>Settings &gt; Access Controls</b>. Wildcards are supported. Splunk searches for the string you enter in all available fields by default. To search a particular field, specify that field. For example, to search only email addresses, type "email=&lt;<i>email address or address fragment</i>&gt;:, or to search only the "Full name" field, type "realname=&lt;<i>name or name fragment</i>&gt;. To search for users in a given role, use "roles=". 
</p><p><img alt="Search bar.jpg" src="images/8/89/Search_bar.jpg" width="366" height="48"></p>
<a name="userlanguageandlocale"></a><h2> <a name="userlanguageandlocale_configure_user_language_and_locale"><span class="mw-headline" id="Configure_user_language_and_locale"> Configure user language and locale</span></a></h2>
<p>When a user logs in, Splunk automatically uses the language that the user's browser is set to. To switch languages, change the browser's locale setting. Locale configurations are browser-specific.
</p><p>Splunk detects locale strings. A locale string contains two components: a language specifier and a localization specifier. This is usually presented as two lowercase letters and two uppercase letters linked by an underscore. For example, "en_US" means US English and "en_GB" means British English. 
</p><p>The user's locale also affects how dates, times, numbers, etc., are formatted, as different countries have different standards for formatting these entities.
</p><p>Splunk provides built-in support for these locales:
</p>
<code><font size="2"><br>de_DE<br>en_GB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>en_US<br>it_IT &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>ja_JP <br>ko_KO &nbsp;&nbsp;&nbsp;&nbsp;<br>zh_CN<br>zh_TW<br></font></code>
<p>If you want to add localization for additional languages, refer to "Translate Splunk" in the Developer manual for guidance. You can then tell your users to specify the appropriate locale in their browsers.
</p>
<h3> <a name="userlanguageandlocale_how_browser_locale_affects_timestamp_formatting"><span class="mw-headline" id="How_browser_locale_affects_timestamp_formatting">How browser locale affects timestamp formatting </span></a></h3>
<p>By default, timestamps in Splunk are formatted according the browser locale.  If the browser is configured for US English, the timestamps are presented in American fashion: <code><font size="2">MM/DD/YYYY:HH:MM:SS</font></code>.  If the browser is configured for British English, then the timestamps will be presented in the European date format: <code><font size="2">DD/MM/YYYY:HH:MM:SS</font></code>.
</p><p>For more information on timestamp formatting, see "Configure timestamp recognition" in the Getting Data In manual.
</p>
<h3> <a name="userlanguageandlocale_override_the_browser_locale"><span class="mw-headline" id="Override_the_browser_locale"> Override the browser locale </span></a></h3>
<p>The locale that Splunk uses for a given session can be changed by modifying the url that you use to access Splunk. Splunk urls follow the form <code><font size="2">http://host:port/locale/...</font></code>. For example, when you access Splunk to log in, the url may appear as <code><font size="2">http://hostname:8000/en-US/account/login</font></code> for US English.  To use British English settings, you can change the locale string to <code><font size="2">http://hostname:8000/en-GB/account/login</font></code>. This session then presents and accepts timestamps in British English format for its duration.
</p><p>Requesting a locale for which the Splunk interface has not been localized results in the message: <code><font size="2">Invalid language Specified</font></code>.
</p><p>Refer to "Translate Splunk" in the Developer Manual for more information about localizing Splunk.
</p>
<a name="configureusertimeouts"></a><h2> <a name="configureusertimeouts_configure_user_session_timeouts"><span class="mw-headline" id="Configure_user_session_timeouts"> Configure user session timeouts</span></a></h2>
<p>The amount of time that elapses before a Splunk user's session times out depends on the interaction among three timeout settings:
</p>
<ul><li> The <code><font size="2">splunkweb</font></code> session timeout. 
</li><li> The <code><font size="2">splunkd</font></code> session timeout.
</li><li> The browser session timeout. 
</li></ul><p>The <code><font size="2">splunkweb</font></code> and <code><font size="2">splunkd</font></code> timeouts determine the maximum idle time in the interaction between browser and Splunk. The browser session timeout determines the maximum idle time in interaction between user and browser.
</p><p>The <code><font size="2">splunkweb</font></code> and <code><font size="2">splunkd</font></code> timeouts generally have the same value, as the same field sets both of them. To set the timeout in Splunk Web:
</p><p><b>1.</b> Click <b>Settings</b> in the upper right-hand corner of Splunk Web.
</p><p><b>2.</b> Under System, click <b>Server settings</b>.  
</p><p><b>3.</b> Click <b>General settings</b>.
</p><p><b>4.</b> In the <b>Session timeout</b> field, enter a timeout value.
</p><p><b>5.</b> Click <b>Save</b>.
</p><p>This sets the user session timeout value for both <code><font size="2">splunkweb</font></code> and <code><font size="2">splunkd</font></code>. Initially, they share the same value of 60 minutes. They will continue to maintain identical values if you change the value through Splunk Web. 
</p><p>If, for some reason, you need to set the timeouts for <code><font size="2">splunkweb</font></code> and <code><font size="2">splunkd</font></code> to different values, you can do so by editing their underlying configuration files, <code><font size="2">web.conf</font></code> (<code><font size="2">tools.sessions.timeout</font></code> attribute) and <code><font size="2">server.conf</font></code> (<code><font size="2">sessionTimeout</font></code> attribute). For all practical purposes, there's no reason to give them different values. In any case, if the user is using SplunkWeb (<code><font size="2">splunkweb</font></code>) to access the Splunk instance (<code><font size="2">splunkd</font></code>), the smaller of the two timeout attributes prevails. So, if <code><font size="2">tools.sessions.timeout</font></code> in <code><font size="2">web.conf</font></code> has a value of "90" (minutes), and <code><font size="2">sessionTimeout</font></code> in <code><font size="2">server.conf</font></code> has a value of "1h" (1 hour; 60 minutes), the session will timeout after 60 minutes.
</p><p>In addition to setting the <code><font size="2">splunkweb</font></code>/<code><font size="2">splunkd</font></code> session value, you can also specify the timeout for the user browser session by editing the <code><font size="2">ui_inactivity_timeout</font></code> value in <code><font size="2">web.conf</font></code>. The Splunk browser session will time out once this value is reached. The default is 60 minutes. If <code><font size="2">ui_inactivity_timeout</font></code> is set to less than 1, there's no timeout -- the session will stay alive while the browser is open.
</p><p>The countdown for the <code><font size="2">splunkweb</font></code>/<code><font size="2">splunkd</font></code> session timeout does not begin until the browser session reaches its timeout value. So, to determine how long the user has before timeout, add the value of <code><font size="2">ui_inactivity_timeout</font></code> to the smaller of the timeout values for <code><font size="2">splunkweb</font></code> and <code><font size="2">splunkd</font></code>. For example, assume the following:
</p>
<ul><li> <code><font size="2">splunkweb</font></code> timeout: 15m
</li></ul><ul><li> <code><font size="2">splunkd</font></code> timeout: 20m
</li></ul><ul><li> browser (<code><font size="2">ui_inactivity_timeout</font></code>) timeout: 10m
</li></ul><p>The user session stays active for 25m (15m+10m). After 25 minutes of no activity, the user will be prompted to login again.
</p><p><b>Note: </b>If you change a timeout value, either in Splunk Web or in configuration files,&Acirc;&nbsp;you must restart Splunk for the change to take effect.
</p>
<h1>Configuration file reference</h1><a name="alertactionsconf"></a><h2> <a name="alertactionsconf_alert_actions.conf"><span class="mw-headline" id="alert_actions.conf">alert_actions.conf</span></a></h2>
<p>The following are the spec and example files for alert_actions.conf.
</p>
<h3> <a name="alertactionsconf_alert_actions.conf.spec"><span class="mw-headline" id="alert_actions.conf.spec">alert_actions.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values for configuring global <br># saved search actions in alert_actions.conf. &nbsp;Saved searches are configured <br># in savedsearches.conf.<br>#<br># There is an alert_actions.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an alert_actions.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see <br># alert_actions.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>maxresults = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set the global maximum number of search results sent via alerts.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 100.<br><br>hostname = [protocol]&lt;host&gt;[:&lt;port&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Sets the hostname used in the web link (url) sent in alerts.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This value accepts two forms.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* hostname<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;examples: splunkserver, splunkserver.example.com<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* protocol://hostname:port<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;examples: http://splunkserver:8000, https://splunkserver.example.com:443<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When this value is a simple hostname, the protocol and port which<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are configured within splunk are used to construct the base of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the url.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When this value begins with 'http://', it is used verbatim. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE: This means the correct port must be specified if it is not<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the default port for http or https.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is useful in cases when the Splunk server is not aware of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;how to construct an externally referenceable url, such as SSO<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;environments, other proxies, or when the Splunk server hostname<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is not generally resolvable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to current hostname provided by the operating system, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or if that fails, "localhost".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When set to empty, default behavior is used.<br><br>ttl &nbsp;&nbsp;&nbsp;&nbsp;= &lt;integer&gt;[p]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* optional argument specifying the minimum time to live (in seconds) <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of the search artifacts, if this action is triggered.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* if p follows integer, then integer is the number of <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduled periods.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If no actions are triggered, the artifacts will have their ttl<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;determined by the "dispatch.ttl" attribute in savedsearches.conf.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 10p <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 86400 (24 hours) &nbsp;&nbsp;for: email, rss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to &nbsp;&nbsp;600 (10 minutes) for: script <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to &nbsp;&nbsp;120 (2 minutes) &nbsp;for: summary_index, populate_lookup <br>&nbsp;<br>maxtime = &lt;integer&gt;[m|s|h|d]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The maximum amount of time that the execution of an action<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is allowed to take before the action is aborted.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Use the d, h, m and s suffixes to define the period of time:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d = day, h = hour, m = minute and s = second.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For example: 5d means 5 days.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 5m for everything except rss.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1m for rss.<br>&nbsp;<br>track_alert = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* indicates whether the execution of this action signifies a trackable<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alert.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br><br>command = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The search command (or pipeline) which is responsible for executing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the action.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Generally the command is a template search pipeline which is realized<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with values from the saved search - to reference saved search<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;field values wrap them in dollar signs ($).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, to reference the savedsearch name use $name$. To<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reference the search, use $search$<br><br>################################################################################<br># EMAIL: these settings are prefaced by the [email] stanza name<br>################################################################################<br><br>[email]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set email notification options under this stanza name.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute/value pairs. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify an entry for each attribute, Splunk will<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use the default value.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>from = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Email address from which the alert originates. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to splunk@$LOCALHOST.<br><br>to &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* to email address receiving alert.<br>&nbsp;<br>cc &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* cc email address receiving alert.<br><br>bcc &nbsp;&nbsp;&nbsp;&nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* bcc email address receiving alert.<br><br>message.report = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify a custom email message for scheduled reports. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Includes the ability to reference attributes from <br>&nbsp;&nbsp;&nbsp;&nbsp;* result, saved search, job<br><br>message.alert = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify a custom email message for alerts. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Includes the ability to reference attributes from <br>&nbsp;&nbsp;&nbsp;&nbsp;* result, saved search, job<br><br>subject = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify an alternate email subject if useNSSubject is false.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to SplunkAlert-&lt;savedsearchname&gt;. &nbsp;<br><br>subject.alert = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify an alternate email subject for an alert.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to SplunkAlert-&lt;savedsearchname&gt;. &nbsp;<br><br>subject.report = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify an alternate email subject for a scheduled report.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to SplunkReport-&lt;savedsearchname&gt;. <br><br>useNSSubject = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to use the namespaced subject (i.e subject.report)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* or subject.<br><br>footer.text = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify an alternate email footer.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to If you believe you've received this email in error, please see your <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunk administrator.\r\n\r\nsplunk &gt; the engine for machine data.<br><br>format = [table|raw|csv]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify the format of inline results in the email.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Acceptable values: &nbsp;table, raw, and csv.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Previously accepted values plain and html are no longer respected<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* and equate to table.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* All emails are sent as HTML messages with an alternative plain text version.<br><br>include.results_link = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to include a link to the results.<br><br>include.search = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to include the search that cause <br>&nbsp;&nbsp;&nbsp;&nbsp;* an email to be sent.<br><br>include.trigger = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to show the trigger condition that <br>&nbsp;&nbsp;&nbsp;&nbsp;* caused the alert to fire. <br><br>include.trigger_time = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to show the time that the alert<br>&nbsp;&nbsp;&nbsp;&nbsp;* was fired. <br><br>include.view_link = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to show the title and a link to <br>&nbsp;&nbsp;&nbsp;&nbsp;* enable the user to edit the saved search. <br><br>sendresults = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the search results are included in the email. The <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results can be attached or inline, see inline (action.email.inline)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>inline = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the search results are contained in the body of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the alert email.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br>priority = [1|2|3|4|5]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set the priority of the email as it appears in the email client.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Value mapping: 1 to highest, 2 to high, 3 to normal, 4 to low, 5 to lowest.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 3.<br><br>mailserver = &lt;host&gt;[:&lt;port&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You must have a Simple Mail Transfer Protocol (SMTP) server available<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to send email. This is not included with Splunk. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The SMTP mail server to use when sending emails.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &lt;host&gt; can be either the hostname or the IP address.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optionally, specify the SMTP &lt;port&gt; that Splunk should connect to.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When the "use_ssl" attribute (see below) is set to 1 (true), you<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;must specify both &lt;host&gt; and &lt;port&gt;.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Example: "example.com:465")<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $LOCALHOST:25.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>use_ssl &nbsp;&nbsp;&nbsp;= [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Whether to use SSL when communicating with the SMTP server.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When set to 1 (true), you must also specify both the server name or<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IP address and the TCP port in the "mailserver" attribute.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>use_tls &nbsp;&nbsp;&nbsp;= [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to use TLS (transport layer security) when <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;communicating with the SMTP server (starttls)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br><br>auth_username &nbsp;&nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The username to use when authenticating with the SMTP server. If this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is not defined or is set to an empty string, no authentication is<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attempted.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE: your SMTP server might reject unauthenticated emails.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty string.<br><br>auth_password &nbsp;&nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The password to use when authenticating with the SMTP server. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normally this value will be set when editing the email settings,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;however you can set a clear text password here and it will be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;encrypted on the next Splunk restart.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty string.<br><br>sendpdf = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to create and send the results as a PDF.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br><br>sendcsv = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to create and send the results as a csv file.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br><br>pdfview = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Name of view to send as a PDF<br><br>reportServerEnabled = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the PDF server is enabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (false).<br><br>reportServerURL = &lt;url&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The URL of the PDF report server, if one is set up and available <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on the network.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For a default locally installed report server, the URL <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is http://localhost:8091/<br><br>reportPaperSize = [letter|legal|ledger|a2|a3|a4|a5]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default paper size for PDFs<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Acceptable values: letter, legal, ledger, a2, a3, a4, a5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "letter".<br><br>reportPaperOrientation = [portrait|landscape]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Paper orientation: portrait or landscape<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "portrait".<br><br>reportIncludeSplunkLogo = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to include a Splunk logo in Integrated PDF Rendering<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1 (true)<br><br>reportCIDFontList = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify the set (and load order) of CID fonts for handling<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Simplified Chinese(gb), Traditional Chinese(cns), <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Japanese(jp), and Korean(kor) in Integrated PDF Rendering.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify in space separated list<br>&nbsp;&nbsp;&nbsp;&nbsp;* If multiple fonts provide a glyph for a given character code, the glyph<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from the first font specified in the list will be used<br>&nbsp;&nbsp;&nbsp;&nbsp;* To skip loading any CID fonts, specify the empty string<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "gb cns jp kor"<br><br>width_sort_columns = &lt;bool&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Whether columns should be sorted from least wide to most wide left to right.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Valid only if format=text<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true<br><br>preprocess_results = &lt;search-string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Supply a search string to Splunk to preprocess results before<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;emailing them. Usually the preprocessing consists of filtering<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;out unwanted internal fields.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty string (no preprocessing)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>################################################################################<br># RSS: these settings are prefaced by the [rss] stanza<br>################################################################################<br><br>[rss]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set RSS notification options under this stanza name.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute/value pairs. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify an entry for each attribute, Splunk will<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use the default value.<br><br>items_count = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Number of saved RSS feeds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Cannot be more than maxresults (in the global settings).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 30.<br><br>################################################################################<br># script: Used to configure any scripts that the alert triggers.<br>################################################################################<br>[script]<br>filename = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The filename, with no path, of the script to trigger.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The script should be located in: $SPLUNK_HOME/bin/scripts/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For system shell scripts on Unix, or .bat or .cmd on windows, there<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are no further requirements.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For other types of scripts, the first line should begin with a #!<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;marker, followed by a path to the interpreter that will run the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;script.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Example: #!C:\Python27\python.exe<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty string.<br><br>################################################################################<br># summary_index: these settings are prefaced by the [summary_index] stanza<br>################################################################################<br>[summary_index]<br>inline = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether the summary index search command will run as part <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of the scheduled search or as a follow-on action. This is useful <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when the results of the scheduled search are expected to be large.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1 (true).<br><br>_name = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The name of the summary index where Splunk will write the events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "summary".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>################################################################################<br># populate_lookup: these settings are prefaced by the [populate_lookup] stanza<br>################################################################################<br>[populate_lookup]<br>dest = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* the name of the lookup table to populate (stanza name in <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transforms.conf) or the lookup file path to where you want the <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data written. If a path is specified it MUST be relative to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$SPLUNK_HOME and a valid lookups directory.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For example: "etc/system/lookups/&lt;file-name&gt;" or <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"etc/apps/&lt;app&gt;/lookups/&lt;file-name&gt;"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The user executing this action MUST have write permissions <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to the app for this action to work properly.<br><br></font></code>
<h3> <a name="alertactionsconf_alert_actions.conf.example"><span class="mw-headline" id="alert_actions.conf.example">alert_actions.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This is an example alert_actions.conf. &nbsp;Use this file to configure alert actions for saved searches.<br>#<br># To use one or more of these configurations, copy the configuration block into alert_actions.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[email]<br># keep the search artifacts around for 24 hours<br>ttl = 86400<br><br># if no @ is found in the address the hostname of the current machine is appended<br>from = splunk<br><br>format = html<br><br>reportServerURL = http://localhost:8091/<br><br>inline = false<br><br>sendresults = true<br><br>hostname = CanAccessFromTheWorld.com<br><br>command = sendemail "to=$action.email.to$" "server=$action.email.mailserver{default=localhost}$" "from=$action.email.from{default=splunk@localhost}$" "subject=$action.email.subject{recurse=yes}$" "format=$action.email.format{default=csv}$" "sssummary=Saved Search [$name$]: $counttype$($results.count$)" "sslink=$results.url$" "ssquery=$search$" "ssname=$name$" "inline=$action.email.inline{default=False}$" "sendresults=$action.email.sendresults{default=False}$" "sendpdf=$action.email.sendpdf{default=False}$" "pdfview=$action.email.pdfview$" "searchid=$search_id$" "graceful=$graceful{default=True}$" maxinputs="$maxinputs{default=1000}$" maxtime="$action.email.maxtime{default=5m}$"<br>_validate-1 = action.email.sendresults, validate( is_bool('action.email.sendresults'), "Value of argument 'action.email.sendresults' must be a boolean")<br><br><br>[rss]<br># at most 30 items in the feed<br>items_count=30<br><br># keep the search artifacts around for 24 hours<br>ttl = 86400<br><br>command = createrss "path=$name$.xml" "name=$name$" "link=$results.url$" "descr=Alert trigger: $name$, results.count=$results.count$ " "count=30" "graceful=$graceful{default=1}$" maxtime="$action.rss.maxtime{default=1m}$"<br><br>[summary_index]<br># don't need the artifacts anytime after they're in the summary index<br>ttl = 120<br><br># make sure the following keys are not added to marker (command, ttl, maxresults, _*)<br>command = summaryindex addtime=true index="$action.summary_index._name{required=yes}$" file="$name$_$#random$.stash" name="$name$" marker="$action.summary_index*{format=$KEY=\\\"$VAL\\\", key_regex="action.summary_index.(?!(?:command|maxresults|ttl|(?:_.*))$)(.*)"}$"<br><br></font></code>

<a name="appconf"></a><h2> <a name="appconf_app.conf"><span class="mw-headline" id="app.conf">app.conf</span></a></h2>
<p>The following are the spec and example files for app.conf.
</p>
<h3> <a name="appconf_app.conf.spec"><span class="mw-headline" id="app.conf.spec">app.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file maintains the state of a given app in Splunk Enterprise. It may also be used<br># to customize certain aspects of an app.<br>#<br># There is no global, default app.conf. Instead, an app.conf may exist in each<br># app in Splunk Enterprise.<br>#<br># You must restart Splunk Enterprise to reload manual changes to app.conf.<br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>#<br># Settings for how this app appears in Launcher (and online on Splunkbase)<br>#<br><br>[launcher]<br><br># global setting<br><br>remote_tab = &lt;bool&gt;<br>* Set whether the Launcher interface will connect to splunkbase.splunk.com.<br>* This setting only applies to the Launcher app and should be not set in any other app<br>* Defaults to true.<br><br># per-application settings<br><br>version = &lt;version string&gt;<br>* Version numbers are a number followed by a sequence of dots and numbers.<br>* Version numbers for releases should use three digits.<br>* Pre-release versions can append a single-word suffix like "beta" or "preview."<br>* Pre-release designations should use lower case and no spaces.<br>* Examples:<br>* &nbsp;&nbsp;&nbsp;1.2.0<br>* &nbsp;&nbsp;&nbsp;3.2.1<br>* &nbsp;&nbsp;&nbsp;11.0.34<br>* &nbsp;&nbsp;&nbsp;2.0beta<br>* &nbsp;&nbsp;&nbsp;1.3beta2<br>* &nbsp;&nbsp;&nbsp;1.0preview<br><br>description = &lt;string&gt;<br>* Short explanatory string displayed underneath the app's title in Launcher.<br>* Descriptions should be 200 characters or less because most users won't read long descriptions!<br><br>author = &lt;name&gt;<br>* For apps you intend to post to Splunkbase, enter the username of your splunk.com account.<br>* For internal-use-only apps, include your full name and/or contact info (e.g. email).<br><br># Your app can include an icon which will show up next to your app<br># in Launcher and on Splunkbase. You can also include a screenshot,<br># which will show up on Splunkbase when the user views info about your<br># app before downloading it. Icons are recommended, although not required.<br># Screenshots are optional.<br>#<br># There is no setting in app.conf for these images. Instead, icon and<br># screenshot images should be placed in the appserver/static dir of<br># your app. They will automatically be detected by Launcher and Splunkbase.<br>#<br># For example:<br>#<br># &nbsp;&nbsp;&nbsp;&nbsp;&lt;app_directory&gt;/appserver/static/appIcon.png &nbsp;&nbsp;&nbsp;(the capital "I" is required!)<br># &nbsp;&nbsp;&nbsp;&nbsp;&lt;app_directory&gt;/appserver/static/screenshot.png<br>#<br># An icon image must be a 36px by 36px PNG file.<br># An app screenshot must be 623px by 350px PNG file.<br><br><br>#<br># [package] defines upgrade-related metadata, and will be<br># used in future versions of Splunk Enterprise to streamline app upgrades.<br>#<br><br>[package]<br><br>id = &lt;appid&gt;<br>* id should be omitted for internal-use-only apps which are not intended<br>&nbsp;&nbsp;&nbsp;&nbsp;to be uploaded to Splunkbase<br>* id is required for all new apps uploaded to Splunkbase. Future versions<br>&nbsp;&nbsp;&nbsp;&nbsp;of Splunk Enterprise will use appid to correlate locally-installed apps and<br>&nbsp;&nbsp;&nbsp;&nbsp;the same app on Splunkbase (e.g. to notify users about app updates)<br>* id must be the same as the folder name in which your app lives in $SPLUNK_HOME/etc/apps<br>* id must adhere to cross-platform folder-name restrictions:<br>&nbsp;&nbsp;- must contain only letters, numbers, "." (dot), and "_" (underscore) characters<br>&nbsp;&nbsp;- must not end with a dot character<br>&nbsp;&nbsp;- must not be any of the following names: CON, PRN, AUX, NUL,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, LPT9<br><br>check_for_updates = &lt;bool&gt;<br>* Set whether Splunk Enterprise should check Splunkbase for updates to this app.<br>* Defaults to true.<br><br><br>#<br># Set install settings for this app<br>#<br><br>[install]<br><br>state = disabled | enabled<br>* Set whether app is disabled or enabled.<br>* If an app is disabled, its configs are ignored.<br>* Defaults to enabled.<br><br>state_change_requires_restart = true | false<br>* Set whether changing an app's state ALWAYS requires a restart of Splunk Enterprise.<br>* State changes include enabling or disabling an app.<br>* When set to true, changing an app's state always requires a restart.<br>* When set to false, modifying an app's state may or may not require a restart<br>&nbsp;&nbsp;depending on what the app contains. This setting cannot be used to avoid all<br>&nbsp;&nbsp;restart requirements!<br>* Defaults to false.<br><br>is_configured = true | false<br>* Stores indication of whether the application's custom setup has been performed<br>* Defaults to false<br><br>build = &lt;integer&gt;<br>* Required.<br>* Must be a positive integer.<br>* Increment this whenever you change files in appserver/static.<br>* Every release must change both "version" and "build" settings.<br>* Ensures browsers don't use cached copies of old static files<br>&nbsp;&nbsp;in new versions of your app.<br>* Build is a single integer, unlike version which can be a complex string<br>&nbsp;&nbsp;like 1.5.18.<br><br>allows_disable = true | false<br>* Set whether an app allows itself to be disabled.<br>* Defaults to true.<br><br>install_source_checksum = &lt;string&gt;<br>* Optional.<br>* Records a checksum of the tarball from which a given app was installed.<br>* Splunk will automatically populate this value upon install; there is no need<br>&nbsp;&nbsp;to set it explicitly within your app.<br><br>#<br># Handle reloading of custom .conf files (4.2+ versions only)<br>#<br><br>[triggers]<br><br>reload.&lt;conf_file_name&gt; = [ simple | rest_endpoints | access_endpoints &lt;handler_url&gt; ]<br>* Splunk Enterprise will reload app configuration after every <br>&nbsp;&nbsp;app-state change: install, update, enable, and disable.<br>* If your app does not use a custom config file (e.g. myconffile.conf) <br>&nbsp;&nbsp;then it won't need a [triggers] stanza, because <br>&nbsp;&nbsp;$SPLUNK_HOME/etc/system/default/app.conf already includes a [triggers]<br>&nbsp;&nbsp;stanza which automatically reloads config files normally used by Splunk Enterprise.<br>* If your app uses a custom config file (e.g. myconffile.conf) and you want to <br>&nbsp;&nbsp;avoid unnecessary Splunk Enterprise restarts, you'll need to add a reload value in<br>&nbsp;&nbsp;the [triggers] stanza.<br>* If you don't include [triggers] settings and your app uses a custom <br>&nbsp;&nbsp;config file, a Splunk Enterprise restart will be required after every state change.<br>* Specifying "simple" implies that Splunk Enterprise will take no special action to reload <br>&nbsp;&nbsp;your custom conf file.<br>* Specify "access_endpoints" and a URL to a REST endpoint, and Splunk Enterprise will call<br>&nbsp;&nbsp;its _reload() method at every app state change.<br>* "rest_endpoints" is reserved for Splunk Enterprise internal use for reloading <br>&nbsp;&nbsp;restmap.conf.<br><br>* Examples: <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[triggers]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Do not force a restart of Splunk Enterprise for state changes of MyApp<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Do not run special code to tell MyApp to reload myconffile.conf<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Apps with custom config files will usually pick this option<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reload.myconffile = simple<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Do not force a restart of Splunk Enterprise for state changes of MyApp.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Splunk Enterprise calls the /admin/myendpoint/_reload method<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# in my custom EAI handler.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Use this advanced option only if MyApp requires custom code to reload <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# its configuration when its state changes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reload.myotherconffile = access_endpoints /admin/myendpoint &nbsp;<br><br>#<br># Set UI-specific settings for this app<br>#<br><br>[ui]<br><br>is_visible = true | false<br>* Indicates if this app should be visible/navigable as a UI app<br>* Apps require at least 1 view to be available from the UI<br><br>is_manageable = true | false<br>* This setting is deprecated. It no longer has any effect.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>label = &lt;string&gt;<br>* Defines the name of the app shown in the Splunk GUI and Launcher<br>* Recommended length between 5 and 80 characters.<br>* Must not include "Splunk For" prefix.<br>* Label is required.<br>* Examples of good labels:<br>&nbsp;&nbsp;&nbsp;&nbsp;IMAP Monitor<br>&nbsp;&nbsp;&nbsp;&nbsp;SQL Server Integration Services<br>&nbsp;&nbsp;&nbsp;&nbsp;FISMA Compliance<br>&nbsp;&nbsp;<br>docs_section_override = &lt;string&gt;<br>* Defines override for auto-generated app-specific documentation links<br>* If not specified, app-specific documentation link will include [&lt;app-name&gt;:&lt;app-version&gt;]<br>* If specified, app-specific documentation link will include [&lt;docs_section_override&gt;]<br>* This only applies to apps with documentation on the Splunk documentation site<br><br>#<br># Credential-verification scripting (4.2+ versions only)<br>#<br><br>[credentials_settings]<br>verify_script = &lt;string&gt;<br>&nbsp;* Optional setting.<br>&nbsp;* Command line to invoke to verify credentials used for this app.<br>&nbsp;* For scripts, the command line should include both the interpreter and the script for it to run.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example: "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/etc/apps/&lt;myapp&gt;/bin/$MY_SCRIPT"<br>&nbsp;* The invoked program is communicated with over standard in / standard out via the same protocol as splunk scripted auth.<br>&nbsp;* Paths incorporating variable expansion or explicit spaces must be quoted.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example, a path including $SPLUNK_HOME should be quoted, as likely will expand to C:\Program Files\Splunk<br><br>[credential:&lt;realm&gt;:&lt;username&gt;]<br>password = &lt;string&gt;<br>* Password that corresponds to the given username for the given realm. Note that realm is optional<br>* The password can be in clear text, however when saved from splunkd the password will always be encrypted<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br><br></font></code>
<h3> <a name="appconf_app.conf.example"><span class="mw-headline" id="app.conf.example">app.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following are example app.conf configurations. Configure properties for your custom application.<br>#<br># There is NO DEFAULT app.conf.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># app.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[launcher]<br>author=&lt;author of app&gt;<br>description=&lt;textual description of app&gt;<br>version=&lt;version of app&gt;<br><br><br></font></code>

<a name="auditconf"></a><h2> <a name="auditconf_audit.conf"><span class="mw-headline" id="audit.conf">audit.conf</span></a></h2>
<p>The following are the spec and example files for audit.conf.
</p>
<h3> <a name="auditconf_audit.conf.spec"><span class="mw-headline" id="audit.conf.spec">audit.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values you can use to configure auditing<br># and event signing in audit.conf.<br>#<br># There is NO DEFAULT audit.conf. To set custom configurations, place an audit.conf in<br># $SPLUNK_HOME/etc/system/local/. For examples, see audit.conf.example. &nbsp;You must restart <br># Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#########################################################################################<br># EVENT HASHING: turn on SHA256 event hashing.<br>#########################################################################################<br><br>[eventHashing]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This stanza turns on event hashing -- every event is SHA256 hashed. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The indexer will encrypt all the signatures in a block.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following attribute/value pairs.<br><br><br>filters=mywhitelist,myblacklist...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* (Optional) Filter which events are hashed.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify filtername values to apply to events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: The order of precedence is left to right. Two special filters are provided by default:<br>blacklist_all and whitelist_all, use them to terminate the list of your filters. For example<br>if your list contains only whitelists, then terminating it with blacklist_all will result in <br>signing of only events that match any of the whitelists. The default implicit filter list <br>terminator is whitelist_all.<br>&nbsp;<br># FILTER SPECIFICATIONS FOR EVENT HASHING<br><br>[filterSpec:&lt;event_whitelist | event_blacklist&gt;:&lt;filtername&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This stanza turns on whitelisting or blacklisting for events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Use filternames in "filters" entry (above).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example [filterSpec:event_whitelist:foofilter].<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>all=[true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The 'all' tag tells the blacklist to stop 'all' events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 'false.'<br><br>source=[string]<br>host=[string]<br>sourcetype=[string]<br># Optional list of blacklisted/whitelisted sources, hosts or sourcetypes (in order from left to right). <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Exact matches only, no wildcarded strings supported.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;source=s1,s2,s3...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host=h1,h2,h3...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sourcetype=st1,st2,st3...<br><br><br>#########################################################################################<br># KEYS: specify your public and private keys for encryption.<br>#########################################################################################<br><br>[auditTrail]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This stanza turns on cryptographic signing for audit trail events (set in inputs.conf) <br>and hashed events (if event hashing is enabled above).<br><br>privateKey=/some/path/to/your/private/key/private_key.pem<br>publicKey=/some/path/to/your/public/key/public_key.pem<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You must have a private key to encrypt the signatures and a public key to decrypt them.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set a path to your own keys<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Generate your own keys using openssl in $SPLUNK_HOME/bin/.<br><br>queueing=[true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Turn off sending audit events to the indexQueue -- tail the audit events instead.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this is set to 'false', you MUST add an inputs.conf stanza to tail<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the audit log in order to have the events reach your index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br></font></code>
<h3> <a name="auditconf_audit.conf.example"><span class="mw-headline" id="audit.conf.example">audit.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This is an example audit.conf. &nbsp;Use this file to configure auditing and event hashing.<br>#<br># There is NO DEFAULT audit.conf.<br>#<br># To use one or more of these configurations, copy the configuration block into audit.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br><br>[auditTrail]<br>privateKey=/some/path/to/your/private/key/private_key.pem<br>publicKey=/some/path/to/your/public/key/public_key.pem<br><br># If this stanza exists, audit trail events will be cryptographically signed.<br># You must have a private key to encrypt the signatures and a public key to decrypt them.<br># Generate your own keys using openssl in $SPLUNK_HOME/bin/.<br><br><br><br># EXAMPLE #1 - hash all events: <br><br>[eventHashing]<br><br># This performs a SHA256 hash on every event other than ones going the _audit index (which are <br># handled their own way).<br># NOTE: All you need to enable hashing is the presence of the stanza 'eventHashing'.<br><br><br><br># EXAMPLE #2 - simple blacklisting<br><br>[filterSpec:event_blacklist:myblacklist]<br>host=somehost.splunk.com, 45.2.4.6, 45.3.5.4<br><br>[eventHashing]<br>filters=myblacklist<br><br># Splunk does NOT hash any events from the hosts listed - they are 'blacklisted'. All other<br># events are hashed.<br><br><br><br># EXAMPLE #3 - multiple blacklisting<br><br>[filterSpec:event_blacklist:myblacklist]<br>host=somehost.splunk.com, 46.45.32.1<br>source=/some/source<br>sourcetype=syslog, apache.error<br><br>[eventHashing]<br>filters=myblacklist<br><br># DO NOT hash all events with the following, sources, sourcetypes and hosts - they are all<br># blacklisted. &nbsp;All other events are hashed.<br><br><br><br># EXAMPLE #4 - whitelisting<br><br>[filterSpec:event_whitelist:mywhitelist]<br>sourcetype=syslog<br>#source=aa, bb &nbsp;(these can be added as well)<br>#host=xx, yy<br><br>[filterSpec:event_blacklist:nothingelse]<br>#The 'all' tag is a special boolean (defaults to false) that says match *all* events<br>all=True<br><br>[eventSigning]<br>filters=mywhitelist, nothingelse<br><br># Hash ONLY those events which are of sourcetype 'syslog'. &nbsp;All other events are NOT hashed.<br># Note that you can have a list of filters and they are executed from left to right for every event.<br># If an event passed a whitelist, the rest of the filters do not execute. &nbsp;Thus placing<br># the whitelist filter before the 'all' blacklist filter says "only hash those events which<br># match the whitelist". <br><br><br></font></code>

<a name="authenticationconf"></a><h2> <a name="authenticationconf_authentication.conf"><span class="mw-headline" id="authentication.conf">authentication.conf</span></a></h2>
<p>The following are the spec and example files for authentication.conf.
</p>
<h3> <a name="authenticationconf_authentication.conf.spec"><span class="mw-headline" id="authentication.conf.spec">authentication.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values for configuring authentication via <br># authentication.conf.<br>#<br># There is an authentication.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an authentication.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># authentication.conf.example. &nbsp;You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[authentication]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following attribute/value pairs.<br><br>authType = [Splunk|LDAP|Scripted]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specify which authentication system to use.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Supported values: Splunk, LDAP, Scripted.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to Splunk.<br><br>authSettings = &lt;authSettings-key&gt;,&lt;authSettings-key&gt;,...<br>&nbsp;&nbsp;&nbsp;&nbsp;* Key to look up the specific configurations of chosen authentication system.<br>&nbsp;&nbsp;&nbsp;&nbsp;* &lt;authSettings-key&gt; is the name of a stanza header that specifies attributes for an LDAP strategy <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or for scripted authentication. Those stanzas are defined below.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For LDAP, specify the LDAP strategy name(s) here. If you want Splunk to query multiple LDAP servers, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enter a comma-separated list of all strategies. Each strategy must be defined in its own stanza. The order in <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which you specify the strategy names will be the order Splunk uses to query their servers when looking for a user.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For scripted authentication, &lt;authSettings-key&gt; should be a single stanza name.<br><br>passwordHashAlgorithm = [SHA512-crypt|SHA256-crypt|SHA512-crypt-&lt;num_rounds&gt;|SHA256-crypt-&lt;num_rounds&gt;|MD5-crypt]<br>&nbsp;&nbsp;&nbsp;&nbsp;* For the default "Splunk" authType, this controls how hashed passwords are stored in the $SPLUNK_HOME/etc/passwd file.<br>&nbsp;&nbsp;&nbsp;&nbsp;* "MD5-crypt" is an algorithm originally developed for FreeBSD in the early 1990's which became a widely used<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;standard among UNIX machines. &nbsp;It was also used by Splunk up through the 5.0.x releases. &nbsp;MD5-crypt runs the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;salted password through a sequence of 1000 MD5 operations.<br>&nbsp;&nbsp;&nbsp;&nbsp;* "SHA256-crypt" and "SHA512-crypt" are newer versions that use 5000 rounds of the SHA256 or SHA512 hash<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functions. &nbsp;This is slower than MD5-crypt and therefore more resistant to dictionary attacks. &nbsp;SHA512-crypt<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is used for system passwords on many versions of Linux.<br>&nbsp;&nbsp;&nbsp;&nbsp;* These SHA-based algorithm can optionally be followed by a number of rounds to use. &nbsp;For example,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SHA512-crypt-10000" will use twice as many rounds of hashing as the default implementation. &nbsp;The<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number of rounds must be at least 1000.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you specify a very large number of rounds (i.e. more than 20x the default value of 5000), splunkd<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;may become unresponsive and connections to splunkd (from splunkweb or CLI) will time out.<br>&nbsp;&nbsp;&nbsp;&nbsp;* This setting only affects new password settings (either when a user is added or a user's password<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is changed) &nbsp;Existing passwords will continue to work but retain their previous hashing algorithm.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The default is "SHA512-crypt".<br><br>#####################<br># LDAP settings<br>#####################<br><br>[&lt;authSettings-key&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with the attribute/value pairs listed below.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For multiple strategies, you will need to specify multiple instances of this stanza, each with its<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;own stanza name and a separate set of attributes.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;authSettings-key&gt; must be one of the values listed in the authSettings attribute, specified above <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in the [authentication] stanza.<br><br>host = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the hostname of LDAP server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Be sure that your Splunk server can resolve the host name.<br><br>SSLEnabled = [0|1]<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL <br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to disabled (0)<br>&nbsp;&nbsp;&nbsp;&nbsp;* See the file $SPLUNK_HOME/etc/openldap/openldap.conf for SSL LDAP settings<br><br>port = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the port that Splunk should use to connect to your LDAP server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to port 389 for non-SSL and port 636 for SSL<br><br>bindDN = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL, leave this blank to retrieve your LDAP entries using anonymous bind (must be supported by the LDAP server)<br>&nbsp;&nbsp;&nbsp;&nbsp;* Distinguished name of the user that will be retrieving the LDAP entries<br>&nbsp;&nbsp;&nbsp;&nbsp;* This user must have read access to all LDAP users and groups you wish to use in Splunk.<br><br>bindDNpassword = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL, leave this blank if anonymous bind is sufficient<br>&nbsp;&nbsp;&nbsp;&nbsp;* Password for the bindDN user.<br><br>userBaseDN = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the distinguished names of LDAP entries whose subtrees contain the users<br>&nbsp;&nbsp;&nbsp;&nbsp;* Enter a ';' delimited list to search multiple trees.<br><br>userBaseFilter = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the LDAP search filter you wish to use when searching for users.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Highly recommended, especially when there are many entries in your LDAP user subtrees<br>&nbsp;&nbsp;&nbsp;&nbsp;* When used properly, search filters can significantly speed up LDAP queries<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example that matches users in the IT or HR department:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* userBaseFilter = (|(department=IT)(department=HR))<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* See RFC 2254 for more detailed information on search filter syntax<br>&nbsp;&nbsp;&nbsp;&nbsp;* This defaults to no filtering.<br><br>userNameAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED <br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the user entry attribute whose value is the username.<br>&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: This attribute should use case insensitive matching for its values, and the values should not contain whitespace<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Users are case insensitive in Splunk<br>&nbsp;&nbsp;&nbsp;&nbsp;* In Active Directory, this is 'sAMAccountName'<br>&nbsp;&nbsp;&nbsp;&nbsp;* A typical attribute for this is 'uid'<br><br>realNameAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the user entry attribute whose value is their real name (human readable).<br>&nbsp;&nbsp;&nbsp;&nbsp;* A typical attribute for this is 'cn'<br><br>emailAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the user entry attribute whose value is their email address.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 'mail'<br><br>groupMappingAttribute &nbsp;= &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the user entry attribute whose value is used by group entries to declare membership.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Groups are often mapped with user DN, so this defaults to 'dn'<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set this if groups are mapped using a different attribute<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Usually only needed for OpenLDAP servers.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A typical attribute used to map users to groups is 'uid'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, assume a group declares that one of its members is 'splunkuser'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This implies that every user with 'uid' value 'splunkuser' will be mapped to that group<br><br>groupBaseDN = [&lt;string&gt;;&lt;string&gt;;...]<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the distinguished names of LDAP entries whose subtrees contain the groups.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Enter a ';' delimited list to search multiple trees.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If your LDAP environment does not have group entries, there is a configuration that can treat each user as its own group<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set groupBaseDN to the same as userBaseDN, which means you will search for groups in the same place as users<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Next, set the groupMemberAttribute and groupMappingAttribute to the same attribute as userNameAttribute<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This means the entry, when treated as a group, will use the username value as its only member<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For clarity, you should probably also set groupNameAttribute to the same value as userNameAttribute as well<br><br>groupBaseFilter = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* The LDAP search filter Splunk uses when searching for static groups<br>&nbsp;&nbsp;&nbsp;&nbsp;* Like userBaseFilter, this is highly recommended to speed up LDAP queries<br>&nbsp;&nbsp;&nbsp;&nbsp;* See RFC 2254 for more information<br>&nbsp;&nbsp;&nbsp;&nbsp;* This defaults to no filtering<br><br>dynamicGroupFilter = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* The LDAP search filter Splunk uses when searching for dynamic groups<br>&nbsp;&nbsp;&nbsp;&nbsp;* Only configure this if you intend to retrieve dynamic groups on your LDAP server<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example: '(objectclass=groupOfURLs)'<br><br>dynamicMemberAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Only configure this if you intend to retrieve dynamic groups on your LDAP server<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is REQUIRED if you want to retrieve dynamic groups<br>&nbsp;&nbsp;&nbsp;&nbsp;* This attribute contains the LDAP URL needed to retrieve members dynamically<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example: 'memberURL'<br><br>groupNameAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the group entry attribute whose value stores the group name.<br>&nbsp;&nbsp;&nbsp;&nbsp;* A typical attribute for this is 'cn' (common name)<br>&nbsp;&nbsp;&nbsp;&nbsp;* Recall that if you are configuring LDAP to treat user entries as their own group, user entries must have this attribute<br><br>groupMemberAttribute = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is the group entry attribute whose values are the groups members<br>&nbsp;&nbsp;&nbsp;&nbsp;* Typical attributes for this are 'member' and 'memberUid'<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example, consider the groupMappingAttribute example above using groupMemberAttribute 'member'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To declare 'splunkuser' as a group member, its attribute 'member' must have the value 'splunkuser'<br><br>nestedGroups = &lt;bool&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Controls whether Splunk will expand nested groups using the 'memberof' extension.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set to 1 if you have nested groups you want to expand and the 'memberof' extension on your LDAP server.<br><br>charset = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* ONLY set this for an LDAP setup that returns non-UTF-8 encoded data. LDAP is supposed to always return UTF-8 encoded <br>data (See RFC 2251), but some tools incorrectly return other encodings.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Follows the same format as CHARSET in props.conf (see props.conf.spec)<br>&nbsp;&nbsp;&nbsp;&nbsp;* An example value would be "latin-1"<br><br>anonymous_referrals = &lt;bool&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set this to 0 to turn off referral chasing<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set this to 1 to turn on anonymous referral chasing<br>&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: We only chase referrals using anonymous bind. We do NOT support rebinding using credentials.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If you do not need referral support, we recommend setting this to 0<br>&nbsp;&nbsp;&nbsp;&nbsp;* If you wish to make referrals work, set this to 1 and ensure your server allows anonymous searching<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1<br><br>sizelimit = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Limits the amount of entries we request in LDAP search<br>&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: The max entries returned is still subject to the maximum imposed by your LDAP server<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Example: If you set this to 5000 and the server limits it to 1000, you'll still only get 1000 entries back<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1000<br><br>timelimit = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Limits the amount of time in seconds we will wait for an LDAP search request to complete<br>&nbsp;&nbsp;&nbsp;&nbsp;* If your searches finish quickly, you should lower this value from the default<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 15<br><br>network_timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL<br>&nbsp;&nbsp;&nbsp;&nbsp;* Limits the amount of time a socket will poll a connection without activity<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is useful for determining if your LDAP server cannot be reached<br>&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: As a connection could be waiting for search results, this value must be higher than 'timelimit'<br>&nbsp;&nbsp;&nbsp;&nbsp;* Like 'timelimit', if you have a fast connection to your LDAP server, we recommend lowering this value<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 20<br><br>#####################<br># Map roles<br>#####################<br><br>[roleMap_&lt;authSettings-key&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* The mapping of Splunk roles to LDAP groups for the LDAP strategy specified by &lt;authSettings-key&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: this role mapping ONLY applies to the specified strategy.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with several Role-to-Group(s) mappings as defined below.<br><br>&lt;Splunk RoleName&gt; = &lt;LDAP group string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Maps a Splunk role (from authorize.conf) to LDAP groups<br>&nbsp;&nbsp;&nbsp;&nbsp;* This LDAP group list is semicolon delimited (no spaces).<br>&nbsp;&nbsp;&nbsp;&nbsp;* List several of these attribute value pairs to map several Splunk roles to LDAP Groups<br><br>#####################<br># Scripted authentication<br>#####################<br><br>[&lt;authSettings-key&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with the following attribute/value pairs:<br><br>scriptPath = &lt;string&gt; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* REQUIRED<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is the full path to the script, including the path to the program that runs it (python)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example: "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/etc/system/bin/$MY_SCRIPT"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Note: If a path contains spaces, it must be quoted. The example above handles the case where <br>$SPLUNK_HOME contains a space<br><br>scriptSearchFilters = [1|0]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* OPTIONAL - Only set this to 1 to call the script to add search filters.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 disables (default)<br><br>[cacheTiming]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Use these settings to adjust how long Splunk will use the answers returned from script functions before calling them again.<br><br>userLoginTTL = &lt;time range string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Timeout for the userLogin script function.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These return values are cached on a per-user basis.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The default is '0' (no caching)<br><br>getUserInfoTTL = &lt;time range string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Timeout for the getUserInfo script function.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These return values are cached on a per-user basis.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The default is '10s'<br><br>getUsersTTL = &lt;time range string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Timeout for the getUsers script function.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* There is only one global getUsers cache (it is not tied to a specific user).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The default is '10s'<br><br>* All timeouts can be expressed in seconds or as a search-like time range<br>* Examples include '30' (30 seconds), '2mins' (2 minutes), '24h' (24 hours), etc.<br>* You can opt to use no caching for a particular function by setting the value to '0'<br>&nbsp;&nbsp;&nbsp;&nbsp;* Be aware that this can severely hinder performance as a result of heavy script invocation<br>* Choosing the correct values for cache timing involves a tradeoff between new information latency and general performance<br>&nbsp;&nbsp;&nbsp;&nbsp;* High values yield better performance from calling the script less, but introduces a latency in picking up changes<br>&nbsp;&nbsp;&nbsp;&nbsp;* Low values will pick up changes in your external auth system more quickly, but may slow down performance due to increased script invocations<br><br>#####################<br># Settings for Splunk Authentication mode<br>#####################<br><br>[splunk_auth]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Settings for Splunk's internal authentication system.<br><br>minPasswordLength = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the minimum permitted password length in characters when passwords are set or modified.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This setting is optional.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If 0, there is no required minimum. &nbsp;In other words there is no constraint.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Password modification attempts which do not meet this requirement will be explicitly rejected.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (disabled).<br><br></font></code>
<h3> <a name="authenticationconf_authentication.conf.example"><span class="mw-headline" id="authentication.conf.example">authentication.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This is an example authentication.conf. authentication.conf is used to configure LDAP and Scripted<br># authentication in addition to Splunk's native authentication.<br>#<br># To use one of these configurations, copy the configuration block into authentication.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must reload auth in manager or restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>##### Use just Splunk's built-in authentication (default):<br>[authentication]<br>authType = Splunk<br><br><br>##### LDAP examples<br><br>#### Basic LDAP configuration example<br>[authentication]<br>authType = LDAP<br>authSettings = ldaphost<br><br>[ldaphost]<br>host = ldaphost.domain.com<br>port = 389<br>SSLEnabled = 0<br>bindDN = cn=Directory Manager<br>bindDNpassword = password<br>userBaseDN = ou=People,dc=splunk,dc=com<br>userBaseFilter = (objectclass=splunkusers)<br>groupBaseDN = ou=Groups,dc=splunk,dc=com<br>groupBaseFilter = (objectclass=splunkgroups)<br>userNameAttribute = uid<br>realNameAttribute = givenName<br>groupMappingAttribute = dn<br>groupMemberAttribute = uniqueMember<br>groupNameAttribute = cn<br>timelimit = 10<br>network_timeout = 15<br><br># This stanza maps roles you have created in authorize.conf to LDAP Groups<br>[roleMap_ldaphost]<br>admin = SplunkAdmins<br><br>#### Example using the same server as 'ldaphost', but treating each user as their own group<br>[authentication]<br>authType = LDAP<br>authSettings = ldaphost_usergroups<br><br>[ldaphost_usergroups]<br>host = ldaphost.domain.com<br>port = 389<br>SSLEnabled = 0<br>bindDN = cn=Directory Manager<br>bindDNpassword = password<br>userBaseDN = ou=People,dc=splunk,dc=com<br>userBaseFilter = (objectclass=splunkusers)<br>groupBaseDN = ou=People,dc=splunk,dc=com<br>groupBaseFilter = (objectclass=splunkusers)<br>userNameAttribute = uid<br>realNameAttribute = givenName<br>groupMappingAttribute = uid<br>groupMemberAttribute = uid<br>groupNameAttribute = uid<br>timelimit = 10<br>network_timeout = 15<br><br>[roleMap_ldaphost_usergroups]<br>admin = admin_user1;admin_user2;admin_user3;admin_user4<br>power = power_user1;power_user2<br>user = user1;user2;user3<br><br>#### Sample Configuration for Active Directory (AD)<br>[authentication]<br>authSettings = AD<br>authType = LDAP<br><br>[AD]<br>SSLEnabled = 1<br>bindDN = ldap_bind@splunksupport.kom<br>bindDNpassword = ldap_bind_user_password<br>groupBaseDN = CN=Groups,DC=splunksupport,DC=kom<br>groupBaseFilter =<br>groupMappingAttribute = dn<br>groupMemberAttribute = member<br>groupNameAttribute = cn<br>host = ADbogus.splunksupport.kom<br>port = 636<br>realNameAttribute = cn<br>userBaseDN = CN=Users,DC=splunksupport,DC=kom<br>userBaseFilter =<br>userNameAttribute = sAMAccountName<br>timelimit = 15<br>network_timeout = 20<br>anonymous_referrals = 0<br><br>[roleMap_AD]<br>admin = SplunkAdmins<br>power = SplunkPowerUsers<br>user = SplunkUsers<br><br>#### Sample Configuration for Sun LDAP Server<br>[authentication]<br>authSettings = SunLDAP<br>authType = LDAP<br><br>[SunLDAP]<br>SSLEnabled = 0<br>bindDN = cn=Directory Manager<br>bindDNpassword = Directory_Manager_Password<br>groupBaseDN = ou=Groups,dc=splunksupport,dc=com<br>groupBaseFilter =<br>groupMappingAttribute = dn<br>groupMemberAttribute = uniqueMember<br>groupNameAttribute = cn<br>host = ldapbogus.splunksupport.com<br>port = 389<br>realNameAttribute = givenName<br>userBaseDN = ou=People,dc=splunksupport,dc=com<br>userBaseFilter =<br>userNameAttribute = uid<br>timelimit = 5<br>network_timeout = 8<br><br>[roleMap_SunLDAP]<br>admin = SplunkAdmins<br>power = SplunkPowerUsers<br>user = SplunkUsers<br><br>#### Sample Configuration for OpenLDAP<br>[authentication]<br>authSettings = OpenLDAP<br>authType = LDAP<br><br>[OpenLDAP]<br>bindDN = uid=directory_bind,cn=users,dc=osx,dc=company,dc=com<br>bindDNpassword = directory_bind_account_password<br>groupBaseFilter =<br>groupNameAttribute = cn<br>SSLEnabled = 0<br>port = 389<br>userBaseDN = cn=users,dc=osx,dc=company,dc=com<br>host = hostname_OR_IP<br>userBaseFilter =<br>userNameAttribute = uid<br>groupMappingAttribute = uid<br>groupBaseDN = dc=osx,dc=company,dc=com<br>groupMemberAttribute = memberUid<br>realNameAttribute = cn<br>timelimit = 5<br>network_timeout = 8<br>dynamicGroupFilter = (objectclass=groupOfURLs)<br>dynamicMemberAttribute = memberURL<br>nestedGroups = 1<br><br>[roleMap_OpenLDAP]<br>admin = SplunkAdmins<br>power = SplunkPowerUsers<br>user = SplunkUsers<br><br><br>##### Scripted Auth examples<br><br>#### The following example is for RADIUS authentication:<br>[authentication]<br>authType = Scripted<br>authSettings = script<br><br>[script]<br>scriptPath = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/share/splunk/authScriptSamples/radiusScripted.py"<br><br># Cache results for 1 second per call<br>[cacheTiming]<br>userLoginTTL &nbsp;&nbsp;&nbsp;= 1<br>getUserInfoTTL &nbsp;= 1<br>getUsersTTL &nbsp;&nbsp;&nbsp;&nbsp;= 1<br><br><br>#### The following example works with PAM authentication:<br>[authentication]<br>authType = Scripted<br>authSettings = script<br><br>[script]<br>scriptPath = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/share/splunk/authScriptSamples/pamScripted.py"<br><br># Cache results for different times per function<br>[cacheTiming]<br>userLoginTTL &nbsp;&nbsp;&nbsp;= 30s<br>getUserInfoTTL &nbsp;= 1min<br>getUsersTTL &nbsp;&nbsp;&nbsp;&nbsp;= 5mins<br><br></font></code>

<a name="authorizeconf"></a><h2> <a name="authorizeconf_authorize.conf"><span class="mw-headline" id="authorize.conf">authorize.conf</span></a></h2>
<p>The following are the spec and example files for authorize.conf.
</p>
<h3> <a name="authorizeconf_authorize.conf.spec"><span class="mw-headline" id="authorize.conf.spec">authorize.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for creating roles in authorize.conf. &nbsp;<br># You can configure roles and granular access controls by creating your own authorize.conf.<br><br># There is an authorize.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an authorize.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># authorize.conf.example. &nbsp;You must restart Splunk to enable configurations.<br># <br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[default]<br>srchFilterSelecting = &lt;boolean&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Determine's whether roles' search filters will be used for selecting or eliminating during role inheritance.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Selecting will OR search filters when combining the filters.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Eliminating will AND search filter when combining the filters.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* All roles will default to true (in other words, selecting).<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example:<br>&nbsp;&nbsp;&nbsp;&nbsp;* role1 srchFilter = sourcetype!=ex1 with selecting=true<br>&nbsp;&nbsp;&nbsp;&nbsp;* role2 srchFilter = sourcetype=ex2 with selecting = false<br>&nbsp;&nbsp;&nbsp;&nbsp;* role3 srchFilter = sourcetype!=ex3 AND index=main with selecting = true<br>&nbsp;&nbsp;&nbsp;&nbsp;* role3 inherits from role2 and role 2 inherits from role1<br>&nbsp;&nbsp;&nbsp;&nbsp;* Resulting srchFilter = ((sourcetype!=ex1) OR (sourcetype!=ex3 AND index=main)) AND ((sourcetype=ex2))<br><br>[capability::&lt;capability&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* DO NOT edit, remove, or add capability stanzas. The existing capabilities are the full set of Splunk system capabilities.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunk adds all of its capabilities this way<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For the default list of capabilities and assignments, see authorize.conf under the 'default' directory<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Descriptions of specific capabilities are listed below.<br><br>[role_&lt;roleName&gt;]<br>&lt;capability&gt; = &lt;enabled&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A capability that is enabled for this role.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can list many of these.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Note that 'enabled' is the only accepted value here, as capabilities are disabled by default.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Roles inherit all capabilities from imported roles, and inherited capabilities cannot be disabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Role names cannot have uppercase characters. User names, however, are case-insensitive.<br><br>importRoles = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Semicolon delimited list of other roles and their associated capabilities that should be imported.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Importing other roles also imports the other aspects of that role, such as allowed indexes to search.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default a role imports no other roles.<br><br>grantableRoles = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Semicolon delimited list of roles that can be granted when edit_user capability is present.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default, a role with edit_user capability can create/edit a user and assign any role to them. But when<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grantableRoles is present, the roles that can be assigned will be restricted to the ones provided.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For a role that has no edit_user capability, grantableRoles has no effect.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to not present.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Example: grantableRoles = role1;role2;role3<br><br>srchFilter = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Semicolon delimited list of search filters for this Role.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default we perform no search filtering.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To override any search filters from imported roles, set this to '*', as the 'admin' role does.<br><br><br>srchTimeWin = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum time span of a search, in seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This time window limit is applied backwards from the latest time specified in a search.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default, searches are not limited to any specific time window.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To override any search time windows from imported roles, set this to '0' (infinite), as the 'admin' role does.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* -1 is a special value that implies no search window has been set for this role<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is equivalent to not setting srchTimeWin at all, which means it can be easily overridden by an imported role<br><br>srchDiskQuota = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum amount of disk space (MB) that can be used by search jobs of a user that belongs to this role<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to '100', for 100 MB.<br><br>srchJobsQuota = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum number of concurrently running historical searches a member of this role can have.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This excludes real-time searches, see rtSrchJobsQuota.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 3.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>rtSrchJobsQuota = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum number of concurrently running real-time searches a member of this role can have.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 6.<br><br>srchMaxTime = &lt;number&gt;&lt;unit&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum amount of time that searches of users from this role will be allowed to run.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Once the search has been ran for this amount of time it will be auto finalized, If the role<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* inherits from other roles, the maximum srchMaxTime value specified in the included roles.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This maximum does not apply to real-time searches.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Examples: 1h, 10m, 2hours, 2h, 2hrs, 100s etc...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 100days<br><br>srchIndexesDefault = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Semicolon delimited list of indexes to search when no index is specified<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These indexes can be wildcarded, with the exception that '*' does not match internal indexes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To match internal indexes, start with '_'. All internal indexes are represented by '_*'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to none, but the UI will automatically populate this with 'main' in manager<br><br>srchIndexesAllowed = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Semicolon delimited list of indexes this role is allowed to search<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follows the same wildcarding semantics as srchIndexesDefault<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to none, but the UI will automatically populate this with '*' in manager<br><br>cumulativeSrchJobsQuota = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum number of concurrently running historical searches that all members of this role can have.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: if a user belongs to multiple roles then s/he will first consume searches from the roles with <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the largest cumulative search quota, when the quota of a role is completely used up then roles<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with lower quotas will be examined. <br><br>cumulativeRTSrchJobsQuota = &lt;number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum number of concurrently running real-time searches that all members of this role can have.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: if a user belongs to multiple roles then s/he will first consume searches from the roles with <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the largest cumulative search quota, when the quota of a role is completely used up then roles<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with lower quotas will be examined. <br><br><br>### Descriptions of Splunk system capabilities<br>[capability::accelerate_datamodel]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to accelerate a datamodel.<br><br>[capability::admin_all_objects]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A role with this capability has access to objects in the system (user objects, search jobs, etc.)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This bypasses any ACL restrictions (similar to root access in a *nix environment)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* We check this capability when accessing manager pages and objects<br><br>[capability::change_authentication]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change authentication settings through the various authentication endpoints.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Also controls whether authentication can be reloaded<br><br>[capability::change_own_password]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory. Some auth systems prefer to have passwords be immutable for some users.<br><br>[capability::delete_by_keyword]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to use the 'delete' search operator. Note that this does not actually delete the raw data on disk.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Delete merely masks the data (via the index) from showing up in search results.<br><br>[capability::edit_deployment_client]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory. The deployment client admin endpoint requires this cap for edit.<br><br>[capability::list_deployment_client]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory.<br><br>[capability::edit_deployment_server]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory. The deployment server admin endpoint requires this cap for edit.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change/create remote inputs that get pushed to the forwarders.<br><br>[capability::list_deployment_server]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory.<br><br>[capability::edit_dist_peer]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to add and edit peers for distributed search.<br><br>[capability::edit_forwarders]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit settings for forwarding data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Used by TCP and Syslog output admin handlers<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Includes settings for SSL, backoff schemes, etc.<br><br>[capability::edit_httpauths]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit and end user sessions through the httpauth-tokens endpoint<br><br>[capability::edit_input_defaults]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change the default hostname for input data in the server settings endpoint.<br><br>[capability::edit_monitor]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to add inputs and edit settings for monitoring files.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Used by the standard inputs endpoint as well as the one-shot input endpoint.<br><br>[capability::edit_roles]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit roles as well as change the mappings from users to roles.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Used by both the users and roles endpoint.<br><br>[capability::edit_scripted]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to create and edit scripted inputs.<br><br>[capability::edit_search_server]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit general distributed search settings like timeouts, heartbeats, and blacklists<br><br>[capability::edit_server]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit general server settings such as the server name, log levels, etc.<br><br>[capability::edit_search_head_clustering]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit and manage search head clustering.<br><br>[capability::edit_splunktcp]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change settings for receiving TCP input from another Splunk instance.<br><br>[capability::edit_splunktcp_ssl]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to list or edit any SSL specific settings for Splunk TCP input.<br><br>[capability::edit_tcp]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change settings for receiving general TCP inputs.<br><br>[capability::edit_udp]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change settings for UDP inputs.<br><br>[capability::edit_user]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to create, edit, or remove users.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Note that Splunk users may edit certain aspects of their information without this capability.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Also required to manage certificates for distributed search.<br><br>[capability::edit_view_html]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to create, edit, or otherwise modify HTML-based views.<br><br>[capability::edit_web_settings]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change the settings for web.conf through the system settings endpoint.<br><br>[capability::get_diag]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to use the /streams/diag endpoint to get remote diag from an instance<br><br>[capability::get_metadata]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to use the 'metadata' search processor.<br><br>[capability::get_typeahead]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required for typeahead. This includes the typeahead endpoint and the 'typeahead' search processor.<br><br>[capability::input_file]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required for inputcsv (except for dispatch=t mode) and inputlookup<br><br>[capability::indexes_edit]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to change any index settings like file size and memory limits.<br><br>[capability::license_tab]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to access and change the license.<br><br>[capability::list_forwarders]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to show settings for forwarding data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Used by TCP and Syslog output admin handlers.<br><br>[capability::list_httpauths]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to list user sessions through the httpauth-tokens endpoint.<br><br>[capability::list_inputs]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to view the list of various inputs.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This includes input from files, TCP, UDP, Scripts, etc.<br><br>[capability::list_search_head_clustering]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to list search head clustering objects like artifacts, delegated jobs, members, captain, etc.<br><br>[capability::output_file]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required for outputcsv (except for dispatch=t mode) and outputlookup<br><br>[capability::request_remote_tok]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to get a remote authentication token.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Used for distributing search to old 4.0.x Splunk instances.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Also used for some distributed peer management and bundle replication.<br><br>[capability::rest_apps_management]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit settings for entries and categories in the python remote apps handler.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* See restmap.conf for more information<br><br>[capability::rest_apps_view]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to list various properties in the python remote apps handler.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* See restmap.conf for more info<br><br>[capability::rest_properties_get]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to get information from the services/properties endpoint.<br><br>[capability::rest_properties_set]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to edit the services/properties endpoint.<br><br>[capability::restart_splunkd]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to restart Splunk through the server control handler.<br><br>[capability::rtsearch]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to run a realtime search.<br><br>[capability::run_debug_commands]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to run debugging commands like 'summarize'<br><br>[capability::schedule_search]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to schedule saved searches.<br><br>[capability::schedule_rtsearch]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to schedule real time saved searches. Note that scheduled_search capability is also required to be enabled<br><br>[capability::search]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Self explanatory - required to run a search.<br><br>[capability::use_file_operator]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to use the 'file' search operator.<br><br>[capability::accelerate_search]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required to save an accelerated search<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* All users have this capability by default<br><br></font></code>
<h3> <a name="authorizeconf_authorize.conf.example"><span class="mw-headline" id="authorize.conf.example">authorize.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This is an example authorize.conf. &nbsp;Use this file to configure roles and capabilities.<br>#<br># To use one or more of these configurations, copy the configuration block into authorize.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must reload auth or restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[role_ninja]<br>rtsearch = enabled<br>importRoles = user<br>srchFilter = host=foo<br>srchIndexesAllowed = *<br>srchIndexesDefault = mail;main<br>srchJobsQuota &nbsp;&nbsp;= 8<br>rtSrchJobsQuota = 8<br>srchDiskQuota &nbsp;&nbsp;= 500<br><br># This creates the role 'ninja', which inherits capabilities from the 'user' role.<br># ninja has almost the same capabilities as power, except cannot schedule searches.<br># The search filter limits ninja to searching on host=foo.<br># ninja is allowed to search all public indexes (those that do not start with underscore), and will<br># search the indexes mail and main if no index is specified in the search.<br># ninja is allowed to run 8 search jobs and 8 real time search jobs concurrently (these counts are independent).<br># ninja is allowed to take up 500 megabytes total on disk for all their jobs.<br><br></font></code>

<a name="collectionsconf"></a><h2> <a name="collectionsconf_collections.conf"><span class="mw-headline" id="collections.conf">collections.conf</span></a></h2>
<p>The following are the spec and example files for collections.conf.
</p>
<h3> <a name="collectionsconf_collections.conf.spec"><span class="mw-headline" id="collections.conf.spec">collections.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file configures the KV Store collections for a given app in Splunk. <br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br>[&lt;collection-name&gt;]<br><br>enforceTypes = true|false<br>* Indicates whether to enforce data types when inserting data into the collection.<br>* When set to true, invalid insert operations fail.<br>* When set to false, invalid insert operations drop only the invalid field.<br>* Defaults to false.<br><br>field.&lt;name&gt; = number|bool|string|time<br>* Field type for a field called &lt;name&gt;.<br>* If the data type is not provided, it is inferred from the provided JSON data type.<br><br>accelerated_fields.&lt;name&gt; = &lt;json&gt;<br>* Acceleration definition for an acceleration called &lt;name&gt;.<br>* Must be a valid JSON document (invalid JSON is ignored).<br>* Example: 'acceleration.foo={"a":1, "b":-1}' is a compound acceleration that first <br>&nbsp;&nbsp;sorts 'a' in ascending order and then 'b' in descending order.<br>* If multiple accelerations with the same definition are in the same collection, <br>&nbsp;&nbsp;the duplicates are skipped.<br>* If the data within a field is too large for acceleration, you will see a warning<br>&nbsp;&nbsp;when you try to create an accelerated field and the acceleration will not be created.<br>* An acceleration is always created on the _key.<br>* The order of accelerations is important. For example, an acceleration of { "a":1, "b":1 } <br>&nbsp;&nbsp;speeds queries on "a" and "a" + "b", but not on "b" alone.<br>* Multiple separate accelerations also speed up queries. For example, separate accelerations <br>&nbsp;&nbsp;{ "a":1 } and { "b": 1 } will speed up queries on "a" + "b", but not as well as <br>&nbsp;&nbsp;a combined acceleration { "a":1, "b":1 }.<br>* Defaults to nothing (no acceleration).<br><br>profilingEnabled = true|false<br>* Indicates whether to enable logging of slow-running operations, as defined in 'profilingThresholdMs'.<br>* Defaults to false.<br><br>profilingThresholdMs = &lt;zero or positive integer&gt;<br>* The threshold for logging a slow-running operation, in milliseconds. <br>* When set to 0, all operations are logged. <br>* This setting is only used when 'profilingEnabled' is true. <br>* This setting impacts the performance of the collection. <br>* Defaults to 100.<br><br></font></code>
<h3> <a name="collectionsconf_collections.conf.example"><span class="mw-headline" id="collections.conf.example">collections.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following is an example collections.conf configuration.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># collections.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[mycollection]<br><br>field.foo = number<br>field.bar = string<br>accelerated_fields.myacceleration = {"foo": 1, "bar": -1}<br><br></font></code>

<a name="commandsconf"></a><h2> <a name="commandsconf_commands.conf"><span class="mw-headline" id="commands.conf">commands.conf</span></a></h2>
<p>The following are the spec and example files for commands.conf.
</p>
<h3> <a name="commandsconf_commands.conf.spec"><span class="mw-headline" id="commands.conf.spec">commands.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for creating search commands for <br># any custom search scripts created. &nbsp;Add your custom search script to $SPLUNK_HOME/etc/searchscripts/<br># or $SPLUNK_HOME/etc/apps/MY_APP/bin/. &nbsp;For the latter, put a custom commands.conf in <br># $SPLUNK_HOME/etc/apps/MY_APP. &nbsp;For the former, put your custom commands.conf <br># in $SPLUNK_HOME/etc/system/local/.<br><br># There is a commands.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;For examples, see <br># commands.conf.example. &nbsp;You must restart Splunk to enable configurations.<br><br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;STANZA_NAME&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Each stanza represents a search command; the command is the stanza name.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The stanza name invokes the command in the search language.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set the following attributes/values for the command. &nbsp;Otherwise, Splunk uses the defaults.<br><br>type = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Type of script: python, perl<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to python.<br><br>filename = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Name of script file for command.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &lt;script-name&gt;.pl for perl.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &lt;script-name&gt;.py for python.<br><br>local = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If true, specifies that the command should be run on the search head only <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br><br>perf_warn_limit = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Issue a performance warning message if more than this many input events are passed to this external command (0 = never)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0 (disabled)<br><br>streaming = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the command is streamable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>maxinputs = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum number of events that can be passed to the command for each invocation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This limit cannot exceed the value of maxresultrows in limits.conf.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 for no limit.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 50000.<br><br>passauth = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to true, passes an authentication token on the start of input.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>run_in_preview = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether to run this command if generating results just for preview rather than final output.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>enableheader = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Indicate whether or not your script is expecting header information or not.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Currently, the only thing in the header information is an auth token.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to true it will expect as input a head section + '\n' then the csv input<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: Should be set to true if you use splunk.Intersplunk<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br>retainsevents = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the command retains events (the way the sort/dedup/cluster commands do) or whether <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it transforms them (the way the stats command does). <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>generating = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether your command generates new events. If no events are passed to the command, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will it generate events?<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>generates_timeorder = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If generating = true, does command generate events in descending time order (latest first)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>overrides_timeorder = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If generating = false and streaming=true, does command change the order of events with respect to time?<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>requires_preop = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the command sequence specified by the 'streaming_preop' key is required for <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;proper execution or is it an optimization only<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is false (streaming_preop not required)<br><br>streaming_preop = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A string that denotes the requested pre-streaming search string.<br><br>required_fields = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A comma separated list of fields that this command may use. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Informs previous commands that they should retain/extract these fields if possible. &nbsp;No error is generated if a field specified is missing.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to '*'<br><br>supports_multivalues = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the command supports multivalues. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If true, multivalues will be treated as python lists of strings, instead of a flat string <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(when using Intersplunk to interpret stdin/stdout).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the list only contains one element, the value of that element will be returned, rather than a list <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(for example, isinstance(val, basestring) == True).<br><br>supports_getinfo = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether the command supports dynamic probing for settings <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(first argument invoked == __GETINFO__ or __EXECUTE__).<br><br>supports_rawargs = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether the command supports raw arguments being passed to it or if it prefers parsed arguments <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(where quotes are stripped).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If unspecified, the default is false<br><br>undo_scheduler_escaping = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether the commands raw arguments need to be unesacped.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is perticularly applies to the commands being invoked by the scheduler.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This applies only if the command supports raw arguments(supports_rawargs).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If unspecified, the default is false<br><br>requires_srinfo = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies if the command requires information stored in SearchResultsInfo. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If true, requires that enableheader be set to true, &nbsp;and the full pathname of the info file (a csv file) <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will be emitted in the header under the key 'infoPath'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If unspecified, the default is false<br><br><br>needs_empty_results = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether or not this search command needs to be called with intermediate empty search results <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If unspecified, the default is true <br><br>changes_colorder = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify whether the script output should be used to change the column ordering of the fields.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is true<br><br>outputheader = &lt;true/false&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to true, output of script should be a header section + blank line + csv output<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If false, script output should be pure csv only<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is false<br><br>clear_required_fields = [true|false]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If true, required_fields represents the *only* fields required. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If false, required_fields are additive to any fields that may be required by subsequent commands.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In most cases, false is appropriate for streaming commands and true for reporting commands<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is false<br><br>stderr_dest = [log|message|none]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What do to with the stderr output from the script<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 'log' means to write the output to the job's search.log. &nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 'message' means to write each line as an search info message. &nbsp;The message level can be set to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adding that level (in ALL CAPS) to the start of the line, e.g. "WARN my warning message." &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 'none' means to discard the stderr output<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to log<br><br></font></code>
<h3> <a name="commandsconf_commands.conf.example"><span class="mw-headline" id="commands.conf.example">commands.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># Configuration for external search commands<br>#<br><br>##############<br># defaults for all external commands, exceptions are below in individual stanzas<br><br># type of script: 'python', 'perl'<br>TYPE = python<br>#default FILENAME would be &lt;stanza-name&gt;.py for python, &lt;stanza-name&gt;.pl for perl and &lt;stanza-name&gt; otherwise<br><br># is command streamable?<br>STREAMING = false<br><br># maximum data that can be passed to command (0 = no limit)<br>MAXINPUTS = 50000<br><br># end defaults<br>#####################<br><br>[crawl]<br>FILENAME = crawl.py<br><br>[createrss]<br>FILENAME = createrss.py<br><br>[diff]<br>FILENAME = diff.py<br><br>[gentimes]<br>FILENAME = gentimes.py<br><br>[head]<br>FILENAME = head.py<br><br>[loglady]<br>FILENAME = loglady.py<br><br>[marklar]<br>FILENAME = marklar.py<br><br>[runshellscript]<br>FILENAME = runshellscript.py<br><br>[sendemail]<br>FILENAME = sendemail.py<br><br>[translate]<br>FILENAME = translate.py<br><br>[transpose]<br>FILENAME = transpose.py<br><br>[uniq]<br>FILENAME = uniq.py<br><br>[windbag]<br>filename = windbag.py<br>supports_multivalues = true<br><br>[xmlkv]<br>FILENAME = xmlkv.py<br><br>[xmlunescape]<br>FILENAME = xmlunescape.py<br><br></font></code>

<a name="crawlconf"></a><h2> <a name="crawlconf_crawl.conf"><span class="mw-headline" id="crawl.conf">crawl.conf</span></a></h2>
<p>The following are the spec and example files for crawl.conf.
</p>
<h3> <a name="crawlconf_crawl.conf.spec"><span class="mw-headline" id="crawl.conf.spec">crawl.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring crawl.<br>#<br># There is a crawl.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a crawl.conf in $SPLUNK_HOME/etc/system/local/. For help, see<br># crawl.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># Set of attribute-values used by crawl. &nbsp;<br># <br># If attribute, ends in _list, the form is:<br>#<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attr = val, val, val, etc.<br>#<br># The space after the comma is necessary, so that "," can be used, as in BAD_FILE_PATTERNS's use of "*,v"<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[default]<br><br>[files]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Sets file crawler-specific attributes under this stanza header.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any of the following attributes.<br><br>root = &lt;semi-colon separate list of directories&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set a list of directories this crawler should search through.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to /;/Library/Logs<br><br>bad_directories_list = &lt;comma-separated list of bad directories&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* List any directories you don't want to crawl.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bin, sbin, boot, mnt, proc, tmp, temp, dev, initrd, help, driver, drivers, share, bak, old, lib, include, doc, docs, man, html, images, tests, js, dtd, org, com, net, class, java, resource, locale, static, testing, src, sys, icons, css, dist, cache, users, system, resources, examples, gdm, manual, spool, lock, kerberos, .thumbnails, libs, old, manuals, splunk, splunkpreview, mail, resources, documentation, applications, library, network, automount, mount, cores, lost\+found, fonts, extensions, components, printers, caches, findlogs, music, volumes, libexec,<br><br>bad_extensions_list = &lt;comma-separated list of file extensions to skip&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* List any file extensions and crawl will skip files that end in those extensions.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0t, a, adb, ads, ali, am, asa, asm, asp, au, bak, bas, bat, bmp, c, cache, cc, cg, cgi, class, clp, com, conf, config, cpp, cs, css, csv, cxx, dat, doc, dot, dvi, dylib, ec, elc, eps, exe, f, f77, f90, for, ftn, gif, h, hh, hlp, hpp, hqx, hs, htm, html, hxx, icns, ico, ics, in, inc, jar, java, jin, jpeg, jpg, js, jsp, kml, la, lai, lhs, lib, license, lo, m, m4, mcp, mid, mp3, mpg, msf, nib, nsmap, o, obj, odt, ogg, old, ook, opt, os, os2, pal, pbm, pdf, pdf, pem, pgm, php, php3, php4, pl, plex, plist, plo, plx, pm, png, po, pod, ppd, ppm, ppt, prc, presets, ps, psd, psym, py, pyc, pyd, pyw, rast, rb, rc, rde, rdf, rdr, res, rgb, ro, rsrc, s, sgml, sh, shtml, so, soap, sql, ss, stg, strings, tcl, tdt, template, tif, tiff, tk, uue, v, vhd, wsdl, xbm, xlb, xls, xlw, xml, xsd, xsl, xslt, jame, d, ac, properties, pid, del, lock, md5, rpm, pp, deb, iso, vim, lng, list<br><br>bad_file_matches_list = &lt;comma-separated list of regex&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Crawl applies the specified regex and skips files that match the patterns.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* There is an implied "$" (end of file name) after each pattern.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*~, *#, *,v, *readme*, *install, (/|^).*, *passwd*, *example*, *makefile, core.*<br><br>packed_extensions_list = &lt;comma-separated list of extensions&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify extensions of compressed files to exclude.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bz, bz2, tbz, tbz2, Z, gz, tgz, tar, zip<br><br>collapse_threshold = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify the minimum number of files a source must have to be considered a directory.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1000.<br><br>days_sizek_pairs_list = &lt;comma-separated hyphenated pairs of integers&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify a comma-separated list of age (days) and size (kb) pairs to constrain what files are crawled. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example: days_sizek_pairs_list = 7-0, 30-1000 tells Splunk to crawl only files last <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;modified within 7 days and at least 0kb in size, or modified within the last 30 days and at least 1000kb in size.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 30-0.<br><br>big_dir_filecount = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Skip directories with files above &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 10000.<br><br>index = &lt;$INDEX&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify index to add crawled files to.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to main.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>max_badfiles_per_dir = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify how far to crawl into a directory for files. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Crawl excludes a directory if it doesn't find valid files within the specified max_badfiles_per_dir.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 100.<br><br><br><br><br>[network]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Sets network crawler-specific attributes under this stanza header.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any of the following attributes.<br><br>host = &lt;host or ip&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* default host to use as a starting point for crawling a network<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 'localhost'.<br><br>subnet = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* default number of bits to use in the subnet mask. Given a host<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with IP 123.123.123.123, a subnet value of 32, would scan only<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that host, and a value or 24 would scan 123.123.123.*.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 32.<br><br></font></code>
<h3> <a name="crawlconf_crawl.conf.example"><span class="mw-headline" id="crawl.conf.example">crawl.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following are example crawl.conf configurations. Configure properties for crawl.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># crawl.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[files]<br>bad_directories_list= bin, sbin, boot, mnt, proc, tmp, temp, home, mail, .thumbnails, cache, old<br>bad_extensions_list= mp3, mpg, jpeg, jpg, &nbsp;m4, mcp, mid<br>bad_file_matches_list= *example*, *makefile, core.*<br>packed_extensions_list= gz, tgz, tar, zip<br>collapse_threshold= 10<br>days_sizek_pairs_list= 3-0,7-1000, 30-10000<br>big_dir_filecount= 100<br>index=main<br>max_badfiles_per_dir=100<br><br><br>[network]<br>host = myserver<br>subnet = 24<br><br><br></font></code>

<a name="datamodelsconf"></a><h2> <a name="datamodelsconf_datamodels.conf"><span class="mw-headline" id="datamodels.conf">datamodels.conf</span></a></h2>
<p>The following are the spec and example files for datamodels.conf.
</p>
<h3> <a name="datamodelsconf_datamodels.conf.spec"><span class="mw-headline" id="datamodels.conf.spec">datamodels.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring datamodels.<br># To configure a datamodel for an app, put your custom datamodels.conf in<br># $SPLUNK_HOME/etc/apps/MY_APP/local/<br><br># For examples, see datamodels.conf.example. &nbsp;You must restart Splunk to enable configurations.<br><br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;datamodel_name&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Each stanza represents a datamodel; the datamodel name is the stanza name.<br><br>acceleration = &lt;bool&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set this to true to enable automatic acceleration of this datamodel<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Automatic acceleration will create auxiliary column stores for the fields and<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;values in the events for this datamodel on a per-bucket basis.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These column stores take additional space on disk so be sure you have the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;proper amount of disk space. Additional space required depends on the number<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of events, fields, and distinct field values in the data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These column stores are created and maintained on a schedule you can specify with<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'acceleration.cron_schedule', and can be later queried with the 'tstats' command<br><br>acceleration.earliest_time = &lt;relative-time-str&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies how far back in time Splunk should keep these column stores (and create if<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;acceleration.backfill_time is not set)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specified by a relative time string, e.g. '-7d' accelerate data within the last 7 days<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to the empty string, meaning keep these stores for all time<br><br>acceleration.backfill_time = &lt;relative-time-str&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* ADVANCED: Specifies how far back in time Splunk should create these column stores<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* ONLY set this parameter if you want to backfill less data than your retention period<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set by 'acceleration.earliest_time'. You may want to use this to limit your time window for<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;creation in a large environment where initially creating all of the stores is an expensive<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;operation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* WARNING: If one of your indexers is down for a period longer than this backfill time, you<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;may miss accelerating a window of your incoming data. It is for this reason we do not recommend<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;setting this to a small window.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* MUST be set to a more recent time than acceleration.earliest_time. For example, if earliest<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time is set to '-1y' to keep the stores for a 1 year window, you could set backfill to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'-20d' to only create stores for data from the last 20 days. However, you could not set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;backfill to '-2y', as that's farther back in time than '-1y'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If empty or unset (default), Splunk will always backfill fully to acceleration.earliest_time<br><br>acceleration.max_time = &lt;unsigned int&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The maximum amount of time that the column store creation search is allowed to run (in seconds)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Note that this is an approximate time, as the 'summarize' search will only finish on clean bucket<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;boundaries to avoid wasted work<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to: 3600<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 implies no limit<br><br>acceleration.cron_schedule = &lt;cron-string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Cron schedule to be used to probe/generate the column stores for this datamodel<br><br>acceleration.manual_rebuilds = &lt;bool&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an ADVANCED command. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Normally, the 'summarize' command will automatically rebuild summaries during the creation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;phase that are considered to be out of-date, such as when the configuration backing the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data model changes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A summary being out of date implies that the datamodel search stored in the metadata<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for this summary no longer matches the current datamodel search, OR the search in the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metadata can no longer be parsed.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to true, out-of-date summaries are not rebuilt by the 'summarize' command.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: If we find a partial summary be out of date, we will always rebuild that summary so<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that a bucket summary only has results corresponding to one datamodel search.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br><br></font></code>
<h3> <a name="datamodelsconf_datamodels.conf.example"><span class="mw-headline" id="datamodels.conf.example">datamodels.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># Configuration for example datamodels<br>#<br><br># An example of accelerating data for the 'mymodel' datamodel for the<br># past five days, generating and checking the column stores every 10 minutes<br>[mymodel]<br>acceleration = true<br>acceleration.earliest_time = -5d<br>acceleration.cron_schedule = */10 * * * *<br><br></font></code>

<a name="datatypesbnfconf"></a><h2> <a name="datatypesbnfconf_datatypesbnf.conf"><span class="mw-headline" id="datatypesbnf.conf">datatypesbnf.conf</span></a></h2>
<p>The following are the spec and example files for datatypesbnf.conf.
</p>
<h3> <a name="datatypesbnfconf_datatypesbnf.conf.spec"><span class="mw-headline" id="datatypesbnf.conf.spec">datatypesbnf.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file effects how the search assistant (typeahead) shows the syntax for search commands<br><br>[&lt;syntax-type&gt;]<br>* The name of the syntax type you're configuring. <br>* Follow this field name with one syntax= definition. <br>* Syntax type can only contain a-z, and -, but cannot begin with -<br><br>syntax = &lt;string&gt;<br>* The syntax for you syntax type.<br>* Should correspond to a regular expression describing the term.<br>* Can also be a &lt;field&gt; or other similar value.<br><br></font></code>
<h3> <a name="datatypesbnfconf_datatypesbnf.conf.example"><span class="mw-headline" id="datatypesbnf.conf.example">datatypesbnf.conf.example</span></a></h3>
<code><font size="2"><br>No example<br></font></code>

<a name="defaultmetaconf"></a><h2> <a name="defaultmetaconf_default.meta.conf"><span class="mw-headline" id="default.meta.conf">default.meta.conf</span></a></h2>
<p>The following are the spec and example files for default.meta.conf.
</p>
<h3> <a name="defaultmetaconf_default.meta.conf.spec"><span class="mw-headline" id="default.meta.conf.spec">default.meta.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br>#<br># *.meta files contain ownership information, access controls, and export<br># settings for Splunk objects like saved searches, event types, and views.<br># Each app has its own default.meta file. <br><br># Interaction of ACLs across app-level, category level, and specific object<br># configuration:<br>* To access/use an object, users must have read access to:<br>&nbsp;&nbsp;* the app containing the object<br>&nbsp;&nbsp;* the generic category within the app (eg [views])<br>&nbsp;&nbsp;* the object itself<br>* If any layer does not permit read access, the object will not be accessible.<br><br>* To update/modify an object, such as to edit a saved search, users must have:<br>&nbsp;&nbsp;* read and write access to the object<br>&nbsp;&nbsp;* read access to the app, to locate the object<br>&nbsp;&nbsp;* read access to the generic category within the app (eg. [savedsearches])<br>* If object does not permit write access to the user, the object will not be<br>&nbsp;&nbsp;modifiable.<br>* If any layer does not permit read access to the user, the object will not be<br>&nbsp;&nbsp;accessible in order to modify<br><br>* In order to add or remove objects from an app, users must have:<br>&nbsp;&nbsp;* write access to the app<br>* If users do not have write access to the app, an attempt to add or remove an<br>&nbsp;&nbsp;object will fail.<br><br><br>* Objects that are exported to other apps or to system context have no change<br>&nbsp;&nbsp;to their accessibility rules. &nbsp;Users must still have read access to the<br>&nbsp;&nbsp;containing app, category, and object, despite the export.<br><br># Set access controls on the app containing this metadata file.<br>[]<br>access = read&nbsp;: [ * ], write&nbsp;: [ admin, power ]<br>* Allow all users to read this app's contents. Unless overridden by other metadata, <br>allow only admin and power users to share objects into this app.<br><br># Set access controls on this app's views.<br>[views]<br>access = read&nbsp;: [ * ], write&nbsp;: [ admin ]<br>* Allow all users to read this app's views. Allow only admin users to create,<br>* remove, share, or unshare views in this app.<br><br># Set access controls on a specific view in this app.<br>[views/index_status]<br>access = read&nbsp;: [ admin ], write&nbsp;: [ admin ]<br>* Allow only admin users to read or modify this view.<br><br># Make this view available in all apps.<br>export = system<br>* To make this view available only in this app, set 'export = none' instead.<br>owner = admin<br>* Set admin as the owner of this view.<br><br></font></code>
<h3> <a name="defaultmetaconf_default.meta.conf.example"><span class="mw-headline" id="default.meta.conf.example">default.meta.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example patterns for the metadata files default.meta and local.meta<br>#<br><br>#This example would make all of the objects in an app globally accessible to all apps<br>[]<br>export=system<br><br></font></code>

<a name="default-modeconf"></a><h2> <a name="default-modeconf_default-mode.conf"><span class="mw-headline" id="default-mode.conf">default-mode.conf</span></a></h2>
<p>The following are the spec and example files for default-mode.conf.
</p>
<h3> <a name="default-modeconf_default-mode.conf.spec"><span class="mw-headline" id="default-mode.conf.spec">default-mode.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file documents the syntax of default-mode.conf for comprehension and<br># troubleshooting purposes.<br><br># default-mode.conf is a file that exists primarily for Splunk Support and<br># Services to configure splunk.<br><br># CAVEATS: <br><br># DO NOT make changes to default-mode.conf without coordinating with Splunk<br># Support or Services. &nbsp;End-user changes to default-mode.conf are not<br># supported.<br>#<br># default-mode.conf *will* be removed in a future version of Splunk, along with<br># the entire configuration scheme that it affects. Any settings present in<br># default-mode.conf files will be completely ignored at this point.<br>#<br># Any number of seemingly reasonable configurations in default-mode.conf <br># might fail to work, behave bizarrely, corrupt your data, iron your<br># cat, cause unexpected rashes, or order unwanted food delivery to your house.<br># Changes here alter the way that pieces of code will communicate which are<br># only intended to be used in a specific configuration.<br><br><br># INFORMATION: <br><br># The main value of this spec file is to assist in reading these files for<br># troubleshooting purposes. &nbsp;default-mode.conf was originally intended to<br># provide a way to describe the alternate setups used by the Splunk Light<br># Forwarder and Splunk Universal Forwarder.<br><br># The only reasonable action is to re-enable input pipelines that are disabled by<br># default in those forwarder configurations. &nbsp;However, keep the prior caveats<br># in mind. &nbsp;Any future means of enabling inputs will have a different form when<br># this mechanism is removed.<br><br># SYNTAX:<br><br>[pipeline:&lt;string&gt;]<br>disabled = true | false<br>disabled_processors = &lt;string&gt;<br><br><br>[pipeline:&lt;string&gt;]<br>&nbsp;* Refers to a particular Splunkd pipeline.<br>&nbsp;* The set of named pipelines is a splunk-internal design. &nbsp;That does not mean<br>&nbsp;&nbsp;&nbsp;that the Splunk design is a secret, but it means it is not external for the<br>&nbsp;&nbsp;&nbsp;purposes of configuration.<br>&nbsp;* Useful information on the data processing system of splunk can be found in<br>&nbsp;&nbsp;&nbsp;the external documentation, for example<br>&nbsp;&nbsp;&nbsp;http://docs.splunk.com/Documentation/Splunk/latest/Deploy/Datapipeline<br><br><br>disabled = true | false<br>&nbsp;* If set to true on a specific pipeline, the pipeline will not be loaded in<br>&nbsp;&nbsp;&nbsp;the system.<br><br>disabled_processors = &lt;processor1&gt;, &lt;processor2&gt;<br>&nbsp;* Processors which normally would be loaded in this pipeline are not loaded if<br>&nbsp;&nbsp;&nbsp;they appear in this list<br>&nbsp;* The set of named procoessors is again a splunk-internal design component.<br><br></font></code>
<h3> <a name="default-modeconf_default-mode.conf.example"><span class="mw-headline" id="default-mode.conf.example">default-mode.conf.example</span></a></h3>
<code><font size="2"><br>No example<br></font></code>

<a name="deploymentconf"></a><h2> <a name="deploymentconf_deployment.conf"><span class="mw-headline" id="deployment.conf">deployment.conf</span></a></h2>
<p>The following are the spec and example files for deployment.conf.
</p>
<h3> <a name="deploymentconf_deployment.conf.spec"><span class="mw-headline" id="deployment.conf.spec">deployment.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># *** DEPRECATED ***<br>#<br>#<br># This configuration has been deprecated in favor of the following:<br># 1.) deploymentclient.conf - for configuring Deployment Clients.<br># 2.) serverclass.conf - for Deployment Server service class configuration.<br># 3.) tenants.conf - for launching multiple Deployment Servers from the same Splunk instance.<br>#<br>#<br># Compatibility:<br># Splunk 4.x Deployment Server is NOT compatible with Splunk 3.x Deployment Clients.<br>#<br><br></font></code>
<h3> <a name="deploymentconf_deployment.conf.example"><span class="mw-headline" id="deployment.conf.example">deployment.conf.example</span></a></h3>
<code><font size="2"><br>No example<br></font></code>

<a name="deploymentclientconf"></a><h2> <a name="deploymentclientconf_deploymentclient.conf"><span class="mw-headline" id="deploymentclient.conf">deploymentclient.conf</span></a></h2>
<p>The following are the spec and example files for deploymentclient.conf.
</p>
<h3> <a name="deploymentclientconf_deploymentclient.conf.spec"><span class="mw-headline" id="deploymentclient.conf.spec">deploymentclient.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values for configuring a deployment client to receive<br># content (apps and configurations) from a deployment server. <br># <br># To customize the way a deployment client behaves, place a deploymentclient.conf in <br># $SPLUNK_HOME/etc/system/local/ on that Splunk instance. Configure what apps or configuration <br># content is deployed to a given deployment client in serverclass.conf. <br># Refer to serverclass.conf.spec and serverclass.conf.example for more information. <br>#<br># You must restart Splunk for changes to this configuration file to take effect.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>#***************************************************************************<br># Configure a Splunk deployment client.<br># <br># Note: At a minimum the [deployment-client] stanza is required in deploymentclient.conf for<br># deployment client to be enabled.<br>#***************************************************************************<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[deployment-client]<br><br>disabled = [false|true]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br>&nbsp;&nbsp;&nbsp;&nbsp;* Enable/Disable deployment client.<br><br>clientName = deploymentClient<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to deploymentClient.<br>&nbsp;&nbsp;&nbsp;&nbsp;* A name that the deployment server can filter on.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Takes precedence over DNS names.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>workingDir = $SPLUNK_HOME/var/run<br>&nbsp;&nbsp;&nbsp;&nbsp;* Temporary folder used by the deploymentClient to download apps and configuration content.<br><br>repositoryLocation = $SPLUNK_HOME/etc/apps<br>&nbsp;&nbsp;&nbsp;&nbsp;* The location into which content is installed after being downloaded from a deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Apps and configuration content must be installed into the default location <br>($SPLUNK_HOME/etc/apps) or it will not be recognized by the Splunk instance on the <br>deployment client. <br>&nbsp;&nbsp;&nbsp;&nbsp;* Note: Apps and configuration content to be deployed may be located in an alternate location on <br>the deployment server. Set both repositoryLocation and serverRepositoryLocationPolicy explicitly to <br>ensure that the content is installed into the correct location ($SPLUNK_HOME/etc/apps) <br>on the deployment client. <br>&nbsp;&nbsp;&nbsp;&nbsp;* The deployment client uses the 'serverRepositoryLocationPolicy' defined below to determine <br>which value of repositoryLocation to use.<br><br>serverRepositoryLocationPolicy = [acceptSplunkHome|acceptAlways|rejectAlways]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to acceptSplunkHome.<br>&nbsp;&nbsp;&nbsp;&nbsp;* acceptSplunkHome - accept the repositoryLocation supplied by the deployment server, only if it <br>is rooted by $SPLUNK_HOME.<br>&nbsp;&nbsp;&nbsp;&nbsp;* acceptAlways - always accept the repositoryLocation supplied by the deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* rejectAlways - reject the server supplied value and use the repositoryLocation specified <br>in the local deploymentclient.conf.<br><br>endpoint=$deploymentServerUri$/services/streams/deployment?name=$serverClassName$:$appName$<br>&nbsp;&nbsp;&nbsp;&nbsp;* The HTTP endpoint from which content should be downloaded.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Note: The deployment server may specify a different endpoint from which to download each set of <br>content (individual apps, etc).<br>&nbsp;&nbsp;&nbsp;&nbsp;* The deployment client will use the serverEndpointPolicy defined below to determine which value <br>to use.<br>&nbsp;&nbsp;&nbsp;&nbsp;* $deploymentServerUri$ will resolve to targetUri defined in the [target-broker] stanza below.<br>&nbsp;&nbsp;&nbsp;&nbsp;* $serverClassName$ and $appName$ mean what they say.<br><br>serverEndpointPolicy = [acceptAlways|rejectAlways]<br>&nbsp;&nbsp;&nbsp;&nbsp;* defaults to acceptAlways<br>&nbsp;&nbsp;&nbsp;&nbsp;* acceptAlways - always accept the endpoint supplied by the server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* rejectAlways - reject the endpoint supplied by the server. Always use the 'endpoint' definition <br>above.<br><br>phoneHomeIntervalInSecs = &lt;integer in seconds&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 60.<br>&nbsp;&nbsp;&nbsp;&nbsp;* This determines how frequently this deployment client should check for new content. <br><br>handshakeRetryIntervalInSecs = &lt;integer in seconds&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to phoneHomeIntervalInSecs<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This sets the handshake retry frequency.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Could be used to tune the initial connection rate on a new server<br><br># Advanced!<br># You should use this property only when you have a hierarchical deployment server installation, and have <br># a Splunk instance that behaves as both a DeploymentClient and a DeploymentServer.<br><br>reloadDSOnAppInstall = [false|true]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br>&nbsp;&nbsp;&nbsp;&nbsp;* Setting this flag to true will cause the deploymentServer on this Splunk instance to be reloaded whenever <br>an app is installed by this deploymentClient.<br><br># The following stanza specifies deployment server connection information<br><br>[target-broker:deploymentServer]<br><br>targetUri= &lt;deploymentServer&gt;:&lt;mgmtPort&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* URI of the deployment server.<br><br>phoneHomeIntervalInSecs = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* see phoneHomeIntervalInSecs above<br><br><br></font></code>
<h3> <a name="deploymentclientconf_deploymentclient.conf.example"><span class="mw-headline" id="deploymentclient.conf.example">deploymentclient.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># Example 1<br># Deployment client receives apps and places them into the same repositoryLocation <br># (locally, relative to $SPLUNK_HOME) as it picked them up from. This is typically $SPLUNK_HOME/etc/apps. <br># There is nothing in [deployment-client] because the deployment client is not overriding the value set <br># on the deployment server side.<br><br>[deployment-client]<br><br>[target-broker:deploymentServer]<br>targetUri= deploymentserver.splunk.mycompany.com:8089<br><br><br># Example 2<br># Deployment server keeps apps to be deployed in a non-standard location on the server side <br># (perhaps for organization purposes). <br># Deployment client receives apps and places them in the standard location. <br># Note: Apps deployed to any location other than $SPLUNK_HOME/etc/apps on the deployment client side <br># will not be recognized and run. <br># This configuration rejects any location specified by the deployment server and replaces it with the <br># standard client-side location. <br><br>[deployment-client]<br>serverRepositoryLocationPolicy = rejectAlways<br>repositoryLocation = $SPLUNK_HOME/etc/apps<br><br>[target-broker:deploymentServer]<br>targetUri= deploymentserver.splunk.mycompany.com:8089<br><br><br># Example 3<br># Deployment client should get apps from an HTTP server that is different from the one specified by <br># the deployment server. <br><br>[deployment-client]<br>serverEndpointPolicy = rejectAlways<br>endpoint = http://apache.mycompany.server:8080/$serverClassName$/$appName$.tar<br><br>[target-broker:deploymentServer]<br>targetUri= deploymentserver.splunk.mycompany.com:8089<br><br><br># Example 4<br># Deployment client should get apps from a location on the file system and not from a location specified <br># by the deployment server<br><br>[deployment-client]<br>serverEndpointPolicy = rejectAlways<br>endpoint = file:/&lt;some_mount_point&gt;/$serverClassName$/$appName$.tar<br><br>[target-broker:deploymentServer]<br>targetUri= deploymentserver.splunk.mycompany.com:8089<br>handshakeRetryIntervalInSecs=20<br><br><br></font></code>

<a name="distsearchconf"></a><h2> <a name="distsearchconf_distsearch.conf"><span class="mw-headline" id="distsearch.conf">distsearch.conf</span></a></h2>
<p>The following are the spec and example files for distsearch.conf.
</p>
<h3> <a name="distsearchconf_distsearch.conf.spec"><span class="mw-headline" id="distsearch.conf.spec">distsearch.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values you can use to configure distributed search.<br>#<br># To set custom configurations, place a distsearch.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;<br># For examples, see distsearch.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># These attributes are all configured on the search head, with the exception of the optional attributes listed <br># under the SEARCH HEAD BUNDLE MOUNTING OPTIONS heading, which are configured on the search peers.<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[distributedSearch]<br>* Set distributed search configuration options under this stanza name.<br>* Follow this stanza name with any number of the following attribute/value pairs. &nbsp;<br>* If you do not set any attribute, Splunk uses the default value (if there is one listed).<br><br>disabled = [true|false]<br>* Toggle distributed search off (true) and on (false).<br>* Defaults to false (your distributed search stanza is enabled by default).<br><br>heartbeatMcastAddr = &lt;IP address&gt;<br>* This setting is deprecated<br><br>heartbeatPort = &lt;port&gt;<br>* This setting is deprecated<br><br>ttl = &lt;integer&gt;<br>* This setting is deprecated<br><br>heartbeatFrequency = &lt;int, in seconds&gt;<br>* This setting is deprecated<br><br>statusTimeout = &lt;int, in seconds&gt;<br>* Set connection timeout when gathering a search peer's basic info (/services/server/info).<br>* Note: Read/write timeouts are automatically set to twice this value.<br>* Defaults to 10.<br><br>removedTimedOutServers = [true|false]<br>* This setting is no longer supported, and will be ignored.<br><br>checkTimedOutServersFrequency = &lt;integer, in seconds&gt;<br>* This setting is no longer supported, and will be ignored.<br><br>autoAddServers = [true|false]<br>* This setting is deprecated<br><br>bestEffortSearch = [true|false]<br>* Whether to remove a peer from search when it does not have any of our bundles. <br>* If set to true searches will never block on bundle replication, even when a peer is first adde - the <br>* peers that don't have any common bundles will simply not be searched.<br>* Defaults to false<br><br>skipOurselves = [true|false]<br>* This setting is deprecated<br><br>servers = &lt;comma separated list of servers&gt;<br>* Initial list of servers. &nbsp;<br><br>disabled_servers = &lt;comma separated list of servers&gt;<br>* A list of configured but disabled search peers.<br><br>shareBundles = [true|false]<br>* Indicates whether this server will use bundle replication to share search time configuration<br>&nbsp;&nbsp;with search peers. <br>* If set to false, the search head assumes that all the search peers can access the correct bundles <br>&nbsp;&nbsp;via share storage and have configured the options listed under "SEARCH HEAD BUNDLE MOUNTING OPTIONS".<br>* Defaults to true.<br><br>useSHPBundleReplication = &lt;bool&gt;|always<br>* Relevant only in search head pooling environments. Whether the search heads in the pool should compete <br>* with each other to decide which one should handle the bundle replication (every time bundle replication <br>* needs to happen) or whether each of them should individually replicate the bundles. <br>* When set to always and bundle mounting is being used then use the search head pool guid rather than <br>* each individual server name to identify bundles (and search heads to the remote peers).<br>* Defaults to true<br><br>trySSLFirst = &lt;bool&gt;<br>* Controls whether the search head attempts HTTPS or HTTP connection when a new peer is added, or <br>* during a restart. If value is missing true is assumed. <br>* Defaults to true<br><br>peerResolutionThreads = &lt;int&gt;<br>* Controls how many threads to use to resolve peers during a restart. Or forced bundle syncronization.<br>* Defaults 0 (Use no threads)<br><br>serverTimeout = &lt;int, in seconds&gt;<br>* DEPRECATED, please use &nbsp;connectionTimeout, sendTimeout, receiveTimeout <br><br>connectionTimeout = &lt;int, in seconds&gt;<br>* Amount of time in seconds to use as a timeout during search peer connection establishment.<br><br>sendTimeout = &lt;int, in seconds&gt;<br>* Amount of time in seconds to use as a timeout while trying to write/send data to a search peer.<br><br>receiveTimeout = &lt;int, in seconds&gt;<br>* Amount of time in seconds to use as a timeout while trying to read/receive data from a search peer.<br><br>authTokenConnectionTimeout = &lt;int, in seconds&gt;<br>* Maximum number of seconds to connect to a remote search peer, when getting its auth token<br>* Default is 5<br><br>authTokenSendTimeout = &lt;int, in seconds&gt;<br>* Maximum number of seconds to send a request to the remote peer, when getting its auth token<br>* Default is 10<br><br>authTokenReceiveTimeout = &lt;int, in seconds&gt;<br>* Maximum number of seconds to receive a response from a remote peer, when getting its auth token<br>* Default is 10<br><br>#******************************************************************************<br># DISTRIBUTED SEARCH KEY PAIR GENERATION OPTIONS<br>#******************************************************************************<br><br>[tokenExchKeys]<br><br>certDir = &lt;directory&gt;<br>* This directory contains the local Splunk instance's distributed search key pair.<br>* This directory also contains the public keys of servers that distribute searches to this Splunk instance.<br><br>publicKey = &lt;filename&gt;<br>* Name of public key file for this Splunk instance.<br><br>privateKey = &lt;filename&gt;<br>* Name of private key file for this Splunk instance.<br><br>genKeyScript = &lt;command&gt;<br>* Command used to generate the two files above.<br><br>#******************************************************************************<br># REPLICATION SETTING OPTIONS<br>#******************************************************************************<br><br>[replicationSettings]<br><br>connectionTimeout = &lt;int, in seconds&gt;<br>* The maximum number of seconds to wait before timing out on initial connection to a peer.<br><br>sendRcvTimeout = &lt;int, in seconds&gt;<br>* The maximum number of seconds to wait for the sending of a full replication to a peer.<br><br>replicationThreads = &lt;int&gt;<br>* The maximum number of threads to use when performing bundle replication to peers.<br>* Must be a positive number<br>* Defaults to 5.<br><br>maxMemoryBundleSize = &lt;int&gt;<br>* The maximum size (in MB) of bundles to hold in memory. If the bundle is larger than this<br>* the bundles will be read and encoded on the fly for each peer the replication is taking place. <br>* Defaults to 10<br><br>maxBundleSize = &lt;int&gt;<br>* The maximum size (in MB) of the bundle for which replication can occur. If the bundle is larger than this<br>* bundle replication will not occur and an error message will be logged.<br>* Defaults to: 1024 (1GB)<br><br>concerningReplicatedFileSize = &lt;int&gt;<br>* Any individual file within a bundle that is larger than this value (in MB) will trigger a splunkd.log message.<br>* Where possible, avoid replicating such files, e.g. by customizing your blacklists.<br>* Defaults to: 50<br><br>allowStreamUpload = auto | true | false<br>* Whether to enable streaming bundle replication for peers.<br>* If set to auto, streaming bundle replication will be used when connecting to peers with a complete implementation of this feature (Splunk 6.0 or higher).<br>* If set to true, streaming bundle replication will be used when connecting to peers with a complete or experimental implementation of this feature (Splunk 4.2.3 or higher).<br>* If set to false, streaming bundle replication will never be used.<br>* Whatever the value of this setting, streaming bundle replication will not be used for peers that completely lack support for this feature.<br>* Defaults to: auto<br><br>allowSkipEncoding = &lt;bool&gt;<br>* Whether to avoid URL-encoding bundle data on upload.<br>* Defaults to: true<br><br>allowDeltaUpload = &lt;bool&gt;<br>* Whether to enable delta-based bundle replication. <br>* Defaults to: true<br><br>sanitizeMetaFiles = &lt;bool&gt;<br>* Whether to sanitize or filter *.meta files before replication.<br>* This feature can be used to avoid unnecessary replications triggered by writes to *.meta files that have no real effect on search behavior.<br>* The types of stanzas that "survive" filtering are configured via the replicationSettings:refineConf stanza.<br>* The filtering process removes comments and cosmetic whitespace.<br>* Defaults to: true<br><br>[replicationSettings:refineConf]<br><br>replicate.&lt;conf_file_name&gt; = &lt;bool&gt;<br>* Controls whether Splunk replicates a particular type of *.conf file, along with any associated permissions in *.meta files.<br>* These settings on their own do not cause files to be replicated. A file must still be whitelisted (via replicationWhitelist) to be eligible for inclusion via these settings.<br>* In a sense, these settings constitute another level of filtering that applies specifically to *.conf files and stanzas with *.meta files.<br>* Defaults to: false<br><br>#******************************************************************************<br># REPLICATION WHITELIST OPTIONS<br>#******************************************************************************<br><br>[replicationWhitelist]<br><br>&lt;name&gt; = &lt;whitelist_pattern&gt;<br>* Controls Splunk's search-time conf replication from search heads to search nodes.<br>* Only files that match a whitelist entry will be replicated.<br>* Conversely, files which are not matched by any whitelist will not be replicated.<br>* Only files located under $SPLUNK_HOME/etc will ever be replicated in this way.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The regex will be matched against the filename, relative to $SPLUNK_HOME/etc.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example: for a file "$SPLUNK_HOME/etc/apps/fancy_app/default/inputs.conf"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this whitelist should match "apps/fancy_app/default/inputs.conf"<br>&nbsp;&nbsp;&nbsp;&nbsp;* Similarly, the etc/system files are available as system/... <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user-specific files are available as users/username/appname/...<br>* The 'name' element is generally just descriptive, with one exception: if &lt;name&gt;<br>&nbsp;&nbsp;begins with "refine.", files whitelisted by the given pattern will also go through<br>&nbsp;&nbsp;another level of filtering configured in the replicationSettings:refineConf stanza.<br>* The whitelist_pattern is the Splunk-style pattern matching, which is primarily<br>&nbsp;&nbsp;regex-based with special local behavior for '...' and '*'.<br>&nbsp;&nbsp;* ... matches anything, while * matches anything besides directory separators. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;See props.conf.spec for more detail on these.<br>&nbsp;&nbsp;* Note '.' will match a literal dot, not any character.<br>* Note that these lists are applied globally across all conf data, not to any<br>&nbsp;&nbsp;particular app, regardless of where they are defined. &nbsp;Be careful to pull in<br>&nbsp;&nbsp;only your intended files.<br><br>#******************************************************************************<br># REPLICATION BLACKLIST OPTIONS<br>#******************************************************************************<br><br>[replicationBlacklist]<br><br>&lt;name&gt; = &lt;blacklist_pattern&gt;<br>* All comments from the replication whitelist notes above also apply here.<br>* Replication blacklist takes precedence over the whitelist, meaning that a<br>&nbsp;&nbsp;file that matches both the whitelist and the blacklist will NOT be replicated.<br>* This can be used to prevent unwanted bundle replication in two common scenarios:<br>&nbsp;&nbsp;&nbsp;* Very large files, which part of an app may not want to be replicated,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;especially if they are not needed on search nodes.<br>&nbsp;&nbsp;&nbsp;* Frequently updated files (for example, some lookups) will trigger retransmission of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all search head data.<br>* Note that these lists are applied globally across all conf data. Especially<br>&nbsp;&nbsp;for blacklisting, be careful to constrain your blacklist to match only data<br>&nbsp;&nbsp;your application will not need.<br><br>#******************************************************************************<br># BUNDLE ENFORCER WHITELIST OPTIONS<br>#******************************************************************************<br><br>[bundleEnforcerWhitelist]<br><br>&lt;name&gt; = &lt;whitelist_pattern&gt;<br>* Peers uses this to make sure knowledge bundle sent by search heads and masters do not contain<br>&nbsp;&nbsp;alien files.<br>* If this stanza is empty, the receiver accepts the bundle unless it contains<br>&nbsp;&nbsp;files matching the rules specified in [bundleEnforcerBlacklist]. Hence, if both <br>&nbsp;&nbsp;[bundleEnforcerWhitelist] and [bundleEnforcerBlacklist] are empty (which is the default),<br>&nbsp;&nbsp;then the receiver accepts all bundles.<br>* If this stanza is not empty, the receiver accepts the bundle only if it contains<br>&nbsp;&nbsp;only files that match the rules specified here but not those in [bundleEnforcerBlacklist].<br>* All rules are regexs. <br>* This stanza is empty by default.<br><br>#******************************************************************************<br># BUNDLE ENFORCER BLACKLIST OPTIONS<br>#******************************************************************************<br><br>[bundleEnforcerBlacklist]<br><br>&lt;name&gt; = &lt;blacklist_pattern&gt;<br>* Peers uses this to make sure knowledge bundle sent by search heads and masters do not contain<br>&nbsp;&nbsp;alien files.<br>* This list overrides [bundleEnforceWhitelist] above. That means the receiver rejects (i.e. removes)<br>&nbsp;&nbsp;the bundle if it contains any file that matches the rules specified here even if that file is allowed<br>&nbsp;&nbsp;by [bundleEnforcerWhitelist].<br>* If this stanza is empty, then only [bundleEnforcerWhitelist] matters.<br>* This stanza is empty by default. <br><br>#******************************************************************************<br># SEARCH HEAD BUNDLE MOUNTING OPTIONS <br># You set these attributes on the search peers only, and only if you also set shareBundles=false <br># in [distributedSearch] on the search head. Use them to achieve replication-less bundle access. The <br># search peers use a shared storage mountpoint to access the search head bundles ($SPLUNK_HOME/etc).<br>#******************************************************************************<br><br>[searchhead:&lt;searchhead-splunk-server-name&gt;]<br>* &lt;searchhead-splunk-server-name&gt; is the name of the related searchhead installation.<br>* This setting is located in server.conf, serverName = &lt;name&gt;<br><br>mounted_bundles = [true|false]<br>* Determines whether the bundles belong to the search head specified in the stanza name are mounted.<br>* You must set this to "true" to use mounted bundles.<br>* Default is "false".<br><br>bundles_location = &lt;path_to_bundles&gt;<br>* The path to where the search head's bundles are mounted. This must be the mountpoint on the search peer, <br>* not on the search head. This should point to a directory that is equivalent to $SPLUNK_HOME/etc/. It must<br>* contain at least the following subdirectories: system, apps, users.<br><br><br>#******************************************************************************<br># DISTRIBUTED SEARCH GROUP DEFINITIONS<br># These are the definitions of the distributed search groups. A search group is<br># a set of search peers as identified by thier host:management-port. Searches <br># may be directed to a search group using the splunk_server_group arguments.The<br># searches will be defined only against the members of that group.<br>#******************************************************************************<br><br>[distributedSearch:&lt;splunk-server-group-name&gt;]<br>* &lt;splunk-server-group-name&gt; is the name of the splunk-server-group that is <br>* defined in this stanza<br><br>servers = &lt;comma separated list of servers&gt;<br>* List of search peers that are members of this group. Comma serparated list<br>* of hot:port in the same format as the servers field of the distributedSearch<br>* stanza<br><br>default = [true|false]<br>* Will set this as the default group of peers against which all searches are run<br>* unless a server-group is not explicitly specified.<br><br></font></code>
<h3> <a name="distsearchconf_distsearch.conf.example"><span class="mw-headline" id="distsearch.conf.example">distsearch.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># These are example configurations for distsearch.conf. Use this file to configure distributed search. &nbsp;For all <br># available attribute/value pairs, see distsearch.conf.spec.<br>#<br># There is NO DEFAULT distsearch.conf.<br>#<br># To use one or more of these configurations, copy the configuration block into distsearch.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[distributedSearch]<br>servers = 192.168.1.1:8059,192.168.1.2:8059<br><br># This entry distributes searches to 192.168.1.1:8059,192.168.1.2:8059.<br># Attributes not set here will use the defaults listed in distsearch.conf.spec.<br><br>#this stanza controls the timing settings for connecting to a remote peer and the send timeout<br>[replicationSettings]<br>connectionTimeout = 10<br>sendRcvTimeout = 60<br><br>#this stanza controls what files are replicated to the other peer each is a regex<br>[replicationWhitelist]<br>allConf = *.conf<br><br># Mounted bundles example.<br># This example shows two distsearch.conf configurations, one for the search head and another for each of the<br># search head's search peers. It shows only the attributes necessary to implement mounted bundles.<br><br># On a search head whose Splunk server name is "searcher01":<br>[distributedSearch]<br>...<br>shareBundles = false<br><br># On each search peer:<br>[searchhead:searcher01]<br>mounted_bundles = true<br>bundles_location = /opt/shared_bundles/searcher01<br><br></font></code>

<a name="eventdiscovererconf"></a><h2> <a name="eventdiscovererconf_eventdiscoverer.conf"><span class="mw-headline" id="eventdiscoverer.conf">eventdiscoverer.conf</span></a></h2>
<p>The following are the spec and example files for eventdiscoverer.conf.
</p>
<h3> <a name="eventdiscovererconf_eventdiscoverer.conf.spec"><span class="mw-headline" id="eventdiscoverer.conf.spec">eventdiscoverer.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br><br># This file contains possible attributes and values you can use to configure event discovery through<br># the search command "typelearner."<br>#<br># There is an eventdiscoverer.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an eventdiscoverer.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see <br># eventdiscoverer.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>ignored_keywords = &lt;comma-separated list of terms&gt; <br>* If you find that event types have terms you do not want considered (for example, "mylaptopname"), <br>add that term to this list.<br>* Terms in this list are never considered for defining an event type. &nbsp;<br>* For more details, refer to $SPLUNK_HOME/etc/system/default/eventdiscoverer.conf).<br>* Default = "sun, mon, tue,..." <br><br>ignored_fields = &lt;comma-separated list of fields&gt;<br>* Similar to ignored_keywords, except these are fields as defined in Splunk instead of terms.<br>* Defaults include time-related fields that would not be useful for defining an event type.<br><br>important_keywords = &lt;comma-separated list of terms&gt; <br>* When there are multiple possible phrases for generating an eventtype search, those phrases with <br>important_keyword terms are favored. &nbsp;For example, "fatal error" would be preferred over <br>"last message repeated", as "fatal" is an important keyword.<br>* Default = "abort, abstract, accept,..." (see $SPLUNK_HOME/etc/system/default/eventdiscoverer.conf).<br><br></font></code>
<h3> <a name="eventdiscovererconf_eventdiscoverer.conf.example"><span class="mw-headline" id="eventdiscoverer.conf.example">eventdiscoverer.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example eventdiscoverer.conf. &nbsp;These settings are used to control the discovery of <br># common eventtypes used by the typelearner search command.<br>#<br># To use one or more of these configurations, copy the configuration block into eventdiscoverer.conf <br># in $SPLUNK_HOME/etc/system/local/. &nbsp;You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># Terms in this list are never considered for defining an eventtype.<br>ignored_keywords = foo, bar, application, kate, charlie<br><br># Fields in this list are never considered for defining an eventtype.<br>ignored_fields = pid, others, directory<br><br></font></code>

<a name="event%20renderersconf"></a><h2> <a name="event%20renderersconf_event_renderers.conf"><span class="mw-headline" id="event_renderers.conf">event_renderers.conf</span></a></h2>
<p>The following are the spec and example files for event_renderers.conf.
</p>
<h3> <a name="event%20renderersconf_event_renderers.conf.spec"><span class="mw-headline" id="event_renderers.conf.spec">event_renderers.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring event rendering properties.<br>#<br># Beginning with version 6.0, Splunk Enterprise does not support the <br># customization of event displays using event renderers.<br>#<br># There is an event_renderers.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an event_renderers.conf in $SPLUNK_HOME/etc/system/local/, or your own custom app directory.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;name&gt;]<br>* Stanza name. This name must be unique. <br><br>eventtype = &lt;event type&gt;<br>* Specify event type name from eventtypes.conf.<br><br>priority = &lt;positive integer&gt;<br>* Highest number wins!! <br><br>template = &lt;valid Mako template&gt;<br>* Any template from the $APP/appserver/event_renderers directory.<br><br>css_class = &lt;css class name suffix to apply to the parent event element class attribute&gt;<br>* This can be any valid css class value. <br>* The value is appended to a standard suffix string of "splEvent-". A css_class value of foo would <br>result in the parent element of the event having an html attribute class with a value of splEvent-foo <br>(for example, class="splEvent-foo"). You can externalize your css style rules for this in <br>$APP/appserver/static/application.css. For example, to make the text red you would add to <br>application.css:.splEvent-foo { color:red; }<br><br></font></code>
<h3> <a name="event%20renderersconf_event_renderers.conf.example"><span class="mw-headline" id="event_renderers.conf.example">event_renderers.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br># DO NOT EDIT THIS FILE!<br># Please make all changes to files in $SPLUNK_HOME/etc/system/local.<br># To make changes, copy the section/stanza you want to change from $SPLUNK_HOME/etc/system/default<br># into ../local and edit there.<br>#<br># This file contains mappings between Splunk eventtypes and event renderers.<br>#<br># Beginning with version 6.0, Splunk Enterprise does not support the <br># customization of event displays using event renderers.<br>#<br><br>[event_renderer_1]<br>eventtype = hawaiian_type<br>priority = 1<br>css_class = EventRenderer1<br><br>[event_renderer_2]<br>eventtype = french_food_type<br>priority = 1<br>template = event_renderer2.html<br>css_class = EventRenderer2<br><br>[event_renderer_3]<br>eventtype = japan_type<br>priority = 1<br>css_class = EventRenderer3<br><br></font></code>

<a name="eventtypesconf"></a><h2> <a name="eventtypesconf_eventtypes.conf"><span class="mw-headline" id="eventtypes.conf">eventtypes.conf</span></a></h2>
<p>The following are the spec and example files for eventtypes.conf.
</p>
<h3> <a name="eventtypesconf_eventtypes.conf.spec"><span class="mw-headline" id="eventtypes.conf.spec">eventtypes.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains all possible attributes and value pairs for an eventtypes.conf file. &nbsp;<br># Use this file to configure event types and their properties. You can also pipe any search<br># to the "typelearner" command to create event types. &nbsp;Event types created this way will be written<br># to $SPLUNK_HOME/etc/systems/local/eventtypes.conf.<br>#<br># There is an eventtypes.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an eventtypes.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># eventtypes.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;$EVENTTYPE&gt;]<br>* Header for the event type<br>* $EVENTTYPE is the name of your event type.<br>* You can have any number of event types, each represented by a stanza and any number of the following <br>attribute/value pairs. &nbsp;<br>* NOTE: If the name of the event type includes field names surrounded by the percent <br>character (for example "%$FIELD%") then the value of $FIELD is substituted into the event type<br>name for that event. &nbsp;For example, an event type with the header [cisco-%code%] that has<br>"code=432" becomes labeled "cisco-432".<br><br>disabled = [1|0]<br>* Toggle event type on or off.<br>* Set to 0 to disable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>search = &lt;string&gt;<br>* Search terms for this event type. <br>* For example: error OR warn.<br><br>priority = &lt;integer, 1 through 10&gt;<br>* Value used to determine the order in which the matching eventtypes of an event are displayed. &nbsp;<br>* 1 is the highest and 10 is the lowest priority. <br><br>description = &lt;string&gt;<br>* Optional human-readable description of this saved search.<br><br>tags = &lt;string&gt;<br>* DEPRECATED - see tags.conf.spec<br><br></font></code>
<h3> <a name="eventtypesconf_eventtypes.conf.example"><span class="mw-headline" id="eventtypes.conf.example">eventtypes.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example eventtypes.conf. &nbsp;Use this file to configure custom eventtypes.<br>#<br># To use one or more of these configurations, copy the configuration block into eventtypes.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br><br># The following example makes an eventtype called "error" based on the search "error OR fatal."<br><br>[error]<br>search = error OR fatal<br><br><br><br># The following example makes an eventtype template because it includes a field name<br># surrounded by the percent character (in this case "%code%"). <br># The value of "%code%" is substituted into the event type name for that event. <br># For example, if the following example event type is instantiated on an event that has a<br># "code=432," it becomes "cisco-432".<br><br>[cisco-%code%]<br>search = cisco<br><br></font></code>

<a name="fieldsconf"></a><h2> <a name="fieldsconf_fields.conf"><span class="mw-headline" id="fields.conf">fields.conf</span></a></h2>
<p>The following are the spec and example files for fields.conf.
</p>
<h3> <a name="fieldsconf_fields.conf.spec"><span class="mw-headline" id="fields.conf.spec">fields.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute and value pairs for:<br># &nbsp;&nbsp;* Telling Splunk how to handle multi-value fields.<br># &nbsp;&nbsp;* Distinguishing indexed and extracted fields.<br># &nbsp;&nbsp;* Improving search performance by telling the search processor how to handle field values.<br><br># Use this file if you are creating a field at index time (not advised).<br>#<br># There is a fields.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a fields.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see fields.conf.example.<br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;field name&gt;]<br>* Name of the field you're configuring. <br>* Follow this stanza name with any number of the following attribute/value pairs.<br>* Field names can only contain a-z, A-Z, 0-9, and &nbsp;_, but cannot begin with a number or _<br><br># TOKENIZER indicates that your configured field's value is a smaller part of a token.<br># For example, your field's value is "123" but it occurs as "foo123" in your event.<br>TOKENIZER = &lt;regular expression&gt;<br>* Use this setting to configure multivalue fields (refer to the online documentation for multivalue <br>fields).<br>* A regular expression that indicates how the field can take on multiple values at the same time.<br>* If empty, the field can only take on a single value. <br>* Otherwise, the first group is taken from each match to form the set of values.<br>* This setting is used by the "search" and "where" commands, the summary and XML outputs of the <br>asynchronous search API, and by the top, timeline and stats commands.<br>* Tokenization of indexed fields (INDEXED = true) is not supported so this attribute is ignored for <br>indexed fields.<br>* Default to empty.<br><br>INDEXED = [true|false]<br>* Indicate whether a field is indexed or not.<br>* Set to true if the field is indexed.<br>* Set to false for fields extracted at search time (the majority of fields).<br>* Defaults to false.<br><br>INDEXED_VALUE = [true|false|&lt;sed-cmd&gt;|&lt;simple-substitution-string&gt;]<br>* Set this to true if the value is in the raw text of the event. &nbsp;<br>* Set this to false if the value is not in the raw text of the event.<br>* Setting this to true expands any search for key=value into a search of value AND key=value <br>(since value is indexed).<br>* For advanced customization, this setting supports sed style substitution. For example, <br>'INDEXED_VALUE=s/foo/bar/g' would take the value of the field, replace all instances of 'foo' with <br>'bar,' and use that new value as the value to search in the index.<br>* This setting also supports a simple substitution based on looking for the literal string <br>'&lt;VALUE&gt;' (including the '&lt;' and '&gt;' characters). &nbsp;For example, 'INDEXED_VALUE=source::*&lt;VALUE&gt;*' <br>would take a search for 'myfield=myvalue' and search for 'source::*myvalue*' in the index as a <br>single term.<br>* For both substitution constructs, if the resulting string starts with a '[', Splunk interprets <br>the string as a Splunk LISPY expression. &nbsp;For example, 'INDEXED_VALUE=[OR &lt;VALUE&gt; source::*&lt;VALUE&gt;]' <br>would turn 'myfield=myvalue' into applying the LISPY expression '[OR myvalue source::*myvalue]' <br>(meaning it matches either 'myvalue' or 'source::*myvalue' terms).<br>* Defaults to true.<br>* NOTE: You only need to set indexed_value if indexed = false.<br><br></font></code>
<h3> <a name="fieldsconf_fields.conf.example"><span class="mw-headline" id="fields.conf.example">fields.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example fields.conf. &nbsp;Use this file to configure dynamic field extractions.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># fields.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># These tokenizers result in the values of To, From and Cc treated as a list,<br># where each list element is an email address found in the raw string of data.<br><br>[To]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br><br>[From]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br><br>[Cc]<br>TOKENIZER = (\w[\w\.\-]*@[\w\.\-]*\w)<br><br></font></code>

<a name="indexesconf"></a><h2> <a name="indexesconf_indexes.conf"><span class="mw-headline" id="indexes.conf">indexes.conf</span></a></h2>
<p>The following are the spec and example files for indexes.conf.
</p>
<h3> <a name="indexesconf_indexes.conf.spec"><span class="mw-headline" id="indexes.conf.spec">indexes.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains all possible options for an indexes.conf file. &nbsp;Use this file to configure <br># Splunk's indexes and their properties.<br>#<br># There is an indexes.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an indexes.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># indexes.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># CAUTION: &nbsp;You can drastically affect your Splunk installation by changing these settings. &nbsp;<br># Consult technical support (http://www.splunk.com/page/submit_issue) if you are not sure how <br># to configure this file.<br>#<br># DO NOT change the attribute QueryLanguageDefinition without consulting technical support.<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>sync = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The index processor syncs events every &lt;integer&gt; number of events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set to 0 to disable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 32767<br><br>defaultDatabase = &lt;index name&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If no index is specified during search, Splunk searches the default index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The specified index displays as the default in Splunk Manager settings.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "main".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>queryLanguageDefinition = &lt;path to file&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* DO NOT EDIT THIS SETTING. SERIOUSLY. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The path to the search language definition file.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $SPLUNK_HOME/etc/searchLanguage.xml.<br><br>blockSignatureDatabase = &lt;index name&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is the index that stores block signatures of events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "_blocksignature".<br><br>memPoolMB = &lt;positive integer&gt;|auto<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Determines how much memory is given to the indexer memory pool. This restricts the number of outstanding events in the indexer at any given time.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must be greater than 0; maximum value is 1048576 (which corresponds to 1 TB)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Setting this too high can lead to splunkd memory usage going up substantially.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Setting this too low can degrade splunkd indexing performance.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Setting this to "auto" or an invalid value will cause Splunk to autotune this parameter.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "auto".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The values derived when "auto" is seen are as follows:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* System Memory Available less than ... | memPoolMB<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 GB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;64 &nbsp;MB<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 GB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;128 MB<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8 GB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;128 MB<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8 GB or higher &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp;&nbsp;&nbsp;512 MB<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Only set this value if you are an expert user or have been advised to by Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CARELESSNESS IN SETTING THIS MAY LEAD TO PERMANENT BRAIN DAMAGE OR LOSS OF JOB.<br><br>indexThreads = &lt;nonnegative integer&gt;|auto<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Determines the number of threads to use for indexing.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must be at least 1 and no more than 16.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This value should not be set higher than the number of processor cores in the box.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If splunkd is also doing parsing and aggregation, the number should be set lower than the total number of <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;processors minus two.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Setting this to "auto" or an invalid value will cause Splunk to autotune this parameter.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "auto".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Only set this value if you are an expert user or have been advised to by Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CARELESSNESS IN SETTING THIS MAY LEAD TO PERMANENT BRAIN DAMAGE OR LOSS OF JOB.<br><br>assureUTF8 = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Verifies that all data retrieved from the index is proper UTF8.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Will degrade indexing performance when enabled (set to true).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Can only be set globally, by specifying in the [default] stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>enableRealtimeSearch = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Enables real-time searches.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br>suppressBannerList = &lt;comma-separated list of strings&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* suppresses index missing warning banner messages for specified indexes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty<br><br>maxRunningProcessGroups = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* splunkd fires off helper child processes like splunk-optimize, recover-metadata, etc. &nbsp;This param limits how many child processes can be running at any given time.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This maximum applies to entire splunkd, not per index. &nbsp;If you have N indexes, there will be at most maxRunningProcessGroups child processes, not N*maxRunningProcessGroups<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must maintain maxRunningProcessGroupsLowPriority &lt; maxRunningProcessGroups<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an advanced parameter; do NOT set unless instructed by Splunk Support<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 8 (note: up until 5.0 it defaulted to 20)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxRunningProcessGroupsLowPriority = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Of the maxRunningProcessGroups (q.v.) helper child processes, at most maxRunningProcessGroupsLowPriority may be low-priority (e.g. fsck) ones.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This maximum applies to entire splunkd, not per index. &nbsp;If you have N indexes, there will be at most maxRunningProcessGroupsLowPriority low-priority child processes, not N*maxRunningProcessGroupsLowPriority<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must maintain maxRunningProcessGroupsLowPriority &lt; maxRunningProcessGroups<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an advanced parameter; do NOT set unless instructed by Splunk Support<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>bucketRebuildMemoryHint = &lt;positive integer&gt;[KB|MB|GB]|auto<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Suggestion for the bucket rebuild process for the size (bytes) of tsidx file it will try to build.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Larger files use more memory in rebuild, but rebuild will fail if there is not enough.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Smaller files make the rebuild take longer during the final optimize step.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Note: this value is not a hard limit on either rebuild memory usage or tsidx size.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an advanced parameter, do NOT set this unless instructed by Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "auto", which varies by the amount of physical RAM on the host<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;less than 2GB RAM = 67108864 (64MB) tsidx<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;2GB to 8GB RAM = 134217728 (128MB) tsidx<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;more than 8GB RAM = 268435456 (256MB) tsidx<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If not "auto", then must be 16MB-1GB.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Value may be specified using a size suffix: "16777216" or "16MB" are equivalent.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Inappropriate use of this parameter will cause splunkd to not start if rebuild is required.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value (in bytes) is 4294967295<br><br>inPlaceUpdates = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If true, metadata updates are written to the .data files directly<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If false, metadata updates are written to a temporary file and then moved into place<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Intended for advanced debugging of metadata issues<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Setting this parameter to false (to use a temporary file) will impact indexing <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;performance, particularly with large numbers of hosts, sources, or sourcetypes <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(~1 million, across all indexes.)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an advanced parameter; do NOT set unless instructed by Splunk Support<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true<br><br>serviceOnlyAsNeeded = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Causes index service (housekeeping tasks) overhead to be incurred only after index activity.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Indexer module problems may be easier to diagnose when this optimization is disabled (set to false).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br>serviceSubtaskTimingPeriod = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Subtasks of indexer service task will be timed on every Nth execution, where N = value of this parameter.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Smaller values will give greater accuracy; larger values will lessen timer overhead.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Timer measurements will be found in metrics.log, marked "group=subtask_seconds, task=indexer_service"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 30<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* We strongly suggest value of this parameter divide evenly into value of 'rotatePeriodInSecs' parameter.<br><br>processTrackerServiceInterval = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls how often indexer checks status of the child OS processes it had launched to see if it can launch new processes for queued requests.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to 0, indexer will check child process status every second.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 15<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxBucketSizeCacheEntries = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This value is not longer needed and its value is ignored.<br><br>tsidxStatsHomePath = &lt;path on server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path that specifies where Splunk creates namespace data with 'tscollect' command<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the directory does not exist, we attempt to create it<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional. If this is unspecified, we default to the 'tsidxstats' directory under $SPLUNK_DB<br><br>hotBucketTimeRefreshInterval = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls how often each index refreshes the available hot bucket<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;times used by the indexes REST endpoint.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Refresh will occur every N times service is performed for each index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For busy indexes, this is a mutiple of seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For idle indexes, this is a multiple of the second-long-periods in<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which data is received.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This tunable is only intended to relax the frequency of these<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;refreshes in the unexpected case that it adversely affects<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;performance in unusual production scenarios.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This time is tracked on a per-index basis, and thus can be adjusted<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on a per-index basis if needed.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If, for some reason, you want have the index information refreshed<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with every service (and accept minor performance overhead), you can<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use the value 1.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 10 (services).<br><br>#******************************************************************************<br># PER INDEX OPTIONS<br># These options may be set under an [&lt;index&gt;] entry.<br>#<br># Index names must consist of only numbers, letters, periods, underscores, and hyphens. <br>#******************************************************************************<br><br>disabled = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Toggles your index entry off and on.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set to true to disable an index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>deleted = true<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If present, means that this index has been marked for deletion: if splunkd is running,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deletion is in progress; if splunkd is stopped, deletion will re-commence on startup.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Normally absent, hence no default.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Do NOT manually set, clear, or modify value of this parameter.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Seriously: LEAVE THIS PARAMETER ALONE.<br><br>homePath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path that contains the hotdb and warmdb for the index. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunkd keeps a file handle open for warmdbs at all times.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* May contain a volume reference (see volume section below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CAUTION: Path MUST be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required. Splunk will not start if an index lacks a valid homePath.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>coldPath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path that contains the colddbs for the index. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Cold databases are opened as needed when searching.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* May contain a volume reference (see volume section below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CAUTION: Path MUST be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required. Splunk will not start if an index lacks a valid coldPath.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>thawedPath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path that contains the thawed (resurrected) databases for the index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* May NOT contain a volume reference.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required. Splunk will not start if an index lacks a valid thawedPath.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>bloomHomePath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Location where the bloomfilter files for the index are stored.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If specified, MUST be defined in terms of a volume definition (see volume section below)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If bloomHomePath is not specified, bloomfilter files for index will be stored inline,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inside bucket directories.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CAUTION: Path must be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>createBloomfilter = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls whether to create bloomfilter files for the index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* TRUE: bloomfilter files will be created. FALSE: not created.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br>summaryHomePath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path where transparent summarization results for data in this index <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;should be stored. Must be different for each index and may be on any disk drive. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* May contain a volume reference (see volume section below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Volume reference must be used if data retention based on data size is desired.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If not specified it defaults to a directory 'summary' in the same location as homePath<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, if homePath is "/opt/splunk/var/lib/splunk/index1/db",<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;then summaryHomePath would be "/opt/splunk/var/lib/splunk/index1/summary".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CAUTION: Path must be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>tstatsHomePath = &lt;path on index server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Location where datamodel acceleration TSIDX data for this index should be stored<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If specified, MUST be defined in terms of a volume definition (see volume section below)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If not specified it defaults to volume:_splunk_summaries/$_index_name/datamodel_summary,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where $_index_name is the name of the index<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* CAUTION: Path must be writable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>maxBloomBackfillBucketAge = &lt;nonnegative integer&gt;[smhd]|infinite<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If a (warm or cold) bloomfilter-less bucket is older than this, we shall not create its bloomfilter when we come across it<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 30d.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When set to 0, bloomfilters are never backfilled<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When set to "infinite", bloomfilters are always backfilled<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NB that if createBloomfilter=false, bloomfilters are never backfilled regardless of the value of this parameter<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value in computed seconds is 2 billion, or 2000000000, which is approximately 68 years.<br><br>enableOnlineBucketRepair = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls asynchronous "online fsck" bucket repair, which runs concurrently with Splunk<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When enabled, you do not have to wait until buckets are repaired, to start Splunk<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When enabled, you might observe a slight performance degradation<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br><br># The following options can be set either per index or globally (as defaults for all indexes).<br># Defaults set globally are overridden if set on a per-index basis.<br><br>maxWarmDBCount = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The maximum number of warm buckets.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Warm buckets are located in the &lt;homePath&gt; for the index. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to zero, it will not retain any warm buckets (will roll them to cold as soon as it can)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 300.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxTotalDataSizeMB = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The maximum size of an index (in MB). <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If an index grows larger than the maximum size, the oldest data is frozen.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This parameter only applies to hot, warm, and cold buckets. &nbsp;It does not apply to thawed buckets.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 500000.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>rotatePeriodInSecs = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls the service period (in seconds): how often splunkd performs certain housekeeping tasks. &nbsp;Among these tasks are:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Check if a new hotdb needs to be created.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Check if there are any cold DBs that should be frozen.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Check whether buckets need to be moved out of hot and cold DBs, due to respective<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size constraints (i.e., homePath.maxDataSizeMB and coldPath.maxDataSizeMB)<br>&nbsp;&nbsp;&nbsp;&nbsp;* This value becomes the default value of the rotatePeriodInSecs attribute for all volumes (see rotatePeriodInSecs in the Volumes section)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 60.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>frozenTimePeriodInSecs = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Number of seconds after which indexed data rolls to frozen.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify a coldToFrozenScript, data is deleted when rolled to frozen.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: Every event in the DB must be older than frozenTimePeriodInSecs before it will roll. Then, the DB <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will be frozen the next time splunkd checks (based on rotatePeriodInSecs attribute).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 188697600 (6 years).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>warmToColdScript = &lt;script path&gt; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies a script to run when moving data from warm to cold. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This attribute is supported for backwards compatibility with versions older than 4.0. &nbsp;Migrating data across <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filesystems is now handled natively by splunkd. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you specify a script here, the script becomes responsible for moving the event data, and Splunk-native data <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;migration will not be used.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The script must accept two arguments:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* First: the warm directory (bucket) to be rolled to cold.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Second: the destination in the cold path.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Searches and other activities are paused while the script is running.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Contact Splunk Support (http://www.splunk.com/page/submit_issue) if you need help configuring this setting.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The script must be in $SPLUNK_HOME/bin or a subdirectory thereof.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to empty.<br><br>coldToFrozenScript = [path to script interpreter] &lt;path to script&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies a script to run when data will leave the splunk index system. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Essentially, this implements any archival tasks before the data is<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deleted out of its default location.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Add "$DIR" (quotes included) to this setting on Windows (see below<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for details).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Script Requirements:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The script must accept one argument:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path to the bucket directory to archive.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Your script should work reliably.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If your script returns success (0), Splunk will complete deleting<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the directory from the managed index location.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If your script return failure (non-zero), Splunk will leave the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bucket in the index, and try calling your script again several<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;minutes later. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If your script continues to return failure, this will eventually<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cause the index to grow to maximum configured size, or fill the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disk.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Your script should complete in a reasonable amount of time.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the script stalls indefinitely, it will occupy slots.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* These slots will not be available for other freeze scripts.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This means that a stalling script for index1 may prevent freezing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of index2.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the string $DIR is present in this setting, it will be expanded to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the absolute path to the directory. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If $DIR is not present, the directory will be added to the end of the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;invocation line of the script.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is important for Windows. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For historical reasons, the entire string is broken up by<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shell-pattern expansion rules.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Since windows paths frequently include spaces, and the windows<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shell breaks on space, the quotes are needed for the script to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;understand the directory.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If your script can be run directly on your platform, you can specify<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;just the script.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Examples of this are:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* .bat and .cmd files on Windows<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* scripts set executable on UNIX with a #! shebang line pointing to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a valid interpreter.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can also specify an explicit path to an interpreter and the script.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Example: &nbsp;/path/to/my/installation/of/python.exe path/to/my/script.py<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunk ships with an example archiving script in that you SHOULD NOT USE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$SPLUNK_HOME/bin called coldToFrozenExample.py<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* DO NOT USE the example for production use, because:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 1 - It will be overwritten on upgrade.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 2 - You should be implementing whatever requirements you need in<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a script of your creation. &nbsp;If you have no such requirements,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use coldToFrozenDir<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Example configuration: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you create a script in bin/ called our_archival_script.py, you could use:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UNIX:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;coldToFrozenScript = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/bin/our_archival_script.py"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Windows:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;coldToFrozenScript = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/bin/our_archival_script.py" "$DIR"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The example script handles data created by different versions of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;splunk differently. Specifically data from before 4.2 and after are<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;handled differently. See "Freezing and Thawing" below:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The script must be in $SPLUNK_HOME/bin or a subdirectory thereof.<br><br>coldToFrozenDir = &lt;path to frozen archive&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An alternative to a coldToFrozen script - simply specify a destination path for the frozen archive<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunk will automatically put frozen buckets in this directory<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For information on how buckets created by different versions are<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;handled, see "Freezing and Thawing" below.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If both coldToFrozenDir and coldToFrozenScript are specified, coldToFrozenDir will take precedence<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* May NOT contain a volume reference.<br><br># Freezing and Thawing (this should move to web docs<br>4.2 and later data: &nbsp;<br>&nbsp;&nbsp;* To archive: remove files except for the rawdata directory, since rawdata<br>&nbsp;&nbsp;&nbsp;&nbsp;contains all the facts in the bucket.<br>&nbsp;&nbsp;* To restore: run splunk rebuild &lt;bucket_dir&gt; on the archived bucket, then<br>&nbsp;&nbsp;&nbsp;&nbsp;atomically move the bucket to thawed for that index<br>4.1 and earlier data:<br>&nbsp;&nbsp;* To archive: gzip the .tsidx files, as they are highly compressable but not<br>&nbsp;&nbsp;&nbsp;&nbsp;recreateable<br>&nbsp;&nbsp;* To restore: unpack the tsidx files within the bucket, then atomically move<br>&nbsp;&nbsp;&nbsp;&nbsp;the bucket to thawed for that index<br><br>compressRawdata = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This parameter is ignored. The splunkd process always compresses raw data.<br><br>maxConcurrentOptimizes = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The number of concurrent optimize processes that can run against the hot DB.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This number should be increased if: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* There are always many small tsidx files in the hot DB.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* After rolling, there are many tsidx files in warm or cold DB.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 6<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxDataSize = &lt;positive integer&gt;|auto|auto_high_volume<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The maximum size in MB for a hot DB to reach before a roll to warm is triggered.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifying "auto" or "auto_high_volume" will cause Splunk to autotune this parameter (recommended).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You should use "auto_high_volume" for high-volume indexes (such as the main<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index); otherwise, use "auto". &nbsp;A "high volume index" would typically be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;considered one that gets over 10GB of data per day.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "auto", which sets the size to 750MB.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* "auto_high_volume" sets the size to 10GB on 64-bit, and 1GB on 32-bit systems.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Although the maximum value you can set this is 1048576 MB, which corresponds to 1 TB, a reasonable <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number ranges anywhere from 100 to 50000. &nbsp;Before proceeding with any higher value, please seek<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;approval of Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you specify an invalid number or string, maxDataSize will be auto tuned.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: The maximum size of your warm buckets may slightly exceed 'maxDataSize', due to post-processing and <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;timing issues with the rolling policy.<br><br>rawFileSizeBytes = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Deprecated in version 4.2 and later. We will ignore this value.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Rawdata chunks are no longer stored in individual files.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you really need to optimize the new rawdata chunks (highly unlikely), edit rawChunkSizeBytes<br><br>rawChunkSizeBytes = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Target uncompressed size in bytes for individual raw slice in the rawdata journal of the index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 131072 (128KB).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If 0 is specified, rawChunkSizeBytes will be set to the default value.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: rawChunkSizeBytes only specifies a target chunk size. The actual chunk size may be slightly larger <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by an amount proportional to an individual event size.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* WARNING: This is an advanced parameter. Only change it if you are instructed to do so by Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 18446744073709551615<br><br>minRawFileSyncSecs = &lt;nonnegative decimal&gt;|disable<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* How frequently we force a filesystem sync while compressing journal slices. &nbsp;During this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;interval, uncompressed slices are left on disk even after they are compressed. &nbsp;Then we<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;force a filesystem sync of the compressed journal and remove the accumulated uncompressed files.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If 0 is specified, we force a filesystem sync after every slice completes compressing.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifying "disable" disables syncing entirely: uncompressed slices are removed as soon as compression is complete<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "disable".<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Some filesystems are very inefficient at performing sync operations, so only enable this if<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you are sure it is needed<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* No exponent may follow the decimal.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 18446744073709551615<br><br>maxMemMB = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The amount of memory to allocate for indexing. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This amount of memory will be allocated PER INDEX THREAD, or, if indexThreads is set to 0, once per index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* IMPORTANT: &nbsp;Calculate this number carefully. splunkd will crash if you set this number higher than the amount<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of memory available.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 5.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The default is recommended for all environments.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br>&nbsp;&nbsp;&nbsp;<br>blockSignSize = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Controls how many events make up a block for block signatures. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this is set to 0, block signing is disabled for this index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A recommended value is 100.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 2000<br><br>maxHotSpanSecs = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Upper bound of timespan of hot/warm buckets in seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 7776000 seconds (90 days).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: If you set this too small, you can get an explosion of hot/warm<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buckets in the filesystem.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you set this parameter to less than 3600, it will be automatically reset to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3600, which will then activate snapping behavior (see below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is an advanced parameter that should be set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with care and understanding of the characteristics of your data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to 3600 (1 hour), or 86400 (1 day), becomes also the lower bound<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of hot bucket timespans. &nbsp;Further, snapping behavior (i.e. ohSnap)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is activated, whereby hot bucket boundaries will be set at exactly the hour<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or day mark, relative to local midnight.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxHotIdleSecs = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Provides a ceiling for buckets to stay in hot status without receiving any data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If a hot bucket receives no data for more than maxHotIdleSecs seconds, Splunk rolls it to warm.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This setting operates independently of maxHotBuckets, which can also cause hot buckets to roll.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A value of 0 turns off the idle check (equivalent to infinite idle time).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxHotBuckets = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Maximum hot buckets that can exist per index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When maxHotBuckets is exceeded, Splunk rolls the least recently used (LRU) hot bucket to warm.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Both normal hot buckets and quarantined hot buckets count towards this total.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This setting operates independently of maxHotIdleSecs, which can also cause hot buckets to roll.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 3.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>quarantinePastSecs = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Events with timestamp of quarantinePastSecs older than "now" will be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dropped into quarantine bucket.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 77760000 (900 days).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is a mechanism to prevent the main hot buckets from being polluted with<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fringe events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>quarantineFutureSecs = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Events with timestamp of quarantineFutureSecs newer than "now" will be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dropped into quarantine bucket.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 2592000 (30 days).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This is a mechanism to prevent main hot buckets from being polluted with<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fringe events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxMetaEntries = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Sets the maximum number of unique lines in .data files in a bucket, which may help to reduce memory consumption<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If exceeded, a hot bucket is rolled to prevent further increase<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If your buckets are rolling due to Strings.data hitting this limit, the culprit may be the 'punct' field<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in your data. &nbsp;If you do not use punct, it may be best to simply disable this (see props.conf.spec)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* There is a delta between when maximum is exceeded and bucket is rolled.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This means a bucket may end up with epsilon more lines than specified, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but this is not a major concern unless excess is significant<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to 0, this setting is ignored (it is treated as infinite)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>syncMeta = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When "true", a sync operation is called before file descriptor is closed on metadata file updates.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This functionality was introduced to improve integrity of metadata files, especially in regards <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to operating system crashes/machine failures.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: Do not change this parameter without the input of a Splunk support professional.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>serviceMetaPeriod = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defines how frequently metadata is synced to disk, in seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 25 (seconds).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You may want to set this to a higher value if the sum of your metadata file sizes is larger than many <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tens of megabytes, to avoid the hit on I/O in the indexing fast path.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>partialServiceMetaPeriod = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Related to serviceMetaPeriod. &nbsp;If set, it enables metadata sync every<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;integer&gt; seconds, but only for records where the sync can be done<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;efficiently in-place, without requiring a full re-write of the metadata<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file. &nbsp;Records that require full re-write will be synced at serviceMetaPeriod.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* &lt;integer&gt; specifies how frequently it should sync. &nbsp;Zero means that this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;feature is turned off and serviceMetaPeriod is the only time when metadata<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sync happens.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the value of partialServiceMetaPeriod is greater than serviceMetaPeriod,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this setting will have no effect.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default it is turned off (zero).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This parameter is ignored if serviceOnlyAsNeeded = true (the default).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>throttleCheckPeriod = &lt;positive integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defines how frequently Splunk checks for index throttling condition, in seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In seconds; defaults to 15<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: Do not change this parameter without the input of a Splunk Support professional.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>maxTimeUnreplicatedWithAcks = &lt;nonnegative decimal&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Important if you have enabled acks on forwarders and have replication enabled (via Clustering)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This parameter puts an upper limit on how long events can sit unacked in a raw slice<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In seconds; defaults to 60<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To disable this, you can set to 0, but this is NOT recommended!!!<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: This is an advanced parameter; make sure you understand the settings on all your forwarders before changing this. &nbsp;This number should not exceed ack timeout configured on any forwarders, and should indeed be set to at most half of the minimum value of that timeout. &nbsp;You can find this setting in outputs.conf readTimeout setting, under tcpout stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 2147483647<br><br>maxTimeUnreplicatedNoAcks = &lt;nonnegative decimal&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Important only if replication is enabled for this index, otherwise ignored<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This parameter puts an upper limit on how long an event can sit in raw slice. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* if there are any ack''d events sharing this raw slice, this paramater will not apply (maxTimeUnreplicatedWithAcks will be used instead)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In seconds; defaults to 60<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 2147483647<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To disable this, you can set to 0; please be careful and understand the consequences before changing this parameter<br><br>isReadOnly = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set to true to make an index read-only.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If true, no new events can be added to the index, but the index is still searchable.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must restart splunkd after changing this parameter; index reload will not suffice.<br><br>homePath.maxDataSizeMB = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum size of homePath (which contains hot and warm buckets).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this size is exceeded, Splunk will move buckets with the oldest value of latest time (for a given bucket)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into the cold DB until homePath is below the maximum size.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this attribute is missing or set to 0, Splunk will not constrain size of homePath.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>coldPath.maxDataSizeMB = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum size of coldPath (which contains cold buckets).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this size is exceeded, Splunk will freeze buckets with the oldest value of latest time (for a given bucket) <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;until coldPath is below the maximum size.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If this attribute is missing or set to 0, Splunk will not constrain size of coldPath<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If we freeze buckets due to enforcement of this policy parameter, and<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;coldToFrozenScript and/or coldToFrozenDir archiving parameters are also<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set on the index, these parameters *will* take into effect<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>disableGlobalMetadata = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: This option was introduced in 4.3.3, but as of 5.0 it is obsolete and ignored if set.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* It used to disable writing to the global metadata. &nbsp;In 5.0 global metadata was removed.<br><br>repFactor = &lt;nonnegative integer&gt;|auto<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Only relevant if this instance is a clustering slave (but see note about "auto" below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* See server.conf spec for details on clustering configuration.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Value of 0 turns off replication for this index.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to "auto", slave will use whatever value the master is configured with<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>minStreamGroupQueueSize = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Minimum size of the queue that stores events in memory before committing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;them to a tsidx file. &nbsp;As Splunk operates, it continually adjusts this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size internally. &nbsp;Splunk could decide to use a small queue size and thus<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generate tiny tsidx files under certain unusual circumstances, such as<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;file system errors. &nbsp;The danger of a very low minimum is that it can<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generate very tiny tsidx files with one or very few events, making it<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;impossible for splunk-optimize to catch up and optimize the tsidx files<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into reasonably sized files.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 2000.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Only set this value if you have been advised to by Splunk Support.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>streamingTargetTsidxSyncPeriodMsec = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Period we force sync tsidx files on streaming targets. &nbsp;This setting is needed<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for multi-site clustering where streaming targets may be primary.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* if set to 0, we never sync (equivalent to infinity)<br><br>#******************************************************************************&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br># Volume settings. &nbsp;This section describes settings that affect the volume-<br># optional and volume-mandatory parameters only.<br>#<br># All volume stanzas begin with "volume:". For example:<br># &nbsp;&nbsp;[volume:volume_name]<br># &nbsp;&nbsp;path = /foo/bar<br>#<br># These volume stanzas can then be referenced by individual index parameters,<br># e.g. homePath or coldPath. &nbsp;To refer to a volume stanza, use the<br># "volume:" prefix. For example, to set a cold DB to the example stanza above,<br># in index "hiro", use:<br># &nbsp;&nbsp;[hiro]<br># &nbsp;&nbsp;coldPath = volume:volume_name/baz<br># This will cause the cold DB files to be placed under /foo/bar/baz. &nbsp;If the<br># volume spec is not followed by a path (e.g. "coldPath=volume:volume_name"),<br># then the cold path would be composed by appending the index name to the<br># volume name ("/foo/bar/hiro").<br>#<br># Note: thawedPath may not be defined in terms of a volume. &nbsp;<br># Thawed allocations are manually controlled by Splunk administrators,<br># typically in recovery or archival/review scenarios, and should not<br># trigger changes in space automatically used by normal index activity.<br>#******************************************************************************&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>path = &lt;path on server&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Required. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Points to the location on the file system where all databases that use this volume will <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reside. &nbsp;You must make sure that this location does not overlap with that of any other <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;volume or index database.<br><br>maxVolumeDataSizeMB = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set, this attribute limits the total size of all databases <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that reside on this volume to the maximum size specified, in MB. &nbsp;Note that<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this it will act only on those indexes which reference this volume, not<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on the total size of the path set in the path attribute of this volume.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the size is exceeded, Splunk will remove buckets with the oldest value of latest time (for a given bucket)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;across all indexes in the volume, until the volume is below the maximum size. &nbsp;This is the trim operation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note that this can cause buckets to be chilled [moved to cold] directly from a hot DB, if those <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buckets happen to have the least value of latest-time (LT) across all indexes in the volume.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br>rotatePeriodInSecs = &lt;nonnegative integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies period of trim operation for this volume.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If not set, the value of global rotatePeriodInSecs attribute is inherited.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Highest legal value is 4294967295<br><br></font></code>
<h3> <a name="indexesconf_indexes.conf.example"><span class="mw-headline" id="indexes.conf.example">indexes.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example indexes.conf. &nbsp;Use this file to configure indexing properties.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># indexes.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br><br># The following example defines a new high-volume index, called "hatch", and<br># sets this to be the default index for both incoming data and search.<br>#<br># Note that you may want to adjust the indexes that your roles have access to<br># when creating indexes (in authorize.conf)<br><br>defaultDatabase = hatch<br><br>[hatch]<br><br>homePath &nbsp;&nbsp;= $SPLUNK_DB/hatchdb/db<br>coldPath &nbsp;&nbsp;= $SPLUNK_DB/hatchdb/colddb<br>thawedPath = $SPLUNK_DB/hatchdb/thaweddb<br>maxDataSize = 10000<br>maxHotBuckets = 10<br><br><br><br># The following example changes the default amount of space used on a per-index basis.<br><br>[default]<br>maxTotalDataSizeMB = 650000<br><br><br># The following example changes the time data is kept around by default.<br># It also sets an export script. &nbsp;NOTE: You must edit this script to set export location before <br># running it.<br><br>[default]<br>maxWarmDBCount = 200<br>frozenTimePeriodInSecs = 432000<br>rotatePeriodInSecs = 30<br>coldToFrozenScript = "$SPLUNK_HOME/bin/python" "$SPLUNK_HOME/bin/myColdToFrozenScript.py"<br><br># This example freezes buckets on the same schedule, but lets Splunk do the freezing process as opposed to a script<br>[default]<br>maxWarmDBCount = 200<br>frozenTimePeriodInSecs = 432000<br>rotatePeriodInSecs = 30<br>coldToFrozenDir = "$SPLUNK_HOME/myfrozenarchive"<br><br>### This example demonstrates the use of volumes ###<br><br># volume definitions; prefixed with "volume:"<br><br>[volume:hot1]<br>path = /mnt/fast_disk<br>maxVolumeDataSizeMB = 100000<br><br>[volume:cold1]<br>path = /mnt/big_disk<br># maxVolumeDataSizeMB not specified: no data size limitation on top of the existing ones<br><br>[volume:cold2]<br>path = /mnt/big_disk2<br>maxVolumeDataSizeMB = 1000000<br><br># index definitions<br><br>[idx1]<br>homePath = volume:hot1/idx1<br>coldPath = volume:cold1/idx1<br><br># thawedPath must be specified, and cannot use volume: syntax<br># choose a location convenient for reconstitition from archive goals<br># For many sites, this may never be used.<br>thawedPath = $SPLUNK_DB/idx1/thaweddb<br><br>[idx2]<br># note that the specific indexes must take care to avoid collisions<br>homePath = volume:hot1/idx2<br>coldPath = volume:cold2/idx2<br>thawedPath = $SPLUNK_DB/idx2/thaweddb<br><br>[idx3]<br>homePath = volume:hot1/idx3<br>coldPath = volume:cold2/idx3<br>thawedPath = $SPLUNK_DB/idx3/thaweddb<br><br>### Indexes may be allocated space in effective groups by sharing volumes &nbsp;###<br><br># perhaps we only want to keep 100GB of summary data and other<br># low-volume information<br>[volume:small_indexes]<br>path = /mnt/splunk_indexes<br>maxVolumeDataSizeMB = 100000<br><br># and this is our main event series, allowing 50 terabytes<br>[volume:large_indexes]<br>path = /mnt/splunk_indexes<br>maxVolumeDataSizeMB = 50000000<br><br># summary and rare_data together will be limited to 100GB<br>[summary]<br>homePath=volume:small_indexes/summary/db<br>coldPath=volume:small_indexes/summary/colddb<br>thawedPath=$SPLUNK_DB/summary/thaweddb<br># low-volume indexes probably don't want a lot of hot buckets<br>maxHotBuckets = 2 <br># if the volume is quite low, and you have data sunset goals you may<br># want to have smaller buckets<br>maxDataSize = 500<br><br><br>[rare_data]<br>homePath=volume:small_indexes/rare_data/db<br>coldPath=volume:small_indexes/rare_data/colddb<br>thawedPath=$SPLUNK_DB/rare_data/thaweddb<br>maxHotBuckets = 2 <br><br># main, and any other large volume indexes you add sharing large_indexes<br># will be together be constrained to 50TB, separately from the 100GB of<br># the small_indexes<br>[main]<br>homePath=volume:large_indexes/main/db<br>coldPath=volume:large_indexes/main/colddb<br>thawedPath=$SPLUNK_DB/main/thaweddb<br># large buckets and more hot buckets are desirable for higher volume<br># indexes, and ones where the variations in the timestream of events is<br># hard to predict.<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br><br>[idx1_large_vol]<br>homePath=volume:large_indexes/idx1_large_vol/db<br>coldPath=volume:large_indexes/idx1_large_vol/colddb<br>homePath=$SPLUNK_DB/idx1_large/thaweddb<br># this index will exceed the default of .5TB requiring a change to maxTotalDataSizeMB<br>maxTotalDataSizeMB = 750000<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br># but the data will only be retained for about 30 days<br>frozenTimePeriodInSecs = 2592000<br><br>### This example demonstrates database size constraining ###<br><br># In this example per-database constraint is combined with volumes. &nbsp;While a<br># central volume setting makes it easy to manage data size across multiple<br># indexes, there is a concern that bursts of data in one index may<br># significantly displace data from others. &nbsp;The homePath.maxDataSizeMB setting<br># can be used to assure that no index will ever take more than certain size,<br># therefore alleviating the concern.<br><br># global settings<br><br># will be inherited by all indexes: no database will exceed 1TB<br>homePath.maxDataSizeMB = 1000000<br><br># volumes<br><br>[volume:caliente]<br>path = /mnt/fast_disk<br>maxVolumeDataSizeMB = 100000<br><br>[volume:frio]<br>path = /mnt/big_disk<br>maxVolumeDataSizeMB = 1000000<br><br># and this is our main event series, allowing about 50 terabytes<br>[volume:large_indexes]<br>path = /mnt/splunk_indexes<br>maxVolumeDataSizeMB = 50000000<br><br># indexes<br><br>[i1]<br>homePath = volume:caliente/i1<br># homePath.maxDataSizeMB is inherited<br>coldPath = volume:frio/i1<br># coldPath.maxDataSizeMB not specified: no limit - old-style behavior<br><br>thawedPath = $SPLUNK_DB/i1/thaweddb<br><br>[i2]<br>homePath = volume:caliente/i2<br># overrides the default maxDataSize<br>homePath.maxDataSizeMB = 1000 &nbsp;<br>coldPath = volume:frio/i2<br># limits the cold DB's<br>coldPath.maxDataSizeMB = 10000 &nbsp;<br>thawedPath = $SPLUNK_DB/i2/thaweddb<br><br>[i3]<br>homePath = /old/style/path<br>homePath.maxDataSizeMB = 1000<br>coldPath = volume:frio/i3<br>coldPath.maxDataSizeMB = 10000<br>thawedPath = $SPLUNK_DB/i3/thaweddb<br><br># main, and any other large volume indexes you add sharing large_indexes<br># will together be constrained to 50TB, separately from the rest of<br># the indexes<br>[main]<br>homePath=volume:large_indexes/main/db<br>coldPath=volume:large_indexes/main/colddb<br>thawedPath=$SPLUNK_DB/main/thaweddb<br># large buckets and more hot buckets are desirable for higher volume indexes<br>maxDataSize = auto_high_volume<br>maxHotBuckets = 10<br><br><br></font></code>

<a name="inputsconf"></a><h2> <a name="inputsconf_inputs.conf"><span class="mw-headline" id="inputs.conf">inputs.conf</span></a></h2>
<p>The following are the spec and example files for inputs.conf.
</p>
<h3> <a name="inputsconf_inputs.conf.spec"><span class="mw-headline" id="inputs.conf.spec">inputs.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br><br># This file contains possible attributes and values you can use to configure inputs,<br># distributed inputs such as forwarders, and file system monitoring in inputs.conf.<br>#<br># There is an inputs.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place an inputs.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see inputs.conf.example.<br># You must restart Splunk to enable new configurations.<br>#<br># To learn more about configuration files (including precedence), see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#*******<br># GENERAL SETTINGS:<br># The following attribute/value pairs are valid for all input types (except file system change monitor,<br># which is described in a separate section in this file).<br># You must first enter a stanza header in square brackets, specifying the input type. See further down <br># in this file for examples. &nbsp;&nbsp;<br># Then, use any of the following attribute/value pairs.<br>#*******<br><br>host = &lt;string&gt;<br>* Sets the host key/field to a static value for this stanza.<br>* Primarily used to control the host field, which will be used for events coming in<br>&nbsp;&nbsp;via this input stanza.<br>* Detail: Sets the host key's initial value. The key is used during parsing/indexing, <br>&nbsp;&nbsp;in particular to set the host field. It is also the host field used at search time.<br>* As a convenience, the chosen string is prepended with 'host::'.<br>* WARNING: Do not quote the &lt;string&gt; value: host=foo, not host="foo".<br>* If set to '$decideOnStartup', will be interpreted as hostname of executing machine;<br>&nbsp;&nbsp;such interpretation will occur on each splunkd startup. &nbsp;This is the default.<br>* If you are running multiple instances of splunk on the same system (hardware or VM),<br>&nbsp;&nbsp;you should probably choose unique values for host to differentiate your data,<br>&nbsp;&nbsp;e.g. myhost-sh-1 or myhost-idx-2.<br><br>index = &lt;string&gt;<br>* Sets the index to store events from this input.<br>* Primarily used to specify the index to store events coming in via this input stanza.<br>* Detail: Sets the index key's initial value. The key is used when selecting an<br>&nbsp;&nbsp;index to store the events.<br>* Defaults to "main" (or whatever you have set as your default index).<br><br>source = &lt;string&gt;<br>* Sets the source key/field for events from this input.<br>* NOTE: Overriding the source key is generally not recommended. &nbsp;Typically, the<br>&nbsp;&nbsp;input layer will provide a more accurate string to aid problem<br>&nbsp;&nbsp;analysis and investigation, accurately recording the file from which the data<br>&nbsp;&nbsp;was retreived. &nbsp;Please consider use of source types, tagging, and search<br>&nbsp;&nbsp;wildcards before overriding this value.<br>* Detail: Sets the source key's initial value. The key is used during<br>&nbsp;&nbsp;parsing/indexing, in particular to set the source field during<br>&nbsp;&nbsp;indexing. &nbsp;It is also the source field used at search time.<br>* As a convenience, the chosen string is prepended with 'source::'.<br>* WARNING: Do not quote the &lt;string&gt; value: source=foo, not source="foo".<br>* Defaults to the input file path.<br><br>sourcetype = &lt;string&gt;<br>* Sets the sourcetype key/field for events from this input.<br>* Primarily used to explicitly declare the source type for this data, as opposed<br>&nbsp;&nbsp;to allowing it to be determined via automated methods. &nbsp;This is typically<br>&nbsp;&nbsp;important both for searchability and for applying the relevant configuration for this<br>&nbsp;&nbsp;type of data during parsing and indexing.<br>* Detail: Sets the sourcetype key's initial value. The key is used during<br>&nbsp;&nbsp;parsing/indexing, in particular to set the source type field during<br>&nbsp;&nbsp;indexing. It is also the source type field used at search time.<br>* As a convenience, the chosen string is prepended with 'sourcetype::'.<br>* WARNING: Do not quote the &lt;string&gt; value: sourcetype=foo, not sourcetype="foo".<br>* If unset, Splunk picks a source type based on various aspects of the data.<br>&nbsp;&nbsp;There is no hard-coded default.<br><br>queue = [parsingQueue|indexQueue]<br>* Specifies where the input processor should deposit the events it reads.<br>* Set queue to "parsingQueue" to apply props.conf and other parsing rules to your data. For more <br>information about props.conf and rules for timestamping and linebreaking, refer to props.conf and the <br>online documentation at http://docs.splunk.com/Documentation.<br>* Set queue to "indexQueue" to send your data directly into the index.<br>* Defaults to parsingQueue.<br><br># Pipeline Key defaulting.<br><br>* Pipeline keys in general can be defaulted in inputs stanzas.<br>* The list of user-available modifiable pipeline keys is described in transforms.conf.spec,<br>&nbsp;&nbsp;See transforms.conf.spec for further information on these keys.<br>* The currently-defined keys which are available literally in inputs stanzas<br>&nbsp;&nbsp;are as follows:<br>queue = &lt;value&gt;<br>_raw &nbsp;= &lt;value&gt;<br>_meta = &lt;value&gt;<br>_time = &lt;value&gt;<br>* Inputs have special support for mapping host, source, sourcetype, and index<br>&nbsp;&nbsp;to their metadata names such as host -&gt; Metadata:Host<br>* Defaulting these values is not recommended, and is<br>&nbsp;&nbsp;generally only useful as a workaround to other product issues.<br>* Defaulting these keys in most cases will override the default behavior of<br>&nbsp;&nbsp;input processors; but this behavior is not guaranteed in all cases.<br>* Values defaulted here, as with all values provided by inputs, may be<br>&nbsp;&nbsp;altered by transforms at parse-time.<br><br># ***********<br># This section contains options for routing data using inputs.conf rather than outputs.conf. <br># Note concerning routing via inputs.conf:<br># This is a simplified set of routing options you can use as data is coming in. <br># For more flexible options or details on configuring required or optional settings, refer to <br># outputs.conf.spec.<br><br>_TCP_ROUTING = &lt;tcpout_group_name&gt;,&lt;tcpout_group_name&gt;,&lt;tcpout_group_name&gt;, ...<br>* Comma-separated list of tcpout group names.<br>* Using this, you can selectively forward the data to specific indexer(s).<br>* Specify the tcpout group the forwarder should use when forwarding the data.<br>&nbsp;&nbsp;The tcpout group names are defined in outputs.conf with [tcpout:&lt;tcpout_group_name&gt;].<br>* Defaults to groups specified in "defaultGroup" in [tcpout] stanza in outputs.conf.<br>* To forward data from the "_internal" index, _TCP_ROUTING must explicitly be set to either "*" or<br>&nbsp;&nbsp;a specific splunktcp target group.<br><br>_SYSLOG_ROUTING = &lt;syslog_group_name&gt;,&lt;syslog_group_name&gt;,&lt;syslog_group_name&gt;, ...<br>* Comma-separated list of syslog group names. <br>* Using this, you can selectively forward the data to specific destinations as syslog events.<br>* Specify the syslog group to use when forwarding the data.<br>&nbsp;&nbsp;The syslog group names are defined in outputs.conf with [syslog:&lt;syslog_group_name&gt;].<br>* Defaults to groups present in "defaultGroup" in [syslog] stanza in outputs.conf.<br>* The destination host must be configured in outputs.conf, using "server=[&lt;ip&gt;|&lt;servername&gt;]:&lt;port&gt;".<br><br>_INDEX_AND_FORWARD_ROUTING = &lt;string&gt;<br>* Only has effect if using selectiveIndexing feature in outputs.conf.<br>* If set for any input stanza, should cause all data coming from that input<br>&nbsp;&nbsp;stanza to be labeled with this setting.<br>* When selectiveIndexing is in use on a forwarder:<br>&nbsp;&nbsp;* data without this label will not be indexed by that forwarder.<br>&nbsp;&nbsp;* data with this label will be indexed in addition to any forwarding.<br>* This setting does not actually cause data to be forwarded or not forwarded in<br>&nbsp;&nbsp;any way, nor does it control where the data is forwarded in multiple-forward path<br>&nbsp;&nbsp;cases.<br>* Defaults to not present.<br><br>#************<br># Blacklist<br>#************<br><br>[blacklist:&lt;path&gt;]<br>* Protect files on the filesystem from being indexed or previewed.<br>* Splunk will treat a file as blacklisted if it starts with any of the defined blacklisted &lt;paths&gt;.<br>* The preview endpoint will return and error when asked to preview a blacklisted file. <br>* The oneshot endpoint and command will also return an error.<br>* When a blacklisted file is monitored (monitor:// or batch://), filestatus endpoint will show an error.<br>* For fschange with sendFullEvent option enabled, contents of backlisted files will not be indexed.<br><br>#*******<br># Valid input types follow, along with their input-specific attributes:<br>#*******<br><br><br>#*******<br># MONITOR:<br>#*******<br><br>[monitor://&lt;path&gt;]<br>* This directs Splunk to watch all files in &lt;path&gt;. <br>* &lt;path&gt; can be an entire directory or just a single file.<br>* You must specify the input type and then the path, so put three slashes in your path if you are starting <br>at the root (to include the slash that goes before the root directory).<br><br># Additional attributes:<br><br>host_regex = &lt;regular expression&gt;<br>* If specified, &lt;regular expression&gt; extracts host from the path to the file for each input file. <br>&nbsp;&nbsp;&nbsp;&nbsp;* Detail: This feature examines the source key; if source is set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;explicitly in the stanza, that string will be matched, not the original filename.<br>* Specifically, the first group of the regex is used as the host. <br>* If the regex fails to match, the default "host =" attribute is used.<br>* If host_regex and host_segment are both set, host_regex will be ignored.<br>* Defaults to unset.<br><br>host_segment = &lt;integer&gt;<br>* If set to N, the Nth "/"-separated segment of the path is set as host. If host_segment=3, for example,<br>&nbsp;&nbsp;the third segment is used.<br>* If the value is not an integer or is less than 1, the default "host =" attribute is used.<br>* On Windows hosts, the drive letter and colon before the slash count as one segment.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example, if you set host_segment=3 and the monitor path is D:\logs\servers\host01, Splunk sets the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host as "servers" because that is the third segment.<br>* Defaults to unset.<br><br>whitelist = &lt;regular expression&gt;<br>* If set, files from this input are monitored only if their path matches the specified regex.<br>* Takes precedence over the deprecated _whitelist attribute, which functions the same way.<br><br>blacklist = &lt;regular expression&gt;<br>* If set, files from this input are NOT monitored if their path matches the specified regex.<br>* Takes precedence over the deprecated _blacklist attribute, which functions the same way.<br><br>Note concerning wildcards and monitor:<br>* You can use wildcards to specify your input path for monitored input. Use "..." for recursive directory <br>&nbsp;&nbsp;matching and "*" for wildcard matching in a single directory segment.<br>* "..." recurses through directories. This means that /foo/.../bar will match foo/bar, foo/1/bar, <br>&nbsp;&nbsp;foo/1/2/bar, etc. <br>* You can use multiple "..." specifications in a single input path. For example: /foo/.../bar/...<br>* The asterisk (*) matches anything in a single path segment; unlike "...", it does not recurse. &nbsp;For example, <br>&nbsp;&nbsp;/foo/*/bar matches the files /foo/bar, /foo/1/bar, /foo/2/bar, etc. However, it does not match /foo/1/2/bar. <br>&nbsp;&nbsp;A second example: /foo/m*r/bar matches /foo/mr/bar, /foo/mir/bar, /foo/moor/bar, etc.<br>* You can combine "*" and "..." as needed: foo/.../bar/* matches any file in the bar directory within the <br>&nbsp;&nbsp;specified path.<br><br>crcSalt = &lt;string&gt;<br>* Use this setting to force Splunk to consume files that have matching CRCs (cyclic redundancy checks). (Splunk only <br>&nbsp;&nbsp;performs CRC checks against the first few lines of a file. This behavior prevents Splunk from indexing the same <br>&nbsp;&nbsp;file twice, even though you may have renamed it -- as, for example, with rolling log files. However, because the <br>&nbsp;&nbsp;CRC is based on only the first few lines of the file, it is possible for legitimately different files to have <br>&nbsp;&nbsp;matching CRCs, particularly if they have identical headers.)<br>* If set, &lt;string&gt; is added to the CRC.<br>* If set to the literal string &lt;SOURCE&gt; (including the angle brackets), the full directory path to the source file <br>&nbsp;&nbsp;is added to the CRC. This ensures that each file being monitored has a unique CRC. &nbsp;&nbsp;When crcSalt is invoked, <br>&nbsp;&nbsp;it is usually set to &lt;SOURCE&gt;.<br>* Be cautious about using this attribute with rolling log files; it could lead to the log file being re-indexed <br>&nbsp;&nbsp;after it has rolled. <br>* Defaults to empty. <br><br>initCrcLength = &lt;integer&gt;<br>* This setting adjusts how much of a file Splunk reads before trying to identify whether it is a file that has<br>&nbsp;&nbsp;already been seen. &nbsp;You may want to adjust this if you have many files with common headers (comment headers,<br>&nbsp;&nbsp;long CSV headers, etc) and recurring filenames.<br>* CAUTION: Improper use of this setting will cause data to be reindexed. &nbsp;You may wish to consult with Splunk<br>&nbsp;&nbsp;Support before adjusting this value - the default is fine for most installations.<br>* Defaults to 256 (bytes).<br>* Must be in the range 256-1048576.<br><br>ignoreOlderThan = &lt;nonnegative integer&gt;[s|m|h|d]<br>* Causes the monitored input to stop checking files for updates if their modtime has passed this threshold.<br>&nbsp;&nbsp;This improves the speed of file tracking operations when monitoring directory hierarchies with large numbers<br>&nbsp;&nbsp;of historical files (for example, when active log files are colocated with old files that are no longer<br>&nbsp;&nbsp;being written to).<br>&nbsp;&nbsp;* As a result, do not select a cutoff that could ever occur for a file<br>&nbsp;&nbsp;&nbsp;&nbsp;you wish to index. &nbsp;Take downtime into account! &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;Suggested value: 14d , which means 2 weeks<br>* A file whose modtime falls outside this time window when seen for the first time will not be indexed at all.<br>* Defaults to 0, meaning no threshold.<br>* Special values for 0 (like '0h' or '0d') also mean the default of no threshold.<br><br>followTail = [0|1]<br>* WARNING: Use of followTail should be considered an advanced administrative action.<br>* Treat this setting as an 'action'. &nbsp;That is, bring splunk up with this<br>&nbsp;&nbsp;setting enabled. &nbsp;Wait enough time for splunk to identify the related files,<br>&nbsp;&nbsp;then disable the setting and restart splunk without it.<br>* DO NOT leave followTail enabled in an ongoing fashion.<br>* Do not use for rolling log files, or files whose names or paths vary.<br>* Can be used to force splunk to skip past all current data for a given stanza. <br>&nbsp;&nbsp;* In more detail: this is intended to mean that if you start up splunk with a<br>&nbsp;&nbsp;&nbsp;&nbsp;stanza configured this way, all data in the file at the time it is first<br>&nbsp;&nbsp;&nbsp;&nbsp;encountered will not be read. &nbsp;Only data arriving after that first<br>&nbsp;&nbsp;&nbsp;&nbsp;encounter time will be read.<br>&nbsp;&nbsp;* This can be used to "skip over" data from old log files, or old portions of<br>&nbsp;&nbsp;&nbsp;&nbsp;log files, to get started on current data right away.<br>* If set to 1, monitoring begins at the end of the file (like tail -f).<br>* If set to 0, Splunk will always start at the beginning of the file. <br>* Defaults to 0.<br><br>alwaysOpenFile = [0|1]<br>* Opens a file to check whether it has already been indexed.<br>* Only useful for files that do not update modtime.<br>* Only needed when monitoring files on Windows, mostly for IIS logs.<br>* This flag should only be used as a last resort, as it increases load and slows down indexing.<br>* Defaults to 0.<br><br>time_before_close = &lt;integer&gt;<br>* Modtime delta required before Splunk can close a file on EOF.<br>* Tells the system not to close files that have been updated in past &lt;integer&gt; seconds.<br>* Defaults to 3.<br><br>recursive = [true|false]<br>* If false, Splunk will not monitor subdirectories found within a monitored directory.<br>* Defaults to true.<br><br>followSymlink = [true|false]<br>* Tells Splunk whether or not to follow any symbolic links within a directory it is monitoring.<br>* If set to false, Splunk will ignore symbolic links found within a monitored directory.<br>* If set to true, Splunk will follow symbolic links and monitor files at the symbolic link's destination.<br>* Additionally, any whitelists or blacklists defined for the stanza also apply to files at the symbolic link's destination.<br>* Defaults to true. <br><br>_whitelist = ...<br>* This setting is deprecated. &nbsp;It is still honored, unless "whitelist" attribute also exists.<br><br>_blacklist = ...<br>* This setting is deprecated. &nbsp;It is still honored, unless "blacklist" attribute also exists.<br><br>dedicatedFD = ...<br>* This setting has been removed. &nbsp;It is no longer needed.<br><br>&nbsp;&nbsp;<br>#****************************************<br># BATCH &nbsp;("Upload a file" in Splunk Web):<br>#****************************************<br><br>NOTE: Batch should only be used for large archives of historic data. If you want to continuously monitor a directory <br>or index small archives, use monitor (see above). Batch reads in the file and indexes it, and then deletes the file <br>from the Splunk instance. <br><br>[batch://&lt;path&gt;]<br>* One time, destructive input of files in &lt;path&gt;.<br>* For continuous, non-destructive inputs of files, use monitor instead.<br><br># Additional attributes:<br><br>move_policy = sinkhole<br>* IMPORTANT: This attribute/value pair is required. You *must* include "move_policy = sinkhole" when defining batch <br>&nbsp;&nbsp;inputs.<br>* This loads the file destructively. &nbsp;<br>* Do not use the batch input type for files you do not want to consume destructively.<br>* As long as this is set, Splunk won't keep track of indexed files. Without the "move_policy = sinkhole" setting, <br>&nbsp;&nbsp;it won't load the files destructively and will keep a track of them. <br><br>host_regex = see MONITOR, above.<br>host_segment = see MONITOR, above.<br>crcSalt = see MONITOR, above.<br><br># IMPORTANT: The following attribute is not used by batch:<br># source = &lt;string&gt;<br><br>followSymlink = [true|false]<br>* Works similarly to monitor, but will not delete files after following a symlink out of the monitored directory.<br><br># The following settings work identically as for [monitor::] stanzas, documented above<br>host_regex = &lt;regular expression&gt;<br>host_segment = &lt;integer&gt;<br>crcSalt = &lt;string&gt;<br>recursive = [true|false]<br>whitelist = &lt;regular expression&gt;<br>blacklist = &lt;regular expression&gt;<br>initCrcLength = &lt;integer&gt;<br><br>#*******<br># TCP: <br>#*******<br><br>[tcp://&lt;remote server&gt;:&lt;port&gt;]<br>* Configure Splunk to listen on a specific port. <br>* If a connection is made from &lt;remote server&gt;, this stanza is used to configure the input.<br>* If &lt;remote server&gt; is empty, this stanza matches all connections on the specified port.<br>* Will generate events with source set to tcp:portnumber, &nbsp;for example: tcp:514<br>* If sourcetype is unspecified, will generate events with set sourcetype to tcp-raw.<br><br># Additional attributes:<br><br>connection_host = [ip|dns|none]<br>* "ip" sets the host to the IP address of the system sending the data. <br>* "dns" sets the host to the reverse DNS entry for IP address of the system sending the data.<br>* "none" leaves the host as specified in inputs.conf, typically the splunk system hostname.<br>* Defaults to "dns".<br><br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>* Maximum size of the in-memory input queue. <br>* Defaults to 500KB.<br><br>persistentQueueSize = &lt;integer&gt;[KB|MB|GB|TB]<br>* Maximum size of the persistent queue file.<br>* Defaults to 0 (no persistent queue).<br>* If set to some value other than 0, persistentQueueSize must be larger than the in-memory queue size <br>&nbsp;&nbsp;(set by queueSize attribute in inputs.conf or maxSize settings in [queue] stanzas in server.conf).<br>* Persistent queues can help prevent loss of transient data. For information on persistent queues and how the <br>&nbsp;&nbsp;queueSize and persistentQueueSize settings interact, see the online documentation.<br><br>requireHeader = &lt;bool&gt;<br>* Require a header be present at the beginning of every stream.<br>* This header may be used to override indexing settings.<br>* Defaults to false.<br><br>listenOnIPv6 = &lt;no | yes | only&gt;<br>* Toggle whether this listening port will listen on IPv4, IPv6, or both<br>* If not present, the setting in the [general] stanza of server.conf will be used<br><br>acceptFrom = &lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to accept connections from. &nbsp;These rules are separated by commas or spaces<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>rawTcpDoneTimeout = &lt;seconds&gt;<br>* Specifies timeout value for sending Done-key.<br>* If a connection over this port remains idle after receiving data for specified seconds,<br>&nbsp;&nbsp;it adds a Done-key, thus declaring the last event has been completely received.<br>* Defaults to 10 second.<br><br>#*******<br># Data distribution:<br>#*******<br><br># Global settings for splunktcp. Used on the receiving side for data forwarded from a forwarder.<br><br>[splunktcp]<br>route = [has_key|absent_key:&lt;key&gt;:&lt;queueName&gt;;...]<br>* Settings for the light forwarder.<br>* Splunk sets these parameters automatically -- you DO NOT need to set them.<br>* The property route is composed of rules delimited by ';'.<br>* Splunk checks each incoming data payload via cooked tcp port against the route rules. <br>* If a matching rule is found, Splunk sends the payload to the specified &lt;queueName&gt;.<br>* If no matching rule is found, Splunk sends the payload to the default queue<br>&nbsp;&nbsp;specified by any queue= for this stanza. If no queue= key is set in<br>&nbsp;&nbsp;the stanza or globally, the events will be sent to the parsingQueue. <br><br>enableS2SHeartbeat = [true|false]<br>* This specifies the global keepalive setting for all splunktcp ports.<br>* This option is used to detect forwarders which may have become unavailable due to network, firewall, etc., problems.<br>* Splunk will monitor each connection for presence of heartbeat, and if the heartbeat is not seen for <br>&nbsp;&nbsp;s2sHeartbeatTimeout seconds, it will close the connection.<br>* Defaults to true (heartbeat monitoring enabled).<br><br>s2sHeartbeatTimeout = &lt;seconds&gt;<br>* This specifies the global timeout value for monitoring heartbeats.<br>* Splunk will close a forwarder connection if heartbeat is not seen for s2sHeartbeatTimeout seconds.<br>* Defaults to 600 seconds (10 minutes).<br><br>inputShutdownTimeout = &lt;seconds&gt;<br>* Used during shutdown to minimize data loss when forwarders are connected to a receiver. <br>&nbsp;&nbsp;During shutdown, the tcp input processor waits for the specified number of seconds and then <br>&nbsp;&nbsp;closes any remaining open connections. If, however, all connections close before the end of <br>&nbsp;&nbsp;the timeout period, shutdown proceeds immediately, without waiting for the timeout.<br><br>stopAcceptorAfterQBlock = &lt;seconds&gt;<br>* Specifies seconds to wait before closing splunktcp port. <br>* If splunk is unable to insert received data into the configured queue for<br>&nbsp;&nbsp;more than the specified number of seconds, it closes the splunktcp port. <br>* This action prevents forwarders establishing new connections to this indexer,<br>&nbsp;&nbsp;and existing forwarders will notice the port is closed upon test-connections<br>&nbsp;&nbsp;and migrate to other indexers. <br>* Once the queue unblocks, and TCP Input can continue processing data, Splunk<br>&nbsp;&nbsp;starts listening on the port again.<br>* This setting should not be adjusted lightly; extreme values may interact<br>&nbsp;&nbsp;poorly with other defaults.<br>* Defaults to 300 seconds (5 minutes).<br><br>listenOnIPv6 = &lt;no | yes | only&gt;<br>* Toggle whether this listening port will listen on IPv4, IPv6, or both<br>* If not present, the setting in the [general] stanza of server.conf will be used<br><br>acceptFrom = &lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to accept connections from. &nbsp;These rules are separated by commas or spaces<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>negotiateNewProtocol = [true|false]<br>* If set to true, allow forwarders that connect to this indexer (or specific port) to send data using the new forwarder protocol.<br>* If set to false, deny the use of the new forwarder protocol during connection negotation.<br>* Defaults to true.<br><br>concurrentChannelLimit = &lt;unsigned integer&gt;<br>* Each forwarder that connects to this indexer may use up to &lt;concurrentChannelLimit&gt; unique channel codes.<br>* In other words, each forwarder may have up to &lt;concurrentChannelLimit&gt; channels in flight concurrently.<br>* Splunk will close a forwarder connection if a forwarder attempts to exceed this value.<br>* This setting only applies when the new forwarder protocol is in use.<br>* Defaults to 300.<br><br># Forwarder-specific settings for splunktcp. <br><br>[splunktcp://[&lt;remote server&gt;]:&lt;port&gt;]<br>* This input stanza is used with Splunk instances receiving data from forwarders ("receivers"). See the topic <br>&nbsp;&nbsp;http://docs.splunk.com/Documentation/Splunk/latest/deploy/Aboutforwardingandreceivingdata for more information.<br>* This is the same as TCP, except the remote server is assumed to be a Splunk instance, most likely a forwarder. <br>* &lt;remote server&gt; is optional. &nbsp;If specified, will only listen for data from &lt;remote server&gt;.<br><br>connection_host = [ip|dns|none]<br>* For splunktcp, the host or connection_host will be used if the remote Splunk instance does not set a host, <br>&nbsp;&nbsp;or if the host is set to "&lt;host&gt;::&lt;localhost&gt;".<br>* "ip" sets the host to the IP address of the system sending the data. <br>* "dns" sets the host to the reverse DNS entry for IP address of the system sending the data.<br>* "none" leaves the host as specified in inputs.conf, typically the splunk system hostname.<br>* Defaults to "ip".<br><br>compressed = [true|false]<br>* Specifies whether receiving compressed data.<br>* Applies to non-SSL receiving only. There is no compression setting required for SSL.<br>* If set to true, the forwarder port(s) should also have compression turned on; otherwise, the receiver will <br>&nbsp;&nbsp;reject the connection.<br>* Defaults to false.<br><br>enableS2SHeartbeat = [true|false]<br>* This specifies the keepalive setting for the splunktcp port.<br>* This option is used to detect forwarders which may have become unavailable due to network, firewall, etc., problems.<br>* Splunk will monitor the connection for presence of heartbeat, and if the heartbeat is not seen for <br>&nbsp;&nbsp;s2sHeartbeatTimeout seconds, it will close the connection.<br>* This overrides the default value specified at the global [splunktcp] stanza.<br>* Defaults to true (heartbeat monitoring enabled).<br><br>s2sHeartbeatTimeout = &lt;seconds&gt;<br>* This specifies the timeout value for monitoring heartbeats.<br>* Splunk will will close the forwarder connection if heartbeat is not seen for s2sHeartbeatTimeout seconds.<br>* This overrides the default value specified at global [splunktcp] stanza.<br>* Defaults to 600 seconds (10 minutes).<br><br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>* Maximum size of the in-memory input queue.<br>* Defaults to 500KB.<br><br>negotiateNewProtocol = [true|false]<br>* See comments for [splunktcp].<br><br>concurrentChannelLimit = &lt;unsigned integer&gt;<br>* See comments for [splunktcp].<br><br>[splunktcp:&lt;port&gt;]<br>* This input stanza is same as [splunktcp://[&lt;remote server&gt;]:&lt;port&gt;] but without any remote server restriction<br>* Please see documentation for [splunktcp://[&lt;remote server&gt;]:&lt;port&gt;] for following supported settings:<br>connection_host = [ip|dns|none]<br>compressed = [true|false]<br>enableS2SHeartbeat = [true|false]<br>s2sHeartbeatTimeout = &lt;seconds&gt;<br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>negotiateNewProtocol = [true|false]<br>concurrentChannelLimit = &lt;unsigned integer&gt;<br><br># SSL settings for data distribution:<br><br>[splunktcp-ssl:&lt;port&gt;]<br>* Use this stanza type if you are receiving encrypted, parsed data from a forwarder.<br>* Set &lt;port&gt; to the port on which the forwarder is sending the encrypted data.<br>* Forwarder settings are set in outputs.conf on the forwarder.<br>* Compression for SSL is enabled by default. On forwarder you can still specify compression<br>&nbsp;&nbsp;using 'useClientSSLCompression' setting in outputs.conf. 'compressed' setting is used for<br>&nbsp;&nbsp;non-SSL. However, if 'compressed' is still specified for SSL, ensure that 'compressed'<br>&nbsp;&nbsp;setting is same as forwarder, as splunktcp protocol expects same 'compressed' setting from <br>&nbsp;&nbsp;forwarder as well.<br><br>connection_host = [ip|dns|none]<br>* For SplunkTCP, the host or connection_host will be used if the remote Splunk instance does not set a host, <br>&nbsp;&nbsp;or if the host is set to "&lt;host&gt;::&lt;localhost&gt;".<br>* "ip" sets the host to the IP address of the system sending the data. <br>* "dns" sets the host to the reverse DNS entry for IP address of the system sending the data.<br>* "none" leaves the host as specified in inputs.conf, typically the splunk system hostname.<br>* Defaults to "ip".<br><br>enableS2SHeartbeat = true|false<br>* See comments for [splunktcp:&lt;port&gt;].<br><br>s2sHeartbeatTimeout = &lt;seconds&gt;<br>* See comments for [splunktcp:&lt;port&gt;].<br><br>listenOnIPv6 = &lt;no | yes | only&gt;<br>* Toggle whether this listening port will listen on IPv4, IPv6, or both<br>* If not present, the setting in the [general] stanza of server.conf will be used<br><br>acceptFrom = &lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to accept connections from. &nbsp;These rules are separated by commas or spaces<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>negotiateNewProtocol = [true|false]<br>* See comments for [splunktcp].<br><br>concurrentChannelLimit = &lt;unsigned integer&gt;<br>* See comments for [splunktcp].<br><br>[tcp-ssl:&lt;port&gt;]<br>* Use this stanza type if you are receiving encrypted, unparsed data from a forwarder or third-party system.<br>* Set &lt;port&gt; to the port on which the forwarder/third-party system is sending unparsed, encrypted data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>listenOnIPv6 = &lt;no | yes | only&gt;<br>* Toggle whether this listening port will listen on IPv4, IPv6, or both<br>* If not present, the setting in the [general] stanza of server.conf will be used<br><br>acceptFrom = &lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to accept connections from. &nbsp;These rules are separated by commas or spaces<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>[SSL]<br>* Set the following specifications for SSL underneath this stanza name:<br><br>serverCert = &lt;path&gt;<br>* Full path to the server certificate.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>password = &lt;string&gt;<br>* Server certificate password, if any.<br><br>rootCA = &lt;string&gt;<br>* Certificate authority list (root file).<br><br>requireClientCert = [true|false]<br>* Determines whether a client must authenticate.<br>* Defaults to false.<br><br>sslVersions = &lt;string&gt;<br>* Comma-separated list of SSL versions to support<br>* The versions available are "ssl2", "ssl3", "tls1.0", "tls1.1", and "tls1.2"<br>* The special version "*" selects all supported versions. &nbsp;The version "tls"<br>&nbsp;&nbsp;selects all versions tls1.0 or newer<br>* If a version is prefixed with "-" it is removed from the list<br>* When configured in FIPS mode ssl2 and ssl3 are always disabled regardless of this configuration<br>* Defaults to "*,-ssl2". &nbsp;(anything newer than SSLv2)<br><br>supportSSLV3Only = [true|false]<br>* DEPRECATED. &nbsp;SSLv2 is now always disabled by default. &nbsp;The exact set of<br>&nbsp;&nbsp;SSL versions allowed is now configurable via the "sslVersions" setting above<br><br>cipherSuite = &lt;cipher suite string&gt;<br>* If set, uses the specified cipher string for the input processors.<br>* If not set, the default cipher string is used.<br>* Provided by OpenSSL. This is used to ensure that the server does not<br>&nbsp;&nbsp;accept connections using weak encryption protocols.<br><br>ecdhCurveName = &lt;string&gt;<br>* ECDH curve to use for ECDH key negotiation<br>* We only support named curves specified by their SHORT name. <br>* The list of valid named curves by their short/long names<br>* can be obtained by executing this command:<br>* $SPLUNK_HOME/bin/splunk cmd openssl ecparam -list_curves<br>* Default is empty string.<br><br>allowSslRenegotiation = true|false<br>* In the SSL protocol, a client may request renegotiation of the connection<br>&nbsp;&nbsp;settings from time to time.<br>* Setting this to false causes the server to reject all renegotiation<br>&nbsp;&nbsp;attempts, breaking the connection. &nbsp;This limits the amount of CPU a<br>&nbsp;&nbsp;single TCP connection can use, but it can cause connectivity problems<br>&nbsp;&nbsp;especially for long-lived connections.<br>* Defaults to true.<br><br>sslQuietShutdown = [true|false]<br>* Enables quiet shutdown mode in SSL<br>* Defaults to false<br><br><br>#*******<br># UDP:<br>#*******<br><br>[udp://&lt;remote server&gt;:&lt;port&gt;]<br>* Similar to TCP, except that it listens on a UDP port.<br>* Only one stanza per port number is currently supported.<br>* Configure Splunk to listen on a specific port. <br>* If &lt;remote server&gt; is specified, the specified port will only accept data from that server.<br>* If &lt;remote server&gt; is empty - [udp://&lt;port&gt;] - the port will accept data sent from any server.<br>* Will generate events with source set to udp:portnumber, for example: udp:514<br>* If sourcetype is unspecified, will generate events with set sourcetype to udp:portnumber .<br><br># Additional attributes:<br><br>connection_host = [ip|dns|none]<br>* "ip" sets the host to the IP address of the system sending the data. <br>* "dns" sets the host to the reverse DNS entry for IP address of the system sending the data.<br>* "none" leaves the host as specified in inputs.conf, typically the splunk system hostname.<br>* Defaults to "ip".<br><br>_rcvbuf = &lt;integer&gt;<br>* Specifies the receive buffer for the UDP port (in bytes). &nbsp;<br>* If the value is 0 or negative, it is ignored. &nbsp;<br>* Defaults to 1,572,864.<br>* Note: If the default value is too large for an OS, Splunk will try to set the value to 1572864/2. If that value also fails, <br>&nbsp;&nbsp;Splunk will retry with 1572864/(2*2). It will continue to retry by halving the value until it succeeds.<br><br>no_priority_stripping = [true|false]<br>* Setting for receiving syslog data. <br>* If this attribute is set to true, Splunk does NOT strip the &lt;priority&gt; syslog field from received events. <br>* NOTE: Do NOT include this attribute if you want to strip &lt;priority&gt;.<br>* Default is false.<br><br>no_appending_timestamp = [true|false]<br>* If this attribute is set to true, Splunk does NOT append a timestamp and host to received events.<br>* NOTE: Do NOT include this attribute if you want to append timestamp and host to received events.<br>* Default is false.<br>&nbsp;<br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>* Maximum size of the in-memory input queue.<br>* Defaults to 500KB.<br><br>persistentQueueSize = &lt;integer&gt;[KB|MB|GB|TB]<br>* Maximum size of the persistent queue file.<br>* Defaults to 0 (no persistent queue).<br>* If set to some value other than 0, persistentQueueSize must be larger than the in-memory queue size <br>&nbsp;&nbsp;(set by queueSize attribute in inputs.conf or maxSize settings in [queue] stanzas in server.conf).<br>* Persistent queues can help prevent loss of transient data. For information on persistent queues and how the <br>&nbsp;&nbsp;queueSize and persistentQueueSize settings interact, see the online documentation.<br><br>listenOnIPv6 = &lt;no | yes | only&gt;<br>* Toggle whether this port will listen on IPv4, IPv6, or both<br>* If not present, the setting in the [general] stanza of server.conf will be used<br><br>acceptFrom = &lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to accept data from. &nbsp;These rules are separated by commas or spaces<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>[udp:&lt;port&gt;]<br>* This input stanza is same as [udp://&lt;remote server&gt;:&lt;port&gt;] but without any remote server restriction<br>* Please see the documentation for [udp://&lt;remote server&gt;:&lt;port&gt;] to follow supported settings:<br>connection_host = [ip|dns|none]<br>_rcvbuf = &lt;integer&gt;<br>no_priority_stripping = [true|false]<br>no_appending_timestamp = [true|false]<br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>persistentQueueSize = &lt;integer&gt;[KB|MB|GB|TB]<br>listenOnIPv6 = &lt;no | yes | only&gt;<br>acceptFrom = &lt;network_acl&gt; ...<br><br>#*******<br># FIFO:<br>#*******<br><br>[fifo://&lt;path&gt;]<br>* This directs Splunk to read from a FIFO at the specified path.<br><br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>* Maximum size of the in-memory input queue.<br>* Defaults to 500KB.<br><br>persistentQueueSize = &lt;integer&gt;[KB|MB|GB|TB]<br>* Maximum size of the persistent queue file.<br>* Defaults to 0 (no persistent queue).<br>* If set to some value other than 0, persistentQueueSize must be larger than the in-memory queue size <br>&nbsp;&nbsp;(set by queueSize attribute in inputs.conf or maxSize settings in [queue] stanzas in server.conf).<br>* Persistent queues can help prevent loss of transient data. For information on persistent queues and how the <br>&nbsp;&nbsp;queueSize and persistentQueueSize settings interact, see the online documentation.<br><br><br>#*******<br># Scripted Input:<br>#*******<br><br>[script://&lt;cmd&gt;]<br>* Runs &lt;cmd&gt; at a configured interval (see below) and indexes the output. &nbsp;<br>* The &lt;cmd&gt; must reside in one of <br>&nbsp;&nbsp;* &nbsp;$SPLUNK_HOME/etc/system/bin/<br>&nbsp;&nbsp;* &nbsp;$SPLUNK_HOME/etc/apps/$YOUR_APP/bin/<br>&nbsp;&nbsp;* &nbsp;&nbsp;$SPLUNK_HOME/bin/scripts/<br>* Script path can be an absolute path, make use of an environment variable such as $SPLUNK_HOME, <br>&nbsp;&nbsp;or use the special pattern of an initial '.' as the first directory to<br>&nbsp;&nbsp;indicate a location inside the current app. &nbsp;&nbsp;Note that the '.' must be<br>&nbsp;&nbsp;followed by a platform-specific directory separator.<br>&nbsp;&nbsp;* For example, on UNIX:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[script://./bin/my_script.sh]<br>&nbsp;&nbsp;&nbsp;&nbsp;Or on Windows:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[script://.\bin\my_program.exe]<br>&nbsp;&nbsp;&nbsp;&nbsp;This '.' pattern is strongly recommended for app developers, and necessary<br>&nbsp;&nbsp;&nbsp;&nbsp;for operation in search head pooling environments.<br>* Splunk on Windows ships with several Windows-only scripted inputs. Check toward the end of the inputs.conf.example <br>&nbsp;&nbsp;for examples of the stanzas for specific Windows scripted inputs that you must add to your inputs.conf file.<br>* &lt;cmd&gt; can also be a path to a file that ends with a ".path" suffix. A file with this suffix is a special type of <br>&nbsp;&nbsp;pointer file that points to a command to be executed. &nbsp;Although the pointer file is bound by the same location<br>&nbsp;&nbsp;restrictions mentioned above, the command referenced inside it can reside anywhere on the file system. &nbsp;<br>&nbsp;&nbsp;This file must contain exactly one line: the path to the command to execute, optionally followed by <br>&nbsp;&nbsp;command line arguments. &nbsp;Additional empty lines and lines that begin with '#' are also permitted and will be ignored.<br><br>interval = [&lt;number&gt;|&lt;cron schedule&gt;]<br>* How often to execute the specified command (in seconds), or a valid cron schedule. <br>* NOTE: when a cron schedule is specified, the script is not executed on start-up.<br>* If specified as a number, may have a fractional component; e.g., 3.14<br>* Splunk's cron implementation does not currently support names of months/days.<br>* Defaults to 60.0 seconds.<br>* The special value 0 will force this scripted input to be executed non-stop; that is, as soon as script exits, we shall re-start it.<br><br>passAuth = &lt;username&gt;<br>* User to run the script as.<br>* If you provide a username, Splunk generates an auth token for that user and passes it to the script via stdin.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>queueSize = &lt;integer&gt;[KB|MB|GB]<br>* Maximum size of the in-memory input queue.<br>* Defaults to 500KB.<br><br>persistentQueueSize = &lt;integer&gt;[KB|MB|GB|TB]<br>* Maximum size of the persistent queue file.<br>* Defaults to 0 (no persistent queue).<br>* If set to some value other than 0, persistentQueueSize must be larger than the in-memory queue size <br>&nbsp;&nbsp;(set by queueSize attribute in inputs.conf or maxSize settings in [queue] stanzas in server.conf).<br>* Persistent queues can help prevent loss of transient data. For information on persistent queues and how the <br>&nbsp;&nbsp;queueSize and persistentQueueSize settings interact, see the online documentation.<br><br>index = &lt;index name&gt;<br>* The index to which the output will be indexed to.<br>* Note: this parameter will be passed as a command-line argument to &lt;cmd&gt; in the format: -index &lt;index name&gt;.<br>&nbsp;&nbsp;If the script does not need the index info, it can simply ignore this argument.<br>* If no index is specified, the default index will be used for the script output.<br><br>send_index_as_argument_for_path = [true|false]<br>* Defaults to true and we will pass the index as an argument when specified for stanzas<br>&nbsp;&nbsp;that begin with 'script://'<br>* The argument is passed as '-index &lt;index name&gt;'.<br>* To avoid passing the index as a command line argument, set this to false<br><br>start_by_shell = [true|false]<br>* If set to true, the specified command will be run via the OS's shell ("/bin/sh -c" on UNIX,<br>&nbsp;&nbsp;"cmd.exe /c" on Windows)<br>* If set to false, the program will be run directly without attempting to expand shell<br>&nbsp;&nbsp;metacharacters.<br>* Defaults to true on UNIX, false on Windows.<br>* Usually the default is fine, but you may want to explicitly set this to false for scripts<br>&nbsp;&nbsp;that you know do not need UNIX shell metacharacter expansion.<br><br>#*******<br># File system change monitor (fschange monitor)<br>#*******<br><br>NOTE: You cannot simultaneously watch a directory using both fschange monitor and monitor (described above).<br><br>[fschange:&lt;path&gt;]<br>* Monitors all add/update/deletes to this directory and its subdirectories.<br>* NOTE: &lt;path&gt; is the direct path. &nbsp;You do not need to preface it with // like other inputs.<br>* Sends an event for every change.<br><br># Additional attributes:<br># NOTE: fschange does not use the same attributes as other input types (described above). &nbsp;Use only the following attributes:<br><br>index = &lt;indexname&gt;<br>* The index in which to store all generated events. <br>* Defaults to _audit, unless you do not set signedaudit (below) or set signedaudit = false, in which case events go <br>&nbsp;&nbsp;into the default index.<br><br>signedaudit = [true|false]<br>* Send cryptographically signed add/update/delete events.<br>* If set to true, events are *always* sent to the _audit index and will *always* have the source type "audittrail".<br>* If set to false, events are placed in the default index and the source type is whatever you specify (or <br>&nbsp;"fs_notification" by default).<br>* You must set signedaudit to false if you want to set the index.<br>* NOTE: You must also enable auditing in audit.conf.<br>* Defaults to false.<br><br>filters = &lt;filter1&gt;,&lt;filter2&gt;,...<br>* Each filter is applied left to right for each file or directory found during the monitor poll cycle. <br>* See "File System Monitoring Filters" below for help defining a filter.<br><br>recurse = [true|false]<br>* If true, recurse directories within the directory specified in [fschange].<br>* Defaults to true.<br><br>followLinks = [true|false]<br>* If true, follow symbolic links. <br>* It is recommended that you do not set this to true; file system loops can occur. <br>* Defaults to false.<br><br>pollPeriod = &lt;integer&gt;<br>* Check this directory for changes every &lt;integer&gt; seconds. <br>* Defaults to 3600 seconds (1 hour).<br><br>hashMaxSize = &lt;integer&gt;<br>* Calculate a SHA256 hash for every file that is less than or equal to &lt;integer&gt; bytes. <br>* This hash is used as an additional method for detecting changes to the file/directory. <br>* Defaults to -1 (disabled).<br><br>fullEvent = [true|false]<br>* Set to true to send the full event if an add or update change is detected. <br>* Further qualified by the sendEventMaxSize attribute. <br>* Defaults to false.<br><br>sendEventMaxSize &nbsp;= &lt;integer&gt;<br>* Only send the full event if the size of the event is less than or equal to &lt;integer&gt; bytes. <br>* This limits the size of indexed file data. <br>* Defaults to -1, which is unlimited.<br><br>sourcetype = &lt;string&gt;<br>* Set the source type for events from this input.<br>* "sourcetype=" is automatically prepended to &lt;string&gt;.<br>* Defaults to audittrail (if signedaudit=true) or fs_notification (if signedaudit=false).<br><br>host = &lt;string&gt;<br>* Set the host for events from this input.<br>* Defaults to whatever host sent the event.<br><br>filesPerDelay = &lt;integer&gt;<br>* Injects a delay specified by delayInMills after processing &lt;integer&gt; files.<br>* This is used to throttle file system monitoring so it consumes less CPU.<br>* Defaults to 10.<br><br>delayInMills = &lt;integer&gt;<br>* The delay in milliseconds to use after processing every &lt;integer&gt; files, as specified in filesPerDelay.<br>* This is used to throttle file system monitoring so it consumes less CPU.<br>* Defaults to 100.<br><br><br>#*******<br># File system monitoring filters:<br>#*******<br><br>[filter:&lt;filtertype&gt;:&lt;filtername&gt;]<br>* Define a filter of type &lt;filtertype&gt; and name it &lt;filtername&gt;.<br>* &lt;filtertype&gt;:<br>&nbsp;&nbsp;* Filter types are either 'blacklist' or 'whitelist.' <br>&nbsp;&nbsp;* A whitelist filter processes all file names that match the regex list.<br>&nbsp;&nbsp;* A blacklist filter skips all file names that match the regex list.<br>* &lt;filtername&gt;<br>&nbsp;&nbsp;* The filter name is used in the comma-separated list when defining a file system monitor.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>regex&lt;integer&gt; = &lt;regex&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>* Blacklist and whitelist filters can include a set of regexes.<br>* The name of each regex MUST be 'regex&lt;integer&gt;', where &lt;integer&gt; starts at 1 and increments. <br>* Splunk applies each regex in numeric order:<br>&nbsp;&nbsp;regex1=&lt;regex&gt;<br>&nbsp;&nbsp;regex2=&lt;regex&gt;<br>&nbsp;&nbsp;...<br><br><br>#*******<br># WINDOWS INPUTS:<br>#*******<br><br>* Windows platform specific input processor.<br># ***********<br># Splunk for Windows ships with several Windows-only scripted inputs. They are defined in the default inputs.conf. &nbsp;<br>&nbsp;<br>* This is a list of the Windows scripted input stanzas:<br>&nbsp;&nbsp;&nbsp;&nbsp;[script://$SPLUNK_HOME\bin\scripts\splunk-wmi.path]<br>&nbsp;&nbsp;&nbsp;&nbsp;[script://$SPLUNK_HOME\bin\scripts\splunk-regmon.path]<br>&nbsp;&nbsp;&nbsp;&nbsp;[script://$SPLUNK_HOME\bin\scripts\splunk-admon.path]<br><br>* By default, some of the scripted inputs are enabled and others are disabled. &nbsp;<br>* Use the "disabled=" parameter to enable/disable any of them.<br>* Here's a short summary of the inputs:<br>&nbsp;&nbsp;* WMI: Retrieves event logs remotely and locally. It can also gather<br>&nbsp;&nbsp;&nbsp;&nbsp;performance data remotely, as well as receive various system notifications.<br>&nbsp;&nbsp;* RegMon: Uses a driver to track and report any changes that occur in the<br>&nbsp;&nbsp;&nbsp;&nbsp;local system's Registry.<br>&nbsp;&nbsp;* ADMon: Indexes existing AD objects and listens for AD changes.<br><br>###<br># The following Windows input specifications are for parsing on non-Windows platforms.<br>###<br>###<br># Performance Monitor<br>###<br><br>[perfmon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Windows Performance Monitor. &nbsp;<br>* Each perfmon:// stanza represents an individually configured performance<br>&nbsp;&nbsp;monitoring input. If you configure the input through Splunk Web, then the<br>&nbsp;&nbsp;value of "$NAME" will match what was specified there. While you can add<br>&nbsp;&nbsp;performance monitor inputs manually, Splunk recommends that you use Splunk Web<br>&nbsp;&nbsp;to configure them, because it is easy to mistype the values for<br>&nbsp;&nbsp;Performance Monitor objects, counters and instances.<br>* Note: The perfmon stanza is for local systems ONLY. To define performance<br>&nbsp;&nbsp;monitor inputs for remote machines, use wmi.conf.<br><br>object = &lt;string&gt;<br>* This is a valid Performance Monitor object as defined within Performance<br>&nbsp;&nbsp;Monitor (for example, "Process," "Server," "PhysicalDisk.")<br>* You can specify a single valid Performance Monitor object, or use a <br>&nbsp;&nbsp;regular expression to specify multiple objects.<br>* This attribute is required, and the input will not run if the attribute is not<br>&nbsp;&nbsp;present.<br>* The object name can be a regular expression (regex).<br>* There is no default.<br><br>counters = &lt;semicolon-separated strings&gt;<br>* This can be a single counter, or multiple valid Performance Monitor counters.<br>* This attribute is required, and the input will not run if the attribute is not<br>&nbsp;&nbsp;present.<br>* '*' is equivalent to all available counters for a given Performance Monitor object.<br>* There is no default.<br><br>instances = &lt;semicolon-separated strings&gt;<br>* This can be a single instance, or multiple valid Performance Monitor<br>&nbsp;&nbsp;instances.<br>* '*' is &nbsp;equivalent to all available instances for a given Performance Monitor<br>&nbsp;&nbsp;counter.<br>* If applicable instances are available for a counter and this attribute is not<br>&nbsp;&nbsp;present, then the input logs data for all available instances (this is the same as<br>&nbsp;&nbsp;setting 'instances = *').<br>* If there are no applicable instances for a counter, then this attribute<br>&nbsp;&nbsp;can be safely omitted.<br>* There is no default.<br><br>interval = &lt;integer&gt;<br>* How often, in seconds, to poll for new data.<br>* This attribute is required, and the input will not run if the attribute is not<br>&nbsp;&nbsp;present.<br>* The recommended setting depends on the Performance Monitor object,<br>&nbsp;&nbsp;counter(s) and instance(s) that you define in the input, and how much <br>&nbsp;&nbsp;performance data you require. &nbsp;Objects with numerous instantaneous<br>&nbsp;&nbsp;or per-second counters, such as "Memory," "Processor" and<br>&nbsp;&nbsp;"PhysicalDisk" should have shorter interval times specified (anywhere<br>&nbsp;&nbsp;from 1-3 seconds). Less volatile counters such as "Terminal Services,"<br>&nbsp;&nbsp;"Paging File" and "Print Queue" can have longer times configured.<br>* Default is 300 seconds.<br><br>mode = &lt;output mode&gt;<br>* Specifies output mode. <br>* Possible values: single, multikv<br><br>samplingInterval = &lt;sampling interval in ms&gt;<br>* Advanced setting. How often, in milliseconds, to poll for new data.<br>* Enables high-frequency performance sampling. The input collects performance data <br>&nbsp;&nbsp;every sampling interval. It then reports averaged data and other statistics at every interval.<br>* The minimum legal value is 100, and the maximum legal value must be less than what the<br>&nbsp;&nbsp;'interval' attribute to.<br>* If not specified, high-frequency sampling does not take place.<br>* Defaults to not specified (disabled).<br><br>stats = &lt;average;count;dev;min;max&gt;<br>* Advanced setting. Reports statistics for high-frequency performance sampling. <br>* Allows values: average, count, dev, min, max. <br>* Can be specified as a semicolon separated list.<br>* If not specified, the input does not produce high-frequency sampling statistics.<br>* Defaults to not specified (disabled).<br><br>disabled = [0|1]<br>* Specifies whether or not the input is enabled.<br>* 1 to disable the input, 0 to enable it.<br>* Defaults to 0 (enabled).<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* If no value is present, defaults to the default index.<br><br>showZeroValue = [0|1]<br>* Specfies whether or not zero value event data should be collected.<br>* 1 captures zero value event data, 0 ignores zero value event data.<br>* Defaults to 0 (ignores zero value event data)<br><br>useEnglishOnly = [true|false]<br>* Controls which Windows perfmon API is used.<br>* If true, PdhAddEnglishCounter() is used to add the counter string.<br>* If false, PdhAddCounter() is used to add the counter string.<br>* Note: if set to true, object regular expression is disabled on<br>&nbsp;&nbsp;non-English language hosts.<br>* Defaults to false.<br><br>###<br># Direct Access File Monitor (does not use file handles)<br># For Windows systems only.<br>###<br><br>[MonitorNoHandle://&lt;path&gt;]<br><br>* This stanza directs Splunk to intercept file writes to the specific file.<br>* &lt;path&gt; must be a fully qualified path name to a specific file.<br>* There can be more than one stanza.<br><br>disabled = [0|1]<br>* Tells Splunk whether or not the input is enabled.<br>* Defaults to 0 (enabled).<br><br>index = &lt;string&gt;<br>* Tells Splunk which index to store incoming data into for this stanza.<br>* This field is optional.<br>* Defaults to the default index.<br><br>###<br># Windows Event Log Monitor<br>###<br><br>[WinEventLog://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Windows event log Monitor. &nbsp;<br>* Each WinEventLog:// stanza represents an individually configured WinEventLog<br>&nbsp;&nbsp;monitoring input. If you you configure the input through Splunk Web, the<br>&nbsp;&nbsp;value of "$NAME" will match what was specified there. While you can add<br>&nbsp;&nbsp;event log monitor inputs manually, Splunk recommends that you use the<br>&nbsp;&nbsp;Manager interface to configure Windows event log monitor inputs because it is<br>&nbsp;&nbsp;easy to mistype the values for event log channels.<br>* Note: The WinEventLog stanza is for local systems ONLY. To define event log<br>&nbsp;&nbsp;monitor inputs for remote machines, use wmi.conf.<br><br>start_from = &lt;string&gt;<br>* Specifies how Splunk should chronologically read the event log channels.<br>* Setting this attribute to 'oldest' tells Splunk to start reading Windows event logs<br>&nbsp;&nbsp;from oldest to newest.<br>* Setting this attribute to 'newest' tells Splunk to start reading Windows event logs <br>&nbsp;&nbsp;in reverse, from newest to oldest. &nbsp;Once the input consumes the backlog of events,<br>&nbsp;&nbsp;it will stop.<br>* 'newest' is not supported in combination with current_only = 1 (This<br>&nbsp;&nbsp;&nbsp;&nbsp;combination does not make much sense.)<br>* Defaults to oldest.<br><br>current_only = [0|1]<br>* If set to 1, the input will only acquire events that arrive while Splunk is<br>&nbsp;&nbsp;running and the input is enabled. &nbsp;Data which was stored in the Windows Event<br>&nbsp;&nbsp;Log while splunk was not running will not be read.<br>&nbsp;&nbsp;This means that there will be gaps in data if splunk is restarted, or<br>&nbsp;&nbsp;experiences downtime.<br>&nbsp;&nbsp;* current_only = 1 is not supported with start_from = 'newest'. (It would<br>&nbsp;&nbsp;&nbsp;&nbsp;not really make sense.)<br>* If set to 0, the input will first get all existing events already stored in<br>&nbsp;&nbsp;the log which have higher event IDs (arrived more recently) than the most<br>&nbsp;&nbsp;recent events acquired, and then continue to monitor events arriving in real<br>&nbsp;&nbsp;time.<br>* Defaults to 0 (false), gathering stored events first before monitoring live events.<br><br>checkpointInterval = &lt;integer&gt;<br>* Sets how frequently the Windows Event Log input should save a checkpoint.<br>* Checkpoints store the eventID of acquired events. This allows Splunk to continue<br>&nbsp;&nbsp;monitoring at the correct event after a shutdown or outage.<br>* The default value is 5.<br><br>disabled = [0|1]<br>* Specifies whether or not the input is enabled.<br>* 1 to disable the input, 0 to enable it.<br>* The default is 0 (enabled).<br><br>evt_resolve_ad_obj = [1|0] <br>* Specifies how Splunk should interact with Active Directory while indexing Windows<br>&nbsp;&nbsp;Event Log events.<br>* A value of 1 tells Splunk to resolve the Active Directory Security IDentifier <br>(SID) objects to their canonical names for a specific Windows event log channel.<br>* When you set this value to 1, you can optionally specify the Domain Controller name<br>&nbsp;&nbsp;and/or DNS name of the domain to bind to with the 'evt_dc_name' attribute. Splunk connects<br>&nbsp;&nbsp;to that server to resolve the AD objects.<br>* A value of 0 tells Splunk not to attempt any resolution. &nbsp;<br>* By default, this attribute is disabled (0) for all channels.<br>* If you enable it, you can negatively impact the rate at which Splunk Enterprise <br>&nbsp;&nbsp;reads events on high-traffic Event Log channels. You can also cause Splunk Enterprise <br>&nbsp;&nbsp;to experience high latency when acquiring these events. This is due to the overhead <br>&nbsp;&nbsp;involved in performing translations.<br><br>evt_dc_name = &lt;string&gt; <br>* Tells Splunk which Active Directory domain controller it should bind to in order to <br>&nbsp;&nbsp;resolve AD objects.<br>* Optional. This parameter can be left empty. <br>* This name can be the NetBIOS name of the domain controller or the fully-<br>qualified DNS name of the domain controller. Either name type can, optionally,<br>be preceded by two backslash characters. &nbsp;The following examples represent<br>correctly formatted domain controller names:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;* "FTW-DC-01"<br>&nbsp;&nbsp;&nbsp;&nbsp;* "\\FTW-DC-01"<br>&nbsp;&nbsp;&nbsp;&nbsp;* "FTW-DC-01.splunk.com"<br>&nbsp;&nbsp;&nbsp;&nbsp;* "\\FTW-DC-01.splunk.com"<br><br>evt_dns_name = &lt;string&gt; <br>* Tells Splunk the fully-qualified DNS name of the domain it should bind to in order to<br>&nbsp;&nbsp;resolve AD objects.<br>* Optional. This parameter can be left empty. &nbsp;<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* If no value is present, defaults to the default index.<br><br># EventLog filtering<br># <br># Filtering at the input layer is desirable to reduce the total processing load<br># in network transfer and computation on the Splunk nodes acquiring and<br># processing the data.<br><br>whitelist = &lt;list of eventIDs&gt; | key=regex [key=regex]<br>blacklist = &lt;list of eventIDs&gt; | key=regex [key=regex]<br><br>whitelist1 = key=regex [key=regex]<br>whitelist2 = key=regex [key=regex]<br>whitelist3 = key=regex [key=regex]<br>whitelist4 = key=regex [key=regex]<br>whitelist5 = key=regex [key=regex]<br>whitelist6 = key=regex [key=regex]<br>whitelist7 = key=regex [key=regex]<br>whitelist8 = key=regex [key=regex]<br>whitelist9 = key=regex [key=regex]<br>blacklist1 = key=regex [key=regex]<br>blacklist2 = key=regex [key=regex]<br>blacklist3 = key=regex [key=regex]<br>blacklist4 = key=regex [key=regex]<br>blacklist5 = key=regex [key=regex]<br>blacklist6 = key=regex [key=regex]<br>blacklist7 = key=regex [key=regex]<br>blacklist8 = key=regex [key=regex]<br>blacklist9 = key=regex [key=regex]<br><br>* These settings are optional.<br>* The base unumbered whitelist and blacklist support two formats, a list of<br>&nbsp;&nbsp;integer event IDs, and a list of key=regex pairs.<br>* Numbered whitelist/blacklist settings such as whitelist1 do not support the<br>&nbsp;&nbsp;Event ID list format.<br><br>* These two formats cannot be combined, only one may be used in a specific line.<br><br>* Numbered whitelist settings are permitted from 1 to 9, so whitelist1 through<br>&nbsp;&nbsp;whitelist9 and blacklist1 through blacklist9 are supported.<br><br>* If no white or blacklist rules are present, all events will be read.<br><br># Formats:<br><br>* Event ID list format:<br>&nbsp;&nbsp;* A comma-seperated list of terms.<br>&nbsp;&nbsp;* Terms may be a single event ID (e.g. 6) or range of event IDs (e.g. 100-200)<br>&nbsp;&nbsp;* Example: 4,5,7,100-200<br>&nbsp;&nbsp;&nbsp;&nbsp;* This would apply to events with IDs 4, 5, 7, or any event ID between 100<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and 200, inclusive.<br>&nbsp;&nbsp;* Provides no additional functionality over the key=regex format, but may be<br>&nbsp;&nbsp;&nbsp;&nbsp;easier to understand than the equivalent:<br>&nbsp;&nbsp;&nbsp;&nbsp;List format: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4,5,7,100-200<br>&nbsp;&nbsp;&nbsp;&nbsp;Regex equivalent: EventCode=%^(4|5|7|1..|200)$%<br><br>* key=regex format &nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;* A whitespace-separated list of event log components to match, and regexes<br>&nbsp;&nbsp;&nbsp;&nbsp;to match against against them.<br>&nbsp;&nbsp;* There can be one match expression or multiple per line.<br>&nbsp;&nbsp;* The key must belong to the set of valid keys provided below.<br>&nbsp;&nbsp;* The regex consists of a leading delimiter, the regex expression, and a<br>&nbsp;&nbsp;&nbsp;&nbsp;trailing delimeter. Examples:&nbsp;%regex%, *regex*, "regex"<br>&nbsp;&nbsp;* When multiple match expressions are present, they are treated as a logical<br>&nbsp;&nbsp;&nbsp;&nbsp;AND. &nbsp;In other words, all expressions must match for the line to apply to<br>&nbsp;&nbsp;&nbsp;&nbsp;the event.<br>&nbsp;&nbsp;* If the value represented by the key does not exist, it is not considered a<br>&nbsp;&nbsp;&nbsp;&nbsp;match, regardless of the regex.<br>&nbsp;&nbsp;* Example:<br>&nbsp;&nbsp;&nbsp;&nbsp;whitelist = EventCode=%^200$% User=%jrodman%<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Include events only if they have EventCode 200 and relate to User jrodman<br><br># Valid keys for the regex format:<br><br>* The following keys are equivalent to the fields which appear in the text of<br>&nbsp;&nbsp;the acquired events: Category CategoryString ComputerName EventCode EventType<br>&nbsp;&nbsp;Keywords LogName Message OpCode RecordNumber Sid SidType SourceName<br>&nbsp;&nbsp;TaskCategory Type User<br>* There are two special keys that do not appear literally in the event.<br>&nbsp;&nbsp;* $TimeGenerated&nbsp;: The time that the computer generated the event<br>&nbsp;&nbsp;* $Timestamp: The time that the event was received and recorded by the Event Log service. <br>* EventType is only available on Server 2003 / XP and earlier<br>* Type is only available on Server 2008 / Vista and later<br>* For a more full definition of these keys, see the web documentation: <br>&nbsp;&nbsp;http://docs.splunk.com/Documentation/Splunk/latest/Data/MonitorWindowsdata#Create_advanced_filters_with_.27whitelist.27_and_.27blacklist.27<br>&nbsp;&nbsp;<br><br><br>suppress_text = [0|1]<br>* Tells Splunk whether or not to include the description of the event text for a given <br>&nbsp;&nbsp;Event Log event.<br>* Optional. This parameter can be left empty.<br>* A value of 1 suppresses the inclusion of the event text description.<br>* A value of 0 includes the event text description.<br>* If no value is present, defaults to 0.<br><br>renderXml= [true|false]<br>* Controls if the Event data is returned as XML or plain text<br>* Defaults to false.<br><br>###<br># Active Directory Monitor<br>###<br><br>[admon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Active Directory Monitor. &nbsp;<br>* Each admon:// stanza represents an individually configured Active Directory<br>&nbsp;&nbsp;monitoring input. If you configure the input with Splunk Web, then the value <br>&nbsp;&nbsp;of "$NAME" will match what was specified there. While you can add<br>&nbsp;&nbsp;Active Directory monitor inputs manually, Splunk recommends that you use the <br>&nbsp;&nbsp;Manager interface to configure Active Directory monitor inputs because it is <br>&nbsp;&nbsp;easy to mistype the values for Active Directory monitor objects.<br><br>targetDc = &lt;string&gt;<br>* Specifies a fully qualified domain name of a valid, network-accessible Active<br>&nbsp;&nbsp;Directory domain controller. <br>* If not specified, Splunk obtains the local computer's DC by default, and<br>&nbsp;&nbsp;binds to its root Distinguished Name (DN).<br><br>startingNode = &lt;string&gt;<br>* Tells Splunk where in the Active Directory directory tree to start monitoring. <br>* If not specified, Splunk attempts to start at the root of the directory<br>&nbsp;&nbsp;tree.<br>* The user that you configure Splunk to run as at installation determines where Splunk <br>&nbsp;&nbsp;starts monitoring.<br><br>monitorSubtree = [0|1]<br>* Tells Splunk whether or not to monitor the subtree(s) of a given Active Directory<br>&nbsp;&nbsp;tree path.<br>* Defaults to 1 (monitor subtrees of a given directory tree path).<br><br>disabled = [0|1]<br>* Tells Splunk whether or not the input is enabled.<br>* Defaults to 0 (enabled.)<br><br>index = &lt;string&gt;<br>* Tells Splunk which index to store incoming data into for this input.<br>* This field is optional.<br>* Defaults to the default index.<br><br>printSchema = [0|1]<br>* Tells Splunk whether or not to print the Active Directory schema.<br>* Defaults to 1 (print schema of Active Directory).<br><br>baseline = [0|1]<br>* Tells Splunk whether or not to query baseline objects.<br>* Baseline objects are objects which currently reside in Active Directory.<br>* Baseline objects also include previously deleted objects.<br>* Defaults to 0 (do not query baseline objects).<br><br>### <br># Windows Registry Monitor<br>###<br><br>[WinRegMon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Windows Registry Monitor. &nbsp;<br>* Each WinRegMon:// stanza represents an individually configured WinRegMon monitoring input.<br>&nbsp;&nbsp;If you configure the inputs with Splunk Web, the value of "$NAME" will match what<br>&nbsp;&nbsp;was specified there. While you can add event log monitor inputs manually, recommends<br>&nbsp;&nbsp;that you use the Manager interface to configure Windows registry monitor inputs because<br>&nbsp;&nbsp;it is easy to mistype the values for Registry hives and keys.<br>* Note: WinRegMon is for local systems ONLY.<br><br>proc = &lt;string&gt;<br>* Tells Splunk which processes this input should monitor for Registry access.<br>* If set, matches against the process name which performed the Registry<br>&nbsp;&nbsp;access.<br>* Events generated by processes that do not match the regular expression get<br>&nbsp;&nbsp;filtered out.<br>* Events generated by processes that match the regular expression pass<br>&nbsp;&nbsp;through.<br>* There is no default.<br><br>hive = &lt;string&gt;<br>* Tells Splunk the Registry hive(s) that this input should monitor for Registry access.<br>* If set, matches against the Registry key which was accessed.<br>* Events that contain hives that do not match the regular expression get<br>&nbsp;&nbsp;filtered out.<br>* Events that contain hives that match the regular expression pass<br>&nbsp;&nbsp;through.<br>* There is no default.<br><br>type = &lt;string&gt;<br>* A regular expression that specifies the type(s) of Registry event(s)<br>&nbsp;&nbsp;that you want Splunk to monitor.<br>* There is no default.<br><br>baseline = [0|1]<br>* Specifies whether or not Splunk should get a baseline of Registry events when it starts.<br>* If set to 1, the input will capture a baseline for the specified hive when the input<br>&nbsp;&nbsp;starts for the first time.<br>* Defaults to 0 (do not baseline the specified hive first before monitoring live events).<br><br>baseline_interval = &lt;integer&gt;<br>* Specifies how often, in seconds, that the Registry Monitor input should capture a baseline<br>&nbsp;&nbsp;for a specific Registry hive or key.<br>* Defaults to 0 (do not establish a baseline).<br><br>disabled = [0|1]<br>* Specifies whether or not the input is enabled.<br>* 1 to disable the input, 0 to enable it.<br>* Defaults to 0 (enabled).<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* If no value is present, defaults to the default index.<br><br>###<br># Windows Host Monitoring<br>###<br><br>[WinHostMon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Windows host monitor. &nbsp;<br>* Each WinHostMon:// stanza represents an WinHostMon monitoring input.<br>&nbsp;&nbsp;If you configure the input in SPlunk web, the value of "$NAME" will match what <br>&nbsp;&nbsp;was specified there.<br>* Note: WinHostMon is for local Windows systems ONLY. You can not monitor Windows host<br>&nbsp;&nbsp;information remotely.<br><br>type = &lt;semicolon-separated strings&gt;<br>* An expression that specifies the type(s) of host inputs<br>&nbsp;&nbsp;that you want Splunk to monitor.<br>* Type can be Computer;Process;Processor;Application;NetworkAdapter;Service;OperatingSystem;Disk;Driver;Roles<br><br>interval = &lt;integer&gt;<br>* Specifies the interval, in seconds, between when the input runs to gather Windows host information. <br><br>disabled = [0|1]<br>* Specifies whether or not the input is enabled.<br>* 1 to disable the input, 0 to enable it.<br>* Defaults to 0 (enabled).<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* If no value is present, defaults to the default index.<br><br>[WinPrintMon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Windows print Monitor. &nbsp;<br>* Each WinPrintMon:// stanza represents an WinPrintMon monitoring input.<br>&nbsp;&nbsp;The value of "$NAME" will match what was specified in<br>&nbsp;&nbsp;Splunk Web.<br>* Note: WinPrintMon is for local systems ONLY.<br><br>type = &lt;semicolon-separated strings&gt;<br>* An expression that specifies the type(s) of print inputs<br>&nbsp;&nbsp;that you want Splunk to monitor.<br><br>baseline = [0|1]<br>* If set to 1, the input will baseline the current print objects when the input<br>&nbsp;&nbsp;is turned on for the first time.<br>* Defaults to 0 (false), not baseline.<br><br>disabled = [0|1]<br>* Specifies whether or not the input is enabled.<br>* 1 to disable the input, 0 to enable it.<br>* Defaults to 0 (enabled).<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* If no value is present, defaults to the default index.<br><br>[WinNetMon://&lt;name&gt;]<br><br>* This section explains possible attribute/value pairs for configuring Splunk's<br>&nbsp;&nbsp;Network Monitor. &nbsp;<br>* Each WinNetMon:// stanza represents an individually configured network<br>&nbsp;&nbsp;monitoring input. &nbsp;The value of "$NAME" will match what was specified in<br>&nbsp;&nbsp;Splunk Web. Splunk recommends that you use the Manager interface to configure<br>&nbsp;&nbsp;Network Monitor inputs because it is easy to mistype the values for<br>&nbsp;&nbsp;Network Monitor monitor objects, counters and instances.<br><br>remoteAddress = &lt;regular expression&gt;<br>* If set, matches against the remote address.<br>* Events with remote addresses that do not match the regular expression get<br>&nbsp;&nbsp;filtered out.<br>* Events with remote addresses that match the regular expression pass<br>&nbsp;&nbsp;through.<br>* Example: 192\.163\..*<br>* Default (missing or empty setting) includes all events<br><br>process = &lt;regular expression&gt;<br>* If set, matches against the process/application name which performed network access<br>* Events generated by processes that do not match the regular expression are<br>&nbsp;&nbsp;filtered out.<br>* Events generated by processes that match the regular expression are passed<br>&nbsp;&nbsp;through.<br>* Default (missing or empty proc setting) includes all processes/applications<br><br>user = &lt;regular expression&gt;<br>* If set, matches against the user name which performed network access<br>* Events generated by users that do not match the regular expression are<br>&nbsp;&nbsp;filtered out.<br>* Events generated by users that match the regular expression are passed<br>&nbsp;&nbsp;through.<br>* Default (missing or empty user setting) includes access by all users<br><br>addressFamily = ipv4;ipv6<br>* If set, matches against address family.<br>* Accepts semicolon separated values, e.g. ipv4;ipv6<br>* Default (missing or empty address family setting) includes ipv4 and ipv6 traffic<br><br>packetType = connect;accept;transport.<br>* If set, matches against packet type<br>* Accepts semicolon separated values, e.g. connect;transport<br>* Default (missing or empty setting) includes all types<br><br>direction = inbound;outbound<br>* If set, matches against direction.<br>* Accepts semicolon separated values, e.g. incoming;outgoing<br>* Default (missing or empty setting) includes all types<br><br>protocol = tcp;udp<br>* If set, matches against protocol ids.<br>* Accepts semicolon separated values<br>* Protocol are defined in http://www.ietf.org/rfc/rfc1700.txt<br>* Example of protocol ids: tcp;udp<br>* Default (missing or empty setting) includes all types<br><br>readInterval = &lt;integer&gt;<br>* Read network driver every readInterval milliseconds.<br>* Advanced option. We recommend that the default value is used unless there is a problem with input performance.<br>* Allows adjusting frequency of calls into kernel driver driver. Higher frequencies may affect network performance, while lower frequencies can cause event loss.<br>* Default value: 100 msec<br>* Minumum: 10 msec, maximum: 1 sec<br><br>driverBufferSize = &lt;integer&gt;<br>* Keep maximum number of network packets in network driver buffer.<br>* Advanced option. We recommend that the default value is used unless there is a problem with input performance.<br>* Controls amount of packets cached in the driver. Lower values may result in event loss. Higher values may increase the size of non-paged memory.<br>* Default: 32768 packets.<br>* Minumum: 128 packets, maximum: 32768 packets<br><br>userBufferSize = &lt;integer&gt;<br>* Maximum size in MB of user mode event buffer.<br>* Advanced option. We recommend that the default value is used unless there is a problem with input performance.<br>* Controls amount of packets cached in the the usre mode. Lower values may result in event loss. Higher values may increase the size of Splunk network monitor memory.<br>* Default: 20 MB.<br>* Minumum: 5 MB, maximum: 500 MB.<br><br>mode = single,multikv<br>* Specifies output mode. Output each event individually or in multikv format.<br>* Default: single.<br><br>multikvMaxEventCount = &lt;integer&gt;<br>* Advanced option. When multikv mode is used output at most &nbsp;multikvMaxEventCount events.<br>* Default: 100 events<br>* Minumum: 10 events, maximum: 500 events<br><br>multikvMaxTimeMs = &lt;integer&gt;<br>* Advanced option. When multikv mode is used output no later than multikvMaxTimeMs milliseconds.<br>* Default: 1000 ms<br>* Minumum: 100 ms, maximum: 5000 ms<br><br>disabled = [0|1]<br>* Tells Splunk whether or not the input is enabled.<br>* Defaults to 0 (enabled.)<br><br>index = &lt;string&gt;<br>* Tells Splunk which index to store incoming data into for this stanza.<br>* This field is optional.<br>* Defaults to the default index.<br><br></font></code>
<h3> <a name="inputsconf_inputs.conf.example"><span class="mw-headline" id="inputs.conf.example">inputs.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example inputs.conf. &nbsp;Use this file to configure data inputs.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># inputs.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># The following configuration directs Splunk to read all the files in the directory /var/log.<br><br>[monitor:///var/log]<br><br><br># The following configuration directs Splunk to read all the files under /var/log/httpd and classify them <br># as sourcetype::access_common. &nbsp;When checking a file for new data, if the file's modtime is from before<br># seven days ago, the file will no longer be checked for changes.<br><br>[monitor:///var/log/httpd]<br>sourcetype = access_common<br>ignoreOlderThan = 7d<br><br><br># The following configuration directs Splunk to read all the files under /mnt/logs. When the path is <br># /mnt/logs/&lt;host&gt;/... it sets the hostname (by file) to &lt;host&gt;.<br><br>[monitor:///mnt/logs]<br>host_segment = 3<br><br><br># The following configuration directs Splunk to listen on TCP port 9997 for raw data from ANY remote server <br># (not just a Splunk instance). The host of the data is set to the IP address of the remote server.<br><br>[tcp://:9997]<br><br><br># The following configuration directs Splunk to listen on TCP port 9995 for raw data from ANY remote server.<br># The host of the data is set as the host name of the remote server. &nbsp;All data will also be<br># assigned the sourcetype "log4j" and the source "tcp:9995".<br><br>[tcp://:9995]<br>connection_host = dns<br>sourcetype = log4j<br>source = tcp:9995<br><br><br># The following configuration directs Splunk to listen on TCP port 9995 for raw data from 10.1.1.10. <br># All data is assigned the host "webhead-1", the sourcetype "access_common" and the<br># the source "//10.1.1.10/var/log/apache/access.log".<br><br>[tcp://10.1.1.10:9995]<br>host = webhead-1<br>sourcetype = access_common<br>source = //10.1.1.10/var/log/apache/access.log<br><br><br># The following configuration sets a global default for data payloads sent from the light forwarder.<br># The route parameter is an ordered set of rules that is evaluated in order for each payload of cooked data.<br><br>[splunktcp]<br>route=has_key:_utf8:indexQueue;has_key:_linebreaker:indexQueue;absent_key:_utf8:parsingQueue;absent_key:_linebreaker:parsingQueue;<br><br><br># The following configuration directs Splunk to listen on TCP port 9996 for<br># splunk cooked event data from ANY splunk forwarder.<br># The host of the data is set to the host name of the remote server ONLY IF the<br># remote data has no host set, or if it is set to "localhost".<br><br>[splunktcp://:9996]<br>connection_host = dns<br><br><br># The following configuration directs Splunk to listen on TCP port 9996 for distributed search data from &nbsp;<br># 10.1.1.100. The data is processed the same as locally indexed data.<br><br>[splunktcp://10.1.1.100:9996]<br><br><br># The following configuration directs Splunk to listen on TCP port 514 for data from <br># syslog.corp.company.net. The data is assigned the sourcetype "syslog" and the host <br># is set to the host name of the remote server.<br><br>[tcp://syslog.corp.company.net:514]<br>sourcetype = syslog<br>connection_host = dns<br><br><br># Set up SSL:<br><br>[SSL]<br>serverCert=$SPLUNK_HOME/etc/auth/server.pem<br>password=password<br>rootCA=$SPLUNK_HOME/etc/auth/cacert.pem<br>requireClientCert=false<br><br>[splunktcp-ssl:9996]<br><br><br># Enable Windows Registry monitoring (Windows only)<br># This example shows how to enable Windows Registry monitoring as a scripted input. <br># Because the Windows Registry can generate a high volume of events, Windows Registry monitoring <br># is also affected by two other configuration files, sysmon.conf and regmon.conf:<br># * sysmon.conf contains global settings for which event types (adds, deletes, renames, and so on) <br># to monitor, which regular expression filters from the regmon-filters.conf file to use, and <br># whether or not Windows registry events are monitored at all.<br># * regmon-filters.conf contains the specific regular expressions you create to refine and filter <br># the hive key paths you want Splunk to monitor.<br># Splunk recommends that you refer to the documentation about Windows Registry monitoring at <br># http://docs.splunk.com/Documentation/Splunk/latest/Data/MonitorWindowsregistrydata <br># for more details.<br># You must make the change shown below in inputs.conf in $SPLUNK_HOME/etc/system/local/. <br># You must restart Splunk to enable configurations. <br><br>[script://$SPLUNK_HOME\bin\scripts\splunk-regmon.path]<br>interval = 60<br>sourcetype = WinRegistry<br>source = WinRegistry<br>disabled = 0<br><br># Enable WMI input (Windows only)<br># This example shows how to enable WMI input as a scripted input.<br># WMI input is also affected by configurations in wmi.conf. <br># Splunk recommends that you refer to the documentation about WMI input at <br># http://docs.splunk.com/Documentation/Splunk/latest/Data/MonitorWMIdata<br># for more details.<br># You must make this change in inputs.conf in $SPLUNK_HOME/etc/apps/windows/local/. <br># You must restart Splunk to enable configurations.<br><br>[script://$SPLUNK_HOME\bin\scripts\splunk-wmi.path]<br>disabled = 0<br><br><br># Use file system change monitor:<br><br>[fschange:/etc/]<br>fullEvent=true<br>pollPeriod=60<br>recurse=true<br>sendEventMaxSize=100000<br>index=main <br><br># Monitor Windows event logs Security, getting the most recent events first,<br># then older, and finally continuing to gather newly arriving events<br><br>[WinEventLog://Security]<br>disabled = 0<br>start_from = newest<br>evt_dc_name =<br>evt_dns_name =<br>evt_resolve_ad_obj = 1<br>checkpointInterval = 5<br><br># Monitor Windows event logs ForwardedEvents, this time only gathering the<br># events happening after first starting to monitor, going forward in time.<br><br>[WinEventLog://ForwardedEvents]<br>disabled = 0<br>start_from = oldest<br>current_only = 1<br>checkpointInterval = 5<br><br>[tcp://9994]<br>queueSize=50KB<br>persistentQueueSize=100MB<br><br># Perfmon: Windows performance monitoring examples<br><br># Important: You must specify the names of objects, counters and instances <br># exactly as they are shown in the Performance Monitor application. &nbsp;Splunk Web<br># is the recommended interface to use to configure performance monitor inputs.<br><br># Important: These stanzas gather performance data from the local system only.<br># Use wmi.conf for performance monitor metrics on remote systems.<br><br># Query the PhysicalDisk performance object and gather disk access data for<br># all physical drives installed in the system. Store this data in the <br># "perfmon" index.<br># Note: If the interval attribute is set to 0, Splunk will reset the interval<br># to 1.<br><br>[perfmon://LocalPhysicalDisk]<br>interval = 0<br>object = PhysicalDisk<br>counters = Disk Bytes/sec;&nbsp;% Disk Read Time;&nbsp;% Disk Write Time;&nbsp;% Disk Time<br>instances = *<br>disabled = 0<br>index = PerfMon<br><br># Gather common memory statistics using the Memory performance object, every <br># 5 seconds. &nbsp;Store the data in the "main" index. &nbsp;Since none of the counters<br># specified have applicable instances, the instances attribute is not required.<br><br>[perfmon://LocalMainMemory]<br>interval = 5<br>object = Memory<br>counters = Committed Bytes; Available Bytes;&nbsp;% Committed Bytes In Use<br>disabled = 0<br>index = main<br><br># Gather data on USB activity levels every 10 seconds. &nbsp;Store this data in the default index.<br><br>[perfmon://USBChanges]<br>interval = 10<br>object = USB<br>counters = Usb Control Data Bytes/Sec<br>instances = *<br>disabled = 0<br><br># Admon: Windows Active Directory monitoring examples<br><br># Monitor the default domain controller for the domain that the computer<br># running Splunk belongs to. &nbsp;Start monitoring at the root node of Active<br># Directory.<br>[admon://NearestDC]<br>targetDc =<br>startingNode =<br><br># Monitor a specific DC, with a specific starting node. &nbsp;Store the events in<br># the "admon" Splunk index. Do not print Active Directory schema. Do not index baseline events.<br>[admon://DefaultTargetDC]<br>targetDc = pri01.eng.ad.splunk.com<br>startingNode = OU=Computers,DC=eng,DC=ad,DC=splunk,DC=com<br>index = admon<br>printSchema = 0<br>baseline = 0<br><br># Monitor two different DCs with different starting nodes.<br>[admon://DefaultTargetDC]<br>targetDc = pri01.eng.ad.splunk.com<br>startingNode = OU=Computers,DC=eng,DC=ad,DC=splunk,DC=com<br><br>[admon://SecondTargetDC]<br>targetDc = pri02.eng.ad.splunk.com<br>startingNode = OU=Computers,DC=hr,DC=ad,DC=splunk,DC=com<br><br></font></code>

<a name="instancecfgconf"></a><h2> <a name="instancecfgconf_instance.cfg.conf"><span class="mw-headline" id="instance.cfg.conf">instance.cfg.conf</span></a></h2>
<p>The following are the spec and example files for instance.cfg.conf.
</p>
<h3> <a name="instancecfgconf_instance.cfg.conf.spec"><span class="mw-headline" id="instance.cfg.conf.spec">instance.cfg.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains the set of attributes and values you can expect to find in the<br># SPLUNK_HOME/etc/instance.cfg file; the instance.cfg file is not to be modified or<br># removed by user. &nbsp;LEAVE THE instance.cfg FILE ALONE.<br>#<br><br>#<br># GLOBAL SETTINGS<br># The [general] stanza defines global settings.<br>#<br>[general]<br><br>guid = &lt;GUID in all-uppercase&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* This setting formerly (before 5.0) belonged in the [general] stanza of server.conf file.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;* Splunk expects that every Splunk instance will have a unique string for this value,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;independent of all other Splunk instances. &nbsp;By default, Splunk will arrange for<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this without user intervention.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;* Currently used by (not exhaustive):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Clustering environments, to identify participating nodes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Splunk introspective searches (Splunk on Splunk, Deployment Monitor, etc.), to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;identify forwarders.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* At startup, the following happens:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If server.conf has a value of 'guid' AND instance.cfg has no value of 'guid',<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;then the value will be erased from server.conf and moved to instance.cfg file.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If server.conf has a value of 'guid' AND instance.cfg has a value of 'guid' AND<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;these values are the same, the value is erased from server.conf file.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If server.conf has a value of 'guid' AND instance.cfg has a value of 'guid' AND<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;these values are different, startup halts and error is shown. &nbsp;Operator must<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;resolve this error. &nbsp;We recommend erasing the value from server.conf file,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and then restarting.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you are hitting this error while trying to mass-clone Splunk installs, please<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;look into the command 'splunk clone-prep-clear-config'; 'splunk help' has help.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* See http://www.ietf.org/rfc/rfc4122.txt for how a GUID (a.k.a. UUID) is constructed.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The standard regexp to match an all-uppercase GUID is "[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12}".<br><br></font></code>
<h3> <a name="instancecfgconf_instance.cfg.conf.example"><span class="mw-headline" id="instance.cfg.conf.example">instance.cfg.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example SPLUNK_HOME/etc/instance.cfg file; the instance.cfg file<br># is not to be modified or removed by user. &nbsp;LEAVE THE instance.cfg FILE ALONE.<br>#<br><br>[general]<br>guid = B58A86D9-DF3D-4BF8-A426-DB85C231B699<br><br></font></code>

<a name="limitsconf"></a><h2> <a name="limitsconf_limits.conf"><span class="mw-headline" id="limits.conf">limits.conf</span></a></h2>
<p>The following are the spec and example files for limits.conf.
</p>
<h3> <a name="limitsconf_limits.conf.spec"><span class="mw-headline" id="limits.conf.spec">limits.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring limits for search commands.<br>#<br># There is a limits.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a limits.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># limits.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br><br># limits.conf settings and DISTRIBUTED SEARCH<br># &nbsp;&nbsp;Unlike most settings which affect searches, limits.conf settings are not<br># &nbsp;&nbsp;provided by the search head to be used by the search peers. &nbsp;This means that if<br># &nbsp;&nbsp;you need to alter search-affecting limits in a distributed environment, typically<br># &nbsp;&nbsp;you will need to modify these settings on the relevant peers and search head for<br># &nbsp;&nbsp;consistent results.<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br># CAUTION: Do not alter the settings in limits.conf unless you know what you are doing. <br># Improperly configured limits may result in splunkd crashes and/or memory overuse.<br><br>* Each stanza controls different parameters of search commands.<br><br>max_mem_usage_mb = &lt;non-negative integer&gt;<br>* Provides a limitation to the amount of RAM a batch of events or results will use<br>&nbsp;&nbsp;in the memory of search processes.<br>* Operates on an estimation of memory use which is not exact.<br>* The limitation is applied in an unusual way; if the number of results or events<br>&nbsp;&nbsp;exceeds maxresults, AND the estimated memory exceeds this limit, the data is<br>&nbsp;&nbsp;spilled to disk.<br>* This means, as a general rule, lower limits will cause a search to use more disk<br>&nbsp;&nbsp;I/O and less RAM, and be somewhat slower, but should cause the same results to<br>&nbsp;&nbsp;typically come out of the search in the end.<br>* This limit is applied currently to a number, but not all search processors.<br>&nbsp;&nbsp;However, more will likely be added as it proves necessary.<br>* The number is thus effectively a ceiling on batch size for many components of<br>&nbsp;&nbsp;search for all searches run on this system.<br>* 0 will specify the size to be unbounded. &nbsp;In this case searches may be allowed to<br>&nbsp;&nbsp;grow to arbitrary sizes.<br>&nbsp;<br>* The 'mvexpand' command uses this value in a different way.<br>&nbsp;&nbsp;* mvexpand has no combined logic with maxresults<br>&nbsp;&nbsp;* If the memory limit is exceeded, output is truncated, not spilled to disk.<br><br>* This value is not exact. The estimation can deviate by an order of magnitude or so<br>&nbsp;&nbsp;to both the smaller and larger sides.<br>* Defaults to 200 (MB)<br><br>min_batch_size_bytes = &lt;integer&gt;<br>* Specifies the size of the file/tar after which the file is handled by the batch reader instead of the trailing processor. <br>* Global parameter, cannot be configured per input.<br>* Note configuring this to a very small value could lead to backing up of jobs at the tailing processor.<br>* defaults to 20 MB <br><br>[searchresults]<br>* This stanza controls search results for a variety of Splunk search commands.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>maxresultrows = &lt;integer&gt;<br>* Configures the maximum number of events are generated by search commands which <br>grow the size of your result set (such as multikv) or that create events. Other search commands are explicitly <br>controlled in specific stanzas below.<br>* This limit should not exceed 50000. Setting this limit higher than 50000 would cause more memory usage. <br>More memory usage would manifest as Splunk out of memory crash when the memory limits are reached.<br>* Defaults to 50000. <br><br>tocsv_maxretry = &lt;integer&gt;<br>* Maximum number of times to retry the atomic write operation.<br>* 1 = no retries.<br>* Defaults to 5.<br><br>tocsv_retryperiod_ms = &lt;integer&gt;<br>* Period of time to wait before each retry.<br>* Defaults to 500.<br><br>* These setting control logging of error messages to info.csv<br>&nbsp;&nbsp;All messages will be logged to search.log regardless of these settings.<br><br>compression_level = &lt;integer&gt;<br>* Compression level to use when writing search results to .csv.gz files<br>* Defaults to 1<br><br>[search_info]<br>* This stanza controls logging of messages to the info.csv file<br>* Messages logged to info.csv are available in to REST api clients,<br>&nbsp;&nbsp;and the Splunk UI, something something, so limiting the messages<br>&nbsp;&nbsp;added to info.csv will mean that these messages will not be<br>&nbsp;&nbsp;available in the UI and/or the REST api something something.<br><br>max_infocsv_messages &nbsp;= &lt;positive integer&gt;<br>* If more than max_infocsv_messages log entries are generated, additional entries will not be<br>&nbsp;&nbsp;logged in info.csv. All entries will still be logged in search.log.<br><br>infocsv_log_level = [DEBUG|INFO|WARN|ERROR]<br>* Limits the messages which are added to info.csv to the stated level<br>&nbsp;&nbsp;and above.<br>* For example, if log_level is WARN, messages of type WARN and higher<br>&nbsp;&nbsp;will be added to info.csv<br>&nbsp;<br>[subsearch]<br>* This stanza controls subsearch results.<br>* NOTE: This stanza DOES NOT control subsearch results when a subsearch is called by<br>&nbsp;&nbsp;commands such as join, append, or appendcols. <br>* Read more about subsearches in the online documentation: <br>&nbsp;&nbsp;http://docs.splunk.com/Documentation/Splunk/latest/Search/Aboutsubsearches<br><br>maxout = &lt;integer&gt;<br>* Maximum number of results to return from a subsearch.<br>* This value cannot be greater than or equal to 10500.<br>* Defaults to 10000.<br><br>maxtime = &lt;integer&gt;<br>* Maximum number of seconds to run a subsearch before finalizing<br>* Defaults to 60.<br><br>ttl = &lt;integer&gt;<br>* Time to cache a given subsearch's results, in seconds.<br>* Do not set this below 120 seconds. <br>* See definition in [search] ttl for more details on how the ttl is computed<br>* Defaults to 300.<br><br>[anomalousvalue]<br><br>maxresultrows = &lt;integer&gt;<br>* Configures the maximum number of events that can be present in memory at one time. <br>* Defaults to searchresults::maxresultsrows (which is by default 50000).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>maxvalues = &lt;integer&gt;<br>* Maximum number of distinct values for a field.<br>* Defaults to 100000.<br><br>maxvaluesize = &lt;integer&gt;<br>* Maximum size in bytes of any single value (truncated to this size if larger).<br>* Defaults to 1000.<br><br>[associate]<br><br>maxfields = &lt;integer&gt;<br>* Maximum number of fields to analyze.<br>* Defaults to 10000.<br><br>maxvalues = &lt;integer&gt;<br>* Maximum number of values for any field to keep track of.<br>* Defaults to 10000.<br><br>maxvaluesize = &lt;integer&gt;<br>* Maximum length of a single value to consider.<br>* Defaults to 1000.<br><br><br>[autoregress]<br><br>maxp = &lt;integer&gt;<br>* Maximum valid period for auto regression <br>* Defaults to 10000.<br><br>maxrange = &lt;integer&gt;<br>* Maximum magnitude of range for p values when given a range.<br>* Defaults to 1000.<br><br>[concurrency]<br>max_count = &lt;integer&gt;<br>* Maximum number of detected concurrencies.<br>* Defaults to 10000000<br><br><br>[ctable]<br>* This stanza controls the contingency, ctable, and counttable commands.<br><br>maxvalues = &lt;integer&gt;<br>* Maximum number of columns/rows to generate (the maximum number of distinct values for the row field and <br>column field).<br>* Defaults to 1000.<br><br><br>[correlate]<br><br>maxfields = &lt;integer&gt;<br>* Maximum number of fields to correlate.<br>* Defaults to 1000.<br><br><br>[discretize]<br>* This stanza set attributes for bin/bucket/discretize.<br><br>default_time_bins = &lt;integer&gt;<br>* When discretizing time for timechart or explicitly via bin, the default bins to use if no span or bins is specified.<br>* Defaults to 100<br><br>maxbins = &lt;integer&gt; <br>* Maximum number of buckets to discretize into.<br>* If maxbins is not specified or = 0, it defaults to searchresults::maxresultrows (which is by default 50000).<br><br>[export]<br>add_timestamp = &lt;bool&gt;<br>* Add a epoch time timestamp to JSON streaming output that reflects the time the results were generated/retrieved <br>* Defaults to false<br><br>add_offset = &lt;bool&gt;<br>* Add an offset/row number to JSON streaming output<br>* Defaults to true<br><br>[extern]<br>perf_warn_limit = &lt;integer&gt;<br>* Warn when external scripted command is applied to more than this many events<br>* set to 0 for no message (message is always INFO level)<br>* Defaults to 10000<br><br>[inputcsv]<br>mkdir_max_retries = &lt;integer&gt;<br>* Maximum number of retries for creating a tmp directory (with random name as subdir of SPLUNK_HOME/var/run/splunk)<br>* Defaults to 100.<br><br>[indexpreview]<br>max_preview_bytes = &lt;integer&gt;<br>* Maximum number of bytes to read from each file during preview<br>* Defaults to 2000000 (2 MB)<br><br>max_results_perchunk = &lt;integer&gt;<br>* Maximum number of results to emit per call to preview data generator<br>* Defaults to 2500<br><br>soft_preview_queue_size = &lt;integer&gt;<br>* Loosely-applied maximum on number of preview data objects held in memory<br>* Defaults to 100<br><br>[join]<br>subsearch_maxout = &lt;integer&gt;<br>* Maximum result rows in output from subsearch to join against.<br>* Defaults to 50000<br><br>subsearch_maxtime = &lt;integer&gt;<br>* Maximum search time (in seconds) before auto-finalization of subsearch.<br>* Defaults to 60 <br><br>subsearch_timeout = &lt;integer&gt;<br>* Maximum time to wait for subsearch to fully finish (in seconds).<br>* Defaults to 120<br><br>[kmeans]<br><br>maxdatapoints = &lt;integer&gt;<br>* Maximum data points to do kmeans clusterings for.<br>* Defaults to 100000000<br><br>maxkvalue = &lt;integer&gt;<br>* Maximum number of clusters to attempt to solve for.<br>* Defaults to 1000<br><br>maxkrange = &lt;integer&gt;<br>* Maximum number of k values to iterate over when specifying a range.<br>* Defaults to 100<br><br>[kv]<br><br>maxcols = &lt;integer&gt;<br>* When non-zero, the point at which kv should stop creating new fields.<br>* Defaults to 512.<br><br>limit = &lt;integer&gt;<br>* Maximum number of keys auto kv can generate.<br>* Defaults to 50.<br><br>maxchars = &lt;integer&gt;<br>* Truncate _raw to this size and then do auto KV.<br>* Defaults to 10240 characters.<br><br>max_extractor_time = &lt;integer&gt;<br>* Maximum amount of CPU time, in milliseconds, that a key-value pair extractor will be allowed to <br>* take before warning. If the extractor exceeds this execution time on any event a warning will be issued<br>* Defaults to 1000<br><br>avg_extractor_time = &lt;integer&gt;<br>* Maximum amount of CPU time, in milliseconds, that the average (over search results) execution time of <br>* a key-value pair extractor will be allowed to take before warning. Once the average becomes larger <br>* than this amount of time a warning will be issued<br>* Defaults to 500<br><br>[lookup]<br><br>max_memtable_bytes = &lt;integer&gt; <br>* Maximum size of static lookup file to use an in-memory index for.<br>* Defaults to 10000000 in bytes (10MB<br><br>max_matches = &lt;integer&gt;<br>* maximum matches for a lookup<br>* range 1 - 1000 <br>* Defaults to 1000<br><br>max_reverse_matches = &lt;integer&gt; <br>* maximum reverse lookup matches (for search expansion)<br>* Defaults to 50<br><br>batch_index_query = &lt;bool&gt;<br>* Should non-memory file lookups (files that are too large) use batched queries to possibly improve performance?<br>* Defaults to true<br><br>batch_response_limit = &lt;integer&gt;<br>* When doing batch requests, the maximum number of matches to retrieve &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>* if more than this limit of matches would otherwise be retrieve, we will fall back to non-batch mode matching<br>* Defaults to 5000000<br><br>max_lookup_messages = &lt;positive integer&gt;<br>* If more than "max_lookup_messages" log entries are generated, additional entries will not be<br>&nbsp;&nbsp;logged in info.csv. All entries will still be logged in search.log.<br><br><br>[metrics]<br><br>maxseries = &lt;integer&gt;<br>* The number of series to include in the per_x_thruput reports in metrics.log.<br>* Defaults to 10.<br><br>interval = &lt;integer&gt;<br>* Number of seconds between logging splunkd metrics to metrics.log.<br>* Minimum of 10.<br>* Defaults to 30.<br><br>[metrics:tcpin_connections]<br><br>aggregate_metrics = [true|false]<br>* For each splunktcp connection from forwarder, splunk logs metrics information every metrics interval.<br>* When there are large number of forwarders connected to indexer, the amount of information logged can take<br>* log of space in metrics.log. When set to true, it will aggregate information across each connection and<br>* report only once per metrics interval.<br>* Defaults to false<br><br>suppress_derived_info = [true|false]<br>* For each forwarder connection, _tcp_Bps, _tcp_KBps, _tcp_avg_thruput, _tcp_Kprocessed is logged in metrics.log.<br>* This can be derived from kb. When set to true, the above derived info will not be emitted.<br>* Defaults to true<br><br>[rare]<br><br>maxresultrows = &lt;integer&gt;<br>* Maximum number of result rows to create.<br>* If not specified, defaults to searchresults::maxresultrows (which is by default 50000).<br><br>maxvalues = &lt;integer&gt;<br>* Maximum number of distinct field vector values to keep track of.<br>* Defaults 100000.<br><br>maxvaluesize = &lt;integer&gt;<br>* Maximum length of a single value to consider.<br>* Defaults to 1000.<br><br>[restapi]<br><br>maxresultrows = &lt;integer&gt;<br>* Maximum result rows to be returned by /events or /results getters from REST API.<br>* Defaults to 50000.<br><br>time_format_reject = &lt;regular expression&gt;<br>* HTTP parameters for time_format and output_time_format which match<br>&nbsp;&nbsp;this regex will be rejected (blacklisted).<br>* The regex will be satisfied by a substring match anywhere in the paramater.<br>* Intended as defense-in-depth against XSS style attacks against browser users<br>&nbsp;&nbsp;by crafting specially encoded URLS for them to access splunkd. <br>* If unset, all parameter strings will be accepted.<br>* To disable this check entirely, set the value to empty.<br>&nbsp;&nbsp;# Example of disabling: time_format_reject =<br>* Defaults to [&lt;&gt;!] , which means that the less-than '&lt;', greater-than '&gt;', and<br>&nbsp;&nbsp;exclaimation point '!' are not allowed.<br><br>jobscontentmaxcount = &lt;integer&gt;<br>* Maximum length of a property in the contents dictionary of an entry from /jobs getter from REST API<br>* Value of 0 disables truncation<br>* Defaults to 0<br><br>[search_metrics]<br>debug_metrics = &lt;bool&gt;<br>* This indicates whether we should output more detailed search metrics for debugging.<br>* This will do things like break out where the time was spent by peer, and may add additional deeper levels of metrics.<br>* This is NOT related to "metrics.log" but to the "Execution Costs" and "Performance" fields in the Search inspector, or the count_map in info.csv.<br>* Defaults to false<br><br>[search]<br>summary_mode = [all|only|none]<br>* Controls if precomputed summary are to be used if possible?<br>* all: use summary if possible, otherwise use raw data<br>* only: use summary if possible, otherwise do not use any data<br>* none: never use precomputed summary data<br>* Defaults to 'all'<br><br>result_queue_max_size = &lt;integer&gt;<br>* Controls the size of the search results queue in dispatch<br>* Default size is set to 100MB<br>* Use caution while playing with this parameter<br><br>use_bloomfilter = &lt;bool&gt;<br>* Control whether to use bloom filters to rule out buckets<br><br>max_id_length = &lt;integer&gt;<br>* Maximum length of custom search job id when spawned via REST api arg id=<br><br>ttl = &lt;integer&gt;<br>* How long search artifacts should be stored on disk once completed, in seconds. The ttl is computed<br>* relative to the modtime of status.csv of the job if such file exists or the modtime of the search<br>* job's artifact directory. If a job is being actively viewed in the Splunk UI then the modtime of <br>* status.csv is constantly updated such that the reaper does not remove the job from underneath.<br>* Defaults to 600, which is equivalent to 10 minutes.<br><br>default_save_ttl = &lt;integer&gt;<br>* How long the ttl for a search artifact should be extended in response to the save control action, in second. &nbsp;0 = indefinitely.<br>* Defaults to 604800 (1 week)<br><br>remote_ttl = &lt;integer&gt;<br>* How long artifacts from searches run in behalf of a search head should be stored on the indexer <br>&nbsp;&nbsp;after completion, in seconds.<br>* Defaults to 600 (10 minutes)<br><br>status_buckets = &lt;integer&gt;<br>* The approximate maximum number buckets to generate and maintain in the timeline.<br>* Defaults to 0, which means do not generate timeline information.<br><br>max_bucket_bytes = &lt;integer&gt;<br>* This setting has been deprecated and has no effect<br><br>max_count = &lt;integer&gt;<br>* The number of events that can be accessible in any given status bucket.<br>* The last accessible event in a call that takes a base and bounds.<br>* Defaults to 10000.<br><br>max_events_per_bucket = &lt;integer&gt;<br>* For searches with status_buckets&gt;0 this will limit the number of events retrieved per timeline bucket.<br>* Defaults to 1000 in code. &nbsp;<br><br>truncate_report = [1|0]<br>* Specifies whether or not to apply the max_count limit to report output.<br>* Defaults to false (0).<br><br>min_prefix_len = &lt;integer&gt;<br>* The minimum length of a prefix before a * to ask the index about.<br>* Defaults to 1.<br><br>cache_ttl = &lt;integer&gt;<br>* The length of time to persist search cache entries (in seconds).<br>* Defaults to 300.<br><br>max_results_perchunk = &lt;integer&gt;<br>* Maximum results per call to search (in dispatch), must be less than or equal to maxresultrows.<br>* Defaults to 2500 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>min_results_perchunk = &lt;integer&gt;<br>* Minimum results per call to search (in dispatch), must be less than or equal to max_results_perchunk.<br>* Defaults to 100 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>max_rawsize_perchunk = &lt;integer&gt;<br>* Maximum raw size of results per call to search (in dispatch).<br>* 0 = no limit. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>* Defaults to 100000000 (100MB)<br>* Not affected by chunk_multiplier<br><br>target_time_perchunk = &lt;integer&gt;<br>* Target duration of a particular call to fetch search results in ms.<br>* Defaults to 2000<br><br>long_search_threshold = &lt;integer&gt;<br>* Time in seconds until a search is considered "long running".<br>* Defaults to 2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>chunk_multiplier = &lt;integer&gt;<br>* max_results_perchunk, min_results_perchunk, and target_time_perchunk are multiplied by this <br>for a long running search.<br>* Defaults to 5<br><br>min_freq = &lt;number&gt;<br>* Minimum frequency of a field required for including in the /summary endpoint as a fraction (&gt;=0 and &lt;=1).<br>* Defaults is 0.01 (1%)<br><br>reduce_freq = &lt;integer&gt;<br>* Attempt to reduce intermediate results every how many chunks (0 = never).<br>* Defaults to 10<br><br>reduce_duty_cycle = &lt;number&gt;<br>* the maximum time to spend doing reduce, as a fraction of total search time<br>* Must be &gt; 0.0 and &lt; 1.0<br>* Defaults to 0.25<br><br>preview_duty_cycle = &lt;number&gt;<br>* the maximum time to spend generating previews, as a fraction of total search time<br>* Must be &gt; 0.0 and &lt; 1.0<br>* Defaults to 0.25<br><br>results_queue_min_size = &lt;integer&gt;<br>* The minimum size for the queue of results that will be kept from peers for processing on the search head.<br>* The queue will be the max of this and the number of peers providing results.<br>* Defaults to 10<br><br>dispatch_quota_retry = &lt;integer&gt;<br>* The maximum number of times to retry to dispatch a search when the quota has been reached.<br>* Defaults to 4<br><br>dispatch_quota_sleep_ms = &lt;integer&gt;<br>* Milliseconds between retrying to dispatch a search if a quota has been reached.<br>* Retries the given number of times, with each successive wait 2x longer than the previous.<br>* Defaults to 100<br><br>base_max_searches = &lt;int&gt;<br>* A constant to add to the maximum number of searches, computed as a multiplier of the CPUs.<br>* Defaults to 6<br><br>max_searches_per_cpu = &lt;int&gt;<br>* The maximum number of concurrent historical searches per CPU. The system-wide limit of <br>historical searches is computed as: <br>&nbsp;&nbsp;max_hist_searches = &nbsp;max_searches_per_cpu x number_of_cpus + base_max_searches<br>* Note: the maximum number of real-time searches is computed as: <br>&nbsp;&nbsp;max_rt_searches = max_rt_search_multiplier x max_hist_searches<br>* Defaults to 1<br><br>max_rt_search_multiplier = &lt;decimal number&gt;<br>* A number by which the maximum number of historical searches is multiplied to determine the maximum<br>* number of concurrent real-time searches <br>* Note: the maximum number of real-time searches is computed as: <br>&nbsp;&nbsp;max_rt_searches = max_rt_search_multiplier x max_hist_searches<br>* Defaults to 1<br><br>max_macro_depth = &lt;int&gt; <br>* Max recursion depth for macros.<br>* Considered a search exception if macro expansion doesn't stop after this many levels.<br>* Must be greater than or equal to 1.<br>* Default is 100<br><br>realtime_buffer = &lt;int&gt;<br>* Maximum number of accessible events to keep for real-time searches from Splunk Web.<br>* Acts as circular buffer once this limit is reached<br>* Must be greater than or equal to 1<br>* Default is 10000<br><br>stack_size = &lt;int&gt;<br>* The stack size (in bytes) of the thread executing the search.<br>* Defaults to 4194304 &nbsp;(4 MB)<br><br>status_cache_size = &lt;int&gt;<br>* The number of search job status data splunkd can cache in RAM. This cache improves performance of <br>&nbsp;&nbsp;the jobs endpoint<br>* Defaults to 10000<br><br>timeline_freq = &lt;timespan&gt; or &lt;ratio&gt; <br>* Minimum amount of time between timeline commits.<br>* If specified as a number &lt; 1 (and &gt; 0), minimum time between commits is computed as a ratio of <br>&nbsp;&nbsp;the amount of time that the search has been running.<br>* defaults to 0 seconds<br><br>preview_freq = &lt;timespan&gt; or &lt;ratio&gt;<br>* Minimum amount of time between results preview updates.<br>* If specified as a number &lt; 1 (and &gt; 0), minimum time between previews is computed as a ratio of <br>the amount of time that the search has been running, or as a ratio of the length of the time window <br>for real-time windowed searches.<br>* Defaults to ratio of 0.05<br><br>max_combiner_memevents = &lt;int&gt;<br>* Maximum size of in-memory buffer for search results combiner, in terms of number of events.<br>* Defaults to 50000 events. <br><br>replication_period_sec &nbsp;= &lt;int&gt;<br>* The minimum amount of time in seconds between two successive bundle replications.<br>* Defaults to 60<br><br>replication_file_ttl = &lt;int&gt;<br>* The TTL (in seconds) of bundle replication tarballs, i.e. *.bundle files.<br>* Defaults to 600 (10m)<br><br>sync_bundle_replication = [0|1|auto]<br>* Flag indicating whether configuration file replication blocks searches or is run asynchronously <br>* When setting this flag to auto Splunk will choose to use asynchronous replication if and only if all the peers <br>* support async bundle replication, otherwise it will fall back into sync replication. <br>* Defaults to auto <br><br>multi_threaded_setup = [0|1]<br>* Flag indicating whether to use multiple threads when setting up distributed search to multiple peers.<br>* Defaults to false (0)<br><br>rr_min_sleep_ms = &lt;int&gt;<br>* Minimum time to sleep when reading results in round-robin mode when no data is available.<br>* Defaults to 10.<br><br>rr_max_sleep_ms = &lt;int&gt;<br>* Maximum time to sleep when reading results in round-robin mode when no data is available.<br>* Defaults to 1000<br><br>rr_sleep_factor = &lt;int&gt;<br>* If no data is available even after sleeping, increase the next sleep interval by this factor.<br>* defaults to 2<br><br>fieldstats_update_freq = &lt;number&gt;<br>* How often to update the field summary statistics, as a ratio to the elapsed run time so far.<br>* Smaller values means update more frequently. &nbsp;0 means as frequently as possible.<br>* Defaults to 0<br><br>fieldstats_update_maxperiod = &lt;int&gt;<br>* Maximum period for updating field summary statistics in seconds<br>* 0 means no maximum, completely dictated by current_run_time * fieldstats_update_freq<br>* defaults to 60<br><br>remote_timeline = [0|1]<br>* If true, allows the timeline to be computed remotely to enable better map/reduce scalability.<br>* defaults to true (1). <br><br>remote_timeline_prefetch = &lt;int&gt;<br>* Each peer should proactively send at most this many full events at the beginning<br>* Defaults to 100.<br><br>remote_timeline_parallel_fetch = &lt;bool&gt;<br>* Connect to multiple peers at the same time when fetching remote events?<br>* Defaults to true<br><br>remote_timeline_min_peers = &lt;int&gt;<br>* Minimum search peers for enabling remote computation of timelines.<br>* Defaults to 1 (1).<br><br>remote_timeline_fetchall = [0|1]<br>* If true, fetches all events accessible through the timeline from the remote peers before the job is <br>&nbsp;&nbsp;considered done.<br>* Defaults to true (1).<br><br>remote_timeline_thread = [0|1]<br>* If true, uses a separate thread to read the full events from remote peers if remote_timeline is used <br>and remote_timeline_fetchall is set to true. (Has no effect if remote_timeline or remote_timeline_fetchall is <br>false).<br>* Defaults to true (1).<br><br>remote_timeline_max_count = &lt;int&gt;<br>* Maximum number of events to be stored per timeline bucket on each search peer, <br>* Defaults to 10000<br><br>remote_timeline_max_size_mb = &lt;int&gt;<br>* Maximum size of disk that remote timeline events should take on each peer<br>* If limit is reached, a DEBUG message is emitted (and should be visible from job inspector/messages<br>* Defaults to 100<br><br>remote_timeline_touchperiod = &lt;int&gt;<br>* How often to touch remote timeline artifacts to keep them from being deleted by the remote peer, while a <br>search is running.<br>* In seconds, 0 means never.<br>* Defaults to 300.<br><br>remote_timeline_connection_timeout = &lt;int&gt;<br>* Connection timeout in seconds for fetching events processed by remote peer timeliner.<br>* Defaults to 5.<br><br>remote_timeline_send_timeout = &lt;int&gt;<br>* Send timeout in seconds for fetching events processed by remote peer timeliner.<br>* Defaults to 10.<br><br>remote_timeline_receive_timeout = &lt;int&gt;<br>* Receive timeout in seconds for fetching events processed by remote peer timeliner.<br>* Defaults to 10. <br><br>default_allow_queue = [0|1]<br>* Unless otherwise specified via REST api argument should an async job spawning request be queued on quota <br>violation (if not, an http error of server too busy is returned) <br>* Defaults to true (1).<br><br>queued_job_check_freq = &lt;int&gt;<br>* Frequency with which to check queued jobs to see if they can be started, in seconds<br>* Defaults to 1.<br><br>enable_history = &lt;bool&gt;<br>* Enable keeping track of searches?<br>* Defaults to true<br><br>max_history_length = &lt;int&gt;<br>* Max number of searches to store in history (per user/app)<br>* Defaults to 1000<br><br>allow_inexact_metasearch = &lt;bool&gt;<br>* Should a metasearch that is inexact be allow. &nbsp;If so, an INFO message will be added to the inexact metasearches. &nbsp;If not, a fatal exception will occur at search parsing time.<br>* Defaults to false<br><br>indexed_as_exact_metasearch = &lt;bool&gt;<br>* Should we allow a metasearch to treat &lt;field&gt;=&lt;value&gt; the same as &lt;field&gt;::&lt;value&gt; if &lt;field&gt; is an indexed field. &nbsp;Allowing this will allow a larger set of metasearches when allow_inexact_metasearch is set to false. &nbsp;However, some of these searches may be inconsistent with the results of doing a normal search.<br>* Defaults to false<br><br>dispatch_dir_warning_size = &lt;int&gt;<br>* The number of jobs in the dispatch directory when to issue a bulletin message warning that performance could be impacted<br>* Defaults to 2000<br><br>allow_reuse = &lt;bool&gt;<br>* Allow normally executed historical searches to be implicitly re-used for newer requests if the newer request allows it?<br>* Defaults to true<br><br>track_indextime_range = &lt;bool&gt;<br>* Track the _indextime range of returned search results?<br>* Defaults to true<br><br>reuse_map_maxsize = &lt;int&gt;<br>* Maximum number of jobs to store in the reuse map <br>* Defaults to 1000<br><br>status_period_ms = &lt;int&gt;<br>* The minimum amout of time, in milliseconds, between successive status/info.csv file updates<br>* This ensures search does not spend significant time just updating these files.<br>&nbsp;&nbsp;* This is typically important for very large number of search peers.<br>&nbsp;&nbsp;* It could also be important for extremely rapid responses from search peers,<br>&nbsp;&nbsp;&nbsp;&nbsp;when the search peers have very little work to do.<br>* Defaults to 1000 (1 second)<br><br>search_process_mode = auto | traditional | debug &lt;debugging-command&gt; [debugging-args ...]<br>* Control how search processes are started<br>* When set to "traditional", Splunk initializes each search process completely from scratch<br>* When set to a string beginning with "debug", Splunk routes searches through the given command, allowing the user the to "plug in" debugging tools<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;debugging-command&gt; must reside in one of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* $SPLUNK_HOME/etc/system/bin/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* $SPLUNK_HOME/etc/apps/$YOUR_APP/bin/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* $SPLUNK_HOME/bin/scripts/<br>&nbsp;&nbsp;&nbsp;&nbsp;* Splunk will pass &lt;debugging-args&gt;, followed by the search command it would normally run, to &lt;debugging-command&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example, given:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;search_process_mode = debug $SPLUNK_HOME/bin/scripts/search-debugger.sh 5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splunk will run a command that looks generally like:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$SPLUNK_HOME/bin/scripts/search-debugger.sh 5 splunkd search --id=... --maxbuckets=... --ttl=... [...]<br>* Defaults to "auto"<br><br>fetch_remote_search_log = [enabled|disabledSavedSearches|disabled]<br>* enabled: all remote search logs will be downloaded barring the oneshot search<br>* disabledSavedSearches: download all remote logs other than saved search logs and oneshot search logs<br>* disabled: irrespective of the search type all remote search log download functionality will be disabled<br>* Defaults to disabledSavedSearches<br>* The previous values:[true|false] are still supported but not recommended for use &nbsp;<br>* The previous value of true maps to the current value of enabled <br>* The previous value of false maps to the current value of disabled<br><br>load_remote_bundles = &lt;bool&gt;<br>* On a search peer, allow remote (search head) bundles to be loaded in splunkd.<br>* Defaults to false.<br><br>use_dispatchtmp_dir = &lt;bool&gt;<br>* Whether to use the dispatchtmp directory for temporary search time files (write temporary files to a different directory from a job's dispatch directory). <br>* Temp files would be written to $SPLUNK_HOME/var/run/splunk/dispatchtmp/&lt;sid&gt;/<br>* In search head pooling performance can be improved by mounting disaptchtmp to the <br>* local file system.<br>* Defaults to true if search head pooling is enabled, false otherwise<br><br>check_splunkd_period = &lt;int&gt;<br>* Amount of time, in seconds, that determines how frequently the search process <br>* (when running a real-time search) checks whether it's parent process (splunkd) is running or not. <br>* Defaults to 60<br><br>allow_batch_mode = &lt;bool&gt;<br>* Whether or not to allow the use of batch mode which searches in disk based batches in a time insensitive manner.<br>* In distributed search environments, this setting is used on the search head.<br>* Defaults to true<br><br>batch_search_max_index_values = &lt;int&gt;<br>* When using batch mode this limits the number of event entries read from the index file. These entries are small<br>* approximately 72 bytes. However batch mode is more efficient when it can read more entries at once.<br>* Setting this value to a smaller number can lead to slower search performance. A balance needs to be struck <br>* between more efficient searching in batch mode and running out of memory on the system with concurrently running searches. <br>* Defaults to 10000000<br><br><br>* These settings control the periodicity of retries to search peers in the event of failure. (Connection <br>* errors, and others.) The interval exists between failure and first retry, as well as successive <br>* retries in the event of further failures.<br><br>batch_retry_min_interval = &lt;int&gt;<br>* When batch mode attempts to retry the search on a peer that failed wait at least this many seconds<br>* Default to 5<br><br>batch_retry_max_interval = &lt;int&gt;<br>* When batch mode attempts to retry the search on a peer that failed wait at most this many seconds<br>* Default to 300<br><br>batch_retry_scaling = &lt;double&gt;<br>* After a retry attempt fails increase the time to wait before trying again by this scaling factor (Value should be &gt; 1.0)<br>* Default 1.5<br><br>batch_wait_after_end = &lt;int&gt;<br>* Batch mode considers the search ended(finished) when all peers without communication failure <br>* have expicitly indicated that they are complete; eg have delivered the complete answer.<br>* After the search is at an end, batch mode will continue to retry with lost-connection peers <br>* for this many seconds.<br>* Default 900<br><br>write_multifile_results_out = &lt;bool&gt;<br>* at the end of the search if results are in multiple files, write out the multiple<br>* files to results_dir directory, under the search results directory.<br>* This will speed up post-processing search, since the results will already be <br>* split into appropriate size files.<br>* Default true<br><br>enable_cumulative_quota = &lt;bool&gt;<br>* whether to enforce cumulative role based quotas <br>* Default false<br><br>remote_reduce_limit = &lt;unsigned long&gt;<br>* the number of results processed by a streaming search before we force a reduce <br>* Note: this option applies only if the search is ran with --runReduce=true (currently on Hunk does this)<br>* Note: a value of 0 is interpreted as unlimited<br>* Defaults to: 1000000<br><br>max_workers_searchparser = &lt;int&gt;<br>* the number of worker threads in processing search result when using round robin policy.<br>* default 5<br><br>max_chunk_queue_size = &lt;int&gt;<br>* the maximum size of the chunk queue<br>* default 10000<br><br>max_tolerable_skew = &lt;positive integer&gt;<br>* Absolute value of the largest timeskew in seconds that we will tolerate between <br>&nbsp;&nbsp;the native clock on the searchhead and the natic clock on the peer (independant of<br>&nbsp;&nbsp;time-zone). <br>* If this timeskew is exceeded we will log a warning. This estimate is &nbsp;approximate and tries <br>&nbsp;&nbsp;to account for network delays.<br><br>-- Unsupported [search] settings: --<br><br>enable_status_cache = &lt;bool&gt;<br>* This is not a user tunable setting. &nbsp;Do not use this setting without working<br>&nbsp;&nbsp;in tandem with Splunk personnel. &nbsp;This setting is not tested at non-default.<br>* This controls whether the status cache is used, which caches information<br>&nbsp;&nbsp;about search jobs (and job artifacts) in memory in main splunkd. &nbsp;<br>* Normally this cacheing is enabled and assists performance. However, when<br>&nbsp;&nbsp;using Search Head Pooling, artifacts in the shared storage location will be<br>&nbsp;&nbsp;changed by other serach heads, so this cacheing is disabled.<br>* Explicit requests to jobs endpoints , eg /services/search/jobs/&lt;sid&gt; are<br>&nbsp;&nbsp;always satisfied from disk, regardless of this setting.<br>* Defaults to true; except in Search Head Pooling environments where it<br>&nbsp;&nbsp;defaults to false.<br><br>status_cache_in_memory_ttl = &lt;positive integer&gt;<br>* This setting has no effect unless search head pooling is enabled, AND<br>&nbsp;&nbsp;enable_status_cache has been set to true.<br>* This is not a user tunable setting. &nbsp;Do not use this setting without working<br>&nbsp;&nbsp;in tandem with Splunk personnel. This setting is not tested at non-default.<br>* If set, controls the number of milliseconds which a status cache entry may be<br>&nbsp;&nbsp;used before it expires.<br>* Defaults to 60000, or 60 seconds.<br><br>[realtime] <br># Default options for indexer support of real-time searches<br># These can all be overriden for a single search via REST API arguments<br><br>local_connect_timeout = &lt;int&gt;<br>* Connection timeout for an indexer's search process when connecting to that indexer's splunkd (in seconds)<br>* Defaults to 5<br><br>local_send_timeout = &lt;int&gt;<br>* Send timeout for an indexer's search process when connecting to that indexer's splunkd (in seconds)<br>* Defaults to 5<br><br>local_receive_timeout = &lt;int&gt;<br>* Receive timeout for an indexer's search process when connecting to that indexer's splunkd (in seconds)<br>* Defaults to 5<br><br>queue_size = &lt;int&gt;<br>* Size of queue for each real-time search (must be &gt;0).<br>* Defaults to 10000<br><br>blocking = [0|1] <br>* Specifies whether the indexer should block if a queue is full.<br>* Defaults to false<br><br>max_blocking_secs = &lt;int&gt;<br>* Maximum time to block if the queue is full (meaningless if blocking = false)<br>* 0 means no limit<br>* Default to 60 <br><br>indexfilter = [0|1]<br>* Specifies whether the indexer should prefilter events for efficiency.<br>* Defaults to true (1).<br><br>default_backfill = &lt;bool&gt;<br>* Specifies if windowed real-time searches should backfill events<br>* Defaults to true<br><br>enforce_time_order = &lt;bool&gt;<br>* Specifies if real-time searches should ensure that events are sorted in ascending time order (the UI will automatically reverse the order that it display events for real-time searches so in effect the latest events will be first)<br>* Defaults to true<br><br>disk_usage_update_period = &lt;int&gt;<br>* Specifies how frequently (in seconds) should the search process estimate the artifact disk usage.<br>* Defaults to 10<br><br>indexed_realtime_use_by_default = &lt;bool&gt;<br>* Should we use the indexedRealtime mode by default<br>* Precedence: SearchHead<br>* Defaults to false<br><br>indexed_realtime_disk_sync_delay = &lt;int&gt;<br>* After indexing there is a non-deterministic period where the files on disk when opened by other<br>* programs might not reflect the latest flush to disk, particularly when a system is under heavy load.<br>* This settings controls the number of seconds to wait for disk flushes to finish when using<br>* indexed/continuous/psuedo realtime search so that we see all of the data.<br>* Precedence: SearchHead overrides Indexers<br>* Defaults to 60 <br><br>indexed_realtime_default_span = &lt;int&gt;<br>* An indexed realtime search is made up of many component historical searches that by default will<br>* span this many seconds. If a component search is not completed in this many seconds the next<br>* historical search will span the extra seconds. To reduce the overhead of running an indexed realtime<br>* search you can change this span to delay longer before starting the next component historical search.<br>* Precendence: Indexers<br>* Defaults to 1<br><br>indexed_realtime_maximum_span = &lt;int&gt;<br>* While running an indexed realtime search, if the component searches regularly take longer than <br>* indexed_realtime_default_span seconds, then indexed realtime search can fall more than <br>* indexed_realtime_disk_sync_delay seconds behind realtime. Use this setting to set a limit <br>* afterwhich we will drop data to return back to catch back up to the specified delay from <br>* realtime, and only search the default span of seconds. <br>* Precedence: API overrides SearchHead overrides Indexers<br>* Defaults to 0 (unlimited) <br><br>indexed_realtime_cluster_update_interval = &lt;int&gt;<br>* While running an indexed realtime search, if we are on a cluster we need to update the list<br>* of allowed primary buckets. This controls the interval that we do this. And it must be less <br>* than the indexed_realtime_disk_sync_delay. If your buckets transition from Brand New to warm <br>* in less than this time indexed realtime will lose data in a clustered environment.<br>* Precendence: Indexers<br>* Default: 30<br><br>alerting_period_ms = &lt;int&gt;<br>* This limits the frequency that we will trigger alerts during a realtime search <br>* 0 means unlimited and we will trigger an alert for every batch of events we read<br>* in dense realtime searches with expensive alerts this can overwhelm the alerting<br>* system. <br>* Precedence: Searchhead<br>* Default: 0<br><br><br>[slc]<br><br>maxclusters = &lt;integer&gt;<br>* Maximum number of clusters to create.<br>* Defaults to 10000.<br><br>[findkeywords]<br>maxevents = &lt;integer&gt;<br>* Maximum number of events used by findkeywords command and the Patterns tab.<br>* Defaults to 50000.<br><br>[sort]<br><br>maxfiles = &lt;integer&gt;<br>* Maximum files to open at once. &nbsp;Multiple passes are made if the number of result chunks <br>exceeds this threshold.<br>* Defaults to 64.<br><br>[stats|sistats]<br><br>maxmem_check_freq = &lt;integer&gt;<br>* How frequently to check to see if we are exceeding the in memory data structure size limit as specified by max_mem_usage_mb, in rows<br>* Defaults to 50000 rows<br><br>maxresultrows = &lt;integer&gt;<br>* Maximum number of rows allowed in the process memory. <br>* When the search process exceeds max_mem_usage_mb and maxresultrows, data is spilled out to the disk<br>* If not specified, defaults to searchresults::maxresultrows (which is by default 50000).<br><br>maxvalues = &lt;integer&gt;<br>* Maximum number of values for any field to keep track of.<br>* Defaults to 0 (unlimited).<br><br>maxvaluesize = &lt;integer&gt;<br>* Maximum length of a single value to consider.<br>* Defaults to 0 (unlimited).<br><br># rdigest is a data structure used to compute approximate order statistics (such as median and percentiles) <br># using sublinear space.<br><br>rdigest_k = &lt;integer&gt;<br>* rdigest compression factor<br>* Lower values mean more compression<br>* After compression, number of nodes guaranteed to be greater than or equal to 11 times k.<br>* Defaults to 100, must be greater than or equal to 2<br><br>rdigest_maxnodes = &lt;integer&gt;<br>* Maximum rdigest nodes before automatic compression is triggered.<br>* Defaults to 1, meaning automatically configure based on k value<br><br>max_stream_window = &lt;integer&gt;<br>* For streamstats command, the maximum allow window size<br>* Defaults to 10000.<br><br>max_valuemap_bytes = &lt;integer&gt;<br>* For sistats command, the maximum encoded length of the valuemap, per result written out<br>* If limit is exceeded, extra result rows are written out as needed. &nbsp;(0 = no limit per row)<br>* Defaults to 100000.<br><br>perc_method = nearest-rank|interpolated<br>* Which method to use for computing percentiles (and medians=50 percentile).<br>* nearest-rank picks the number with 0-based rank R = floor((percentile/100)*count)<br>* interpolated means given F = (percentile/100)*(count-1), pick ranks R1 = floor(F) and R2 = ceiling(F). &nbsp;Answer = (R2 * (F - R1)) + (R1 * (1 - (F - R1)))<br>* See wikipedia percentile entries on nearest rank and "alternative methods" <br>* Defaults to interpolated<br><br>approx_dc_threshold = &lt;integer&gt;<br>* When using approximate distinct count (i.e. estdc(&lt;field&gt;) in stats/chart/timechart), do not use approximated results if the actual number of distinct values is less than this number<br>* Defaults to 1000<br><br>dc_digest_bits = &lt;integer&gt;<br>* 2^&lt;integer&gt; bytes will be size of digest used for approximating distinct count.<br>* Defaults to 10 (equivalent to 1KB)<br>* Must be &gt;= 8 (128B) and &lt;= 16 (64KB)<br><br>natural_sort_output = &lt;bool&gt;<br>* Do a natural sort on the output of stats if output size is &lt;= maxresultrows<br>* Natural sort means that we sort numbers numerically and non-numbers lexicographically<br>* Defaults to true<br><br>list_maxsize = &lt;int&gt;<br>* Maximum number of list items to emit when using the list() function stats/sistats<br>* Defaults to 100<br><br>sparkline_maxsize = &lt;int&gt;<br>* Maximum number of elements to emit for a sparkline<br>* Defaults to value of list_maxsize setting<br><br>default_partitions = &lt;int&gt;<br>* Number of partitions to split incoming data into for parallel/multithreaded reduce<br>* Defaults to 1<br><br>partitions_limit = &lt;int&gt;<br>* Maximum number of partitions to split into that can be specified via the 'partitions' option.<br>* When exceeded, the number of partitions is reduced to this limit.<br>* Defaults to 100<br><br>[thruput]<br><br>maxKBps = &lt;integer&gt;<br>* If specified and not zero, this limits the speed through the thruput processor to the specified <br>rate in kilobytes per second.<br>* To control the CPU load while indexing, use this to throttle the number of events this indexer <br>processes to the rate (in KBps) you specify. <br><br>[journal_compression]<br><br>threads = &lt;integer&gt;<br>* Specifies the maximum number of indexer threads which will be work on compressing hot bucket journal data.<br>* Defaults to the number of CPU threads of the host machine<br>* This setting does not typically need to be modified.<br><br>[top]<br><br>maxresultrows = &lt;integer&gt;<br>* Maximum number of result rows to create.<br>* If not specified, defaults to searchresults::maxresultrows (usually 50000).<br><br>maxvalues = &lt;integer&gt;<br>* Maximum number of distinct field vector values to keep track of.<br>* Defaults to 100000.<br><br>maxvaluesize = &lt;integer&gt;<br>* Maximum length of a single value to consider.<br>* Defaults to 1000.<br><br>[summarize]<br>hot_bucket_min_new_events = &lt;integer&gt;<br>* The minimum number of new events that need to be added to the hot bucket (since last summarization)<br>* before a new summarization can take place. To disable hot bucket summarization set this value to a <br>* large positive number.<br>* Defaults to 100000<br><br>sleep_seconds = &lt;integer&gt;<br>* The amount of time to sleep between polling of summarization complete status.<br>* Default to 5<br><br>stale_lock_seconds = &lt;integer&gt;<br>* The amount of time to have elapse since the mod time of a .lock file before summarization considers <br>* that lock file stale and removes it<br>* Default to 600<br><br>max_summary_ratio = &lt;float&gt;<br>* A number in the [0-1) range that indicates the maximum ratio of summary data / bucket size at which <br>* point the summarization of that bucket, for the particual search, will be disabled. Use 0 to disable.<br>* Defaults to 0<br><br>max_summary_size = &lt;int&gt;<br>* Size of summary, in bytes, at which point we'll start applying the max_summary_ratio. Use 0 to disable.<br>* Defaults to 0<br><br>max_time = &lt;int&gt;<br>* The maximum amount of time, seconds, that a summary search process is allowed to run. Use 0 to disable.<br>* Defaults to 0<br><br>indextime_lag = &lt;unsigned int&gt;<br>* The amount of lag time to give indexing to ensure that it has synced any received events to disk. Effectively,<br>* the data that has been received in the past indextime_lag will NOT be summarized.<br>* Do not change this value unless directed by Splunk support.<br>* Defaults to 90<br><br>[transactions]<br><br>maxopentxn = &lt;integer&gt;<br>* Specifies the maximum number of not yet closed transactions to keep in the open pool before starting to evict transactions.<br>* Defaults to 5000.<br><br>maxopenevents = &lt;integer&gt;<br>* Specifies the maximum number of events (which are) part of open transactions before transaction eviction starts happening, using LRU policy.<br>* Defaults to 100000.<br><br>[inputproc]<br><br>max_fd = &lt;integer&gt;<br>* Maximum number of file descriptors that Splunk will keep open, to capture any trailing data from <br>files that are written to very slowly.<br>* Defaults to 100.<br><br>time_before_close = &lt;integer&gt;<br>* MOVED. &nbsp;This setting is now configured per-input in inputs.conf.<br>* Specifying this setting in limits.conf is DEPRECATED, but for now will override the setting for all <br>monitor inputs.<br><br>tailing_proc_speed = &lt;integer&gt;<br>* REMOVED. &nbsp;This setting is no longer used.<br><br>file_tracking_db_threshold_mb = &lt;integer&gt;<br>* this setting controls the trigger point at which the file tracking db (also commonly known as the "fishbucket" or btree) rolls over. &nbsp;A new database is created in its place. &nbsp;Writes are targetted at new db. &nbsp;Reads are first targetted at new db, and we fall back to old db for read failures. &nbsp;Any reads served from old db successfully will be written back into new db.<br>* MIGRATION NOTE: if this setting doesn't exist, the initialization code in splunkd triggers an automatic migration step that reads in the current value for "maxDataSize" under the "_thefishbucket" stanza in indexes.conf and writes this value into etc/system/local/limits.conf.<br><br>[scheduler]<br>max_searches_perc = &lt;integer&gt;<br>* The maximum number of searches the scheduler can run, as a percentage of the maximum number of concurrent <br>searches, see [search] max_searches_per_cpu for how to set the system wide maximum number of searches.<br>* Defaults to 50.<br><br>auto_summary_perc = &lt;integer&gt;<br>* The maximum number of concurrent searches to be allocated for auto summarization, as a percentage of the<br>concurrent searches that the scheduler can run. <br>* Auto summary searches include:<br>&nbsp;&nbsp;* Searches which generate the data for the Report Acceleration feature.<br>&nbsp;&nbsp;* Searches which generate the data for Data Model acceleration.<br>* Note: user scheduled searches take precedence over auto summary searches.<br>* Defaults to 50.<br><br>max_action_results = &lt;integer&gt;<br>* The maximum number of results to load when triggering an alert action.<br>* Defaults to 50000<br><br>action_execution_threads = &lt;integer&gt;<br>* Number of threads to use to execute alert actions, change this number if your alert actions take a long <br>time to execute. <br>* This number is capped at 10.<br>* Defaults to 2<br><br>actions_queue_size = &lt;integer&gt;<br>* The number of alert notifications to queue before the scheduler starts blocking, set to 0 for infinite size.<br>* Defaults to 100<br><br>actions_queue_timeout = &lt;integer&gt;<br>* The maximum amount of time, in seconds to block when the action queue size is full.<br>* Defaults to 30<br><br>alerts_max_count = &lt;integer&gt;<br>* Maximum number of unexpired alerts information to keep for the alerts manager, when this number is reached <br>Splunk will start discarding the oldest alerts.<br>* Defaults to 50000<br><br>alerts_max_history = &lt;integer&gt;[s|m|h|d]<br>* Maximum time in the past to search for previously triggered alerts. These alerts will then be displayed on the Activity -&gt; Triggered Alerts page.<br>* Increasing this number beyond default may cause slowdown.<br>* Defaults to 7 days.<br><br>alerts_scoping = host|splunk_server|all<br>* Determines the scoping to use on the search to populate the triggered alerts page. Choosing splunk_server will result in the search query using splunk_server=local, host will result in the search query using host=&lt;search-head-host-name&gt;, and all will have no scoping added to the search query.<br>* Defaults to splunk_server.<br><br>alerts_expire_period = &lt;integer&gt;<br>* The amount of time between expired alert removal<br>* This period controls how frequently the alerts list is scanned, the only benefit from reducing this is <br>better resolution in the number of alerts fired at the savedsearch level.<br>* Change not recommended.<br>* Defaults to 120.<br><br>persistance_period = &lt;integer&gt;<br>* The period (in seconds) between scheduler state persistance to disk. The scheduler currently persists <br>the suppression and fired-unexpired alerts to disk. <br>* This is relevant only in search head pooling mode.<br>* Defaults to 30.<br><br>max_lock_files = &lt;int&gt;<br>* The number of most recent lock files to keep around. <br>* This setting only applies in search head pooling.<br><br>max_lock_file_ttl = &lt;int&gt;<br>* Time (in seconds) that must pass before reaping a stale lock file .<br>* Only applies in search head pooling.<br><br>max_per_result_alerts = &lt;int&gt;<br>* Maximum number of alerts to trigger for each saved search instance (or real-time results preview for RT alerts)<br>* Only applies in non-digest mode alerting. Use 0 to disable this limit<br>* Defaults to 500<br><br>max_per_result_alerts_time = &lt;int&gt;<br>* Maximum number of time to spend triggering alerts for each saved search instance (or real-time results preview for RT alerts)<br>* Only applies in non-digest mode alerting. Use 0 to disable this limit.<br>* Defaults to 300<br><br>scheduled_view_timeout = &lt;int&gt;[s|m|h|d]<br>* The maximum amount of time that a scheduled view (pdf delivery) would be allowed to render<br>* Defaults to 60m<br><br>concurrency_message_throttle_time = &lt;int&gt;[s|m|h|d]<br>* Amount of time controlling throttling between messages warning about scheduler concurrency limits<br>*Defaults to 10m<br><br>shp_dispatch_to_slave = &lt;bool&gt;<br>* by default the scheduler should distribute jobs throughout the pool.<br>* Defaults to true<br><br>[auto_summarizer]<br>cache_timeout = &lt;integer&gt;<br>* The amount of time, in seconds, to cache auto summary details and search hash codes<br>* Defaults to 600 - 10 minutes <br><br>search_2_hash_cache_timeout = &lt;integer&gt;<br>* The amount of time, in seconds, to cache search hash codes<br>* Defaults to the value of cache_timeout i.e. 600 - 10 minutes<br><br>maintenance_period = &lt;integer&gt;<br>* The period of time, in seconds, that the auto summarization maintenance happens<br>* Defaults to 1800 (30 minutes)<br><br>allow_event_summarization = &lt;bool&gt;<br>* Whether auto summarization of searches whose remote part returns events rather than results will be allowed.<br>* Defaults to false<br><br>max_verify_buckets = &lt;int&gt;<br>* When verifying buckets, stop after verifying this many buckets if no failures have been found<br>* 0 means never<br>* Defaults to 100<br><br>max_verify_ratio = &lt;number&gt;<br>* Maximum fraction of data in each bucket to verify<br>* Defaults to 0.1 (10%)<br><br>max_verify_bucket_time = &lt;int&gt;<br>* Maximum time to spend verifying each bucket, in seconds<br>* Defaults to 15 (seconds)<br><br>verify_delete = &lt;bool&gt;<br>* Should summaries that fail verification be automatically deleted?<br>* Defaults to false<br><br>max_verify_total_time = &lt;int&gt;<br>* Maximum total time in seconds to spend doing verification, regardless if any buckets have failed or not<br>* Defaults to 0 (no limit)<br><br>max_run_stats = &lt;int&gt;<br>* Maximum number of summarization run statistics to keep track and expose via REST.<br>* Defaults to 48<br><br>return_actions_with_normalized_ids = [yes|no|fromcontext]<br>* Report acceleration summaries are stored under a signature/hash which can be regular or normalized.<br>* Normalization improves the re-use of pre-built summaries but is not supported before 5.0. This config<br>* will determine the default value of how normalization works (regular/normalized)<br>* Default value is "fromcontext", which would mean the end points and summaries would be operating based on context.<br>* normalization strategy can also be changed via admin/summarization REST calls with the "use_normalization"<br>* parameter which can take the values "yes"/"no"/"fromcontext"<br><br>normalized_summaries = &lt;bool&gt;<br>* Turn on/off normalization of report acceleration summaries.<br>* Default = false and will become true in 6.0<br><br>detailed_dashboard = &lt;bool&gt;<br>* Turn on/off the display of both normalized and regular summaries in the Report <br>* acceleration summary dashboard and details.<br>* Default = false<br><br>shc_accurate_access_counts = &lt;bool&gt;<br>* Only relevant if you are using search head clustering<br>* Turn on/off to make acceleration summary access counts accurate on the captain.<br>* by centralizing the access requests on the captain.<br>* Default = false<br><br>[show_source]<br>max_count = &lt;integer&gt;<br>* Maximum number of events accessible by show_source. <br>* The show source command will fail when more than this many events are in the same second as the requested event.<br>* Defaults to 10000<br><br>max_timebefore = &lt;timespan&gt;<br>* Maximum time before requested event to show.<br>* Defaults to '1day' (86400 seconds)<br><br>max_timeafter = &lt;timespan&gt;<br>* Maximum time after requested event to show.<br>* Defaults to '1day' (86400 seconds)<br><br>distributed = &lt;bool&gt;<br>* Controls whether we will do a distributed search for show source to get events from all servers and indexes<br>* Turning this off results in better performance for show source, but events will only come from the initial server and index<br>* NOTE: event signing and verification is not supported in distributed mode<br>* Defaults to true<br><br>distributed_search_limit = &lt;unsigned int&gt;<br>* Sets a limit on the maximum events we will request when doing the search for distributed show source<br>* As this is used for a larger search than the initial non-distributed show source, it is larger than max_count<br>* Splunk will rarely return anywhere near this amount of results, as we will prune the excess results<br>* The point is to ensure the distributed search captures the target event in an environment with many events<br>* Defaults to 30000<br><br>[typeahead]<br>maxcount = &lt;integer&gt;<br>* Maximum number of typeahead results to find.<br>* Defaults to 1000<br><br>use_cache = [0|1]<br>* Specifies whether the typeahead cache will be used if use_cache is not specified in the command line or endpoint.<br>* Defaults to true.<br><br>fetch_multiplier = &lt;integer&gt;<br>* A multiplying factor that determines the number of terms to fetch from the index, fetch = fetch_multiplier x count.<br>* Defaults to 50<br><br>cache_ttl_sec = &lt;integer&gt;<br>* How long the typeahead cached results are valid, in seconds.<br>* Defaults to 300. <br><br>min_prefix_length = &lt;integer&gt;<br>* The minimum string prefix after which to provide typeahead.<br>* Defaults to 1.<br><br>max_concurrent_per_user = &lt;integer&gt;<br>* The maximum number of concurrent typeahead searches per user. Once this maximum is reached only cached <br>* typeahead results might be available<br>* Defaults to 3.<br><br>[typer]<br>maxlen = &lt;int&gt;<br>* In eventtyping, pay attention to first &lt;int&gt; characters of any attribute (such as _raw), including individual <br>tokens. Can be overridden by supplying the typer operator with the argument maxlen (for example, "|typer maxlen=300").<br>* Defaults to 10000.<br><br>[authtokens]<br>expiration_time = &lt;integer&gt;<br>* Expiration time of auth tokens in seconds.<br>* Defaults to 3600<br><br>[sample]<br><br>maxsamples = &lt;integer&gt;<br>* Defaults to 10000<br><br>maxtotalsamples = &lt;integer&gt;<br>* Defaults to 100000<br><br>[metadata]<br>maxresultrows = &lt;integer&gt;<br>&nbsp;* the maximum number of results in a single chunk fetched by the metadata command<br>&nbsp;* a smaller value will require less memory on the search head in setups with<br>&nbsp;&nbsp;&nbsp;large number of peers and many metadata results, though, setting this too<br>&nbsp;&nbsp;&nbsp;small will decrease the search performance<br>&nbsp;* default is 10000<br>&nbsp;* do not change unless instructed to do so by Splunk Support<br>maxcount = &lt;integer&gt;<br>&nbsp;* the total number of metadata search results returned by the search head;<br>&nbsp;&nbsp;&nbsp;after the maxcount is reached, any addtional metadata results received from<br>&nbsp;&nbsp;&nbsp;the search peers will be ignored (not returned)<br>&nbsp;* a larger number incurs additional memory usage on the search head<br>&nbsp;* default is 100000<br><br>[set]<br>maxresultrows = &lt;integer&gt;<br>&nbsp;* the maximum number of results the set command will use from each resultset to compute the required set operation<br><br>[input_channels]<br>max_inactive = &lt;integer&gt;<br>* internal setting, do not change unless instructed to do so by Splunk Support<br><br>lowater_inactive = &lt;integer&gt;<br>* internal setting, do not change unless instructed to do so by Splunk Support<br><br>inactive_eligibility_age_seconds = &lt;integer&gt;<br>* internal setting, do not change unless instructed to do so by Splunk Support<br><br>[ldap]<br>max_users_to_precache = &lt;unsigned integer&gt;<br>* The maximum number of users we will attempt to precache from LDAP after reloading auth<br>* Set this to 0 to turn off precaching<br><br>allow_multiple_matching_users = &lt;bool&gt;<br>* This controls whether we allow login when we find multiple entries with the same value for the username attribute<br>* When multiple entries are found, we choose the first user DN lexicographically<br>* Setting this to false is more secure as it does not allow any ambiguous login, but users with duplicate entries will not be able to login.<br>* Defaults to true<br><br>[spath]<br>extraction_cutoff = &lt;integer&gt;<br>* For extract-all spath extraction mode, only apply extraction to the first &lt;integer&gt; number of bytes<br>* Defaults to 5000<br><br>extract_all = &lt;boolean&gt;<br>* Controls whether we respect automatic field extraction when spath is invoked manually.<br>* If true, we extract all fields regardless of settings. &nbsp;If false, we only extract fields used by later splunk commands.<br><br>[reversedns]<br>rdnsMaxDutyCycle = &lt;integer&gt;<br>* generate diagnostic WARN in splunkd.log if reverse dns lookups are taking <br>* more than this percent of time<br>* range 0-100<br>* default 10<br><br>[viewstates]<br><br>enable_reaper = &lt;boolean&gt;<br>* Controls whether the viewstate reaper runs<br>* Defaults to true<br><br>reaper_freq = &lt;integer&gt;<br>* Controls how often the viewstate reaper runs<br>* Defaults to 86400 (1 day)<br><br>reaper_soft_warn_level = &lt;integer&gt;<br>* Controls what the reaper considers an acceptable number of viewstates<br>* Defaults to 1000<br><br>ttl = &lt;integer&gt;<br>* Controls the age at which a viewstate is considered eligible for reaping<br>* Defaults to 86400 (1 day)<br><br>[geostats]<br><br>maxzoomlevel = &lt;integer&gt;<br>* contols the number of zoom levels that geostats will cluster events on<br><br>zl_0_gridcell_latspan = &lt;float&gt;<br>* contols what is the grid spacing in terms of latitude degrees at the lowest zoom level, which is zoom-level 0<br>* grid-spacing at other zoom levels are auto created from this value by reducing by a factor of 2 at each zoom-level.<br><br>zl_0_gridcell_longspan = &lt;float&gt;<br>* contols what is the grid spacing in terms of longitude degrees at the lowest zoom level, which is zoom-level 0<br>* grid-spacing at other zoom levels are auto created from this value by reducing by a factor of 2 at each zoom-level.<br><br>filterstrategy = &lt;integer&gt;<br>* controls the selection strategy on the geoviz map. Allowed values are 1 and 2<br><br>[iplocation]<br><br>db_path = &lt;path&gt;<br>* Location of GeoIP database in MMDB format<br>* If not set, defaults to database included with splunk<br><br>[tscollect]<br>squashcase = &lt;boolean&gt;<br>* The default value of the 'squashcase' argument if not specified by the command<br>* Defaults to false<br><br>keepresults = &lt;boolean&gt;<br>* The default value of the 'keepresults' argument if not specified by the command<br>* Defaults to false<br><br>optimize_max_size_mb = &lt;unsigned int&gt;<br>* The maximum size in megabytes of files to create with optimize<br>* Specify 0 for no limit (may create very large tsidx files)<br>* Defaults to 1024<br><br>[tstats]<br>apply_search_filter = &lt;boolean&gt;<br>* Controls whether we apply role-based search filters when users run tstats on normal index data<br>* Note: we never apply search filters to data collected with tscollect or datamodel acceleration<br>* Defaults to true<br><br>summariesonly = &lt;boolean&gt;<br>* The default value of 'summariesonly' arg if not specified by the command<br>* When running tstats on an accelerated datamodel, summariesonly=false implies a mixed mode where we will fall back to search for missing TSIDX data<br>* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;summariesonly=true overrides this mixed mode to only generate results from TSIDX data, which may be incomplete<br>* Defaults to false<br><br>allow_old_summaries = &lt;boolean&gt;<br>* The default value of 'allow_old_summaries' arg if not specified by the command<br>* When running tstats on an accelerated datamodel, allow_old_summaries=false ensures we check that the datamodel search in each bucket's summary metadata is considered up to date with the current datamodel search. Only summaries that are considered up to date will be used to deliver results.<br>* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allow_old_summaries=true overrides this behavior and will deliver results even from bucket summaries that are considered out of date with the current datamodel.<br>* Defaults to false<br><br>chunk_size = &lt;unsigned int&gt;<br>* ADVANCED: The default value of 'chunk_size' arg if not specified by the command<br>* This argument controls how many events are retrieved at a time within a single TSIDX file when answering queries<br>* Consider lowering this value if tstats queries are using too much memory (cannot be set lower than 10000)<br>* Larger values will tend to cause more memory to be used (per search) and might have performance benefits.<br>* Smaller values will tend to reduce performance and might reduce memory used (per search). <br>* Altering this value without careful measurement is not advised.<br>* Defaults to 10000000<br><br>[pdf]<br>max_rows_per_table = &lt;unsigned int&gt;<br>* The maximum number of rows that will be rendered for a table within integrated PDF rendering<br>* Defaults to 1000<br><br>render_endpoint_timeout = &lt;unsigned int&gt;<br>* The number of seconds after which the pdfgen render endpoint will timeout if it has not yet finished rendering the PDF output <br>* Defaults to 3600<br><br>[kvstore]<br>max_accelerations_per_collection = &lt;unsigned int&gt;<br>* The maximum number of accelerations that can be assigned to a single collection<br>* Valid values range from 0 to 50<br>* Defaults to 10<br><br>max_fields_per_acceleration = &lt;unsigned int&gt;<br>* The maximum number of fields that can be part of a compound acceleration (i.e. an acceleration with multiple keys)<br>* Valid values range from 0 to 50<br>* Defaults to 10<br><br>max_rows_per_query = &lt;unsigned int&gt;<br>* The maximum number of rows that will be returned for a single query to a collection.<br>* If the query returns more rows than the specified value, then returned result set will contain the number of rows specified in this value.<br>* Defaults to 50000<br><br>max_queries_per_batch = &lt;unsigned int&gt;<br>* The maximum number of queries that can be run in a single batch<br>* Defaults to 1000<br><br>max_size_per_result_mb = &lt;unsigned int&gt;<br>* The maximum size of the result that will be returned for a single query to a collection in MB.<br>* Defaults to 50 MB<br><br>max_size_per_batch_save_mb = &lt;unsigned int&gt;<br>* The maximum size of a batch save query in MB<br>* Defaults to 50 MB<br><br>max_documents_per_batch_save = &lt;unsigned int&gt;<br>* The maximum number of documents that can be saved in a single batch<br>* Defaults to 1000<br><br>max_size_per_batch_result_mb = &lt;unsigned int&gt;<br>* The maximum size of the result set from a set of batched queries<br>* Defaults to 100 MB<br><br></font></code>
<h3> <a name="limitsconf_limits.conf.example"><span class="mw-headline" id="limits.conf.example">limits.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br># CAUTION: Do not alter the settings in limits.conf unless you know what you are doing. <br># Improperly configured limits may result in splunkd crashes and/or memory overuse.<br><br><br>[searchresults]<br>maxresultrows = 50000<br># maximum number of times to try in the atomic write operation (1 = no retries)<br>tocsv_maxretry = 5<br># retry period is 1/2 second (500 milliseconds)<br>tocsv_retryperiod_ms = 500<br><br>[subsearch]<br># maximum number of results to return from a subsearch<br>maxout = 100<br># maximum number of seconds to run a subsearch before finalizing<br>maxtime = 10<br># time to cache a given subsearch's results<br>ttl = 300<br><br>[anomalousvalue]<br>maxresultrows = 50000<br># maximum number of distinct values for a field<br>maxvalues = 100000<br># maximum size in bytes of any single value (truncated to this size if larger)<br>maxvaluesize = 1000<br><br>[associate]<br>maxfields = 10000<br>maxvalues = 10000<br>maxvaluesize = 1000<br><br># for the contingency, ctable, and counttable commands<br>[ctable]<br>maxvalues = 1000<br><br>[correlate]<br>maxfields = 1000<br><br># for bin/bucket/discretize<br>[discretize]<br>maxbins = 50000 <br># if maxbins not specified or = 0, defaults to searchresults::maxresultrows<br><br>[inputcsv]<br># maximum number of retries for creating a tmp directory (with random name in SPLUNK_HOME/var/run/splunk)<br>mkdir_max_retries = 100<br><br>[kmeans]<br>maxdatapoints = 100000000<br><br>[kv]<br># when non-zero, the point at which kv should stop creating new columns<br>maxcols = 512<br><br>[rare]<br>maxresultrows = 50000<br># maximum distinct value vectors to keep track of<br>maxvalues = 100000<br>maxvaluesize = 1000<br><br>[restapi]<br># maximum result rows to be returned by /events or /results getters from REST API &nbsp;<br>maxresultrows = 50000<br><br>[search]<br># how long searches should be stored on disk once completed<br>ttl = 86400<br><br># the approximate maximum number of timeline buckets to maintain<br>status_buckets = 300<br><br># the last accessible event in a call that takes a base and bounds<br>max_count = 10000<br><br># the minimum length of a prefix before a * to ask the index about<br>min_prefix_len = 1<br><br># the length of time to persist search cache entries (in seconds)<br>cache_ttl = 300<br><br>[slc]<br># maximum number of clusters to create<br>maxclusters = 10000<br><br>[findkeywords]<br>#events to use in findkeywords command (and patterns UI)<br>maxevents = 50000<br><br>[stats]<br>maxresultrows = 50000<br>maxvalues = 10000<br>maxvaluesize = 1000<br><br>[top]<br>maxresultrows = 50000<br># maximum distinct value vectors to keep track of<br>maxvalues = 100000<br>maxvaluesize = 1000<br><br></font></code>

<a name="literalsconf"></a><h2> <a name="literalsconf_literals.conf"><span class="mw-headline" id="literals.conf">literals.conf</span></a></h2>
<p>The following are the spec and example files for literals.conf.
</p>
<h3> <a name="literalsconf_literals.conf.spec"><span class="mw-headline" id="literals.conf.spec">literals.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains attribute/value pairs for configuring externalized strings in literals.conf.<br>#<br># There is a literals.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a literals.conf in $SPLUNK_HOME/etc/system/local/. For examples, see <br># literals.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># For the full list of all literals that can be overridden, check out<br># $SPLUNK_HOME/etc/system/default/literals.conf.<br><br>###############################################################################################<br>#<br># CAUTION: <br>#<br># &nbsp;&nbsp;&nbsp;- You can destroy Splunk's performance by editing literals.conf incorrectly. &nbsp;<br>#<br># &nbsp;&nbsp;&nbsp;- Only edit the attribute values (on the right-hand side of the '=').<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DO NOT edit the attribute names (left-hand side of the '=').<br>#<br># &nbsp;&nbsp;&nbsp;- When strings contain "%s", do not add or remove any occurrences of&nbsp;%s, <br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or reorder their positions.<br>#<br># &nbsp;&nbsp;&nbsp;- When strings contain HTML tags, take special care to make sure<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that all tags and quoted attributes are properly closed, and<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that all entities such as &amp; are escaped.<br>#<br><br><br><br><br><br></font></code>
<h3> <a name="literalsconf_literals.conf.example"><span class="mw-headline" id="literals.conf.example">literals.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example literals.conf, which is used to<br># configure the externalized strings in Splunk.<br>#<br># For the full list of all literals that can be overwritten, consult<br># the far longer list in $SPLUNK_HOME/etc/system/default/literals.conf<br>#<br><br>[ui]<br>PRO_SERVER_LOGIN_HEADER = Login to Splunk (guest/guest)<br>INSUFFICIENT_DISK_SPACE_ERROR = The server's free disk space is too low. &nbsp;Indexing will temporarily pause until more disk space becomes available.<br>SERVER_RESTART_MESSAGE = This Splunk Server's configuration has been changed. &nbsp;The server needs to be restarted by an administrator.<br>UNABLE_TO_CONNECT_MESSAGE = Could not connect to splunkd at&nbsp;%s.<br><br><br></font></code>

<a name="macrosconf"></a><h2> <a name="macrosconf_macros.conf"><span class="mw-headline" id="macros.conf">macros.conf</span></a></h2>
<p>The following are the spec and example files for macros.conf.
</p>
<h3> <a name="macrosconf_macros.conf.spec"><span class="mw-headline" id="macros.conf.spec">macros.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for search language macros.<br><br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[&lt;STANZA_NAME&gt;]<br>* Each stanza represents a search macro that can be referenced in any search.<br>* The stanza name is the name of the macro if the macro takes no arguments. &nbsp;Otherwise, <br>&nbsp;&nbsp;the stanza name is the macro name appended with "(&lt;numargs&gt;)", where &lt;numargs&gt; is the number <br>&nbsp;&nbsp;of arguments that this macro takes.<br>* Macros can be overloaded. In other words, they can have the same name but a different number <br>&nbsp;&nbsp;of arguments. If you have [foobar], [foobar(1)], [foobar(2)], etc., they are not the same<br>&nbsp;&nbsp;macro.<br>* Macros can be used in the search language by enclosing the macro name and any argument list<br>&nbsp;&nbsp;within tick marks, for example:`foobar(arg1,arg2)` or `footer`.<br>* Splunk does not expand macros when they are inside of quoted values, for example: <br>&nbsp;&nbsp;"foo`bar`baz".<br><br>args = &lt;string&gt;,&lt;string&gt;,...<br>* A comma-delimited string of argument names.<br>* Argument names can only contain alphanumeric characters, underscores '_', and hyphens '-'.<br>* If the stanza name indicates that this macro takes no arguments, this attribute will <br>&nbsp;&nbsp;be ignored.<br>* This list cannot contain any repeated elements.<br><br>definition = &lt;string&gt;<br>* The string that the macro will expand to, with the argument substitutions made. (The <br>&nbsp;&nbsp;exception is when iseval = true, see below.)<br>* Arguments to be substituted must be wrapped by dollar signs ($), for example: "the last <br>&nbsp;&nbsp;part of this string will be replaced by the value of argument foo $foo$". &nbsp;&nbsp;<br>* Splunk replaces the $&lt;arg&gt;$ pattern globally in the string, even inside of quotes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>validation = &lt;string&gt;<br>* A validation string that is an 'eval' expression. &nbsp;This expression must evaluate to a <br>&nbsp;&nbsp;boolean or a string.<br>* Use this to verify that the macro's argument values are acceptable.<br>* If the validation expression is boolean, validation succeeds when it returns true. &nbsp;If it<br>&nbsp;&nbsp;returns false or is NULL, validation fails, and Splunk returns the error message defined by <br>&nbsp;&nbsp;the attribute, errormsg.<br>* If the validation expression is not boolean, Splunk expects it to return a string or NULL.<br>&nbsp;&nbsp;If it returns NULL, validation is considered a success. Otherwise, the string returned <br>&nbsp;&nbsp;is the error string.<br><br>errormsg = &lt;string&gt;<br>* The error message to be displayed if validation is a boolean expression and it does not <br>&nbsp;&nbsp;evaluate to true.<br><br>iseval = &lt;true/false&gt;<br>* If true, the definition attribute is expected to be an eval expression that returns a <br>&nbsp;&nbsp;string that represents the expansion of this macro.<br>* Defaults to false.<br><br>description = &lt;string&gt;<br>* OPTIONAL. Simple english description of what the macro does.<br><br></font></code>
<h3> <a name="macrosconf_macros.conf.example"><span class="mw-headline" id="macros.conf.example">macros.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># Example macros.conf<br>#<br><br># macro foobar that takes no arguments can be invoked via `foobar`<br>[foobar]<br># the defintion of a macro can invoke another macro. &nbsp;nesting can be indefinite and cycles will be detected and result in an error<br>definition = `foobar(foo=defaultfoo)`<br><br><br># macro foobar that takes one argument, invoked via `foobar(someval)`<br>[foobar(1)]<br>args = foo<br># note this is definition will include the leading and trailing quotes, i.e.<br># something `foobar(someval)`<br># would expand to<br># something "foo = someval"<br>definition = "foo = $foo$"<br><br># macro that takes two arguments<br># note that macro arguments can be named so this particular macro could be invoked equivalently as<br># `foobar(1,2)` `foobar(foo=1,bar=2)` or `foobar(bar=2,foo=1)`<br>[foobar(2)]<br>args = foo, bar<br>definition = "foo = $foo$, bar = $bar$"<br><br># macro that takes one argument that does validation<br>[foovalid(1)]<br>args = foo<br>definition = "foovalid = $foo$"<br># the validation eval function takes any even number of arguments (&gt;=2) where the first argument is<br># a boolean expression, the 2nd a string, the third boolean, 4th a string, etc etc etc<br>validation = validate(foo&gt;15,"foo must be greater than 15",foo&lt;=100,"foo must be &lt;= 100")<br><br># macro showing simple boolean validation, where if foo &gt; bar is not true, errormsg is displayed<br>[foovalid(2)]<br>args = foo, bar<br>definition = "foo = $foo$ and bar = $bar$"<br>validation = foo &gt; bar<br>errormsg = foo must be greater than bar<br><br># example of an eval-based definition. &nbsp;For example in this case `fooeval(10,20)` would get replaced by 10 + 20<br>[fooeval(2)]<br>args = foo, bar<br>definition = if (bar &gt; 0, "$foo$ + $bar$", "$foo$ - $bar$")<br>iseval = true<br><br></font></code>

<a name="multikvconf"></a><h2> <a name="multikvconf_multikv.conf"><span class="mw-headline" id="multikv.conf">multikv.conf</span></a></h2>
<p>The following are the spec and example files for multikv.conf.
</p>
<h3> <a name="multikvconf_multikv.conf.spec"><span class="mw-headline" id="multikv.conf.spec">multikv.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute and value pairs for creating multikv rules.<br># Multikv is the process of extracting events from table-like events, such as the <br># output of top, ps, ls, netstat, etc.<br>#<br># There is NO DEFAULT multikv.conf. &nbsp;To set custom configurations, place a multikv.conf in <br># $SPLUNK_HOME/etc/system/local/. For examples, see multikv.conf.example. <br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br># <br># NOTE: Only configure multikv.conf if Splunk's default multikv behavior does not meet your<br># needs.<br><br># A table-like event includes a table consisting of four sections: <br>#<br>#---------------------------------------------------------------------------------------<br># Section Name | Description<br>#---------------------------------------------------------------------------------------<br># pre &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| optional: info/description (for example: the system summary output in top)<br># header &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| optional: if not defined, fields are named Column_N<br># body &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| required: the body of the table from which child events are constructed<br># post &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| optional: info/description<br>#---------------------------------------------------------------------------------------<br><br># NOTE: Each section must have a definition and a processing component. See below.<br><br>[&lt;multikv_config_name&gt;]<br>* Name of the stanza to use with the multikv search command, for example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'.... | multikv conf=&lt;multikv_config_name&gt; rmorig=f | ....'<br>* Follow this stanza name with any number of the following attribute/value pairs.<br><br>#####################<br># Section Definition<br>#####################<br># Define where each section begins and ends.<br><br>&lt;Section Name&gt;.start = &lt;regex&gt; &nbsp;&nbsp;&nbsp;&nbsp;<br>* A line matching this regex denotes the start of this section (inclusive).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>OR<br><br>&lt;Section Name&gt;.start_offset = &lt;int&gt; &nbsp;&nbsp;&nbsp;&nbsp;<br>* Line offset from the start of an event or the end of the previous section (inclusive).<br>* Use this if you cannot define a regex for the start of the section.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&lt;Section Name&gt;.member = &lt;regex&gt; &nbsp;&nbsp;&nbsp;&nbsp;<br>* A line membership test.<br>* Member if lines match the regex.<br><br>&lt;Section Name&gt;.end = &lt;regex&gt; &nbsp;&nbsp;&nbsp;&nbsp;<br>* A line matching this regex denotes the end of this section (exclusive).<br><br>OR<br><br>&lt;Section Name&gt;.linecount = &lt;int&gt; &nbsp;&nbsp;&nbsp;<br>* Specify the number of lines in this section.<br>* Use this if you cannot specify a regex for the end of the section.<br><br>#####################<br># Section processing<br>#####################<br># Set processing for each section.<br><br>&lt;Section Name&gt;.ignore = [_all_|_none_|_regex_ &lt;regex-list&gt;] &nbsp;<br>* Determines which member lines will be ignored and not processed further.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&lt;Section Name&gt;.replace = &lt;quoted-str&gt; = &lt;quoted-str&gt;, &lt;quoted-str&gt; = &lt;quoted-str&gt;,... <br>* List of the form: "toReplace" = "replaceWith".<br>* Can have any number of quoted string pairs.<br>* For example: "%" = "_", "#" = "_"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&lt;Section Name&gt;.tokens = [&lt;chopper&gt;|&lt;tokenizer&gt;|&lt;aligner&gt;|&lt;token-list&gt;]<br>* See below for definitions of each possible token: chopper, tokenizer, aligner,<br>&nbsp;&nbsp;token-list.<br><br>&lt;chopper&gt; = _chop_, &lt;int-list&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>* Transform each string into a list of tokens specified by &lt;int-list&gt;.<br>* &lt;int-list&gt; is a list of (offset, length) tuples.<br><br>&lt;tokenizer&gt; = _tokenize_ &lt;max_tokens (int)&gt; &lt;delims&gt; (&lt;consume-delims&gt;)?<br>* Tokenize the string using the delim characters. <br>* This generates at most max_tokens number of tokens.<br>* Set max_tokens to: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* -1 for complete tokenization.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 to inherit from previous section (usually header).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A non-zero number for a specific token count.<br>* If tokenization is limited by the max_tokens, the rest of the string is added onto the <br>&nbsp;&nbsp;last token.<br>* &lt;delims&gt; is a comma-separated list of delimiting chars.<br>* &lt;consume-delims&gt; - boolean, whether to consume consecutive delimiters. Set to false/0 if you want <br>* &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;consecutive delimiters to be treated as empty values. Defaults to true.<br><br>&lt;aligner&gt; = _align_, &lt;header_string&gt;, &lt;side&gt;, &lt;max_width&gt;<br>* Generates tokens by extracting text aligned to the specified header fields.<br>* header_string: a complete or partial header field value the columns are aligned with.<br>* side: either L or R (for left or right align, respectively).<br>* max_width: the maximum width of the extracted field.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set max_width to -1 for automatic width. This expands the field until any of the <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;following delimiters are found: " ", "\t"<br><br>&lt;token_list&gt; = _token_list_ &lt;comma-separated list&gt;<br>* Defines a list of static tokens in a section. <br>* This is useful for tables with no header, for example: the output of 'ls -lah' <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which misses a header altogether. <br><br></font></code>
<h3> <a name="multikvconf_multikv.conf.example"><span class="mw-headline" id="multikv.conf.example">multikv.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example multi key/value extraction configurations.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># multikv.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># This example breaks up the output from top: <br><br># Sample output:<br><br># Processes: 56 total, 2 running, 54 sleeping... 221 threads 10:14:07<br>#.....<br>#<br># &nbsp;&nbsp;PID COMMAND &nbsp;%CPU TIME &nbsp;&nbsp;&nbsp;&nbsp;#TH #PRTS #MREGS RPRVT RSHRD RSIZE &nbsp;VSIZE <br># 29960 mdimport 0.0% &nbsp;0:00.29 &nbsp;3 &nbsp;&nbsp;&nbsp;60 &nbsp;&nbsp;&nbsp;50 &nbsp;1.10M &nbsp;2.55M 3.54M &nbsp;38.7M<br># 29905 pickup &nbsp;&nbsp;0.0% &nbsp;0:00.01 &nbsp;1 &nbsp;&nbsp;&nbsp;16 &nbsp;&nbsp;&nbsp;17 &nbsp;&nbsp;164K &nbsp;&nbsp;832K &nbsp;764K &nbsp;26.7M<br>#....<br><br>[top_mkv] <br># pre table starts at "Process..." and ends at line containing "PID"<br>pre.start = "Process" <br>pre.end = "PID" <br>pre.ignore = _all_ <br><br># specify table header location and processing <br>header.start = "PID" <br>header.linecount = 1 <br>header.replace = "%" = "_", "#" = "_"<br>header.tokens = _tokenize_, -1," "<br>&nbsp;<br># table body ends at the next "Process" line (ie start of another top) tokenize <br># and inherit the number of tokens from previous section (header)<br>body.end = "Process"<br>body.tokens &nbsp;= _tokenize_, &nbsp;0, " "<br><br><br><br>## This example handles the output of 'ls -lah' command:<br>#<br># total 2150528 <br># drwxr-xr-x 88 john john 2K &nbsp;&nbsp;Jan 30 07:56 . <br># drwxr-xr-x 15 john john 510B Jan 30 07:49 .. <br># -rw------- 1 &nbsp;john john 2K &nbsp;&nbsp;Jan 28 11:25 .hiden_file <br># drwxr-xr-x 20 john john 680B Jan 30 07:49 my_dir <br># -r--r--r-- 1 &nbsp;john john 3K &nbsp;&nbsp;Jan 11 09:00 my_file.txt<br><br><br>[ls-lah-cpp]<br>pre.start &nbsp;&nbsp;&nbsp;&nbsp;= "total"<br>pre.linecount = 1<br><br># the header is missing, so list the column names<br>header.tokens = _token_list_, mode, links, user, group, size, date, name<br><br># The ends when we have a line starting with a space <br>body.end &nbsp;&nbsp;&nbsp;&nbsp;= "^\s*$"<br># This filters so that only lines that contain with .cpp are used<br>body.member &nbsp;= "\.cpp" <br># concatenates the date into a single unbreakable item<br>body.replace = "(\w{3})\s+(\d{1,2})\s+(\d{2}:\d{2})" ="\1_\2_\3"<br><br># ignore dirs <br>body.ignore = _regex_ "^drwx.*", <br>body.tokens &nbsp;= _tokenize_, 0, " "<br><br><br></font></code>

<a name="outputsconf"></a><h2> <a name="outputsconf_outputs.conf"><span class="mw-headline" id="outputs.conf">outputs.conf</span></a></h2>
<p>The following are the spec and example files for outputs.conf.
</p>
<h3> <a name="outputsconf_outputs.conf.spec"><span class="mw-headline" id="outputs.conf.spec">outputs.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># Forwarders require outputs.conf; non-forwarding Splunk instances do not use it. &nbsp;It determines how the <br># forwarder sends data to receiving Splunk instances, either indexers or other forwarders.<br>#<br># To configure forwarding, create an outputs.conf file in $SPLUNK_HOME/etc/system/local/. <br># For examples of its use, see outputs.conf.example.<br>#<br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># NOTE: To learn more about forwarding, see the documentation at <br># http://docs.splunk.com/Documentation/Splunk/latest/Deploy/Aboutforwardingandreceivingdata<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>############<br>TCP Output stanzas<br>############<br># There are three levels of TCP Output stanzas: <br># * Global: [tcpout]<br># * Target group: [tcpout:&lt;target_group&gt;]<br># * Single server: [tcpout-server://&lt;ip address&gt;:&lt;port&gt;]<br>#<br># Settings at more specific levels override settings at higher levels. For example, an attribute set for a single<br># server overrides the value of that attribute, if any, set at that server's target group stanza. See the online <br># documentation on configuring forwarders for details.<br>#<br># This spec file first describes the three levels of stanzas (and any attributes unique to a particular level). <br># It then describes the optional attributes, which can be set at any &nbsp;of the three levels.<br><br><br>#----TCP Output Global Configuration -----<br># The global configurations specified here in the [tcpout] stanza can be overwritten in stanzas for specific <br># target groups, as described later. Note that the defaultGroup and indexAndForward attributes can only be set<br># here, at the global level.<br>#<br># Starting with 4.2, the [tcpout] stanza is no longer required.<br><br>[tcpout]<br><br>defaultGroup = &lt;target_group&gt;, &lt;target_group&gt;, ...<br>* Comma-separated list of one or more target group names, specified later in [tcpout:&lt;target_group&gt;] stanzas.<br>* The forwarder sends all data to the specified groups.<br>* If you don't want to forward data automatically, don't set this attribute.<br>* Can be overridden by an inputs.conf _TCP_ROUTING setting, which in turn can be overridden by a <br>&nbsp;&nbsp;props.conf/transforms.conf modifier.<br>* Starting with 4.2, this attribute is no longer required. <br><br>indexAndForward = [true|false]<br>* Index all data locally, in addition to forwarding it.<br>* This is known as an "index-and-forward" configuration.<br>* This attribute is only available for heavy forwarders.<br>* This attribute is available only at the top level [tcpout] stanza. It cannot be overridden in a target group.<br>* Defaults to false.<br><br>#----Target Group Configuration -----<br><br># If multiple servers are specified in a target group, the forwarder performs auto load-balancing, sending data <br># alternately to each available server in the group. For example, assuming you have three servers (server1, server2,<br># server3) and autoLBFrequency=30, the forwarder sends all data to server1 for 30 seconds, then it sends all data<br># to server2 for the next 30 seconds, then all data to server3 for the next 30 seconds, finally cycling back to server1. <br>#<br># You can have as many target groups as you want.<br># If more than one target group is specified, the forwarder sends all data to each target group. <br># This is known as "cloning" the data.<br><br><br>[tcpout:&lt;target_group&gt;]<br><br>server = [&lt;ip&gt;|&lt;servername&gt;]:&lt;port&gt;, [&lt;ip&gt;|&lt;servername&gt;]:&lt;port&gt;, ...<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Takes a comma separated list of one or more systems to send data to over<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a tcp socket.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Typically used to specify receiving splunk systems, although it can be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;used to send data to non-splunk systems (see sendCookedData setting).<br>&nbsp;&nbsp;&nbsp;&nbsp;* For each mentioned system, the following are required:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* IP or servername where one or system is listening.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Port on which syslog server is listening.<br><br>blockWarnThreshold = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default value is 100<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Sets the output pipleline send failure count threshold after which a failure message<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will be displayed as banner on UI &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To disable any warnings to be sent to UI on blocked output queue condition, set this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to a large value (2 million for example)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>#----Single server configuration -----<br><br># You can define specific configurations for individual indexers on a server-by-server<br># basis. &nbsp;However, each server must also be part of a target group.<br><br>[tcpout-server://&lt;ip address&gt;:&lt;port&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Optional. &nbsp;There is no requirement to have any tcpout-server stanzas.<br><br>############<br>#----TCPOUT ATTRIBUTES----&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>############<br># These attributes are optional and can appear in any of the three stanza levels.<br><br>[tcpout&lt;any of above&gt;]<br><br>#----General Settings----<br><br>sendCookedData = [true|false]<br>* If true, events are cooked (have been processed by Splunk).<br>* If false, events are raw and untouched prior to sending.<br>* Set to false if you are sending to a third-party system.<br>* Defaults to true.<br><br>heartbeatFrequency = &lt;integer&gt;<br>* How often (in seconds) to send a heartbeat packet to the receiving server.<br>* Heartbeats are only sent if sendCookedData=true.<br>* Defaults to 30 seconds.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>blockOnCloning = [true|false]<br>* If true, TcpOutputProcessor blocks till at least one of the cloned group gets events. This will<br>&nbsp;&nbsp;not drop events when all the cloned groups are down.<br>* If false, TcpOutputProcessor will drop events when all the cloned groups are down and queues for<br>&nbsp;&nbsp;the cloned groups are full. When at least one of the cloned groups is up and queues are not full,<br>&nbsp;&nbsp;the events are not dropped.<br>* Defaults to true.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>compressed = [true|false]<br>* Applies to non-SSL forwarding only. For SSL useClientSSLCompression setting is used.<br>* If true, forwarder sends compressed data.<br>* If set to true, the receiver port must also have compression turned on (in its inputs.conf file).<br>* Defaults to false.<br><br>negotiateNewProtocol = [true|false]<br>* When setting up a connection to an indexer, try to negotiate the use of the new forwarder protocol.<br>* If set to false, the forwarder will not query the indexer for support for the new protocol, and the connection will fall back on the traditional protocol.<br>* Defaults to true.<br><br>channelReapInterval = &lt;integer&gt;<br>* Controls how often, in milliseconds, channel codes are reaped, i.e. made available for re-use.<br>* This value sets the minimum time between reapings; in practice, consecutive reapings may be separated by greater than &lt;channelReapInterval&gt; milliseconds.<br>* Defaults to 60000 (1 minute)<br><br>channelTTL = &lt;integer&gt;<br>* Controls how long, in milliseconds, a channel may remain "inactive" before it is reaped, i.e. before its code is made available for re-use by a different channel.<br>* Defaults to 300000 (5 minutes)<br><br>channelReapLowater = &lt;integer&gt;<br>* If the number of active channels is above &lt;channelReapLowater&gt;, we reap old channels in order to make their channel codes available for re-use.<br>* If the number of active channels is below &lt;channelReapLowater&gt;, we do not reap channels, no matter how old they are.<br>* This value essentially determines how many active-but-old channels we keep "pinned" in memory on both sides of a splunk-to-splunk connection.<br>* A non-zero value helps ensure that we do not waste network resources by "thrashing" channels in the case of a forwarder sending a trickle of data.<br>* Defaults to 10.<br><br>#----Queue Settings----&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>maxQueueSize = [&lt;integer&gt;|&lt;integer&gt;[KB|MB|GB]|auto]<br>* This attribute sets the maximum size of the forwarder's output queue. <br>* The size can be limited based on the number of entries, or on the total memory used by the items<br>&nbsp;&nbsp;in the queue.<br>* If specified as a lone integer (for example, maxQueueSize=100), maxQueueSize indicates<br>&nbsp;&nbsp;the maximum count of queued items.<br>* If specified as an integer followed by KB, MB, or GB (for example, maxQueueSize=100MB),<br>&nbsp;&nbsp;maxQueueSize indicates the maximum RAM size of all the items in the queue.<br>* If set to auto, chooses a value depending on whether useACK is enabled.<br>&nbsp;&nbsp;* If useACK=false, uses 500KB<br>&nbsp;&nbsp;* If useACK=true, uses 7MB<br>* If the useACK setting is enabled, the maximum size of the wait queue is set to to 3x this value.<br>&nbsp;&nbsp;* Although the wait queue and the output queue sizes are both controlled by this attribute, they<br>&nbsp;&nbsp;&nbsp;&nbsp;are separate.<br>* Limiting the queue sizes by quantity is largely historical. &nbsp;However, should you choose to<br>&nbsp;&nbsp;configure queues based on quantity, keep the following in mind:<br>&nbsp;&nbsp;* Queued items can be events or blocks of data.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Non-parsing forwarders, such as universal forwarders, will send blocks, which may be<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;up to 64KB.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Parsing forwarders, such as heavy forwarders, will send events, which will be the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size of the events. &nbsp;For some events these are as small as a few hundred bytes. &nbsp;In unusual<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cases (data dependent), customers may arrange to produce events that are multiple megabytes.<br>* Defaults to auto<br>&nbsp;&nbsp;* If useACK is enabled, effectively defaults the wait queue to 21MB<br><br>dropEventsOnQueueFull = &lt;integer&gt;<br>* If set to a positive number, wait &lt;integer&gt; seconds before throwing out all new events until the output queue has space.<br>* Setting this to -1 or 0 will cause the output queue to block when it gets full, causing further blocking up the processing chain.<br>* If any target group's queue is blocked, no more data will reach any other target group.<br>* Using auto load-balancing is the best way to minimize this condition, because, in that case, multiple receivers must be down <br>&nbsp;&nbsp;(or jammed up) before queue blocking can occur.<br>* Defaults to -1 (do not drop events).<br>* DO NOT SET THIS VALUE TO A POSITIVE INTEGER IF YOU ARE MONITORING FILES!<br><br>dropClonedEventsOnQueueFull = &lt;integer&gt;<br>* If set to a positive number, do not block completely, but wait up to &lt;integer&gt; seconds to queue events to a group. If it<br>&nbsp;&nbsp;cannot enqueue to a group for more than &lt;integer&gt; seconds, begin dropping events for the group. It makes sure that at least<br>&nbsp;&nbsp;one group in the cloning configuration will get events. It blocks if event cannot be delivered to any of the cloned groups.<br>* If set to -1, the TcpOutputProcessor will make sure that each group will get all of the events. &nbsp;If one of the groups is down,<br>&nbsp;&nbsp;then Splunk will block everything.<br>* Defaults to 5.<br><br>#----Backoff Settings When Unable To Send Events to Indexer----<br># The settings in this section determine forwarding behavior when there<br># are repeated failures in sending events to an indexer ("sending failures").<br><br>maxFailuresPerInterval = &lt;integer&gt;<br>* Specifies the maximum number failures allowed per interval before backoff<br>&nbsp;&nbsp;takes place. The interval is defined below.<br>* Defaults to 2.<br><br>secsInFailureInterval = &lt;integer&gt;<br>* Number of seconds in an interval. If the number of write failures exceeds maxFailuresPerInterval<br>&nbsp;&nbsp;in the specified secsInFailureInterval seconds, the forwarder applies backoff. The backoff time <br>&nbsp;&nbsp;period range is 1-10 * autoLBFrequency.<br>* Defaults to 1.<br><br>backoffOnFailure = &lt;positive integer&gt;<br>* Number of seconds a forwarder will wait before attempting another connection attempt.<br>* Defaults to 30<br><br>maxConnectionsPerIndexer = &lt;integer&gt;<br>* Maximum number of allowed connections per indexer. In presence of failures, the max number of connection <br>&nbsp;&nbsp;attempt per indexer at any point in time.<br>* Defaults to 2.<br><br>connectionTimeout = &lt;integer&gt;<br>* Time out period if connection establishment does not finish in &lt;integer&gt; seconds.<br>* Defaults to 20 seconds.<br><br>readTimeout = &lt;integer&gt;<br>* Time out period if read from socket does not finish in &lt;integer&gt; seconds.<br>* This timeout is used to read acknowledgment when indexer acknowledgment is used (useACK=true).<br>* Defaults to 300 seconds.<br><br>writeTimeout = &lt;integer&gt;<br>* Time out period if write on socket does not finish in &lt;integer&gt; seconds.<br>* Defaults to 300 seconds.<br><br>tcpSendBufSz = &lt;integer&gt;<br>* TCP send buffer size in &lt;integer&gt; bytes.<br>* Useful to improve thruput with small size events like windows events.<br>* Only set this value if you are a TCP/IP expert.<br>* Defaults to system default.<br><br>dnsResolutionInterval = &lt;integer&gt;<br>* Specifies base time interval in seconds at which indexer dns names will be resolved to ip address.<br>&nbsp;&nbsp;This is used to compute runtime dnsResolutionInterval as follows:<br>&nbsp;&nbsp;runtime interval = dnsResolutionInterval + (number of indexers in server settings - 1)*30.<br>&nbsp;&nbsp;DNS resolution interval is extended by 30 second for each additional indexer in server setting.<br>* Defaults to 300 seconds.<br><br>forceTimebasedAutoLB = [true|false]<br>* Will force existing streams to switch to newly elected indexer every AutoLB cycle.<br>* Defaults to false<br><br>#----Index Filter Settings.<br># These attributes are only applicable under the global [tcpout] stanza. This filter does not work if it is created <br># under any other stanza.<br>forwardedindex.&lt;n&gt;.whitelist = &lt;regex&gt;<br>forwardedindex.&lt;n&gt;.blacklist = &lt;regex&gt;<br>* These filters determine which events get forwarded, based on the indexes they belong to.<br>* This is an ordered list of whitelists and blacklists, which together decide if events should be forwarded to an index.<br>* The order is determined by &lt;n&gt;. &lt;n&gt; must start at 0 and continue with positive integers, in sequence. There cannot be any<br>&nbsp;&nbsp;gaps in the sequence. (For example, forwardedindex.0.whitelist, forwardedindex.1.blacklist, forwardedindex.2.whitelist, ...). <br>* The filters can start from either whitelist or blacklist. They are tested from forwardedindex.0 to forwardedindex.&lt;max&gt;.<br>* If both forwardedindex.&lt;n&gt;.whitelist and forwardedindex.&lt;n&gt;.blacklist are present for the same value of n, then<br>&nbsp;&nbsp;forwardedindex.&lt;n&gt;.whitelist is honored. forwardedindex.&lt;n&gt;.blacklist is ignored in this case.<br>* You should not normally need to change these filters from their default settings in $SPLUNK_HOME/system/default/outputs.conf.<br>* Filtered out events are not indexed if local indexing is not enabled.<br><br>forwardedindex.filter.disable = [true|false]<br>* If true, disables index filtering. Events for all indexes are then forwarded.<br>* Defaults to false.<br><br>#----Automatic Load-Balancing <br>autoLB = true<br>* Automatic load balancing is the only way to forward data. Round-robin method is not supported anymore.<br>* Defaults to true.<br><br>autoLBFrequency = &lt;seconds&gt;<br>* Every autoLBFrequency seconds, a new indexer is selected randomly from the list of indexers provided in the server attribute <br>&nbsp;&nbsp;of the target group stanza.<br>* Defaults to 30 (seconds).<br><br>#----SSL Settings----<br><br># To set up SSL on the forwarder, set the following attribute/value pairs.<br># If you want to use SSL for authentication, add a stanza for each receiver that must be <br># certified.<br><br>sslPassword = &lt;password&gt;<br>* The password associated with the CAcert.<br>* The default Splunk CAcert uses the password "password".<br>* There is no default value.<br><br>sslCertPath = &lt;path&gt;<br>* If specified, this connection will use SSL. &nbsp;<br>* This is the path to the client certificate.<br>* There is no default value.<br><br>sslCipher = &lt;string&gt;<br>* If set, uses the specified cipher string for the input processors.<br>* If not set, the default cipher string is used.<br>* Provided by OpenSSL. This is used to ensure that the server does not<br>&nbsp;&nbsp;accept connections using weak encryption protocols.<br><br>ecdhCurveName = &lt;string&gt;<br>* ECDH curve to use for ECDH key negotiation<br>* We only support named curves specified by their SHORT name. <br>* (see struct ASN1_OBJECT in asn1.h)<br>* The list of valid named curves by their short/long names<br>* can be obtained by executing this command:<br>* $SPLUNK_HOME/bin/splunk cmd openssl ecparam -list_curves<br>* Default is empty string.<br><br>sslRootCAPath = &lt;path&gt;<br>* The path to the root certificate authority file (optional).<br>* There is no default value.<br><br>sslVerifyServerCert = [true|false]<br>* If true, you must make sure that the server you are connecting to is a valid one (authenticated). &nbsp;<br>* Both the common name and the alternate name of the server are then checked for a match.<br>* Defaults to false.<br><br>sslCommonNameToCheck = &lt;string&gt;<br>* Check the common name of the server's certificate against this name.<br>* If there is no match, assume that Splunk is not authenticated against this server. &nbsp;<br>* You must specify this setting if sslVerifyServerCert is true.<br><br>sslAltNameToCheck = &lt;string&gt;<br>* Check the alternate name of the server's certificate against this name.<br>* If there is no match, assume that Splunk is not authenticated against this server. &nbsp;<br>* You must specify this setting if sslVerifyServerCert is true.<br><br>useClientSSLCompression = [true|false]<br>* Enables compression on SSL.<br>* Defaults to value of useClientSSLCompression from [sslConfig] stanza in server.conf.<br><br>sslQuietShutdown = [true|false]<br>* Enables quiet shutdown mode in SSL<br>* Defaults to false<br><br>#----Indexer Acknowledgment ----<br># Indexer acknowledgment ensures that forwarded data is reliably delivered to the receiver.<br># If the receiver is an indexer, it indicates that the indexer has received the data, indexed it, and written <br># it to the file system. If the receiver is an intermediate forwarder, it indicates that the intermediate<br># forwarder has successfully forwarded the data to the terminating indexer and has received acknowledgment from &nbsp;<br># that indexer. <br><br># Important: Indexer acknowledgment is a complex feature that requires careful planning. Before using it, <br># read the online topic describing it in the Distributed Deployment manual.<br><br>useACK = [true|false]<br>* When set to true, the forwarder will retain a copy of each sent event, until the receiving system<br>&nbsp;&nbsp;sends an acknowledgement.<br>&nbsp;&nbsp;* The receiver will send an acknowledgement when it has fully handled it (typically written it to<br>&nbsp;&nbsp;&nbsp;&nbsp;disk in indexing)<br>&nbsp;&nbsp;* In the event of receiver misbehavior (acknowledgement is not received), the data will be re-sent<br>&nbsp;&nbsp;&nbsp;&nbsp;to an alternate receiver.<br>&nbsp;&nbsp;* Note: the maximum memory used for the outbound data queues will increase significantly by <br>&nbsp;&nbsp;&nbsp;&nbsp;default (500KB -&gt; &nbsp;28MB) when useACK is enabled. This is intended for correctness and performance.<br>* When set to false, the forwarder will consider the data fully processed when it finishes writing<br>&nbsp;&nbsp;it to the network socket.<br>* This attribute can be set at the [tcpout] or [tcpout:&lt;target_group&gt;] stanza levels. You cannot set<br>&nbsp;&nbsp;it for individual servers at the [tcpout-server: ...] stanza level.<br>* Defaults to false.<br><br>############<br>#----Syslog output----<br>############<br># The syslog output processor is not available for universal or light forwarders.<br><br># The following configuration is used to send output using syslog:<br><br>[syslog]<br>defaultGroup = &lt;target_group&gt;, &lt;target_group&gt;, ...<br><br>[syslog:&lt;target_group&gt;]<br><br>#----REQUIRED SETTINGS----<br># Required settings for a syslog output group:<br><br>server = [&lt;ip&gt;|&lt;servername&gt;]:&lt;port&gt;<br>* IP or servername where syslog server is running.<br>* Port on which server is listening. You must specify the port. Syslog, by default, uses 514.<br><br>#----OPTIONAL SETTINGS----&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br># Optional settings for syslog output:<br><br>type = [tcp|udp]<br>* Protocol used. <br>* Default is udp.<br><br>priority = &lt;priority_value&gt; | NO_PRI<br>* The priority_value should specified as "&lt;integer&gt;" (an integer surrounded by angle brackets). For <br>&nbsp;&nbsp;example, specify &nbsp;a priority of 34 like this: &lt;34&gt;<br>* The integer must be one to three digits in length.<br>* The value you enter will appear in the syslog header.<br>* Mimics the number passed via syslog interface call, documented via man syslog.<br>* The integer can be computed as (&lt;facility&gt; * 8) + &lt;severity&gt;. For example, if &lt;facility&gt; is 4 <br>&nbsp;&nbsp;(security/authorization messages) and &lt;severity&gt; is 2 (critical conditions), the priority <br>&nbsp;&nbsp;will be 34 = (4 * 8) + 2. Set the attribute to: &lt;34&gt;<br>* The table of facility and severity (and their values) can be referenced in RFC3164, eg <br>&nbsp;&nbsp;http://www.ietf.org/rfc/rfc3164.txt section 4.1.1<br>* Defaults to &lt;13&gt;, or a facility of "user" or typically unspecified application,<br>&nbsp;&nbsp;and severity of "Notice".<br>* If you do not wish to add priority, set 'NO_PRI' as priority value.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example: priority = NO_PRI<br>* The table is reproduced briefly here, some of these are archaic.<br>&nbsp;&nbsp;Facility:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 kernel messages<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1 user-level messages<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 mail system<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 system daemons<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4 security/authorization messages<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5 messages generated internally by syslogd<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6 line printer subsystem<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7 network news subsystem<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8 UUCP subsystem<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9 clock daemon<br>&nbsp;&nbsp;&nbsp;&nbsp;10 security/authorization messages<br>&nbsp;&nbsp;&nbsp;&nbsp;11 FTP daemon<br>&nbsp;&nbsp;&nbsp;&nbsp;12 NTP subsystem<br>&nbsp;&nbsp;&nbsp;&nbsp;13 log audit<br>&nbsp;&nbsp;&nbsp;&nbsp;14 log alert<br>&nbsp;&nbsp;&nbsp;&nbsp;15 clock daemon<br>&nbsp;&nbsp;&nbsp;&nbsp;16 local use 0 &nbsp;(local0)<br>&nbsp;&nbsp;&nbsp;&nbsp;17 local use 1 &nbsp;(local1)<br>&nbsp;&nbsp;&nbsp;&nbsp;18 local use 2 &nbsp;(local2)<br>&nbsp;&nbsp;&nbsp;&nbsp;19 local use 3 &nbsp;(local3)<br>&nbsp;&nbsp;&nbsp;&nbsp;20 local use 4 &nbsp;(local4)<br>&nbsp;&nbsp;&nbsp;&nbsp;21 local use 5 &nbsp;(local5)<br>&nbsp;&nbsp;&nbsp;&nbsp;22 local use 6 &nbsp;(local6)<br>&nbsp;&nbsp;&nbsp;&nbsp;23 local use 7 &nbsp;(local7)<br>&nbsp;&nbsp;Severity:<br>&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;Emergency: system is unusable<br>&nbsp;&nbsp;&nbsp;&nbsp;1 &nbsp;Alert: action must be taken immediately<br>&nbsp;&nbsp;&nbsp;&nbsp;2 &nbsp;Critical: critical conditions<br>&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;Error: error conditions<br>&nbsp;&nbsp;&nbsp;&nbsp;4 &nbsp;Warning: warning conditions<br>&nbsp;&nbsp;&nbsp;&nbsp;5 &nbsp;Notice: normal but significant condition<br>&nbsp;&nbsp;&nbsp;&nbsp;6 &nbsp;Informational: informational messages<br>&nbsp;&nbsp;&nbsp;&nbsp;7 &nbsp;Debug: debug-level messages<br><br>syslogSourceType = &lt;string&gt;<br>* Specifies an additional rule for handling data, in addition to that provided by<br>&nbsp;&nbsp;the 'syslog' source type.<br>* This string is used as a substring match against the sourcetype key. &nbsp;For<br>&nbsp;&nbsp;example, if the string is set to 'syslog', then all source types containing the<br>&nbsp;&nbsp;string 'syslog' will receive this special treatment.<br>* To match a source type explicitly, use the pattern "sourcetype::sourcetype_name".<br>&nbsp;&nbsp;&nbsp;&nbsp;* Example: syslogSourceType = sourcetype::apache_common<br>* Data which is 'syslog' or matches this setting is assumed to already be in <br>&nbsp;&nbsp;syslog format. <br>* Data which does not match the rules has a header, potentially a timestamp,<br>&nbsp;&nbsp;and a hostname added to the front of the event. &nbsp;This is how Splunk causes<br>&nbsp;&nbsp;arbitrary log data to match syslog expectations.<br>* Defaults to unset.<br><br>timestampformat = &lt;format&gt;<br>* If specified, the formatted timestamps are added to the start of events forwarded to syslog.<br>* As above, this logic is only applied when the data is not syslog, or the syslogSourceType.<br>* The format is a strftime-style timestamp formatting string. This is the same implementation used in <br>&nbsp;&nbsp;the 'eval' search command, splunk logging, and other places in splunkd.<br>&nbsp;&nbsp;&nbsp;&nbsp;* &nbsp;For example:&nbsp;%b&nbsp;%e&nbsp;%H:%M:%S<br>&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;%b - Abbreviated month name (Jan, Feb, ...)<br>&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;%e - Day of month<br>&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;%H - Hour<br>&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;%M - Minute<br>&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;%s - Second<br>* For a more exhaustive list of the formatting specifiers, refer to the online documentation.<br>* Note that the string is not quoted.<br>* Defaults to unset, which means that no timestamp will be inserted into the front of events.<br><br>dropEventsOnQueueFull = &lt;integer&gt;<br>* If set to a positive number, wait &lt;integer&gt; seconds before throwing out all new events until the output queue has space.<br>* Setting this to -1 or 0 will cause the output queue to block when it gets full, causing further blocking up the processing chain.<br>* If any target group's queue is blocked, no more data will reach any other target group.<br>* Defaults to -1 (do not drop events).<br><br>maxEventSize = &lt;integer&gt;<br>* If specified, sets the maximum size of an event that splunk will transmit. <br>* All events excedding this size will be truncated.<br>* Defaults to 1024 bytes.<br><br>#---- Routing Data to Syslog Server -----<br># To route data to syslog server:<br># 1) Decide which events to route to which servers.<br># 2) Edit the props.conf, transforms.conf, and outputs.conf files on the forwarders.<br><br># Edit $SPLUNK_HOME/etc/system/local/props.conf and set a TRANSFORMS-routing attribute as shown here:<br><br>&nbsp;[&lt;spec&gt;]<br>&nbsp;TRANSFORMS-routing=&lt;unique_stanza_name&gt;<br><br>* &lt;spec&gt; can be: <br>&nbsp;&nbsp;* &lt;sourcetype&gt;, the source type of an event <br>&nbsp;&nbsp;* host::&lt;host&gt;, where &lt;host&gt; is the host for an event <br>&nbsp;&nbsp;* source::&lt;source&gt;, where &lt;source&gt; is the source for an event <br><br>* Use the &lt;unique_stanza_name&gt; when creating your entry in transforms.conf.<br><br># Edit $SPLUNK_HOME/etc/system/local/transforms.conf and set rules to match your props.conf stanza: <br><br>&nbsp;&nbsp;[&lt;unique_stanza_name&gt;]<br>&nbsp;&nbsp;REGEX=&lt;your_regex&gt;<br>&nbsp;&nbsp;DEST_KEY=_SYSLOG_ROUTING<br>&nbsp;&nbsp;FORMAT=&lt;unique_group_name&gt;<br><br>* &lt;unique_stanza_name&gt; must match the name you created in props.conf. <br>* Enter the regex rules in &lt;your_regex&gt; to determine which events get conditionally routed. <br>* DEST_KEY should be set to _SYSLOG_ROUTING to send events via SYSLOG.<br>* Set FORMAT to &lt;unique_group_name&gt;. This should match the syslog group name you create in outputs.conf.<br><br>############<br>#----IndexAndForward Processor-----<br>############<br># The IndexAndForward processor determines the default behavior for indexing data on full Splunk. It has the "index"<br># property, which determines whether indexing occurs.<br>#<br># When Splunk is not configured as a forwarder, "index" is set to "true". That is, the Splunk instance indexes data by<br># default.<br>#<br># When Splunk is configured as a forwarder, the processor turns "index" to "false". That is, the Splunk instance does not<br># index data by default.<br>#<br># The IndexAndForward processor has no effect on the universal forwarder, which can never index data.<br>#<br># If the [tcpout] stanza configures the indexAndForward attribute, the value of that attribute overrides the default <br># value of "index". However, if you set "index" in the [indexAndForward] stanza, described below, it supersedes any <br># value set in [tcpout].<br><br>[indexAndForward]<br>index = [true|false]<br>* If set to true, data is indexed.<br>* If set to false, data is not indexed.<br>* Default depends on whether the Splunk instance is configured as a forwarder, modified by any value configured for the <br>&nbsp;&nbsp;indexAndForward attribute in [tcpout].<br><br>selectiveIndexing = [true|false]<br>* When index is 'true', all events are indexed. Setting selectiveIndexing to 'true' allows you to index only specific events<br>&nbsp;&nbsp;that has key '_INDEX_AND_FORWARD_ROUTING' set.<br>* '_INDEX_AND_FORWARD_ROUTING' can be set in inputs.conf as:<br>&nbsp;&nbsp;[&lt;input_stanza&gt;]<br>&nbsp;&nbsp;_INDEX_AND_FORWARD_ROUTING = local<br>* Defaults to false.<br><br><br></font></code>
<h3> <a name="outputsconf_outputs.conf.example"><span class="mw-headline" id="outputs.conf.example">outputs.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example outputs.conf. &nbsp;Use this file to configure forwarding in a distributed<br># set up.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># outputs.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># Specify a target group for an IP:PORT which consists of a single receiver.<br># This is the simplest possible configuration; it sends data to the host at 10.1.1.197 on port 9997.<br><br>[tcpout:group1]<br>server=10.1.1.197:9997<br><br><br># Specify a target group for a hostname which consists of a single receiver.<br><br>[tcpout:group2]<br>server=myhost.Splunk.com:9997<br><br><br># Specify a target group made up of two receivers. &nbsp;In this case, the data will be<br># distributed using AutoLB between these two receivers. &nbsp;You can specify as many<br># receivers as you wish here. You can combine host name and IP if you wish.<br># NOTE: Do not use this configuration with SplunkLightForwarder.<br><br>[tcpout:group3]<br>server=myhost.Splunk.com:9997,10.1.1.197:6666<br><br><br># You can override any of the global configuration values on a per-target group basis.<br># All target groups that do not override a global config will inherit the global config.<br><br># Send every event to a receiver at foo.Splunk.com:9997 with a maximum queue size of 100,500 events.<br><br>[tcpout:group4]<br>server=foo.Splunk.com:9997<br>heartbeatFrequency=45<br>maxQueueSize=100500<br><br><br># Clone events to groups indexer1 and indexer2. Also, index all this data locally as well.<br><br>[tcpout]<br>indexAndForward=true<br><br>[tcpout:indexer1]<br>server=Y.Y.Y.Y:9997<br><br>[tcpout:indexer2]<br>server=X.X.X.X:6666<br><br><br># Clone events between two data balanced groups.<br><br>[tcpout:indexer1]<br>server=A.A.A.A:1111, B.B.B.B:2222<br><br>[tcpout:indexer2]<br>server=C.C.C.C:3333, D.D.D.D:4444<br><br># Syslout output configuration<br># This example sends only events generated by the splunk daemon to a remote<br># syslog host:<br><br>[syslog:syslog-out1]<br>disabled = false<br>server = X.X.X.X:9099<br>type = tcp<br>priority = &lt;34&gt;<br>timestampformat =&nbsp;%b&nbsp;%e&nbsp;%H:%M:%S<br><br><br># New in 4.0: Auto Load Balancing<br>#<br># This example balances output between two indexers running on<br># 1.2.3.4:4433 and 1.2.4.5:4433.<br># To achieve this you'd create a DNS entry for splunkLB pointing<br># to the two IP addresses of your indexers:<br>#<br># &nbsp;&nbsp;$ORIGIN example.com.<br># &nbsp;&nbsp;splunkLB A 1.2.3.4<br># &nbsp;&nbsp;splunkLB A 1.2.3.5<br><br>[tcpout]<br>defaultGroup = lb<br><br>[tcpout:lb]<br>server = splunkLB.example.com:4433<br>autoLB = true<br><br># Alternatively, you can autoLB sans DNS:<br><br>[tcpout]<br>defaultGroup = lb<br><br>[tcpout:lb]<br>server = 1.2.3.4:4433, 1.2.3.5:4433<br>autoLB = true<br><br><br># Compression<br>#<br># This example sends compressed events to the remote indexer.<br># NOTE: Compression can be enabled TCP or SSL outputs only.<br># The receiver input port should also have compression enabled.<br><br>[tcpout]<br>server = splunkServer.example.com:4433<br>compressed = true<br><br><br># SSL<br>#<br># This example sends events to an indexer via SSL using splunk's<br># self signed cert:<br><br>[tcpout]<br>server = splunkServer.example.com:4433<br>sslPassword = password <br>sslCertPath = $SPLUNK_HOME/etc/auth/server.pem<br>sslRootCAPath = $SPLUNK_HOME/etc/auth/ca.pem<br><br>#<br># The following example shows how to route events to syslog server<br># This is similar to tcpout routing, but DEST_KEY is set to _SYSLOG_ROUTING<br>#<br>1. Edit $SPLUNK_HOME/etc/system/local/props.conf and set a TRANSFORMS-routing attribute:<br>[default]<br>TRANSFORMS-routing=errorRouting<br><br>[syslog]<br>TRANSFORMS-routing=syslogRouting<br><br>2. Edit $SPLUNK_HOME/etc/system/local/transforms.conf and set errorRouting and syslogRouting rules:<br>[errorRouting]<br>REGEX=error<br>DEST_KEY=_SYSLOG_ROUTING<br>FORMAT=errorGroup<br><br>[syslogRouting]<br>REGEX=.<br>DEST_KEY=_SYSLOG_ROUTING<br>FORMAT=syslogGroup<br><br>3. Edit $SPLUNK_HOME/etc/system/local/outputs.conf and set which syslog outputs go to with servers or groups: <br>[syslog]<br>defaultGroup=everythingElseGroup<br><br>[syslog:syslogGroup]<br>server = 10.1.1.197:9997<br><br>[syslog:errorGroup]<br>server=10.1.1.200:9999<br><br>[syslog:everythingElseGroup]<br>server=10.1.1.250:6666<br><br>#<br># Perform selective indexing and forwarding<br>#<br># With a heavy forwarder only, you can index and store data locally, as well as forward the data onwards<br># to a receiving indexer. There are two ways to do this:<br>1. In outputs.conf:<br>[tcpout]<br>defaultGroup = indexers<br><br>[indexAndForward]<br>index=true<br>selectiveIndexing=true<br><br>[tcpout:indexers]<br>server = 10.1.1.197:9997, 10.1.1.200:9997<br><br>2. In inputs.conf, Add _INDEX_AND_FORWARD_ROUTING for any data that you want index locally, and<br>_TCP_ROUTING=&lt;target_group&gt; for data to be forwarded.<br><br>[monitor:///var/log/messages/]<br>_INDEX_AND_FORWARD_ROUTING=local<br><br>[monitor:///var/log/httpd/]<br>_TCP_ROUTING=indexers<br><br><br></font></code>

<a name="pdf%20serverconf"></a><h2> <a name="pdf%20serverconf_pdf_server.conf"><span class="mw-headline" id="pdf_server.conf">pdf_server.conf</span></a></h2>
<p>The following are the spec and example files for pdf_server.conf.
</p>
<h3> <a name="pdf%20serverconf_pdf_server.conf.spec"><span class="mw-headline" id="pdf_server.conf.spec">pdf_server.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values you can use to configure Splunk's pdf server.<br>#<br># There is a pdf_server.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a pdf_server.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see pdf_server.conf.example.<br># You must restart the pdf server to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[settings]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set general Splunk Web configuration options under this stanza name.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following attribute/value pairs. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify an entry for each attribute, Splunk will use the default value.<br><br>startwebserver = [0|1]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set whether or not to start the server.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 disables Splunk Web, 1 enables it.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1.<br><br>httpport = &lt;port_number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must be present for the server to start.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If omitted or 0 the server will NOT start an http listener.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If using SSL, set to the HTTPS port number.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 9000.<br><br>enableSplunkWebSSL = [True|False]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Toggle between http or https.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set to true to enable https and SSL.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to False.<br>&nbsp;&nbsp;&nbsp;<br>privKeyPath = /certs/privkey.pem<br>caCertPath = /certs/cert.pem<br>&nbsp;&nbsp;&nbsp;* Specify paths and names for Web SSL certs.<br>&nbsp;&nbsp;&nbsp;* Path is relative to $SPLUNK_HOME/share/splunk.<br><br>supportSSLV3Only = [True|False]<br>&nbsp;&nbsp;&nbsp;* Allow only SSLv3 connections if true.<br>&nbsp;&nbsp;&nbsp;* NOTE: Enabling this may cause some browsers problems.<br>&nbsp;&nbsp;&nbsp;<br>root_endpoint = &lt;URI_prefix_string&gt;<br>&nbsp;&nbsp;&nbsp;* Defines the root URI path on which the appserver will listen.<br>&nbsp;&nbsp;&nbsp;* Default setting is '/'.<br>&nbsp;&nbsp;&nbsp;* For example: if you want to proxy the splunk UI at http://splunk:8000/splunkui, then set root_endpoint = /splunkui<br><br>static_endpoint = &lt;URI_prefix_string&gt;<br>&nbsp;&nbsp;&nbsp;* Path to static content.<br>&nbsp;&nbsp;&nbsp;* The path here is automatically appended to root_endpoint defined above.<br>&nbsp;&nbsp;&nbsp;* Default is /static.<br><br>static_dir = &lt;relative_filesystem_path&gt;<br>&nbsp;&nbsp;&nbsp;* The directory that actually holds the static content.<br>&nbsp;&nbsp;&nbsp;* This can be an absolute URL if you want to put it elsewhere.<br>&nbsp;&nbsp;&nbsp;* Default is share/splunk/search_mrsparkle/exposed.<br><br>enable_gzip = [True|False]<br>&nbsp;&nbsp;&nbsp;* Determines if web server applies gzip compression to responses.<br>&nbsp;&nbsp;&nbsp;* Defaults to True.<br><br><br><br>#<br># cherrypy HTTP server config<br>#<br><br>server.thread_pool = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the numbers of threads the app server is allowed to maintain.<br>&nbsp;&nbsp;&nbsp;* Defaults to 10.<br>&nbsp;&nbsp;&nbsp;<br>server.socket_host = &lt;ip_address&gt;<br>&nbsp;&nbsp;&nbsp;* Host values may be any IPv4 or IPv6 address, or any valid hostname.<br>&nbsp;&nbsp;&nbsp;* The string 'localhost' is a synonym for '127.0.0.1' (or '::1', if your hosts file prefers IPv6).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The string '0.0.0.0' is a special IPv4 entry meaning "any active interface" (INADDR_ANY), and<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'::' is the similar IN6ADDR_ANY for IPv6. <br>&nbsp;&nbsp;&nbsp;* The empty string or None are not allowed.<br>&nbsp;&nbsp;&nbsp;* Defaults to 0.0.0.0<br><br>log.access_file = &lt;filename&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the HTTP access log filename.<br>&nbsp;&nbsp;&nbsp;* Stored in default Splunk /var/log directory.<br>&nbsp;&nbsp;&nbsp;* Defaults to pdf_access.log<br><br>log.error_file = &lt;filename&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the HTTP error log filename.<br>&nbsp;&nbsp;&nbsp;* Stored in default Splunk /var/log directory.<br>&nbsp;&nbsp;&nbsp;* Defaults to pdf_service.log<br><br>log.screen = [True|False]<br>&nbsp;&nbsp;&nbsp;* Indicates if runtime output is displayed inside an interactive tty.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br>&nbsp;&nbsp;&nbsp;<br>request.show_tracebacks = [True|False]<br>&nbsp;&nbsp;&nbsp;* Indicates if an exception traceback is displayed to the user on fatal exceptions.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>engine.autoreload_on = [True|False]<br>&nbsp;&nbsp;&nbsp;* Indicates if the app server will auto-restart if it detects a python file has changed.<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>tools.sessions.on = True<br>&nbsp;&nbsp;&nbsp;&nbsp;* Indicates if user session support is enabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Should always be True<br><br>tools.sessions.timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the number of minutes of inactivity before a user session expires.<br>&nbsp;&nbsp;&nbsp;* Defaults to 60<br><br>response.timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the number of seconds to wait for the server to complete a response.<br>&nbsp;&nbsp;&nbsp;* Some requests such as uploading large files can take a long time.<br>&nbsp;&nbsp;&nbsp;* Defaults to 7200<br><br>tools.sessions.storage_type = [file]<br>tools.sessions.storage_path = &lt;filepath&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the session information storage mechanisms.<br>&nbsp;&nbsp;&nbsp;* Comment out these two lines to use RAM based sessions instead.<br>&nbsp;&nbsp;&nbsp;* Use an absolute path to store sessions outside of the Splunk directory tree.<br>&nbsp;&nbsp;&nbsp;* Defaults to storage_type=file, storage_path=var/run/splunk<br><br>tools.decode.on = [True|False]<br>&nbsp;&nbsp;&nbsp;* Indicates if all strings that come into CherryPy controller methods are decoded as unicode (assumes UTF-8 encoding).<br>&nbsp;&nbsp;&nbsp;* WARNING: Disabling this will likely break the application, as all incoming strings are assumed<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to be unicode.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>tools.encode.on = [True|False]<br>&nbsp;&nbsp;&nbsp;* Encodes all controller method response strings into UTF-8 str objects in Python.<br>&nbsp;&nbsp;&nbsp;* WARNING: Disabling this will likely cause high byte character encoding to fail.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>tools.encode.encoding = &lt;codec&gt;<br>&nbsp;&nbsp;&nbsp;* Force all outgoing characters to be encoded into UTF-8.<br>&nbsp;&nbsp;&nbsp;* This only works with tools.encode.on set to True.<br>&nbsp;&nbsp;&nbsp;* By setting this to utf-8, CherryPy's default behavior of observing the Accept-Charset header<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is overwritten and forces utf-8 output. Only change this if you know a particular browser<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;installation must receive some other character encoding (Latin-1, iso-8859-1, etc.).<br>&nbsp;&nbsp;&nbsp;* WARNING: Change this at your own risk.<br>&nbsp;&nbsp;&nbsp;* Defaults to utf-8<br><br>pid_path = &lt;filepath&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the path to the PID file.<br>&nbsp;&nbsp;&nbsp;* Defaults to var/run/splunk/splunkweb.pid.<br><br>firefox_cmdline = &lt;cmdline&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies additional arguments to pass to Firefox.<br>&nbsp;&nbsp;&nbsp;* This should normally not be set.<br><br>max_queue = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the maximum size of the backlog of pending report requests.<br>&nbsp;&nbsp;&nbsp;* Once the backlog is reached the server will return an error on receiving additional requests.<br>&nbsp;&nbsp;&nbsp;* Defaults to 10.<br><br>max_concurrent = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the maximum number of copies of Firefox that the report server will use concurrently to render reports.<br>&nbsp;&nbsp;&nbsp;* Increase only if the host machine has multiple cores and plenty of spare memory.<br>&nbsp;&nbsp;&nbsp;* Defaults to 2.<br><br>Xvfb = &lt;path&gt;<br>&nbsp;&nbsp;* Pathname to the Xvfb program.<br>&nbsp;&nbsp;* Defaults to searching the PATH.<br><br>xauth = &lt;path&gt;<br>&nbsp;&nbsp;* Pathname to the xauth program.<br>&nbsp;&nbsp;* Defaults to searching the PATH.<br><br>mcookie = &lt;path&gt;<br>&nbsp;&nbsp;* Pathname to the mcookie program.<br>&nbsp;&nbsp;* Defaults to searching the PATH.<br><br>appserver_ipaddr = &lt;ip_networks&gt;<br>&nbsp;&nbsp;* If set, the PDF server will only query Splunk app servers on IP addresses within the IP networks<br>&nbsp;&nbsp;&nbsp;&nbsp;specified here.<br>&nbsp;&nbsp;* Networks can be specified as a prefix (10.1.0.0/16) or using a netmask (10.1.0.0/255.255.0.0).<br>&nbsp;&nbsp;* IPv6 addresses are also supported.<br>&nbsp;&nbsp;* Individual IP addresses can also be listed (1.2.3.4).<br>&nbsp;&nbsp;* Multiple networks should be comma separated.<br>&nbsp;&nbsp;* Defaults to accepting any IP address.<br><br>client_ipaddr = &lt;ip_networks&gt;<br>&nbsp;&nbsp;* If set, the PDF server will only accept requests from hosts whose IP address falls within the IP<br>&nbsp;&nbsp;&nbsp;&nbsp;networks specified here.<br>&nbsp;&nbsp;* Generally this setting should match the appserver_ipaddr setting.<br>&nbsp;&nbsp;* Format matches appserver_ipaddr.<br>&nbsp;&nbsp;* Defaults to accepting any IP address.<br><br>screenshot_enabled = [True|False]<br>&nbsp;&nbsp;* If enabled allows screenshots of the X server to be taken for debugging purposes.<br>&nbsp;&nbsp;* Enabling this is a potential security hole as anyone on an IP address matching client_ipaddr will be<br>&nbsp;&nbsp;&nbsp;&nbsp;able to see reports in progress.<br>&nbsp;&nbsp;* Defaults to False.<br><br><br></font></code>
<h3> <a name="pdf%20serverconf_pdf_server.conf.example"><span class="mw-headline" id="pdf_server.conf.example">pdf_server.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example pdf_server.conf. &nbsp;Use this file to configure pdf server process settings.<br>#<br># To use one or more of these configurations, copy the configuration block into pdf_server.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart the pdf server to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br><br># This stanza heading must precede any changes.<br>[settings]<br><br># Change the default port number:<br>httpport = 12900<br><br># Lock down access to the IP address of specific appservers <br># that will utilize the pdf server<br>appserver_ipaddr = 192.168.3.0/24,192.168.2.2<br>client_ipaddr = 192.168.3.0/24,192.168.2.2<br><br><br></font></code>

<a name="procmon-filtersconf"></a><h2> <a name="procmon-filtersconf_procmon-filters.conf"><span class="mw-headline" id="procmon-filters.conf">procmon-filters.conf</span></a></h2>
<p>The following are the spec and example files for procmon-filters.conf.
</p>
<h3> <a name="procmon-filtersconf_procmon-filters.conf.spec"><span class="mw-headline" id="procmon-filters.conf.spec">procmon-filters.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># *** DEPRECATED ***<br>#<br>#<br># This file contains potential attribute/value pairs to use when configuring Windows registry<br># monitoring. The procmon-filters.conf file contains the regular expressions you create to refine<br># and filter the processes you want Splunk to monitor. You must restart Splunk to enable configurations. <br># <br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>#### find out if this file is still being used.<br><br>[&lt;stanza name&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Name of the filter being defined.<br><br>proc = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Regex specifying process image that you want Splunk to monitor.<br><br>type = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Regex specifying the type(s) of process event that you want Splunk to monitor. <br><br>hive = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Not used in this context, but should always have value ".*" <br><br></font></code>
<h3> <a name="procmon-filtersconf_procmon-filters.conf.example"><span class="mw-headline" id="procmon-filters.conf.example">procmon-filters.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example registry monitor filters. To create your own filter, use <br># the information in procmon-filters.conf.spec.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># procmon-filters.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[default]<br>hive = .*<br><br>[not-splunk-optimize]<br>proc = (?&lt;!splunk-optimize.exe)$<br>type = create|exit|image<br><br></font></code>

<a name="propsconf"></a><h2> <a name="propsconf_props.conf"><span class="mw-headline" id="props.conf">props.conf</span></a></h2>
<p>The following are the spec and example files for props.conf.
</p>
<h3> <a name="propsconf_props.conf.spec"><span class="mw-headline" id="props.conf.spec">props.conf.spec</span></a></h3>
<code><font size="2"><br><br>#<br># This file contains possible attribute/value pairs for configuring Splunk's processing<br># properties via props.conf.<br>#<br># Props.conf is commonly used for:<br>#<br># * Configuring linebreaking for multiline events.<br># * Setting up character set encoding.<br># * Allowing processing of binary files.<br># * Configuring timestamp recognition.<br># * Configuring event segmentation.<br># * Overriding Splunk's automated host and source type matching. You can use props.conf to:<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Configure advanced (regex-based) host and source type overrides.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Override source type matching for data from a particular source.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set up rule-based source type recognition.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Rename source types.<br># * Anonymizing certain types of sensitive incoming data, such as credit card or social<br># &nbsp;&nbsp;security numbers, using sed scripts.<br># * Routing specific events to a particular index, when you have multiple indexes.<br># * Creating new index-time field extractions, including header-based field extractions.<br># &nbsp;&nbsp;NOTE: We do not recommend adding to the set of fields that are extracted at index time<br># &nbsp;&nbsp;unless it is absolutely necessary because there are negative performance implications.<br># * Defining new search-time field extractions. You can define basic search-time field<br># &nbsp;&nbsp;extractions entirely through props.conf. But a transforms.conf component is required if<br># &nbsp;&nbsp;you need to create search-time field extractions that involve one or more of the following:<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Reuse of the same field-extracting regular expression across multiple sources,<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;source types, or hosts.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Application of more than one regex to the same source, source type, or host.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Delimiter-based field extractions (they involve field-value pairs that are<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separated by commas, colons, semicolons, bars, or something similar).<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Extraction of multiple values for the same field (multivalued field extraction).<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Extraction of fields with names that begin with numbers or underscores.<br># * Setting up lookup tables that look up fields from external sources.<br># * Creating field aliases.<br>#<br># NOTE: Several of the above actions involve a corresponding transforms.conf configuration.<br>#<br># You can find more information on these topics by searching the Splunk documentation<br># (http://docs.splunk.com/Documentation/Splunk).<br>#<br># There is a props.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations,<br># place a props.conf in $SPLUNK_HOME/etc/system/local/. For help, see<br># props.conf.example.<br>#<br># You can enable configurations changes made to props.conf by typing the following search string<br># in Splunk Web:<br>#<br># | extract reload=T<br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># For more information about using props.conf in conjunction with distributed Splunk<br># deployments, see the Distributed Deployment Manual.<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;spec&gt;]<br>* This stanza enables properties for a given &lt;spec&gt;.<br>* A props.conf file can contain multiple stanzas for any number of different &lt;spec&gt;.<br>* Follow this stanza name with any number of the following attribute/value pairs, as appropriate<br>&nbsp;&nbsp;for what you want to do.<br>* If you do not set an attribute for a given &lt;spec&gt;, the default is used.<br><br>&lt;spec&gt; can be:<br>1. &lt;sourcetype&gt;, the source type of an event.<br>2. host::&lt;host&gt;, where &lt;host&gt; is the host, or host-matching pattern, for an event.<br>3. source::&lt;source&gt;, where &lt;source&gt; is the source, or source-matching pattern, for an event.<br>4. rule::&lt;rulename&gt;, where &lt;rulename&gt; is a unique name of a source type classification rule.<br>5. delayedrule::&lt;rulename&gt;, where &lt;rulename&gt; is a unique name of a delayed source type<br>&nbsp;&nbsp;&nbsp;classification rule.<br>These are only considered as a last resort before generating a new source type based on the<br>source seen.<br><br>**[&lt;spec&gt;] stanza precedence:**<br><br>For settings that are specified in multiple categories of matching [&lt;spec&gt;] stanzas,<br>[host::&lt;host&gt;] settings override [&lt;sourcetype&gt;] settings. Additionally,<br>[source::&lt;source&gt;] settings override both [host::&lt;host&gt;] and<br>[&lt;sourcetype&gt;] settings.<br><br>**Considerations for Windows file paths:**<br><br>When you specify Windows-based file paths as part of a [source::&lt;source&gt;] stanza, you must<br>escape any backslashes contained within the specified file path.<br><br>Example: [source::c:\\path_to\\file.txt]<br><br>**[&lt;spec&gt;] stanza patterns:**<br><br>When setting a [&lt;spec&gt;] stanza, you can use the following regex-type syntax:<br>... recurses through directories until the match is met<br>&nbsp;&nbsp;&nbsp;&nbsp;or equivalently, matches any number of characters.<br>* &nbsp;&nbsp;matches anything but the path separator 0 or more times.<br>&nbsp;&nbsp;&nbsp;&nbsp;The path separator is '/' on unix, or '\' on windows.<br>&nbsp;&nbsp;&nbsp;&nbsp;Intended to match a partial or complete directory or filename.<br>| &nbsp;&nbsp;is equivalent to 'or'<br>( ) are used to limit scope of |.<br><br>\\ = matches a literal backslash '\'.<br><br>Example: [source::....(?&lt;!tar.)(gz|tgz)]<br><br>&nbsp;This matches any file ending with '.gz' or '.bz2', provided this is not<br>&nbsp;preceded by 'tar.', so tar.bz2 and tar.gz would not be matched.<br><br>**[source::&lt;source&gt;] and [host::&lt;host&gt;] stanza match language:**<br><br>Match expressions must match the entire name, not just a substring. If you are familiar<br>with regular expressions, match expressions are based on a full implementation of PCRE with the<br>translation of ..., * and . Thus . matches a period, * matches non-directory separators,<br>and ... matches any number of any characters.<br><br>For more information see the wildcards section at:<br>http://docs.splunk.com/Documentation/Splunk/latest/Data/Specifyinputpathswithwildcards<br><br>**[&lt;spec&gt;] stanza pattern collisions:**<br><br>Suppose the source of a given input matches multiple [source::&lt;source&gt;] patterns. If the<br>[&lt;spec&gt;] stanzas for these patterns each supply distinct settings, Splunk applies all of these<br>settings.<br><br>However, suppose two [&lt;spec&gt;] stanzas supply the same setting. In this case, Splunk chooses<br>the value to apply based on the ASCII order of the patterns in question.<br><br>For example, take this source:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;source::az<br><br>and the following colliding patterns:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...a...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = a<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...z...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = z<br><br>In this case, the settings provided by the pattern [source::...a...] take precedence over those<br>provided by [source::...z...], and sourcetype ends up with "a" as its value.<br><br>To override this default ASCII ordering, use the priority key:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...a...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = a<br>&nbsp;&nbsp;&nbsp;&nbsp;priority = 5<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[source::...z...]<br>&nbsp;&nbsp;&nbsp;&nbsp;sourcetype = z<br>&nbsp;&nbsp;&nbsp;&nbsp;priority = 10<br><br>Assigning a higher priority to the second stanza causes sourcetype to have the value "z".<br><br>**Case-sensitivity for [&lt;spec&gt;] stanza matching:**<br><br>By default, [source::&lt;source&gt;] and [&lt;sourcetype&gt;] stanzas match in a case-sensitive manner,<br>while [host::&lt;host&gt;] stanzas match in a case-insensitive manner. This is a convenient default,<br>given that DNS names are case-insensitive.<br><br>To force a [host::&lt;host&gt;] stanza to match in a case-sensitive manner use the "(?-i)" option in<br>its pattern.<br><br>For example:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[host::foo]<br>&nbsp;&nbsp;&nbsp;&nbsp;FIELDALIAS-a = a AS one<br><br>&nbsp;&nbsp;&nbsp;&nbsp;[host::(?-i)bar]<br>&nbsp;&nbsp;&nbsp;&nbsp;FIELDALIAS-b = b AS two<br><br>The first stanza will actually apply to events with host values of "FOO" or<br>"Foo" . The second stanza, on the other hand, will not apply to events with<br>host values of "BAR" or "Bar".<br><br>**Building the final [&lt;spec&gt;] stanza:**<br><br>The final [&lt;spec&gt;] stanza is built by layering together (1) literal-matching stanzas (stanzas<br>which match the string literally) and (2) any regex-matching stanzas, according to the value of<br>the priority field.<br><br>If not specified, the default value of the priority key is:<br>* 0 for pattern-matching stanzas.<br>* 100 for literal-matching stanzas.<br><br>NOTE: Setting the priority key to a value greater than 100 causes the pattern-matched [&lt;spec&gt;]<br>stanzas to override the values of the literal-matching [&lt;spec&gt;] stanzas.<br><br>The priority key can also be used to resolve collisions between [&lt;sourcetype&gt;] patterns and<br>[host::&lt;host&gt;] patterns. However, be aware that the priority key does *not* affect precedence<br>across &lt;spec&gt; types. For example, [&lt;spec&gt;] stanzas with [source::&lt;source&gt;] patterns take<br>priority over stanzas with [host::&lt;host&gt;] and [&lt;sourcetype&gt;] patterns, regardless of their<br>respective priority key values.<br><br><br>#******************************************************************************<br># The possible attributes/value pairs for props.conf, and their<br># default values, are:<br>#******************************************************************************<br><br># International characters and character encoding.<br><br>CHARSET = &lt;string&gt;<br>* When set, Splunk assumes the input from the given [&lt;spec&gt;] is in the specified encoding.<br>* Can only be used as the basis of [&lt;sourcetype&gt;] or [source::&lt;spec&gt;], not [host::&lt;spec&gt;].<br>* A list of valid encodings can be retrieved using the command "iconv -l" on most *nix systems.<br>* If an invalid encoding is specified, a warning is logged during initial configuration and<br>&nbsp;&nbsp;further input from that [&lt;spec&gt;] is discarded.<br>* If the source encoding is valid, but some characters from the [&lt;spec&gt;] are not valid in the<br>&nbsp;&nbsp;specified encoding, then the characters are escaped as hex (for example, "\xF3").<br>* When set to "AUTO", Splunk attempts to automatically determine the character encoding and<br>&nbsp;&nbsp;convert text from that encoding to UTF-8.<br>* For a complete list of the character sets Splunk automatically detects, see the online<br>&nbsp;&nbsp;documentation.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to ASCII.<br><br><br>#******************************************************************************<br># Line breaking<br>#******************************************************************************<br><br># Use the following attributes to define the length of a line.<br><br>TRUNCATE = &lt;non-negative integer&gt;<br>* Change the default maximum line length (in bytes).<br>* Although this is in bytes, line length is rounded down when this would<br>&nbsp;&nbsp;otherwise land mid-character for multi-byte characters.<br>* Set to 0 if you never want truncation (very long lines are, however, often a sign of<br>&nbsp;&nbsp;garbage data).<br>* Defaults to 10000 bytes.<br><br>LINE_BREAKER = &lt;regular expression&gt;<br>* Specifies a regex that determines how the raw text stream is broken into initial events,<br>&nbsp;&nbsp;before line merging takes place. (See the SHOULD_LINEMERGE attribute, below)<br>* Defaults to ([\r\n]+), meaning data is broken into an event for each line, delimited by <br>&nbsp;&nbsp;any number of carriage return or newline characters.<br>* The regex must contain a capturing group -- a pair of parentheses which<br>&nbsp;&nbsp;defines an identified subcomponent of the match.<br>* Wherever the regex matches, Splunk considers the start of the first<br>&nbsp;&nbsp;capturing group to be the end of the previous event, and considers the end<br>&nbsp;&nbsp;of the first capturing group to be the start of the next event.<br>* The contents of the first capturing group are discarded, and will not be<br>&nbsp;&nbsp;present in any event. &nbsp;You are telling Splunk that this text comes between<br>&nbsp;&nbsp;lines.<br>* NOTE: You get a significant boost to processing speed when you use LINE_BREAKER to delimit<br>&nbsp;&nbsp;multiline events (as opposed to using SHOULD_LINEMERGE to reassemble individual lines into<br>&nbsp;&nbsp;multiline events).<br>&nbsp;&nbsp;* When using LINE_BREAKER to delimit events, SHOULD_LINEMERGE should be set<br>&nbsp;&nbsp;&nbsp;&nbsp;to false, to ensure no further combination of delimited events occurs.<br>&nbsp;&nbsp;* Using LINE_BREAKER to delimit events is discussed in more detail in the web <br>&nbsp;&nbsp;&nbsp;&nbsp;documentation at the following url: <br>&nbsp;&nbsp;&nbsp;&nbsp;http://docs.splunk.com/Documentation/Splunk/latest/Data/indexmulti-lineevents<br><br>** Special considerations for LINE_BREAKER with branched expressions &nbsp;**<br><br>When using LINE_BREAKER with completely independent patterns separated by<br>pipes, some special issues come into play.<br>&nbsp;&nbsp;&nbsp;&nbsp;EG. LINE_BREAKER = pattern1|pattern2|pattern3<br><br>Note, this is not about all forms of alternation, eg there is nothing<br>particular special about<br>&nbsp;&nbsp;&nbsp;&nbsp;example: LINE_BREAKER = ([\r\n])+(one|two|three)<br>where the top level remains a single expression.<br>&nbsp;<br>A caution: Relying on these rules is NOT encouraged. &nbsp;Simpler is better, in<br>both regular expressions and the complexity of the behavior they rely on.<br>If possible, it is strongly recommended that you reconstruct your regex to<br>have a leftmost capturing group that always matches.<br><br>It may be useful to use non-capturing groups if you need to express a group<br>before the text to discard.<br>&nbsp;&nbsp;&nbsp;&nbsp;EG. LINE_BREAKER = (?:one|two)([\r\n]+)<br>&nbsp;&nbsp;&nbsp;&nbsp;* This will match the text one, or two, followed by any amount of newlines<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or carriage returns. &nbsp;The one-or-two group is non-capturing via the&nbsp;?:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prefix and will be skipped by LINE_BREAKER.<br><br>* A branched expression can match without the first capturing group matching,<br>&nbsp;&nbsp;so the line breaker behavior becomes more complex.<br>&nbsp;&nbsp;Rules:<br>&nbsp;&nbsp;1: If the first capturing group is part of a match, it is considered the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;linebreak, as normal.<br>&nbsp;&nbsp;2: If the first capturing group is not part of a match, the leftmost<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;capturing group which is part of a match will be considered the linebreak.<br>&nbsp;&nbsp;3: If no capturing group is part of the match, the linebreaker will assume<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that the linebreak is a zero-length break immediately preceding the match.<br><br>Example 1: &nbsp;LINE_BREAKER = end(\n)begin|end2(\n)begin2|begin3<br><br>&nbsp;&nbsp;* A line ending with 'end' followed a line beginning with 'begin' would<br>&nbsp;&nbsp;&nbsp;&nbsp;match the first branch, and the first capturing group would have a match<br>&nbsp;&nbsp;&nbsp;&nbsp;according to rule 1. &nbsp;That particular newline would become a break<br>&nbsp;&nbsp;&nbsp;&nbsp;between lines.<br>&nbsp;&nbsp;* A line ending with 'end2' followed by a line beginning with 'begin2'<br>&nbsp;&nbsp;&nbsp;&nbsp;would match the second branch and the second capturing group would have a<br>&nbsp;&nbsp;&nbsp;&nbsp;match. &nbsp;That second capturing group would become the linebreak according<br>&nbsp;&nbsp;&nbsp;&nbsp;to rule 2, and the associated newline would become a break between lines.<br>&nbsp;&nbsp;* The text 'begin3' anywhere in the file at all would match the third<br>&nbsp;&nbsp;&nbsp;&nbsp;branch, and there would be no capturing group with a match. &nbsp;A linebreak<br>&nbsp;&nbsp;&nbsp;&nbsp;would be assumed immediately prior to the text 'begin3' so a linebreak<br>&nbsp;&nbsp;&nbsp;&nbsp;would be inserted prior to this text in accordance with rule 3.<br>&nbsp;&nbsp;&nbsp;&nbsp;This means that a linebreak will occur before the text 'begin3' at any<br>&nbsp;&nbsp;&nbsp;&nbsp;point in the text, whether a linebreak character exists or not.<br><br>Example 2: Example 1 would probably be better written as follows. &nbsp;This is<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not equivalent for all possible files, but for most real files<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;would be equivalent.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LINE_BREAKER = end2?(\n)begin(2|3)?<br><br>LINE_BREAKER_LOOKBEHIND = &lt;integer&gt;<br>* When there is leftover data from a previous raw chunk, LINE_BREAKER_LOOKBEHIND indicates the<br>&nbsp;&nbsp;number of bytes before the end of the raw chunk (with the next chunk concatenated) that<br>&nbsp;&nbsp;Splunk applies the LINE_BREAKER regex. You may want to increase this value from its default<br>&nbsp;&nbsp;if you are dealing with especially large or multiline events.<br>* Defaults to 100 (bytes).<br><br># Use the following attributes to specify how multiline events are handled.<br><br>SHOULD_LINEMERGE = [true|false]<br>* When set to true, Splunk combines several lines of data into a single multiline event, based<br>&nbsp;&nbsp;on the following configuration attributes.<br>* Defaults to true.<br><br># When SHOULD_LINEMERGE is set to true, use the following attributes to define how Splunk builds<br># multiline events.<br><br>BREAK_ONLY_BEFORE_DATE = [true|false]<br>* When set to true, Splunk creates a new event only if it encounters a new line with a date.<br>&nbsp;&nbsp;* Note, when using DATETIME_CONFIG = CURRENT or NONE, this setting is not meaningful, as<br>&nbsp;&nbsp;&nbsp;&nbsp;timestamps are not identified.<br>* Defaults to true.<br><br>BREAK_ONLY_BEFORE = &lt;regular expression&gt;<br>* When set, Splunk creates a new event only if it encounters a new line that matches the<br>&nbsp;&nbsp;regular expression.<br>* Defaults to empty.<br><br>MUST_BREAK_AFTER = &lt;regular expression&gt;<br>* When set and the regular expression matches the current line, Splunk creates a new event for<br>&nbsp;&nbsp;the next input line.<br>* Splunk may still break before the current line if another rule matches.<br>* Defaults to empty.<br><br>MUST_NOT_BREAK_AFTER = &lt;regular expression&gt;<br>* When set and the current line matches the regular expression, Splunk does not break on any<br>&nbsp;&nbsp;subsequent lines until the MUST_BREAK_AFTER expression matches.<br>* Defaults to empty.<br><br>MUST_NOT_BREAK_BEFORE = &lt;regular expression&gt;<br>* When set and the current line matches the regular expression, Splunk does not break the<br>&nbsp;&nbsp;last event before the current line.<br>* Defaults to empty.<br><br>MAX_EVENTS = &lt;integer&gt;<br>* Specifies the maximum number of input lines to add to any event.<br>* Splunk breaks after the specified number of lines are read.<br>* Defaults to 256 (lines).<br><br><br>#******************************************************************************<br># Timestamp extraction configuration<br>#******************************************************************************<br><br>DATETIME_CONFIG = &lt;filename relative to $SPLUNK_HOME&gt;<br>* Specifies which file configures the timestamp extractor, which identifies timestamps from the<br>&nbsp;&nbsp;event text.<br>* This configuration may also be set to "NONE" to prevent the timestamp extractor from running<br>&nbsp;&nbsp;or "CURRENT" to assign the current system time to each event.<br>&nbsp;&nbsp;* "CURRENT" will set the time of the event to the time that the event was merged from lines, or<br>&nbsp;&nbsp;&nbsp;&nbsp;worded differently, the time it passed through the aggregator processor.<br>&nbsp;&nbsp;* "NONE" will leave the event time set to whatever time was selected by the input layer<br>&nbsp;&nbsp;&nbsp;&nbsp;* For data sent by splunk forwarders over the splunk protocol, the input layer will be the time<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that was selected on the forwarder by its input behavior (as below).<br>&nbsp;&nbsp;&nbsp;&nbsp;* For file-based inputs (monitor, batch) the time chosen will be the modification timestamp on<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the file being read.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For other inputs, the time chosen will be the current system time when the event is read from<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the pipe/socket/etc.<br>&nbsp;&nbsp;* Both "CURRENT" and "NONE" explicitly disable the per-text timestamp identification, so<br>&nbsp;&nbsp;&nbsp;&nbsp;the default event boundary detection (BREAK_ONLY_BEFORE_DATE = true) is likely to not work as<br>&nbsp;&nbsp;&nbsp;&nbsp;desired. &nbsp;When using these settings, use SHOULD_LINEMERGE and/or the BREAK_ONLY_* , MUST_BREAK_*<br>&nbsp;&nbsp;&nbsp;&nbsp;settings to control event merging.<br>* Defaults to /etc/datetime.xml (for example, $SPLUNK_HOME/etc/datetime.xml).<br><br>TIME_PREFIX = &lt;regular expression&gt;<br>* If set, splunk scans the event text for a match for this regex in event text before attempting <br>&nbsp;&nbsp;to extract a timestamp.<br>* The timestamping algorithm only looks for a timestamp in the text following the end of the <br>&nbsp;&nbsp;first regex match.<br>* For example, if TIME_PREFIX is set to "abc123", only text following the first occurrence of the <br>&nbsp;&nbsp;text abc123 will be used for timestamp extraction.<br>* If the TIME_PREFIX cannot be found in the event text, timestamp extraction will not occur.<br>* Defaults to empty.<br><br>MAX_TIMESTAMP_LOOKAHEAD = &lt;integer&gt;<br>* Specifies how far (in characters) into an event Splunk should look for a timestamp.<br>* This constraint to timestamp extraction is applied from the point of the TIME_PREFIX-set location.<br>* For example, if TIME_PREFIX positions a location 11 characters into the event, and <br>&nbsp;&nbsp;MAX_TIMESTAMP_LOOKAHEAD is set to 10, timestamp extraction will be constrained to characters <br>&nbsp;&nbsp;11 through 20.<br>* If set to 0, or -1, the length constraint for timestamp recognition is<br>&nbsp;&nbsp;effectively disabled. &nbsp;This can have negative performance implications which<br>&nbsp;&nbsp;scale with the length of input lines (or with event size when LINE_BREAKER<br>&nbsp;&nbsp;is redefined for event splitting).<br>* Defaults to 150 (characters).<br><br>TIME_FORMAT = &lt;strptime-style format&gt;<br>* Specifies a strptime format string to extract the date.<br>* strptime is an industry standard for designating time formats.<br>* For more information on strptime, see "Configure timestamp recognition" in<br>&nbsp;&nbsp;the online documentation.<br>* TIME_FORMAT starts reading after the TIME_PREFIX. If both are specified, the TIME_PREFIX<br>&nbsp;&nbsp;regex must match up to and including the character before the TIME_FORMAT date.<br>* For good results, the &lt;strptime-style format&gt; should describe the day of the year and the<br>&nbsp;&nbsp;time of day.<br>* Defaults to empty.<br><br>TZ = &lt;timezone identifier&gt;<br>* The algorithm for determining the time zone for a particular event is as follows:<br>* If the event has a timezone in its raw text (for example, UTC, -08:00), use that.<br>* If TZ is set to a valid timezone string, use that.<br>* If the event was forwarded, and the forwarder-indexer connection is using the<br>&nbsp;&nbsp;6.0+ forwarding protocol, use the timezone provided by the forwarder.<br>* Otherwise, use the timezone of the system that is running splunkd.<br>* Defaults to empty.<br><br>TZ_ALIAS = &lt;key=value&gt;[,&lt;key=value&gt;]...<br>* Provides splunk admin-level control over how timezone strings extracted from events are<br>&nbsp;&nbsp;interpreted.<br>&nbsp;&nbsp;* For example, EST can mean Eastern (US) Standard time, or Eastern (Australian) Standard time.<br>&nbsp;&nbsp;&nbsp;&nbsp;There are many other three letter timezone acronyms with many expansions.<br>* There is no requirement to use TZ_ALIAS if the traditional Splunk default mappings for these<br>&nbsp;&nbsp;values have been as expected. &nbsp;For example, EST maps to the Eastern US by default.<br>* Has no effect on TZ value; this only affects timezone strings from event text, either from<br>&nbsp;&nbsp;any configured TIME_FORMAT, or from pattern-based guess fallback.<br>* The setting is a list of key=value pairs, separated by commas.<br>&nbsp;&nbsp;* The key is matched against the text of the timezone specifier of the event, and the value is the<br>&nbsp;&nbsp;&nbsp;&nbsp;timezone specifier to use when mapping the timestamp to UTC/GMT. <br>&nbsp;&nbsp;* The value is another TZ specifier which expresses the desired offset.<br>&nbsp;&nbsp;* Example: TZ_ALIAS = EST=GMT+10:00 (See props.conf.example for more/full examples)<br>* Defaults to unset.<br><br>MAX_DAYS_AGO = &lt;integer&gt;<br>* Specifies the maximum number of days past, from the current date, that an extracted date<br>&nbsp;&nbsp;can be valid.<br>* For example, if MAX_DAYS_AGO = 10, Splunk ignores dates that are older than 10 days ago.<br>* Defaults to 2000 (days), maximum 10951.<br>* IMPORTANT: If your data is older than 2000 days, increase this setting.<br><br>MAX_DAYS_HENCE = &lt;integer&gt;<br>* Specifies the maximum number of days in the future from the current date that an extracted<br>&nbsp;&nbsp;date can be valid.<br>* For example, if MAX_DAYS_HENCE = 3, dates that are more than 3 days in the future are ignored.<br>* The default value includes dates from one day in the future.<br>* If your servers have the wrong date set or are in a timezone that is one day ahead, increase<br>&nbsp;&nbsp;this value to at least 3.<br>* Defaults to 2 (days), maximum 10950.<br>* IMPORTANT:False positives are less likely with a tighter window, change with caution.<br><br>MAX_DIFF_SECS_AGO = &lt;integer&gt;<br>* If the event's timestamp is more than &lt;integer&gt; seconds BEFORE the previous timestamp, only<br>&nbsp;&nbsp;accept the event if it has the same exact time format as the majority of timestamps from the <br>&nbsp;&nbsp;source.<br>* IMPORTANT: If your timestamps are wildly out of order, consider increasing this value.<br>* Note: if the events contain time but not date (date determined another way, such as from a<br>&nbsp;&nbsp;filename) this check will only consider the hour. (No one second granularity for this purpose.)<br>* Defaults to 3600 (one hour), maximum 2147483646.<br><br>MAX_DIFF_SECS_HENCE = &lt;integer&gt;<br>* If the event's timestamp is more than &lt;integer&gt; seconds AFTER the previous timestamp, only<br>&nbsp;&nbsp;accept the event if it has the same exact time format as the majority of timestamps from the <br>&nbsp;&nbsp;source.<br>* IMPORTANT: If your timestamps are wildly out of order, or you have logs that are written<br>&nbsp;&nbsp;less than once a week, consider increasing this value.<br>* Defaults to 604800 (one week), maximum 2147483646.<br><br><br>#******************************************************************************<br># Structured Data Header Extraction and configuration<br>#******************************************************************************<br><br>* This feature and all of its settings apply at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br><br># Special characters for Structured Data Header Extraction: <br># Some unprintable characters can be described with escape sequences. The attributes<br># that can use these characters specifically mention that capability in their descriptions below.<br># \f&nbsp;: form feed &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;byte: 0x0c<br># \s&nbsp;: space &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;byte: 0x20<br># \t&nbsp;: horizontal tab &nbsp;byte: 0x09<br># \v&nbsp;: vertical tab &nbsp;&nbsp;&nbsp;byte: 0x0b<br><br>INDEXED_EXTRACTIONS = &lt; CSV|W3C|TSV|PSV|JSON &gt;<br>* Tells Splunk the type of file and the extraction and/or parsing method Splunk should use<br>&nbsp;&nbsp;on the file.<br>&nbsp;&nbsp;CSV &nbsp;- Comma separated value format<br>&nbsp;&nbsp;TSV &nbsp;- Tab-separated value format<br>&nbsp;&nbsp;PSV &nbsp;- pipe "|" separated value format<br>&nbsp;&nbsp;W3C &nbsp;- W3C Extended Extended Log File Format<br>&nbsp;&nbsp;JSON - JavaScript Object Notation format<br>* These settings default the values of the remaining settings to the<br>&nbsp;&nbsp;appropriate values for these known formats.<br>* Defaults to unset.<br><br>PREAMBLE_REGEX = &lt;regex&gt;<br>* Some files contain preamble lines. This attribute specifies a regular expression which<br>&nbsp;&nbsp;allows Splunk to ignore these preamble lines, based on the pattern specified.<br><br>FIELD_HEADER_REGEX = &lt;regex&gt;<br>* A regular expression that specifies a pattern for prefixed headers. Note that the actual header<br>&nbsp;&nbsp;starts after the pattern and it is not included in the header field.<br>* This attribute supports the use of the special characters described above.<br><br>HEADER_FIELD_LINE_NUMBER = &lt;integer&gt;<br>* Tells Splunk the line number of the line within the file that contains the header fields.<br>&nbsp;&nbsp;If set to 0, Splunk attempts to locate the header fields within the file automatically.<br>* The default value is set to 0.<br><br>FIELD_DELIMITER = &lt;character&gt;<br>* Tells Splunk which character delimits or separates fields in the specified file or source.<br>* This attribute supports the use of the special characters described above.<br><br>HEADER_FIELD_DELIMITER = &lt;character&gt;<br>* Tells Splunk which character delimits or separates header fields in the specified file or source.<br>* This attribute supports the use of the special characters described above.<br><br>FIELD_QUOTE = &lt;character&gt;<br>* Tells Splunk the character to use for quotes in the specified file or source.<br>* This attribute supports the use of the special characters described above.<br><br>HEADER_FIELD_QUOTE = &lt;character&gt;<br>* Specifies Splunk the character to use for quotes in the header of the specified file or source.<br>* This attribute supports the use of the special characters described above.<br><br>TIMESTAMP_FIELDS = [ &lt;string&gt;,..., &lt;string&gt;] <br>* Some CSV and structured files have their timestamp encompass multiple fields in the event <br>&nbsp;&nbsp;separated by delimiters. This attribue tells Splunk to specify all such fields which<br>&nbsp;&nbsp;constitute the timestamp in a comma-separated fashion.<br>* If not specified, Splunk tries to automatically extract the timestamp of the event.<br><br>FIELD_NAMES = [ &lt;string&gt;,..., &lt;string&gt;] <br>* Some CSV and structured files might have missing headers. This attribute tells Splunk to<br>&nbsp;&nbsp;specify the header field names directly.<br><br>MISSING_VALUE_REGEX = &lt;regex&gt;<br>* Tells Splunk the placeholder to use in events where no value is present.<br><br>#******************************************************************************<br># Field extraction configuration<br>#******************************************************************************<br><br>NOTE: If this is your first time configuring field extractions in props.conf, review<br>the following information first.<br><br>There are three different "field extraction types" that you can use to configure field<br>extractions: TRANSFORMS, REPORT, and EXTRACT. They differ in two significant ways: 1) whether<br>they create indexed fields (fields extracted at index time) or extracted fields (fields<br>extracted at search time), and 2), whether they include a reference to an additional component<br>called a "field transform," which you define separately in transforms.conf.<br><br>**Field extraction configuration: index time versus search time**<br><br>Use the TRANSFORMS field extraction type to create index-time field extractions. Use the<br>REPORT or EXTRACT field extraction types to create search-time field extractions.<br><br>NOTE: Index-time field extractions have performance implications. Creating additions to<br>Splunk's default set of indexed fields is ONLY recommended in specific circumstances.<br>Whenever possible, extract fields only at search time.<br><br>There are times when you may find that you need to change or add to your set of indexed<br>fields. For example, you may have situations where certain search-time field extractions are<br>noticeably impacting search performance. This can happen when the value of a search-time<br>extracted field exists outside of the field more often than not. For example, if you commonly<br>search a large event set with the expression company_id=1 but the value 1 occurs in many<br>events that do *not* have company_id=1, you may want to add company_id to the list of fields<br>extracted by Splunk at index time. This is because at search time, Splunk will want to check<br>each instance of the value 1 to see if it matches company_id, and that kind of thing slows<br>down performance when you have Splunk searching a large set of data.<br><br>Conversely, if you commonly search a large event set with expressions like company_id!=1<br>or NOT company_id=1, and the field company_id nearly *always* takes on the value 1, you<br>may want to add company_id to the list of fields extracted by Splunk at index time.<br><br>For more information about index-time field extraction, search the documentation for<br>"index-time extraction." For more information about search-time field extraction, search<br>the online documentation for "search-time extraction."<br><br>**Field extraction configuration: field transforms vs. "inline" (props.conf only) configs**<br><br>The TRANSFORMS and REPORT field extraction types reference an additional component called<br>a field transform, which you define separately in transforms.conf. Field transforms contain<br>a field-extracting regular expression and other attributes that govern the way that the<br>transform extracts fields. Field transforms are always created in conjunction with field<br>extraction stanzas in props.conf; they do not stand alone.<br><br>The EXTRACT field extraction type is considered to be "inline," which means that it does<br>not reference a field transform. It contains the regular expression that Splunk uses to<br>extract fields at search time. You can use EXTRACT to define a field extraction entirely<br>within props.conf--no transforms.conf component is required.<br><br>**Search-time field extractions: Why use REPORT if EXTRACT will do?**<br><br>It's a good question. And much of the time, EXTRACT is all you need for search-time field<br>extraction. But when you build search-time field extractions, there are specific cases that<br>require the use of REPORT and the field transform that it references. Use REPORT if you want<br>to:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Reuse the same field-extracting regular expression across multiple sources, source<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;types, or hosts. If you find yourself using the same regex to extract fields across<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;several different sources, source types, and hosts, set it up as a transform, and then<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reference it in REPORT extractions in those stanzas. If you need to update the regex<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you only have to do it in one place. Handy!<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Apply more than one field-extracting regular expression to the same source, source<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type, or host. This can be necessary in cases where the field or fields that you want<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to extract from a particular source, source type, or host appear in two or more very<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;different event patterns.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Use a regular expression to extract fields from the values of another field (also<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;referred to as a "source key").<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set up delimiter-based field extractions. Useful if your event data presents<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;field-value pairs (or just field values) separated by delimiters such as commas,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spaces, bars, and so on.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Configure extractions for multivalued fields. You can have Splunk append additional<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;values to a field as it finds them in the event data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Extract fields with names beginning with numbers or underscores. Ordinarily, Splunk's<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key cleaning functionality removes leading numeric characters and underscores from<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;field names. If you need to keep them, configure your field transform to turn key<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cleaning off.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Manage formatting of extracted fields, in cases where you are extracting multiple fields,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or are extracting both the field name and field value.<br><br>**Precedence rules for TRANSFORMS, REPORT, and EXTRACT field extraction types**<br><br>* For each field extraction, Splunk takes the configuration from the highest precedence<br>&nbsp;&nbsp;configuration stanza (see precedence rules at the beginning of this file).<br>* If a particular field extraction is specified for a source and a source type, the field<br>&nbsp;&nbsp;extraction for source wins out.<br>* Similarly, if a particular field extraction is specified in ../local/ for a &lt;spec&gt;, it<br>&nbsp;&nbsp;overrides that field extraction in ../default/.<br><br><br>TRANSFORMS-&lt;class&gt; = &lt;transform_stanza_name&gt;, &lt;transform_stanza_name2&gt;,...<br>* Used for creating indexed fields (index-time field extractions).<br>* &lt;class&gt; is a unique literal string that identifies the namespace of the field you're extracting.<br>&nbsp;&nbsp;**Note:** &lt;class&gt; values do not have to follow field name syntax restrictions. You can use <br>&nbsp;&nbsp;characters other than a-z, A-Z, and 0-9, and spaces are allowed. &lt;class&gt; values are not subject<br>&nbsp;&nbsp;to key cleaning. <br>* &lt;transform_stanza_name&gt; is the name of your stanza from transforms.conf.<br>* Use a comma-separated list to apply multiple transform stanzas to a single TRANSFORMS<br>&nbsp;&nbsp;extraction. Splunk applies them in the list order. For example, this sequence ensures that<br>&nbsp;&nbsp;the [yellow] transform stanza gets applied first, then [blue], and then [red]:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[source::color_logs]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TRANSFORMS-colorchange = yellow, blue, red<br><br>REPORT-&lt;class&gt; = &lt;transform_stanza_name&gt;, &lt;transform_stanza_name2&gt;,...<br>* Used for creating extracted fields (search-time field extractions) that reference one or more<br>&nbsp;&nbsp;transforms.conf stanzas.<br>* &lt;class&gt; is a unique literal string that identifies the namespace of the field you're extracting.<br>&nbsp;&nbsp;**Note:** &lt;class&gt; values do not have to follow field name syntax restrictions. You can use <br>&nbsp;&nbsp;characters other than a-z, A-Z, and 0-9, and spaces are allowed. &lt;class&gt; values are not subject<br>&nbsp;&nbsp;to key cleaning. <br>* &lt;transform_stanza_name&gt; is the name of your stanza from transforms.conf.<br>* Use a comma-separated list to apply multiple transform stanzas to a single REPORT extraction.<br>&nbsp;&nbsp;Splunk applies them in the list order. For example, this sequence insures that the [yellow]<br>&nbsp;&nbsp;transform stanza gets applied first, then [blue], and then [red]:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[source::color_logs]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;REPORT-colorchange = yellow, blue, red<br><br>EXTRACT-&lt;class&gt; = [&lt;regex&gt;|&lt;regex&gt; in &lt;src_field&gt;]<br>* Used to create extracted fields (search-time field extractions) that do not reference<br>&nbsp;&nbsp;transforms.conf stanzas.<br>* Performs a regex-based field extraction from the value of the source field.<br>* &lt;class&gt; is a unique literal string that identifies the namespace of the field you're extracting.<br>&nbsp;&nbsp;**Note:** &lt;class&gt; values do not have to follow field name syntax restrictions. You can use <br>&nbsp;&nbsp;characters other than a-z, A-Z, and 0-9, and spaces are allowed. &lt;class&gt; values are not subject<br>&nbsp;&nbsp;to key cleaning. <br>* The &lt;regex&gt; is required to have named capturing groups. When the &lt;regex&gt; matches, the named<br>&nbsp;&nbsp;capturing groups and their values are added to the event.<br>* Use '&lt;regex&gt; in &lt;src_field&gt;' to match the regex against the values of a specific field.<br>&nbsp;&nbsp;Otherwise it just matches against _raw (all raw event data).<br>* NOTE: &lt;src_field&gt; can only contain alphanumeric characters and underscore (a-z, A-Z, 0-9, and _).<br>* If your regex needs to end with 'in &lt;string&gt;' where &lt;string&gt; is *not* a field name, change<br>&nbsp;&nbsp;the regex to end with '[i]n &lt;string&gt;' to ensure that Splunk doesn't try to match &lt;string&gt;<br>&nbsp;&nbsp;to a field name.<br><br>KV_MODE = [none|auto|auto_escaped|multi|json|xml]<br>* Used for search-time field extractions only.<br>* Specifies the field/value extraction mode for the data.<br>* Set KV_MODE to one of the following:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* none: if you want no field/value extraction to take place.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* auto: extracts field/value pairs separated by equal signs.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* auto_escaped: extracts fields/value pairs separated by equal signs and honors \" and \\ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as escaped sequences within quoted values, e.g field="value with \"nested\" quotes"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* multi: invokes the multikv search command to expand a tabular event into multiple events.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* xml&nbsp;: automatically extracts fields from XML data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* json: automatically extracts fields from JSON data.<br>* Setting to 'none' can ensure that one or more user-created regexes are not overridden by<br>&nbsp;&nbsp;automatic field/value extraction for a particular host, source, or source type, and also<br>&nbsp;&nbsp;increases search performance.<br>* Defaults to auto.<br>* The 'xml' and 'json' modes will not extract any fields when used on data that isn't of the <br>&nbsp;&nbsp;correct format (JSON or XML).<br><br>AUTO_KV_JSON = [true|false]<br>* Used for search-time field extractions only.<br>* Specifies whether to try json extraction automatically.<br>* Defaults to true.<br><br>KV_TRIM_SPACES = true|false<br>* Modifies the behavior of KV_MODE when set to auto, and auto_escaped.<br>* Traditionally, automatically identified fields have leading and trailing<br>&nbsp;&nbsp;whitespace removed from their values.<br>&nbsp;&nbsp;* Example event: 2014-04-04 10:10:45 myfield=" apples "<br>&nbsp;&nbsp;&nbsp;&nbsp;would result in a field called 'myfield' with a value of 'apples'.<br>* If this value is set to false, then external whitespace then this outer space<br>&nbsp;&nbsp;is retained.<br>&nbsp;&nbsp;* Example: 2014-04-04 10:10:45 myfield=" apples "<br>&nbsp;&nbsp;&nbsp;&nbsp;would result in a field called 'myfield' with a value of ' apples '.<br>* The trimming logic applies only to space characters, not tabs, or other<br>&nbsp;&nbsp;whitespace.<br>* NOTE: The Splunk UI currently has limitations with displaying and<br>&nbsp;&nbsp;interactively clicking on &nbsp;fields that have leading or trailing whitespace.<br>&nbsp;&nbsp;Field values with leading or trailing spaces may not look distinct in the<br>&nbsp;&nbsp;event viewer, and clicking on a field value will typically insert the term<br>&nbsp;&nbsp;into the search string without its embedded spaces.<br>&nbsp;&nbsp;* These warts are not specific to this feature. &nbsp;Any such embedded spaces<br>&nbsp;&nbsp;&nbsp;&nbsp;will behave this way.<br>&nbsp;&nbsp;* The Splunk search language and included commands will respect the spaces.<br>* Defaults to true.<br><br>CHECK_FOR_HEADER = [true|false]<br>* Used for index-time field extractions only.<br>* Set to true to enable header-based field extraction for a file.<br>* If the file has a list of columns and each event contains a field value (without field name), <br>&nbsp;&nbsp;Splunk picks a suitable header line to use to for extracting field names.<br>* If the file has a list of columns and each event contains a field value (without a field<br>&nbsp;&nbsp;name), Splunk picks a suitable header line to use for field extraction.<br>* Can only be used on the basis of [&lt;sourcetype&gt;] or [source::&lt;spec&gt;], not [host::&lt;spec&gt;].<br>* Disabled when LEARN_SOURCETYPE = false.<br>* Will cause the indexed source type to have an appended numeral; for example, sourcetype-2,<br>&nbsp;&nbsp;sourcetype-3, and so on.<br>* The field names are stored in etc/apps/learned/local/props.conf.<br>&nbsp;&nbsp;* Because of this, this feature will not work in most environments where the<br>&nbsp;&nbsp;&nbsp;&nbsp;data is forwarded.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to false.<br><br>SEDCMD-&lt;class&gt; = &lt;sed script&gt;<br>* Only used at index time.<br>* Commonly used to anonymize incoming data at index time, such as credit card or social<br>&nbsp;&nbsp;security numbers. For more information, search the online documentation for "anonymize<br>&nbsp;&nbsp;data."<br>* Used to specify a sed script which Splunk applies to the _raw field.<br>* A sed script is a space-separated list of sed commands. Currently the following subset of<br>&nbsp;&nbsp;sed commands is supported:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* replace (s) and character substitution (y).<br>* Syntax:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* replace - s/regex/replacement/flags<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* regex is a perl regular expression (optionally containing capturing groups).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* replacement is a string to replace the regex match. Use \n for back <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;references, where "n" is a single digit.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* flags can be either: g to replace all matches, or a number to replace a <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;specified match.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* substitute - y/string1/string2/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* substitutes the string1[i] with string2[i]<br><br>FIELDALIAS-&lt;class&gt; = (&lt;orig_field_name&gt; AS &lt;new_field_name&gt;)+<br>* Use this to apply aliases to a field. The original field is not removed. This just means<br>&nbsp;&nbsp;that the original field can be searched on using any of its aliases.<br>* You can create multiple aliases for the same field.<br>* &lt;orig_field_name&gt; is the original name of the field.<br>* &lt;new_field_name&gt; is the alias to assign to the field.<br>* You can include multiple field alias renames in the same stanza.<br>* Field aliasing is performed at search time, after field extraction, but before <br>&nbsp;&nbsp;calculated fields (EVAL-* statements) and lookups.<br>&nbsp;&nbsp;This means that:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Any field extracted at search time can be aliased.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can specify a lookup based on a field alias.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You cannot alias a calculated field.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>EVAL-&lt;fieldname&gt; = &lt;eval statement&gt;<br>* Use this to automatically run the &lt;eval statement&gt; and assign the value of the output <br>&nbsp;&nbsp;to &lt;fieldname&gt;. This creates a "calculated field."<br>* When multiple EVAL-* statements are specified, they behave as if <br>&nbsp;&nbsp;they are run in parallel, rather than in any particular sequence. &nbsp;<br>&nbsp;&nbsp;For example say you have two statements: EVAL-x = y*2 and EVAL-y=100. In this case, "x" <br>&nbsp;&nbsp;will be assigned the original value of "y * 2," not the value of "y" after it is set to 100.<br>* Splunk processes calculated fields after field extraction and field aliasing but before <br>&nbsp;&nbsp;lookups. This means that:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can use a field alias in the eval statement for a calculated field.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You cannot use a field added through a lookup in an eval statement for a calculated<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;field. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>LOOKUP-&lt;class&gt; = $TRANSFORM (&lt;match_field&gt; (AS &lt;match_field_in_event&gt;)?)+ (OUTPUT|OUTPUTNEW <br>(&lt;output_field&gt; (AS &lt;output_field_in_event&gt;)? )+ )?<br>* At search time, identifies a specific lookup table and describes how that lookup table should<br>&nbsp;&nbsp;be applied to events.<br>* &lt;match_field&gt; specifies a field in the lookup table to match on.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default Splunk looks for a field with that same name in the event to match with<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(if &lt;match_field_in_event&gt; is not provided)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You must provide at least one match field. Multiple match fields are allowed.<br>* &lt;output_field&gt; specifies a field in the lookup entry to copy into each matching event,<br>&nbsp;&nbsp;where it will be in the field &lt;output_field_in_event&gt;.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify an &lt;output_field_in_event&gt; value, Splunk uses &lt;output_field&gt;.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A list of output fields is not required.<br>* If they are not provided, all fields in the lookup table except for the match fields (and<br>&nbsp;&nbsp;the timestamp field if it is specified) will be output for each matching event.<br>* If the output field list starts with the keyword "OUTPUTNEW" instead of "OUTPUT",<br>&nbsp;&nbsp;then each outputfield is only written out if it did not previous exist. Otherwise,<br>&nbsp;&nbsp;the output fields are always overridden. Any event that has all of the &lt;match_field&gt; values<br>&nbsp;&nbsp;but no matching entry in the lookup table clears all of the output fields.<br>&nbsp;&nbsp;NOTE that OUTPUTNEW behavior has changed since 4.1.x (where *none* of the output fields were<br>&nbsp;&nbsp;written to if *any* of the output fields previously existed).<br>* Splunk processes lookups after it processes field extractions, field aliases, and <br>&nbsp;&nbsp;calculated fields (EVAL-* statements). This means that you can use extracted fields,<br>&nbsp;&nbsp;aliased fields, and calculated fields to specify lookups. But you can't use fields<br>&nbsp;&nbsp;discovered by lookups in the configurations of extracted fields, aliased fields, or <br>&nbsp;&nbsp;calculated fields. <br>* The LOOKUP- prefix is actually case-insensitive. Acceptable variants include:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LOOKUP_&lt;class&gt; = [...]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LOOKUP&lt;class&gt; &nbsp;= [...]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lookup_&lt;class&gt; = [...]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lookup&lt;class&gt; &nbsp;= [...]<br><br>#******************************************************************************<br># Binary file configuration<br>#******************************************************************************<br><br>NO_BINARY_CHECK = [true|false]<br>* When set to true, Splunk processes binary files and does not check to see if they are binary first.<br>* Can only be used on the basis of [&lt;sourcetype&gt;], or [source::&lt;source&gt;], not [host::&lt;host&gt;].<br>* Defaults to false (Splunk checks to see if the file is binary, and ignores it if it is.)<br>* This setting applies at input time, when Splunk first reads data. <br>&nbsp;&nbsp;Use the setting on Splunk systems that have configured inputs acquiring the data.<br><br>detect_trailing_nulls = [auto|true|false]<br>* When enabled, Splunk will try to avoid reading in null bytes at the end of a file.<br>* When false, splunk will assume that all the bytes in the file should be read and indexed.<br>* Set this value to false for UTF-16 and other encodings (CHARSET) values that<br>&nbsp;&nbsp;can have null bytes as part of the character text.<br>* Subtleties of 'true' vs 'auto':<br>&nbsp;&nbsp;* 'true' is the splunk-on-windows historical behavior of trimming all null bytes.<br>&nbsp;&nbsp;* 'auto' is currently a synonym for true but will be extended to be sensitive<br>&nbsp;&nbsp;&nbsp;&nbsp;to the charset selected (ie quantized for multi-byte encodings, and<br>&nbsp;&nbsp;&nbsp;&nbsp;disabled for unsafe variable-width encdings)<br>* This feature was introduced to work around programs which foolishly<br>&nbsp;&nbsp;pre-allocate their log files with nulls and fill in data later. &nbsp;The<br>&nbsp;&nbsp;well-known case is Internet Information Server.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to false on *nix, true on windows.<br><br>#******************************************************************************<br># Segmentation configuration<br>#******************************************************************************<br><br>SEGMENTATION = &lt;segmenter&gt;<br>* Specifies the segmenter from segmenters.conf to use at index time for the host,<br>&nbsp;&nbsp;source, or sourcetype specified by &lt;spec&gt; in the stanza heading.<br>* Defaults to indexing.<br><br>SEGMENTATION-&lt;segment selection&gt; = &lt;segmenter&gt;<br>* Specifies that Splunk Web should use the specific segmenter (from segmenters.conf) for the<br>&nbsp;&nbsp;given &lt;segment selection&gt; choice.<br>* Default &lt;segment selection&gt; choices are: all, inner, outer, raw. For more information<br>&nbsp;&nbsp;see the Admin Manual.<br>* Do not change the set of default &lt;segment selection&gt; choices, unless you have some overriding<br>&nbsp;&nbsp;reason for doing so. In order for a changed set of &lt;segment selection&gt; choices to appear in<br>&nbsp;&nbsp;Splunk Web, you will need to edit the Splunk Web UI.<br><br>#******************************************************************************<br># File checksum configuration<br>#******************************************************************************<br><br>CHECK_METHOD = [endpoint_md5|entire_md5|modtime]<br>* Set CHECK_METHOD endpoint_md5 to have Splunk checksum of the first and last 256 bytes of a<br>&nbsp;&nbsp;file. When it finds matches, Splunk lists the file as already indexed and indexes only new<br>&nbsp;&nbsp;data, or ignores it if there is no new data.<br>* Set CHECK_METHOD = entire_md5 to use the checksum of the entire file.<br>* Set CHECK_METHOD = modtime to check only the modification time of the file.<br>* Settings other than endpoint_md5 cause Splunk to index the entire file for each detected<br>&nbsp;&nbsp;change.<br>* Important: this option is only valid for [source::&lt;source&gt;] stanzas. &nbsp;<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to endpoint_md5.<br><br>initCrcLength = &lt;integer&gt;<br>* See documentation in inputs.conf.spec.<br><br>#******************************************************************************<br># Small file settings<br>#******************************************************************************<br><br>PREFIX_SOURCETYPE = [true|false]<br>* NOTE: this attribute is only relevant to the "[too_small]" sourcetype.<br>* Determines the source types that are given to files smaller than 100 lines, and are therefore<br>&nbsp;&nbsp;not classifiable.<br>* PREFIX_SOURCETYPE = false sets the source type to "too_small."<br>* PREFIX_SOURCETYPE = true sets the source type to "&lt;sourcename&gt;-too_small", where "&lt;sourcename&gt;"<br>&nbsp;&nbsp;is a cleaned up version of the filename.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The advantage of PREFIX_SOURCETYPE = true is that not all small files are classified as<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the same source type, and wildcard searching is often effective.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, a Splunk search of "sourcetype=access*" will retrieve "access" files as well<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as "access-too_small" files.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to true.<br><br><br>#******************************************************************************<br># Sourcetype configuration<br>#******************************************************************************<br><br>sourcetype = &lt;string&gt;<br>* Can only be set for a [source::...] stanza.<br>* Anything from that &lt;source&gt; is assigned the specified source type.<br>* Is used by file-based inputs, at input time (when accessing logfiles) such as<br>&nbsp;&nbsp;on a forwarder, or indexer monitoring local files.<br>* sourcetype assignment settings on a system receiving forwarded splunk data<br>&nbsp;&nbsp;will not be applied to forwarded data.<br>* For logfiles read locally, data from logfiles matching &lt;source&gt; is assigned<br>&nbsp;&nbsp;the specified source type.<br>* Defaults to empty.<br><br># The following attribute/value pairs can only be set for a stanza that begins<br># with [&lt;sourcetype&gt;]:<br><br>rename = &lt;string&gt;<br>* Renames [&lt;sourcetype&gt;] as &lt;string&gt; at search time<br>* With renaming, you can search for the [&lt;sourcetype&gt;] with sourcetype=&lt;string&gt;<br>* To search for the original source type without renaming it, use the field _sourcetype.<br>* Data from a a renamed sourcetype will only use the search-time configuration for the target <br>&nbsp;&nbsp;sourcetype. Field extractions (REPORTS/EXTRACT) for this stanza sourcetype will be ignored.<br>* Defaults to empty.<br><br>invalid_cause = &lt;string&gt;<br>* Can only be set for a [&lt;sourcetype&gt;] stanza.<br>* Splunk does not read any file sources of a sourcetype with invalid_cause set.<br>* Set &lt;string&gt; to "archive" to send the file to the archive processor (specified in<br>&nbsp;&nbsp;unarchive_cmd).<br>* Set to any other string to throw an error in the splunkd.log if you are running<br>&nbsp;&nbsp;Splunklogger in debug mode.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to empty.<br><br>is_valid = [true|false]<br>* Automatically set by invalid_cause.<br>* This setting applies at input time, when data is first read by Splunk, such<br>&nbsp;&nbsp;as on a forwarder.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* DO NOT SET THIS.<br>* Defaults to true.<br><br>unarchive_cmd = &lt;string&gt;<br>* Only called if invalid_cause is set to "archive".<br>* This field is only valid on [source::&lt;source&gt;] stanzas.<br>* &lt;string&gt; specifies the shell command to run to extract an archived source.<br>* Must be a shell command that takes input on stdin and produces output on stdout.<br>* Use _auto for Splunk's automatic handling of archive files (tar, tar.gz, tgz, tbz, tbz2, zip)<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to empty.<br><br>unarchive_sourcetype = &lt;string&gt;<br>* Sets the source type of the contents of the matching archive file. Use this field instead<br>&nbsp;&nbsp;of the sourcetype field to set the source type of archive files that have the following<br>&nbsp;&nbsp;extensions: gz, bz, bz2, Z.<br>* If this field is empty (for a matching archive file props lookup) Splunk strips off the<br>&nbsp;&nbsp;archive file's extension (.gz, bz etc) and lookup another stanza to attempt to determine the<br>&nbsp;&nbsp;sourcetype.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to empty.<br><br>LEARN_SOURCETYPE = [true|false]<br>* Determines whether learning of known or unknown sourcetypes is enabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For known sourcetypes, refer to LEARN_MODEL.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For unknown sourcetypes, refer to the rule:: and delayedrule:: configuration (see below).<br>* Setting this field to false disables CHECK_FOR_HEADER as well (see above).<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to true.<br><br>LEARN_MODEL = [true|false]<br>* For known source types, the file classifier adds a model file to the learned directory.<br>* To disable this behavior for diverse source types (such as sourcecode, where there is no good<br>exemplar to make a sourcetype) set LEARN_MODEL = false.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to false.<br><br>maxDist = &lt;integer&gt;<br>* Determines how different a source type model may be from the current file.<br>* The larger the maxDist value, the more forgiving Splunk will be with differences.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, if the value is very small (for example, 10), then files of the specified<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sourcetype should not vary much.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A larger value indicates that files of the given source type can vary quite a bit.<br>* If you're finding that a source type model is matching too broadly, reduce its maxDist<br>&nbsp;&nbsp;value by about 100 and try again. If you're finding that a source type model is being too<br>&nbsp;&nbsp;restrictive, increase its maxDist value by about 100 and try again.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to 300.<br><br># rule:: and delayedrule:: configuration<br><br>MORE_THAN&lt;optional_unique_value&gt;_&lt;number&gt; = &lt;regular expression&gt; (empty)<br>LESS_THAN&lt;optional_unique_value&gt;_&lt;number&gt; = &lt;regular expression&gt; (empty)<br><br>* These settingg apply at input time, when data is first read by Splunk, such<br>&nbsp;&nbsp;as on a forwarder.<br><br>An example:<br><br>[rule::bar_some]<br>sourcetype = source_with_lots_of_bars<br># if more than 80% of lines have "----", but fewer than 70% have "####" declare this a<br># "source_with_lots_of_bars"<br>MORE_THAN_80 = ----<br>LESS_THAN_70 = ####<br><br>A rule can have many MORE_THAN and LESS_THAN patterns, and all are required for the rule<br>to match.<br><br>#******************************************************************************<br># Annotation Processor configured<br>#******************************************************************************<br><br>ANNOTATE_PUNCT = [true|false]<br>* Determines whether to index a special token starting with "punct::"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The "punct::" key contains punctuation in the text of the event.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It can be useful for finding similar events<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If it is not useful for your dataset, or if it ends up taking<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;too much space in your index it is safe to disable it<br>* Defaults to true.<br><br>#******************************************************************************<br># Header Processor configuration<br>#******************************************************************************<br><br>HEADER_MODE = &lt;empty&gt; | always | firstline | none<br>* Determines whether to use the inline ***SPLUNK*** directive to rewrite index-time fields.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If "always", any line with ***SPLUNK*** can be used to rewrite index-time fields.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If "firstline", only the first line can be used to rewrite index-time fields.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If "none", the string ***SPLUNK*** is treated as normal data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If &lt;empty&gt;, scripted inputs take the value "always" and file inputs take the value "none".<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Defaults to &lt;empty&gt;.<br><br>#******************************************************************************<br># Internal settings<br>#******************************************************************************<br><br># NOT YOURS. DO NOT SET.<br><br>_actions = &lt;string&gt;<br>* Internal field used for user-interface control of objects.<br>* Defaults to "new,edit,delete".<br><br>pulldown_type = &lt;bool&gt;<br>* Internal field used for user-interface control of source types.<br>* Defaults to empty.<br><br>given_type = &lt;string&gt;<br>* Internal field used by the CHECK_FOR_HEADER feature to remember the original sourcetype.<br>* This setting applies at input time, when data is first read by Splunk. <br>&nbsp;&nbsp;The setting is used on a Splunk system that has configured inputs acquiring the data.<br>* Default to unset.<br><br>#******************************************************************************<br># Sourcetype Category and Descriptions<br>#******************************************************************************<br><br>description = &lt;string&gt;<br>* Field used to describe the sourcetype. Does not affect indexing behaviour.<br>* Defaults to unset.<br>category = &lt;string&gt;<br>* Field used to classify sourcetypes for organization in the front end. Case sensitive. Does not affect indexing behaviour.<br>* Defaults to unset.<br><br></font></code>
<h3> <a name="propsconf_props.conf.example"><span class="mw-headline" id="props.conf.example">props.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following are example props.conf configurations. Configure properties for your data.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># props.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br>########<br># Line merging settings<br>########<br><br># The following example linemerges source data into multi-line events for apache_error sourcetype.<br><br>[apache_error]<br>SHOULD_LINEMERGE = True<br><br><br><br>########<br># Settings for tuning<br>########<br><br># The following example limits the amount of characters indexed per event from host::small_events.<br><br>[host::small_events]<br>TRUNCATE = 256<br><br># The following example turns off DATETIME_CONFIG (which can speed up indexing) from any path<br># that ends in /mylogs/*.log.<br>#<br># In addition, the default splunk behavior of finding event boundaries<br># via per-event timestamps can't work with NONE, so we disable<br># SHOULD_LINEMERGE, essentially declaring that all events in this file are<br># single-line.<br><br>[source::.../mylogs/*.log]<br>DATETIME_CONFIG = NONE<br>SHOULD_LINEMERGE = false<br><br><br>&nbsp;&nbsp;<br>########<br># Timestamp extraction configuration<br>########<br><br># The following example sets Eastern Time Zone if host matches nyc*.<br><br>[host::nyc*]<br>TZ = US/Eastern<br><br><br># The following example uses a custom datetime.xml that has been created and placed in a custom app<br># directory. This sets all events coming in from hosts starting with dharma to use this custom file.<br><br>[host::dharma*]<br>DATETIME_CONFIG = &lt;etc/apps/custom_time/datetime.xml&gt;<br><br>########<br>## Timezone alias configuration<br>########<br><br># The following example uses a custom alias to disambiguate the Australian meanings of EST/EDT <br><br>TZ_ALIAS = EST=GMT+10:00,EDT=GMT+11:00<br><br># The following example gives a sample case wherein, one timezone field is being replaced by/interpreted as<br># another. <br><br>TZ_ALIAS = EST=AEST,EDT=AEDT <br><br>########<br># Transform configuration<br>########<br><br># The following example creates a search field for host::foo if tied to a stanza in transforms.conf.<br><br>[host::foo]<br>TRANSFORMS-foo=foobar<br><br># The following example creates an extracted field for sourcetype access_combined<br># if tied to a stanza in transforms.conf.<br><br>[eventtype::my_custom_eventtype]<br>REPORT-baz = foobaz<br><br><br># The following stanza extracts an ip address from _raw<br>[my_sourcetype]<br>EXTRACT-extract_ip = (?&lt;ip&gt;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})<br><br># The following example shows how to configure lookup tables<br>[my_lookuptype]<br>LOOKUP-foo = mylookuptable userid AS myuserid OUTPUT username AS myusername<br><br># The following shows how to specify field aliases<br>FIELDALIAS-foo = user AS myuser id AS myid<br><br><br>########<br># Sourcetype configuration<br>########<br><br># The following example sets a sourcetype for the file web_access.log for a unix path.<br><br>[source::.../web_access.log]<br>sourcetype = splunk_web_access <br><br># The following example sets a sourcetype for the Windows file iis6.log. &nbsp;Note: Backslashes within Windows file paths must be escaped.<br><br>[source::...\\iis\\iis6.log]<br>sourcetype = iis_access<br><br># The following example untars syslog events.<br><br>[syslog]<br>invalid_cause = archive<br>unarchive_cmd = gzip -cd -<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br># The following example learns a custom sourcetype and limits the range between different examples<br># with a smaller than default maxDist.<br><br>[custom_sourcetype]<br>LEARN_MODEL = true<br>maxDist = 30<br><br><br># rule:: and delayedrule:: configuration<br># The following examples create sourcetype rules for custom sourcetypes with regex.<br><br><br>[rule::bar_some]<br>sourcetype = source_with_lots_of_bars<br>MORE_THAN_80 = ----<br><br><br>[delayedrule::baz_some]<br>sourcetype = my_sourcetype<br>LESS_THAN_70 = ####<br><br><br>########&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br># File configuration<br>########<br><br># Binary file configuration<br># The following example indexes binary files from the sourcetype "imported_records".<br><br>[imported_records]<br>NO_BINARY_CHECK = true <br>&nbsp;&nbsp;&nbsp;&nbsp;<br><br># File checksum configuration<br># The following example checks the entirety of every file in the web_access dir rather than <br># skipping files that appear to be the same.<br><br>[source::.../web_access/*]<br>CHECK_METHOD = entire_md5<br><br></font></code>

<a name="pubsubconf"></a><h2> <a name="pubsubconf_pubsub.conf"><span class="mw-headline" id="pubsub.conf">pubsub.conf</span></a></h2>
<p>The following are the spec and example files for pubsub.conf.
</p>
<h3> <a name="pubsubconf_pubsub.conf.spec"><span class="mw-headline" id="pubsub.conf.spec">pubsub.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values for configuring a client of the PubSub system (broker).<br>#<br># To set custom configurations, place a pubsub.conf in $SPLUNK_HOME/etc/system/local/. <br># For examples, see pubsub.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#******************************************************************<br># Configure the physical location where deploymentServer is running.<br># This configuration is used by the clients of the pubsub system.<br>#******************************************************************<br>[pubsub-server:deploymentServer]<br><br>disabled = &lt;false or true&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* defaults to 'false'<br><br>targetUri = &lt;IP:Port&gt;|&lt;hostname:Port&gt;|direct<br>&nbsp;&nbsp;&nbsp;&nbsp;* specify either the url of a remote server in case the broker is remote, or just the keyword "direct" when broker is in-process.<br>&nbsp;&nbsp;&nbsp;&nbsp;* It is usually a good idea to co-locate the broker and the Deployment Server on the same Splunk. In such a configuration, all <br>&nbsp;&nbsp;&nbsp;&nbsp;* deployment clients would have targetUri set to deploymentServer:port.<br><br>#******************************************************************<br># The following section is only relevant to Splunk developers.<br>#******************************************************************<br><br># This "direct" configuration is always available, and cannot be overridden.<br>[pubsub-server:direct]<br>disabled = false<br>targetUri = direct<br><br>[pubsub-server:&lt;logicalName&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* It is possible for any Splunk to be a broker. If you have multiple brokers, assign a logicalName that is used by the clients to refer to it.<br><br>disabled = &lt;false or true&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* defaults to 'false'<br><br>targetUri = &lt;IP:Port&gt;|&lt;hostname:Port&gt;|direct<br>&nbsp;&nbsp;&nbsp;&nbsp;* The Uri of a Splunk that is being used as a broker.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The keyword "direct" implies that the client is running on the same Splunk instance as the broker.<br><br><br></font></code>
<h3> <a name="pubsubconf_pubsub.conf.example"><span class="mw-headline" id="pubsub.conf.example">pubsub.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br><br>[pubsub-server:deploymentServer]<br>disabled=false<br>targetUri=somehost:8089<br><br>[pubsub-server:internalbroker]<br>disabled=false<br>targetUri=direct<br><br><br></font></code>

<a name="restmapconf"></a><h2> <a name="restmapconf_restmap.conf"><span class="mw-headline" id="restmap.conf">restmap.conf</span></a></h2>
<p>The following are the spec and example files for restmap.conf.
</p>
<h3> <a name="restmapconf_restmap.conf.spec"><span class="mw-headline" id="restmap.conf.spec">restmap.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute and value pairs for creating new Representational State Transfer<br># (REST) endpoints.<br>#<br># There is a restmap.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a restmap.conf in $SPLUNK_HOME/etc/system/local/. For help, see<br># restmap.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># NOTE: You must register every REST endpoint via this file to make it available.<br><br>###########################<br># Global stanza<br><br>[global]<br>* This stanza sets global configurations for all REST endpoints. <br>* Follow this stanza name with any number of the following attribute/value pairs.<br><br>allowGetAuth=[true|false]<br>* Allow user/password to be passed as a GET parameter to endpoint services/auth/login. &nbsp;<br>* Setting this to true, while convenient, may result in user/password getting logged as cleartext<br>&nbsp;&nbsp;in Splunk's logs *and* any proxy servers in between. <br>* Defaults to false. &nbsp;<br><br>pythonHandlerPath=&lt;path&gt;<br>* Path to 'main' python script handler.<br>* Used by the script handler to determine where the actual 'main' script is located. &nbsp;<br>* Typically, you should not need to change this.<br>* Defaults to $SPLUNK_HOME/bin/rest_handler.py.<br><br><br>###########################<br># Applicable to all REST stanzas<br># Stanza definitions below may supply additional information for these.<br>#<br><br>[&lt;rest endpoint name&gt;:&lt;endpoint description string&gt;]<br>match=&lt;path&gt;<br>* Specify the URI that calls the handler. <br>* For example if match=/foo, then https://$SERVER:$PORT/services/foo calls this handler. &nbsp;<br>* NOTE: You must start your path with a /.<br><br>requireAuthentication=[true|false]<br>* This optional attribute determines if this endpoint requires authentication. &nbsp;<br>* Defaults to 'true'.<br><br>authKeyStanza=&lt;stanza&gt;<br>* This optional attribute determines the location of the pass4SymmKey in the server.conf to be used for endpoint authentication. &nbsp;<br>* Defaults to 'general' stanza.<br>* Only applicable if the requireAuthentication is set true.<br><br>capability=&lt;capabilityName&gt;<br>capability.&lt;post|delete|get|put&gt;=&lt;capabilityName&gt;<br>* Depending on the HTTP method, check capabilities on the authenticated session user.<br>* If you use 'capability.post|delete|get|put,' then the associated method is checked <br>against the authenticated user's role. <br>* If you just use 'capability,' then all calls get checked against this capability (regardless <br>of the HTTP method).<br><br>acceptFrom=&lt;network_acl&gt; ...<br>* Lists a set of networks or addresses to allow this endpoint to be accessed from.<br>* This shouldn't be confused with the setting of the same name in the<br>&nbsp;&nbsp;[httpServer] stanza of server.conf which controls whether a host can<br>&nbsp;&nbsp;make HTTP requests at all<br>* Each rule can be in the following forms:<br>* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>* &nbsp;&nbsp;4. A single '*' which matches anything<br>* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;except the 10.1.*.* network.<br>* Defaults to "*" (accept from anywhere)<br><br>includeInAccessLog=[true|false]<br>* If this is set to false, requests to this endpoint will not appear in splunkd_access.log<br>* Defaults to 'true'.<br><br>###########################<br># Per-endpoint stanza <br># Specify a handler and other handler-specific settings. &nbsp;<br># The handler is responsible for implementing arbitrary namespace underneath each REST endpoint.<br><br>[script:&lt;uniqueName&gt;]<br>* NOTE: The uniqueName must be different for each handler.<br>* Call the specified handler when executing this endpoint.<br>* The following attribute/value pairs support the script handler.<br><br>scripttype=python<br>* Tell the system what type of script to execute when using this endpoint.<br>* Defaults to python.<br>* Python is currently the only option for scripttype.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>handler=&lt;SCRIPT&gt;.&lt;CLASSNAME&gt;<br>* The name and class name of the file to execute. &nbsp;<br>* The file *must* live in an application's bin subdirectory.<br>* For example, $SPLUNK_HOME/etc/apps/&lt;APPNAME&gt;/bin/TestHandler.py has a class called<br>&nbsp;&nbsp;MyHandler (which, in the case of python must be derived from a base class called <br>&nbsp;&nbsp;'splunk.rest.BaseRestHandler'). The tag/value pair for this is: "handler=TestHandler.MyHandler".<br><br>xsl=&lt;path to XSL transform file&gt;<br>* Optional.<br>* Perform an optional XSL transform on data returned from the handler.<br>* Only use this if the data is XML.<br><br>script=&lt;path to a script executable&gt;<br>* Optional.<br>* Execute a script which is *not* derived from 'splunk.rest.BaseRestHandler'.<br>* Put the path to that script here. &nbsp;<br>* This is rarely used.<br>* Do not use this unless you know what you are doing.<br><br>output_modes=&lt;csv list&gt;<br>* Specifies which output formats can be requested from this endpoint.<br>* Valid values are: json, xml.<br>* Defaults to xml.<br><br><br>#############################<br># 'admin'<br># The built-in handler for the Extensible Administration Interface. <br># Exposes the listed EAI handlers at the given URL.<br>#<br><br>[admin:&lt;uniqueName&gt;]<br><br>match=&lt;partial URL&gt;<br>* URL which, when accessed, will display the handlers listed below.<br><br>members=&lt;csv list&gt;<br>* List of handlers to expose at this URL.<br>* See https://localhost:8089/services/admin for a list of all possible handlers.<br><br>#############################<br># 'admin_external'<br># Register Python handlers for the Extensible Administration Interface.<br># Handler will be exposed via its "uniqueName".<br>#<br><br>[admin_external:&lt;uniqueName&gt;]<br><br>handlertype=&lt;script type&gt;<br>* Currently only the value 'python' is valid.<br><br>handlerfile=&lt;unique filename&gt;<br>* Script to execute.<br>* For bin/myAwesomeAppHandler.py, specify only myAwesomeAppHandler.py.<br><br>handleractions=&lt;comma separated list&gt;<br>* List of EAI actions supported by this handler.<br>* Valid values are: create, edit, list, delete, _reload.<br><br>#########################<br># Validation stanzas<br># Add stanzas using the following definition to add arg validation to<br># the appropriate EAI handlers.<br><br>[validation:&lt;handler-name&gt;]<br><br>&lt;field&gt; = &nbsp;&lt;validation-rule&gt; <br><br>* &lt;field&gt; is the name of the field whose value would be validated when an object is being saved. <br>* &lt;validation-rule&gt; is an eval expression using the validate() function to evaluate arg correctness <br>&nbsp;&nbsp;and return an error message. If you use a boolean returning function, a generic message is displayed. <br>* &lt;handler-name&gt; is the name of the REST endpoint which this stanza applies to handler-name is what is<br>&nbsp;&nbsp;used to access the handler via /servicesNS/&lt;user&gt;/&lt;app/admin/&lt;handler-name&gt;.<br>* For example:<br>* action.email.sendresult = validate( isbool('action.email.sendresults'), "'action.email.sendresults' must<br>&nbsp;&nbsp;be a boolean value").<br>* NOTE: use ' or $ to enclose field names that contain non alphanumeric characters.<br><br>#############################<br># 'eai'<br># Settings to alter the behavior of EAI handlers in various ways.<br># These should not need to be edited by users.<br>#<br><br>[eai:&lt;EAI handler name&gt;]<br><br>showInDirSvc = [true|false]<br>* Whether configurations managed by this handler should be enumerated via the<br>&nbsp;&nbsp;directory service, used by SplunkWeb's "All Configurations" management page.<br>&nbsp;&nbsp;Defaults to false.<br><br>desc = &lt;human readable string&gt;<br>* Allows for renaming the configuration type of these objects when enumerated<br>&nbsp;&nbsp;via the directory service.<br><br><br>#############################<br># Miscellaneous<br># The un-described parameters in these stanzas all operate according to the<br># descriptions listed under "script:", above.<br># These should not need to be edited by users - they are here only to quiet<br># down the configuration checker.<br>#<br><br>[input:...]<br>dynamic = [true|false]<br>* If set to true, listen on the socket for data.<br>* If false, data is contained within the request body.<br>* Defaults to false.<br><br>[peerupload:...]<br>path = &lt;directory path&gt;<br>* Path to search through to find configuration bundles from search peers.<br>untar = [true|false]<br>* Whether or not a file should be untarred once the transfer is complete.<br><br></font></code>
<h3> <a name="restmapconf_restmap.conf.example"><span class="mw-headline" id="restmap.conf.example">restmap.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example REST endpoint configurations.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># restmap.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># The following are default REST configurations. &nbsp;To create your own endpoints, modify <br># the values by following the spec outlined in restmap.conf.spec.<br><br><br># /////////////////////////////////////////////////////////////////////////////<br># &nbsp;global settings<br># /////////////////////////////////////////////////////////////////////////////<br><br>[global]<br><br># indicates if auths are allowed via GET params<br>allowGetAuth=false<br><br>#The default handler (assuming that we have PYTHONPATH set)<br>pythonHandlerPath=$SPLUNK_HOME/bin/rest_handler.py<br><br><br><br># /////////////////////////////////////////////////////////////////////////////<br># &nbsp;internal C++ handlers<br># NOTE: These are internal Splunk-created endpoints. &nbsp;3rd party developers can only use script or<br># search can be used as handlers. &nbsp;(Please see restmap.conf.spec for help with configurations.)<br># /////////////////////////////////////////////////////////////////////////////<br><br>[SBA:sba]<br>match=/properties<br>capability=get_property_map<br><br>[asyncsearch:asyncsearch]<br>match=/search<br>capability=search<br><br><br></font></code>

<a name="savedsearchesconf"></a><h2> <a name="savedsearchesconf_savedsearches.conf"><span class="mw-headline" id="savedsearches.conf">savedsearches.conf</span></a></h2>
<p>The following are the spec and example files for savedsearches.conf.
</p>
<h3> <a name="savedsearchesconf_savedsearches.conf.spec"><span class="mw-headline" id="savedsearches.conf.spec">savedsearches.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute/value pairs for saved search entries in savedsearches.conf.<br># You can configure saved searches by creating your own savedsearches.conf.<br>#<br># There is a default savedsearches.conf in $SPLUNK_HOME/etc/system/default. To set custom<br># configurations, place a savedsearches.conf in $SPLUNK_HOME/etc/system/local/.<br># For examples, see savedsearches.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#*******<br># The possible attribute/value pairs for savedsearches.conf are:<br>#*******<br><br>[&lt;stanza name&gt;]<br>* Create a unique stanza name for each saved search.<br>* Follow the stanza name with any number of the following attribute/value pairs.<br>* If you do not specify an attribute, Splunk uses the default.<br><br>disabled = [0|1]<br>* Disable your search by setting to 1.<br>* If set to 1, this saved search is not visible in Splunk Web.<br>* Defaults to 0.<br><br>search = &lt;string&gt;<br>* Actual search terms of the saved search.<br>* For example, search = index::sampledata http NOT 500.<br>* Your search can include macro searches for substitution.<br>* To learn more about creating a macro search, search the documentation for "macro search."<br>* Multi-line search strings currently have some limitations. &nbsp;For example use<br>&nbsp;&nbsp;with the search command '|savedseach' does not currently work with multi-line<br>&nbsp;&nbsp;search strings.<br>* Defaults to empty string.<br><br>dispatchAs = [user|owner]<br>* When the saved search is dispatched via the "saved/searches/{name}/dispatch" endpoint, <br>&nbsp;&nbsp;this setting controls, what user that search is dispatched as.<br>* This setting is only meaningful for shared saved searches.<br>* When dispatched as user it will be executed as if the requesting user owned the search.<br>* When dispatched as owner it will be executed as if the owner of the search dispatched <br>&nbsp;&nbsp;it no matter what user requested it.<br>* Defaults to owner<br><br><br>#*******<br># Scheduling options<br>#*******<br><br>enableSched = [0|1]<br>* Set this to 1 to run your search on a schedule.<br>* Defaults to 0.<br><br>cron_schedule = &lt;cron string&gt;<br>* The cron schedule used to execute this search.<br>* For example: */5 * * * * &nbsp;causes the search to execute every 5 minutes.<br>* Cron lets you use standard cron notation to define your scheduled search interval.<br>&nbsp;&nbsp;In particular, cron can accept this type of notation: 00,20,40 * * * *, which runs the search<br>&nbsp;&nbsp;every hour at hh:00, hh:20, hh:40. Along the same lines, a cron of 03,23,43 * * * * runs the<br>&nbsp;&nbsp;search every hour at hh:03, hh:23, hh:43.<br>* Splunk recommends that you schedule your searches so that they are staggered over time. This<br>&nbsp;&nbsp;reduces system load. Running all of them every 20 minutes (*/20) means they would all launch<br>&nbsp;&nbsp;at hh:00 (20, 40) and might slow your system every 20 minutes.<br>* Splunk's cron implementation does not currently support names of months/days.<br>* Defaults to empty string.<br><br>schedule = &lt;cron-style string&gt;<br>* This field is DEPRECATED as of 4.0.<br>* For more information, see the pre-4.0 spec file.<br>* Use cron_schedule to define your scheduled search interval.<br><br><br>max_concurrent = &lt;int&gt;<br>* The maximum number of concurrent instances of this search the scheduler is allowed to run.<br>* Defaults to 1.<br><br>realtime_schedule = [0|1]<br>* Controls the way the scheduler computes the next execution time of a scheduled search.<br>* If this value is set to 1, the scheduler bases its determination of the next scheduled search<br>&nbsp;&nbsp;execution time on the current time.<br>* If this value is set to 0, the scheduler bases its determination of the next scheduled search<br>&nbsp;&nbsp;on the last search execution time. This is called continuous scheduling.<br>* &nbsp;&nbsp;&nbsp;If set to 1, the scheduler might skip some execution periods to make sure that the scheduler<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is executing the searches running over the most recent time range.<br>* &nbsp;&nbsp;&nbsp;If set to 0, the scheduler never skips scheduled execution periods. However, the execution<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of the saved search might fall behind depending on the scheduler's load. Use continuous<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduling whenever you enable the summary index option.<br>* The scheduler tries to execute searches that have realtime_schedule set to 1 before it<br>&nbsp;&nbsp;executes searches that have continuous scheduling (realtime_schedule = 0).<br>* Defaults to 1<br><br><br>#*******<br># Notification options<br>#*******<br><br>counttype = number of events | number of hosts | number of sources | always<br>* Set the type of count for alerting.<br>* Used with relation and quantity (below).<br>* NOTE: If you specify "always," do not set relation or quantity (below).<br>* Defaults to always.<br><br>relation = greater than | less than | equal to | not equal to | drops by | rises by<br>* Specifies how to compare against counttype.<br>* Defaults to empty string.<br><br>quantity = &lt;integer&gt;<br>* Specifies a value for the counttype and relation, to determine the condition under which an<br>&nbsp;&nbsp;alert is triggered by a saved search.<br>* You can think of it as a sentence constructed like this: &lt;counttype&gt; &lt;relation&gt; &lt;quantity&gt;.<br>* For example, "number of events [is] greater than 10" sends an alert when the count of events<br>&nbsp;&nbsp;is larger than by 10.<br>* For example, "number of events drops by 10%" sends an alert when the count of events drops by<br>&nbsp;&nbsp;10%.<br>* Defaults to an empty string.<br><br>alert_condition = &lt;search string&gt;<br>* Contains a conditional search that is evaluated against the results of the saved search.<br>&nbsp;&nbsp;Alerts are triggered if the specified search yields a non-empty search result list.<br>* NOTE: If you specify an alert_condition, do not set counttype, relation, or quantity.<br>* Defaults to an empty string.<br><br><br>#*******<br># generic action settings.<br># For a comprehensive list of actions and their arguments, refer to alert_actions.conf.<br>#*******<br><br>action.&lt;action_name&gt; = 0 | 1<br>* Indicates whether the action is enabled or disabled for a particular saved search.<br>* The action_name can be: email | populate_lookup | script | summary_index<br>* For more about your defined alert actions see alert_actions.conf.<br>* Defaults to an empty string.<br><br>action.&lt;action_name&gt;.&lt;parameter&gt; = &lt;value&gt;<br>* Overrides an action's parameter (defined in alert_actions.conf) with a new &lt;value&gt; for this<br>&nbsp;&nbsp;saved search only.<br>* Defaults to an empty string.<br><br><br>#******<br># Settings for email action<br>#******<br><br>action.email = 0 | 1<br>* Enables or disables the email action.<br>* Defaults to 0.<br><br>action.email.to = &lt;email list&gt;<br>* REQUIRED. This setting is not defined in alert_actions.conf.<br>* Set a comma-delimited list of recipient email addresses.<br>* Defaults to empty string.<br><br>action.email.from = &lt;email address&gt;<br>* Set an email address to use as the sender's address.<br>* Defaults to splunk@&lt;LOCALHOST&gt; (or whatever is set in alert_actions.conf).<br><br>action.email.subject = &lt;string&gt;<br>* Set the subject of the email delivered to recipients.<br>* Defaults to SplunkAlert-&lt;savedsearchname&gt; (or whatever is set in alert_actions.conf).<br><br>action.email.mailserver = &lt;string&gt;<br>* Set the address of the MTA server to be used to send the emails.<br>* Defaults to &lt;LOCALHOST&gt; (or whatever is set in alert_actions.conf).<br><br>action.email.maxresults = &lt;integer&gt;<br>* Set the maximum number of results to be emailed.<br>* Any alert-level results threshold greater than this number will be capped at<br>&nbsp;&nbsp;this level.<br>* This value affects all methods of result inclusion by email alert: inline, CSV<br>&nbsp;&nbsp;and PDF.<br>* Note that this setting is affected globally by "maxresults" in the [email]<br>&nbsp;&nbsp;stanza of alert_actions.conf.<br>* Defaults to 10000<br><br><br>#******<br># Settings for script action<br>#******<br><br>action.script = 0 | 1<br>* Enables or disables the script action.<br>* 1 to enable, 0 to disable.<br>* Defaults to 0<br><br>action.script.filename = &lt;script filename&gt;<br>* The filename, with no path, of the shell script to execute. <br>* The script should be located in: $SPLUNK_HOME/bin/scripts/<br>* For system shell scripts on Unix, or .bat or .cmd on windows, there<br>&nbsp;&nbsp;are no further requirements.<br>* For other types of scripts, the first line should begin with a #!<br>&nbsp;&nbsp;marker, followed by a path to the interpreter that will run the<br>&nbsp;&nbsp;script.<br>&nbsp;&nbsp;* Example: #!C:\Python27\python.exe<br>* Defaults to empty string.<br><br><br>#*******<br># Settings for summary index action<br>#*******<br><br>action.summary_index = 0 | 1<br>* Enables or disables the summary index action.<br>* Defaults to 0.<br><br>action.summary_index._name = &lt;index&gt;<br>* Specifies the name of the summary index where the results of the scheduled search are saved.<br>* Defaults to summary.<br><br>action.summary_index.inline = &lt;bool&gt;<br>* Determines whether to execute the summary indexing action as part of the scheduled search.<br>* NOTE: This option is considered only if the summary index action is enabled and is always<br>&nbsp;&nbsp;executed (in other words, if counttype = always).<br>* Defaults to true.<br><br>action.summary_index.&lt;field&gt; = &lt;string&gt;<br>* Specifies a field/value pair to add to every event that gets summary indexed by this search.<br>* You can define multiple field/value pairs for a single summary index search.<br><br><br>#*******<br># Settings for lookup table population parameters<br>#*******<br><br>action.populate_lookup = 0 | 1<br>* Enables or disables the lookup population action.<br>* Defaults to 0.<br><br>action.populate_lookup.dest = &lt;string&gt;<br>* Can be one of the following two options:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A lookup name from transforms.conf.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A path to a lookup .csv file that Splunk should copy the search results to, relative to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$SPLUNK_HOME.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: This path must point to a .csv file in either of the following directories:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* etc/system/lookups/<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* etc/apps/&lt;app-name&gt;/lookups<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: the destination directories of the above files must already exist<br>* Defaults to empty string.<br><br>run_on_startup = true | false<br>* Toggles whether this search runs when Splunk starts or any edit that changes search related args happen<br>* (which includes: search and dispatch.* args).<br>* If set to true the search is ran as soon as possible during startup or after edit<br>* otherwise the search is ran at the next scheduled time.<br>* We recommend that you set run_on_startup to true for scheduled searches that populate lookup<br>* tables or generate artifacts used by dashboards.<br>* Defaults to false.<br><br><br>#*******<br># dispatch search options<br>#*******<br><br>dispatch.ttl = &lt;integer&gt;[p]<br>* Indicates the time to live (in seconds) for the artifacts of the scheduled search, if no<br>&nbsp;&nbsp;actions are triggered.<br>* If the integer is followed by the letter 'p' Splunk interprets the ttl as a multiple of the<br>&nbsp;&nbsp;scheduled search's execution period (e.g. if the search is scheduled to run hourly and ttl is set to 2p<br>&nbsp;&nbsp;the ttl of the artifacts will be set to 2 hours).<br>* If an action is triggered Splunk changes the ttl to that action's ttl. If multiple actions are<br>&nbsp;&nbsp;triggered, Splunk applies the largest action ttl to the artifacts. To set the action's ttl, refer<br>&nbsp;&nbsp;to alert_actions.conf.spec.<br>* For more info on search's ttl please see limits.conf.spec [search] ttl<br>* Defaults to 2p (that is, 2 x the period of the scheduled search).<br><br>dispatch.buckets &nbsp;= &lt;integer&gt;<br>* The maximum number of timeline buckets.<br>* Defaults to 0.<br><br>dispatch.max_count = &lt;integer&gt;<br>* The maximum number of results before finalizing the search.<br>* Defaults to 500000.<br><br>dispatch.max_time = &lt;integer&gt;<br>* Indicates the maximum amount of time (in seconds) before finalizing the search.<br>* Defaults to 0.<br><br>dispatch.lookups = 1| 0<br>* Enables or disables lookups for this search.<br>* Defaults to 1.<br><br>dispatch.earliest_time = &lt;time-str&gt;<br>* Specifies the earliest time for this search. Can be a relative or absolute time.<br>* If this value is an absolute time, use the dispatch.time_format to format the value.<br>* Defaults to empty string.<br><br>dispatch.latest_time = &lt;time-str&gt;<br>* Specifies the latest time for this saved search. Can be a relative or absolute time.<br>* If this value is an absolute time, use the dispatch.time_format to format the value.<br>* Defaults to empty string.<br><br>dispatch.index_earliest= &lt;time-str&gt;<br>* Specifies the earliest index time for this search. Can be a relative or absolute time.<br>* If this value is an absolute time, use the dispatch.time_format to format the value.<br>* Defaults to empty string.<br><br>dispatch.index_latest= &lt;time-str&gt;<br>* Specifies the latest index time for this saved search. Can be a relative or absolute time.<br>* If this value is an absolute time, use the dispatch.time_format to format the value.<br>* Defaults to empty string.<br><br>dispatch.time_format = &lt;time format str&gt;<br>* Defines the time format that Splunk uses to specify the earliest and latest time.<br>* Defaults to&nbsp;%FT%T.%Q%:z<br><br>dispatch.spawn_process = 1 | 0<br>* Specifies whether Splunk spawns a new search process when this saved search is executed.<br>* Default is 1.<br><br>dispatch.auto_cancel = &lt;int&gt;<br>* If specified, the job automatically cancels after this many seconds of inactivity. (0 means never auto-cancel) <br>* Default is 0.<br><br>dispatch.auto_pause = &lt;int&gt;<br>* If specified, the search job pauses after this many seconds of inactivity. (0 means never auto-pause.)<br>* To restart a paused search job, specify unpause as an action to POST search/jobs/{search_id}/control.<br>* auto_pause only goes into effect once. Unpausing after auto_pause does not put auto_pause into effect again. <br>* Default is 0.<br><br>dispatch.reduce_freq = &lt;int&gt;<br>* Specifies how frequently Splunk should run the MapReduce reduce phase on accumulated map values.<br>* Defaults to 10.<br><br>dispatch.rt_backfill = &lt;bool&gt;<br>* Specifies whether to do real-time window backfilling for scheduled real time searches<br>* Defaults to false.<br><br>dispatch.indexedRealtime = &lt;bool&gt;<br>* Specifies whether to use indexed-realtime mode when doing realtime searches.<br>* Defaults to false<br><br>restart_on_searchpeer_add = 1 | 0<br>* Specifies whether to restart a real-time search managed by the scheduler when a search peer<br>&nbsp;&nbsp;becomes available for this saved search.<br>* NOTE: The peer can be a newly added peer or a peer that has been down and has become available.<br>* Defaults to 1.<br><br><br>#*******<br># auto summarization options<br>#*******<br>auto_summarize &nbsp;= &lt;bool&gt;<br>* Whether the scheduler should ensure that the data for this search is automatically summarized<br>* Defaults to false.<br><br>auto_summarize.command = &lt;string&gt;<br>* A search template to be used to construct the auto summarization for this search.<br>* DO NOT change unles you know what you're doing<br><br>auto_summarize.timespan = &lt;time-specifier&gt; (, &lt;time-specifier&gt;)*<br>* Comma delimite list of time ranges that each summarized chunk should span. This comprises the list<br>* of available granularity levels for which summaries would be available. For example a timechart over<br>* the last month whose granuality is at the day level should set this to 1d. If you're going to need the<br>* same data summarized at the hour level because you need to have weekly charts then use: 1h,1d<br><br>auto_summarize.cron_schedule = &lt;cron-string&gt;<br>* Cron schedule to be used to probe/generate the summaries for this search<br><br>auto_summarize.dispatch.&lt;arg-name&gt; = &lt;string&gt;<br>* Any dispatch.* options that need to be overridden when running the summary search.<br><br>auto_summarize.suspend_period &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= &lt;time-specifier&gt;<br>* Amount of time to suspend summarization of this search if the summarization is deemed unhelpful<br>* Defaults to 24h<br><br>auto_summarize.max_summary_size &nbsp;&nbsp;&nbsp;&nbsp;= &lt;unsigned int&gt;<br>* The minimum summary size when to start testing it's helpfulness<br>* Defaults to 52428800 (5MB)<br><br>auto_summarize.max_summary_ratio &nbsp;&nbsp;&nbsp;= &lt;positive float&gt;<br>* The maximum ratio of summary_size/bucket_size when to stop summarization and deem it unhelpful for a bucket<br>* NOTE: the test is only performed if the summary size is larger than auto_summarize.max_summary_size<br>* Defaults to: 0.1<br><br>auto_summarize.max_disabled_buckets = &lt;unsigned int&gt;<br>* The maximum number of buckets with the suspended summarization before the summarization search is completely<br>* stopped and the summarization of the search is suspended for auto_summarize.suspend_period<br>* Defaults to: 2<br><br>auto_summarize.max_time = &lt;unsigned in&gt;<br>* The maximum amount of time that the summary search is allowed to run. Note that this is an approximate time<br>* and the summarize search will be stopped at clean bucket boundaries.<br>* Defaults to: 3600<br><br>auto_summarize.hash = &lt;string&gt;<br>auto_summarize.normalized_hash = &lt;string&gt;<br>* These are auto generated settings.<br><br>#*******<br># alert suppression/severity/expiration/tracking/viewing settings<br>#*******<br><br>alert.suppress = 0 | 1<br>* Specifies whether alert suppression is enabled for this scheduled search.<br>* Defaults to 0.<br><br>alert.suppress.period = &lt;time-specifier&gt;<br>* Sets the suppression period. Use [number][time-unit] to specify a time.<br>* For example: 60 = 60 seconds, 1m = 1 minute, 1h = 60 minutes = 1 hour etc<br>* Honored if and only if alert.suppress = 1<br>* Defaults to empty string.<br><br>alert.suppress.fields = &lt;comma-delimited-field-list&gt;<br>* List of fields to use when suppressing per-result alerts. This field *must* be specified<br>* if the digest mode is disabled and suppression is enabled.<br>* Defaults to empty string.<br><br>alert.severity = &lt;int&gt;<br>* Sets the alert severity level.<br>* Valid values are: 1-debug, 2-info, 3-warn, 4-error, 5-severe, 6-fatal<br>* Defaults to 3.<br><br>alert.expires = &lt;time-specifier&gt;<br>* Sets the period of time to show the alert in the dashboard. Use [number][time-unit] to specify a time.<br>* For example: 60 = 60 seconds, 1m = 1 minute, 1h = 60 minutes = 1 hour etc<br>* Defaults to 24h.<br><br>alert.digest_mode = true | false<br>* Specifies whether Splunk applies the alert actions to the entire result set or on each<br>* individual result.<br>* Defaults to true.<br><br>alert.track = true | false | auto<br>* Specifies whether to track the actions triggered by this scheduled search.<br>* auto &nbsp;- determine whether to track or not based on the tracking setting of each action,<br>* do not track scheduled searches that always trigger actions.<br>* true &nbsp;- force alert tracking.<br>* false - disable alert tracking for this search.<br>* Defaults to auto.<br><br>alert.display_view = &lt;string&gt;<br>* Name of the UI view where the emailed link for per result alerts should point to.<br>* If not specified, the value of request.ui_dispatch_app will be used, if that <br>* is missing then "search" will be used<br>* Defaults to empty string<br><br>#*******<br># UI-specific settings<br>#*******<br><br>displayview =&lt;string&gt;<br>* Defines the default UI view name (not label) in which to load the results.<br>* Accessibility is subject to the user having sufficient permissions.<br>* Defaults to empty string.<br><br>vsid = &lt;string&gt;<br>* Defines the viewstate id associated with the UI view listed in 'displayview'.<br>* Must match up to a stanza in viewstates.conf.<br>&nbsp;* Defaults to empty string.<br><br>is_visible = true | false<br>* Specifies whether this saved search should be listed in the visible saved search list.<br>* Defaults to true.<br><br>description = &lt;string&gt;<br>* Human-readable description of this saved search.<br>* Defaults to empty string.<br><br>request.ui_dispatch_app &nbsp;= &lt;string&gt;<br>* Specifies a field used by Splunk UI to denote the app this search should be dispatched in.<br>* Defaults to empty string.<br><br>request.ui_dispatch_view = &lt;string&gt;<br>* Specifies a field used by Splunk UI to denote the view this search should be displayed in.<br>* Defaults to empty string.<br><br>#******<br># Display Formatting Options<br>#******<br><br># General options<br>display.general.enablePreview = 0 | 1<br>display.general.type = [events|statistics|visualizations]<br>display.general.timeRangePicker.show = 0 | 1<br>display.general.migratedFromViewState = 0 | 1<br><br># Event options<br>display.events.fields = [&lt;string&gt;(, &lt;string&gt;)*]<br>display.events.type = [raw|list|table]<br>display.events.rowNumbers = 0 | 1<br>display.events.maxLines = &lt;int&gt;<br>display.events.raw.drilldown = [inner|outer|full|none]<br>display.events.list.drilldown = [inner|outer|full|none]<br>display.events.list.wrap = 0 | 1<br>display.events.table.drilldown = 0 | 1<br>display.events.table.wrap = 0 | 1<br><br># Statistics options<br>display.statistics.rowNumbers = 0 | 1<br>display.statistics.wrap = 0 | 1<br>display.statistics.overlay = [none|heatmap|highlow]<br>display.statistics.drilldown = [row|cell|none]<br><br># Visualization options<br>display.visualizations.show = 0 | 1<br>display.visualizations.type = [charting|singlevalue|mapping]<br>display.visualizations.chartHeight = &lt;int&gt;<br>display.visualizations.charting.chart = [line|area|column|bar|pie|scatter|bubble|radialGauge|fillerGauge|markerGauge]<br>display.visualizations.charting.chart.stackMode = [default|stacked|stacked100]<br>display.visualizations.charting.chart.nullValueMode = [gaps|zero|connect]<br>display.visualizations.charting.chart.overlayFields = &lt;string&gt;<br>display.visualizations.charting.drilldown = [all|none]<br>display.visualizations.charting.chart.style = [minimal|shiny]<br>display.visualizations.charting.layout.splitSeries = 0 | 1<br>display.visualizations.charting.legend.placement = [right|bottom|top|left|none]<br>display.visualizations.charting.legend.labelStyle.overflowMode = [ellipsisEnd|ellipsisMiddle|ellipsisStart]<br>display.visualizations.charting.axisTitleX.text = &lt;string&gt;<br>display.visualizations.charting.axisTitleY.text = &lt;string&gt;<br>display.visualizations.charting.axisTitleY2.text = &lt;string&gt;<br>display.visualizations.charting.axisTitleX.visibility = [visible|collapsed]<br>display.visualizations.charting.axisTitleY.visibility = [visible|collapsed]<br>display.visualizations.charting.axisTitleY2.visibility = [visible|collapsed]<br>display.visualizations.charting.axisX.scale = linear|log<br>display.visualizations.charting.axisY.scale = linear|log<br>display.visualizations.charting.axisY2.scale = linear|log|inherit<br>display.visualizations.charting.axisLabelsX.majorLabelStyle.overflowMode = [ellipsisMiddle|ellipsisNone]<br>display.visualizations.charting.axisLabelsX.majorLabelStyle.rotation = [-90|-45|0|45|90]<br>display.visualizations.charting.axisLabelsX.majorUnit = &lt;float&gt; | auto<br>display.visualizations.charting.axisLabelsY.majorUnit = &lt;float&gt; | auto<br>display.visualizations.charting.axisLabelsY2.majorUnit = &lt;float&gt; | auto<br>display.visualizations.charting.axisX.minimumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisY.minimumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisY2.minimumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisX.maximumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisY.maximumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisY2.maximumNumber = &lt;float&gt; | auto<br>display.visualizations.charting.axisY2.enabled = 0 | 1<br>display.visualizations.charting.chart.sliceCollapsingThreshold = &lt;float&gt;<br>display.visualizations.charting.gaugeColors = [&lt;hex&gt;(, &lt;hex&gt;)*]<br>display.visualizations.charting.chart.rangeValues = [&lt;string&gt;(, &lt;string&gt;)*]<br>display.visualizations.charting.chart.bubbleMaximumSize = &lt;int&gt;<br>display.visualizations.charting.chart.bubbleMinimumSize = &lt;int&gt;<br>display.visualizations.charting.chart.bubbleSizeBy = [area|diameter]<br>display.visualizations.singlevalue.beforeLabel = &lt;string&gt;<br>display.visualizations.singlevalue.afterLabel = &lt;string&gt;<br>display.visualizations.singlevalue.underLabel = &lt;string&gt;<br>display.visualizations.mapHeight = &lt;int&gt;<br>display.visualizations.mapping.drilldown = [all|none]<br>display.visualizations.mapping.map.center = (&lt;float&gt;,&lt;float&gt;)<br>display.visualizations.mapping.map.zoom = &lt;int&gt;<br>display.visualizations.mapping.markerLayer.markerOpacity = &lt;float&gt;<br>display.visualizations.mapping.markerLayer.markerMinSize = &lt;int&gt;<br>display.visualizations.mapping.markerLayer.markerMaxSize = &lt;int&gt;<br>display.visualizations.mapping.data.maxClusters = &lt;int&gt;<br>display.visualizations.mapping.tileLayer.url = &lt;string&gt;<br>display.visualizations.mapping.tileLayer.minZoom = &lt;int&gt;<br>display.visualizations.mapping.tileLayer.maxZoom = &lt;int&gt;<br><br># Patterns options<br>display.page.search.patterns.sensitivity = &lt;float&gt;<br><br># Page options<br>display.page.search.mode = [fast|smart|verbose]<br>display.page.search.timeline.format = [hidden|compact|full]<br>display.page.search.timeline.scale = [linear|log]<br>display.page.search.showFields = 0 | 1<br>display.page.search.tab = [events|statistics|visualizations|patterns]<br># Deprecated<br>display.page.pivot.dataModel = &lt;string&gt;<br><br>#*******<br># Other settings<br>#*******<br><br>embed.enabled = 0 | 1<br>* Specifies whether a saved search is shared for access with a guestpass.<br>* Search artifacts of a search can be viewed via a guestpass only if:<br>&nbsp;&nbsp;* A token has been generated that is associated with this saved search.<br>&nbsp;&nbsp;&nbsp;&nbsp;The token is asociated with a particular user and app context.<br>&nbsp;&nbsp;* The user to whom the token belongs has permissions to view that search.<br>&nbsp;&nbsp;* The saved search has been scheduled and there are artifacts available.<br>&nbsp;&nbsp;&nbsp;&nbsp;Only artifacts are available via guestpass: we never dispatch a search.<br>&nbsp;&nbsp;* The save search is not disabled, it is scheduled, it is not real-time,<br>&nbsp;&nbsp;&nbsp;&nbsp;and it is not an alert.<br><br>#*******<br># deprecated settings<br>#*******<br><br>sendresults = &lt;bool&gt;<br>* use action.email.sendresult<br><br>action_rss = &lt;bool&gt;<br>* use action.rss<br><br>action_email = &lt;string&gt;<br>* use action.email and action.email.to<br><br>role = &lt;string&gt;<br>* see saved search permissions<br><br>userid = &lt;string&gt;<br>* see saved search permissions<br><br>query &nbsp;= &lt;string&gt;<br>* use search<br><br>nextrun &nbsp;= &lt;int&gt;<br>* not used anymore, the scheduler maintains this info internally<br><br>qualifiedSearch = &lt;string&gt;<br>* not used anymore, Splunk computes this value during runtime<br><br></font></code>
<h3> <a name="savedsearchesconf_savedsearches.conf.example"><span class="mw-headline" id="savedsearches.conf.example">savedsearches.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example saved searches and alerts.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># savedsearches.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># The following searches are example searches. &nbsp;To create your own search, modify <br># the values by following the spec outlined in savedsearches.conf.spec.<br><br><br>[Daily indexing volume by server]<br>search = index=_internal todaysBytesIndexed LicenseManager-Audit NOT source=*web_service.log NOT source=*web_access.log | eval Daily<br>_Indexing_Volume_in_MBs = todaysBytesIndexed/1024/1024 | timechart avg(Daily_Indexing_Volume_in_MBs) by host<br>dispatch.earliest_time = -7d<br><br>[Errors in the last 24 hours]<br>search = error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )<br>dispatch.earliest_time = -1d<br><br>[Errors in the last hour]<br>search = error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )<br>dispatch.earliest_time = -1h<br><br>[KB indexed per hour last 24 hours]<br>search = index=_internal metrics group=per_index_thruput NOT debug NOT sourcetype=splunk_web_access | timechart fixedrange=t span=1h<br>&nbsp;sum(kb) | rename sum(kb) as totalKB<br>dispatch.earliest_time = -1d<br><br>[Messages by minute last 3 hours]<br>search = index=_internal eps "group=per_source_thruput" NOT filetracker | eval events=eps*kb/kbps | timechart fixedrange=t span=1m s<br>um(events) by series<br>dispatch.earliest_time = -3h<br><br>[Splunk errors last 24 hours]<br>search = index=_internal " error " NOT debug source=*/splunkd.log*<br>dispatch.earliest_time = -24h<br><br><br><br><br></font></code>

<a name="searchbnfconf"></a><h2> <a name="searchbnfconf_searchbnf.conf"><span class="mw-headline" id="searchbnf.conf">searchbnf.conf</span></a></h2>
<p>The following are the spec and example files for searchbnf.conf.
</p>
<h3> <a name="searchbnfconf_searchbnf.conf.spec"><span class="mw-headline" id="searchbnf.conf.spec">searchbnf.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br>#<br># This file contain descriptions of stanzas and attribute/value pairs<br># for configuring search-assistant via searchbnf.conf<br>#<br># There is a searchbnf.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;It<br># should not be modified. &nbsp;If your application has its own custom<br># python search commands, your application can include its own<br># searchbnf.conf to describe the commands to the search-assistant.<br># <br># To learn more about configuration files (including precedence)<br># please see the documentation located at<br># http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;search-commandname&gt;-command]<br>&nbsp;&nbsp;&nbsp;&nbsp;* This stanza enables properties for a given &lt;search-command&gt;.<br>&nbsp;&nbsp;&nbsp;&nbsp;* A searchbnf.conf file can contain multiple stanzas for any<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number of commands. &nbsp;* Follow this stanza name with any number<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of the following attribute/value pairs.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If you do not set an attribute for a given &lt;spec&gt;, the default<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is used. The default values are empty.<br>&nbsp;&nbsp;&nbsp;&nbsp;* An example stanza name might be "geocode-command", for a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"geocode" command.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Search command stanzas can refer to definitions defined in<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;others stanzas, and they do not require "-command", appended to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;them. &nbsp;For example:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[geocode-command]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;syntax = geocode &lt;geocode-option&gt;*<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[geocode-option]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;syntax = (maxcount=&lt;int&gt;) | (maxhops=&lt;int&gt;)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...<br><br><br>#******************************************************************************<br># The possible attributes/value pairs for searchbnf.conf<br>#******************************************************************************<br><br><br>SYNTAX = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Describes the syntax of the search command. &nbsp;See the head of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;searchbnf.conf for details.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required<br><br>SIMPLESYNTAX = &lt;string&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;* Optional simpler version of the syntax to make it easier to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;understand at the expense of completeness. &nbsp;Typically it removes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rarely used options or alternate ways of saying the same thing.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example, a search command might accept values such as<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"m|min|mins|minute|minutes", but that would unnecessarily<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clutter the syntax description for the user. &nbsp;In this can, the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simplesyntax can just pick the one (e.g., "minute").<br><br>ALIAS = &lt;commands list&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Alternative names for the search command. &nbsp;This further cleans<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;up the syntax so the user does not have to know that<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'savedsearch' can also be called by 'macro' or 'savedsplunk'.<br><br>DESCRIPTION = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Detailed text description of search command. &nbsp;Description can continue on the next line if the line ends in "\"<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required<br><br>SHORTDESC = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* A short description of the search command. &nbsp;The full DESCRIPTION<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;may take up too much screen real-estate for the search assistant.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required<br><br>EXAMPLE = &lt;string&gt;<br>COMMENT = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* 'example' should list out a helpful example of using the search<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;command, and 'comment' should describe that example.<br>&nbsp;&nbsp;&nbsp;&nbsp;* 'example' and 'comment' can be appended with matching indexes to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allow multiple examples and corresponding comments.<br>&nbsp;&nbsp;&nbsp;&nbsp;* For example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;example2 = geocode maxcount=4<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;command2 = run geocode on up to four values<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;example3 = geocode maxcount=-1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;comment3 = run geocode on all values<br><br>USAGE = public|private|deprecated<br>&nbsp;&nbsp;&nbsp;&nbsp;* Determines if a command is public, private, depreciated. &nbsp;The<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;search assistant only operates on public commands.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required<br><br>TAGS = &lt;tags list&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* List of tags that describe this search command. &nbsp;Used to find<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;commands when the use enters a synonym (e.g. "graph" -&gt; "chart")<br><br>RELATED = &lt;commands list&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* List of related commands to help user when using one command to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learn about others.<br><br><br>#******************************************************************************<br># Optional attributes primarily used internally at Splunk<br>#******************************************************************************<br><br>maintainer, appears-in, note, supports-multivalue, optout-in<br><br><br><br></font></code>
<h3> <a name="searchbnfconf_searchbnf.conf.example"><span class="mw-headline" id="searchbnf.conf.example">searchbnf.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following are example stanzas for searchbnf.conf configurations. <br>#<br><br>##################<br># selfjoin<br>##################<br>[selfjoin-command]<br>syntax = selfjoin (&lt;selfjoin-options&gt;)* &lt;field-list&gt; <br>shortdesc = Join results with itself.<br>description = Join results with itself. &nbsp;Must specify at least one field to join on.<br>usage = public<br>example1 = selfjoin id<br>comment1 = Joins results with itself on 'id' field.<br>related = join<br>tags = join combine unite<br><br>[selfjoin-options]<br>syntax = overwrite=&lt;bool&gt; | max=&lt;int&gt; | keepsingle=&lt;int&gt;<br>description = The selfjoin joins each result with other results that\<br>&nbsp;&nbsp;have the same value for the join fields. &nbsp;'overwrite' controls if\<br>&nbsp;&nbsp;fields from these 'other' results should overwrite fields of the\<br>&nbsp;&nbsp;result used as the basis for the join (default=true). &nbsp;max indicates\<br>&nbsp;&nbsp;the maximum number of 'other' results each main result can join with.\<br>&nbsp;&nbsp;(default = 1, 0 means no limit). &nbsp;'keepsingle' controls whether or not\<br>&nbsp;&nbsp;results with a unique value for the join fields (and thus no other\<br>&nbsp;&nbsp;results to join with) should be retained. &nbsp;(default = false)<br><br></font></code>

<a name="segmentersconf"></a><h2> <a name="segmentersconf_segmenters.conf"><span class="mw-headline" id="segmenters.conf">segmenters.conf</span></a></h2>
<p>The following are the spec and example files for segmenters.conf.
</p>
<h3> <a name="segmentersconf_segmenters.conf.spec"><span class="mw-headline" id="segmenters.conf.spec">segmenters.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring segmentation of events in <br># segementers.conf.<br>#<br># There is a default segmenters.conf in $SPLUNK_HOME/etc/system/default. To set custom <br># configurations, place a segmenters.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;<br># For examples, see segmenters.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;SegmenterName&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Name your stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following attribute/value pairs.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you don't specify an attribute/value pair, Splunk will use the default.<br><br>MAJOR = &lt;space separated list of breaking characters&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set major breakers.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Major breakers are words, phrases or terms in your data that are surrounded by set breaking characters.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* By default, major breakers are set to most characters and blank spaces.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Typically, major breakers are single characters.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Default is [ ] &lt; &gt; ( ) { } |&nbsp;!&nbsp;; , ' " * \n \r \s \t &amp;&nbsp;? +&nbsp;%21&nbsp;%26&nbsp;%2526&nbsp;%3B&nbsp;%7C&nbsp;%20&nbsp;%2B&nbsp;%3D --&nbsp;%2520&nbsp;%5D&nbsp;%5B&nbsp;%3A&nbsp;%0A&nbsp;%2C&nbsp;%28&nbsp;%29<br>&nbsp;&nbsp;&nbsp;&nbsp;* Please note: \s represents a space; \n, a newline; \r, a carriage return; and \t, a tab.<br><br><br>MINOR = &lt;space separated list of strings&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set minor breakers.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In addition to the segments specified by the major breakers, for each minor breaker found, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splunk indexes the token from the last major breaker to the current minor breaker and<br>&nbsp;&nbsp;&nbsp;&nbsp;from the last minor breaker to the current minor breaker.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Default is /&nbsp;: = @ . - $ #&nbsp;% \\ _<br><br>INTERMEDIATE_MAJORS = &nbsp;true | false<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set this to "true" if you want an IP address to appear in typeahead as a, a.b, a.b.c, a.b.c.d<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The typical performance hit by setting to "true" is 30%.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is "false".<br><br>FILTER = &lt;regular expression&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set, segmentation will only take place if the regular expression matches. <br>&nbsp;&nbsp;&nbsp;&nbsp;* Furthermore, segmentation will only take place on the first group of the matching regex.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Default is empty.<br><br>LOOKAHEAD = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set how far into a given event (in characters) Splunk segments.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* LOOKAHEAD applied after any FILTER rules.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To disable segmentation, set to 0.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to -1 (read the whole event).<br><br>MINOR_LEN = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify how long a minor token can be. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Longer minor tokens are discarded without prejudice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to -1.<br><br>MAJOR_LEN = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify how long a major token can be. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Longer major tokens are discarded without prejudice.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to -1.<br><br>MINOR_COUNT = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify how many minor segments to create per event.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* After the specified number of minor tokens have been created, later ones are<br>&nbsp;&nbsp;&nbsp;&nbsp;discarded without prejudice.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to -1.<br><br>MAJOR_COUNT = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specify how many major segments are created per event. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* After the specified number of major segments have been created, later ones are<br>&nbsp;&nbsp;&nbsp;&nbsp;discarded without prejudice.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Default to -1.<br><br></font></code>
<h3> <a name="segmentersconf_segmenters.conf.example"><span class="mw-headline" id="segmenters.conf.example">segmenters.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># The following are examples of segmentation configurations.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># segmenters.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br># Example of a segmenter that doesn't index the date as segments in syslog data:<br><br>[syslog]<br>FILTER = ^.*?\d\d:\d\d:\d\d\s+\S+\s+(.*)$<br><br><br># Example of a segmenter that only indexes the first 256b of events:<br><br>[limited-reach]<br>LOOKAHEAD = 256<br><br><br># Example of a segmenter that only indexes the first line of an event:<br><br>[first-line]<br>FILTER = ^(.*?)(\n|$)<br><br><br># Turn segmentation off completely:<br><br>[no-segmentation]<br>LOOKAHEAD = 0<br><br><br></font></code>

<a name="serverconf"></a><h2> <a name="serverconf_server.conf"><span class="mw-headline" id="server.conf">server.conf</span></a></h2>
<p>The following are the spec and example files for server.conf.
</p>
<h3> <a name="serverconf_server.conf.spec"><span class="mw-headline" id="server.conf.spec">server.conf.spec</span></a></h3>
<code><font size="2">
#   Version 6.2.3
#
# This file contains the set of attributes and values you can use to configure server options
# in server.conf.
#
# There is a server.conf in $SPLUNK_HOME/etc/system/default/.  To set custom configurations, 
# place a server.conf in $SPLUNK_HOME/etc/system/local/.  For examples, see server.conf.example.
# You must restart Splunk to enable configurations.
#
# To learn more about configuration files (including precedence) please see the documentation 
# located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles

# GLOBAL SETTINGS
# Use the [default] stanza to define any global settings.
#     * You can also define global settings outside of any stanza, at the top of the file.
#     * Each conf file should have at most one default stanza. If there are multiple default
#       stanzas, attributes are combined. In the case of multiple definitions of the same
#       attribute, the last definition in the file wins.
#     * If an attribute is defined at both the global level and in a specific stanza, the
#       value in the specific stanza takes precedence.


##########################################################################################
# General Server Configuration
##########################################################################################
[general]
serverName = &lt;ASCII string&gt;
    * The name used to identify this Splunk instance for features such as distributed search.
    * Defaults to &lt;hostname&gt;-&lt;user running splunk&gt;.
    * May not be an empty string
    * May contain environment variables
    * After any environment variables have been expanded, the server name (if not an IPv6
      address) can only contain letters, numbers, underscores, dots, and dashes; and
      it must start with a letter, number, or an underscore.  

hostnameOption = &lt;ASCII string&gt;
    * The option used to specify the detail in the server name used to identify this Splunk instance.
    * Can be one of fullyqualifiedname , clustername, shortname
    * Is applicable to Windows only
    * May not be an empty string

sessionTimeout = &lt;nonnegative integer&gt;[smhd]
    * The amount of time before a user session times out, expressed as a search-like time range
    * Examples include '24h' (24 hours), '3d' (3 days), '7200s' (7200 seconds, or two hours)
    * Defaults to '1h' (1 hour)

trustedIP = &lt;IP address&gt;
    * All logins from this IP address are trusted, meaning password is no longer required
    * Only set this if you are using Single Sign On (SSO)

allowRemoteLogin = always|never|requireSetPassword
    * Controls remote management by restricting general login. Note that this does not apply to trusted
      SSO logins from trustedIP.
    * If 'always', enables authentication so that all remote login attempts are allowed.
    * If 'never', only local logins to splunkd will be allowed. Note that this will still allow
      remote management through splunkweb if splunkweb is on the same server.
    * If 'requireSetPassword' (default):
         * In the free license, remote login is disabled.
         * In the pro license, remote login is only disabled for "admin" user if default password of "admin" has not been changed.

access_logging_for_phonehome = true|false
    * Enables/disables logging to splunkd_access.log for client phonehomes
    * defaults to true (logging enabled)

hangup_after_phonehome = true|false
    * Controls whether or not the (deployment) server hangs up the connection after the phonehome is done.
    * By default we use persistent HTTP 1.1 connections with the server to handle phonehomes. This may show higher memory usage for a large number of clients. 
    * In case we have more than maximum concurrent tcp connection number of deployment clients, persistent connections don't help with the reuse of connections anyway, so setting this to false helps bring down memory usage.
    * defaults to false (persistent connections for phonehome)
    
pass4SymmKey = &lt;passphrase string&gt;
    * Authenticates traffic between:
       *   License master and its license slaves.
       *   Members of a cluster; see Note 1 below.
       *   Deployment server (DS) and its deployment clients (DCs); see Note 2 below.
    * Note 1: Clustering may override the passphrase specified here, in
      the [clustering] stanza.  A clustering searchhead connecting to multiple masters
      may further override in the [clustermaster:stanza1] stanza.
    * Note 2: By default, DS-DCs passphrase auth is disabled.  To enable DS-DCs passphrase auth, you must *also* add the
      following line to the [broker:broker] stanza in restmap.conf:
																		requireAuthentication = true
    * In all scenarios, *every* node involved must
      set the same passphrase in the same stanza(s) (i.e. [general] and/or [clustering]);
      otherwise, respective communication (licensing and deployment in case of [general] stanza,
      clustering in case of [clustering] stanza) will not proceed.

listenOnIPv6 = no|yes|only
    * By default, splunkd will listen for incoming connections (both REST
      and TCP inputs) using IPv4 only
    * To enable IPv6 support in splunkd, set this to 'yes'.  splunkd will simultaneously
      listen for connections on both IPv4 and IPv6
    * To disable IPv4 entirely, set this to 'only', which will cause splunkd
      to exclusively accept connections over IPv6.  You will probably also
      need to change mgmtHostPort in web.conf (use '[::1]' instead of '127.0.0.1')
    * Note that any setting of SPLUNK_BINDIP in your environment or splunk-launch.conf
      will override this value.  In that case splunkd will listen on the exact address
      specified.

connectUsingIpVersion = auto|4-first|6-first|4-only|6-only
    * When making outbound TCP connections (for forwarding eventdata, making
      distributed search requests, etc) this controls whether the connections will
      be made via IPv4 or IPv6.
    * If a host is available over both IPv4 and IPv6 and this is set to '4-first', then
      we will connect over IPv4 first and fallback to IPv6 if the connection fails.
    * If it is set to '6-first' then splunkd will try IPv6 first and fallback to IPv4 on failure
    * If this is set to '4-only' then splunkd will only attempt to make connections over IPv4.
    * Likewise, if this is set to '6-only', then splunkd will only attempt to connect to the IPv6 address.
    * The default value of 'auto' will select a reasonable value based on listenOnIPv6 setting
      If that value is set to 'no' it will act like '4-only'.  If it is set to 'yes' it will
      act like '6-first' and if it is set to 'only' it will act like '6-only'.
    * Note that connections to literal addresses are unaffected by this.  For example,
      if a forwarder is configured to connect to "10.1.2.3" the connection will be made over
      IPv4 regardless of this setting.

guid = &lt;globally unique identifier for this instance&gt;
    * This setting now (as of 5.0) belongs in the [general] stanza of SPLUNK_HOME/etc/instance.cfg file;
      please see specfile of instance.cfg for more information.

useHTTPServerCompression = true|false
    * Whether splunkd HTTP server should support gzip content encoding. For more info on how 
      content encoding works, see http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html (section 14.3).
    * Defaults to true.

defaultHTTPServerCompressionLevel = &lt;integer&gt;
    * If useHTTPServerCompression is enabled, this setting constrols the
      compression "level" we attempt
    * This number must be in the range 1 through 9
    * Higher numbers produce smaller compressed results but require more CPU usage
    * The default value of 6 is appropriate for most environments

skipHTTPCompressionAcl = &lt;network_acl&gt;
    * Lists a set of networks or addresses to skip compressing data for.
      These are addresses that are considered so close that network speed
      is never an issue, so any CPU time spent compressing a response is
      wasteful.
    * Note that the server may still respond with compressed data if it
      already has a compressed version of the data available.
    * These rules are separated by commas or spaces
    * Each rule can be in the following forms:
    *   1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")
    *   2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")
    *   3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")
    *   4. A single '*' which matches anything
    * Entries can also be prefixed with '!' to negate their meaning.
    * Defaults to localhost addresses.

site = &lt;site-id&gt;
    * Specifies the site that this splunk instance belongs to when multisite is enabled.
    * Valid values for site-id include site1 to site63

useHTTPClientCompression = true|false|on-http|on-https
    * Whether gzip compression should be supported when Splunkd acts as a client (including distributed searches). Note that  
      in order for the content to be compressed, the HTTP server that the client is connecting to should also support compression.
    * If the connection is being made over https and useClientSSLCompression=true (see below), then setting this 
      option to true would result in double compression work without much compression gain. It is recommended that this
      value be set to on-http (or to true, and useClientSSLCompression to false).
    * Defaults to false.

embedSecret = &lt;string&gt;
    * When using report embedding, normally the generated URLs can only
      be used on the search head they were generated on
    * If "embedSecret" is set, then the token in the URL will be encrypted
      with this key.  Then other search heads with the exact same setting
      can also use the same URL.
    * This is needed if you want to use report embedding across multiple
      nodes on a search head pool.

instanceType = &lt;string&gt;
    * Should not be modified by users.
    * Informs components (such as the SplunkWeb Manager section) which
      environment Splunk is running in, to allow for more customized behaviors.
    * Defaults to "splunk", meaning no special behaviors.

##########################################################################################
# SSL Configuration details
##########################################################################################

[sslConfig]
    * Set SSL for communications on Splunk back-end under this stanza name.
        * NOTE: To set SSL (eg HTTPS) for Splunk Web and the browser, use web.conf.
    * Follow this stanza name with any number of the following attribute/value pairs.  
    * If you do not specify an entry for each attribute, Splunk will use the default value.

enableSplunkdSSL = true|false
    * Enables/disables SSL on the splunkd management port (8089) and KV store port (8191).
    * Defaults to true.
    * Note: Running splunkd without SSL is not generally recommended. 
    * Distributed search will often perform better with SSL enabled.

useClientSSLCompression = true|false
    * Turns on HTTP client compression. 
    * Server-side compression is turned on by default; setting this on the client side enables 
      compression between server and client.  
    * Enabling this potentially gives you much faster distributed searches across multiple Splunk instances.
    * Defaults to true.
    
useSplunkdClientSSLCompression = true|false
    * Controls whether SSL compression would be used when splunkd is acting as an HTTP client,
      usually during certificate exchange, bundle replication, remote calls etc. 
    * NOTE: this setting is effective if, and only if, useClientSSLCompression is set to true
    * NOTE: splunkd is not involved in data transfer in distributed search, the search in a separate process is.
    * Defaults to true.
 
sslVersions = &lt;versions_list&gt;
    * Comma-separated list of SSL versions to support
    * The versions available are "ssl2", "ssl3", "tls1.0", "tls1.1", and "tls1.2"
    * The special version "*" selects all supported versions.  The version "tls"
      selects all versions tls1.0 or newer
    * If a version is prefixed with "-" it is removed from the list
    * When configured in FIPS mode ssl2 and ssl3 are always disabled regardless of this configuration
    * Defaults to "*,-ssl2".  (anything newer than SSLv2)

supportSSLV3Only = true|false
    * DEPRECATED.  SSLv2 is now always disabled by default.  The exact set of
      SSL versions allowed is now configurable via the "sslVersions" setting above

sslVerifyServerCert = true|false
        * Used by distributed search: when making a search request to another
          server in the search cluster.
        * Used by distributed deployment clients: when polling a deployment
          server.
        * If this is set to true, you should make sure that the server that is
          being connected to is a valid one (authenticated).  Both the common
          name and the alternate name of the server are then checked for a
          match if they are specified in this configuration file.  A
          certificiate is considered verified if either is matched.
        * Default is false.

sslCommonNameToCheck = &lt;commonName&gt;
        * If this value is set, and 'sslVerifyServerCert' is set to true,
          splunkd will limit most outbound HTTPS connections to hosts which use
          a cert with this common name. 
        * 'sslCommonNameList' is a multivalue extension of this setting, certs
          which match 'sslCommonNameList' or 'sslCommonNameToCheck' will be
          accepted.
        * The most important scenario is distributed search.
        * This feature does not work with the deployment server and client
          communication over SSL.
        * Optional.  Defaults to no common name checking.

sslCommonNameList = &lt;commonName1&gt;, &lt;commonName2&gt;, ...
        * If this value is set, and 'sslVerifyServerCert' is set to true,
          splunkd will limit most outbound HTTPS connections to hosts which use
          a cert with one of the listed common names. 
        * The most important scenario is distributed search.
        * Optional.  Defaults to no common name checking.

sslAltNameToCheck = &lt;alternateName1&gt;, &lt;alternateName2&gt;, ...
        * If this value is set, and 'sslVerifyServerCert' is set to true,
          splunkd will also be willing to verify certificates which have a
          so-called "Subject Alternate Name" that matches any of the alternate
          names in this list.
            * Subject Alternate Names are effectively extended descriptive
              fields in SSL certs beyond the commonName.  A common practice for
              HTTPS certs is to use these values to store additional valid
              hostnames or domains where the cert should be considered valid.
        * Accepts a comma-separated list of Subject Alternate Names to consider
          valid.
        * Items in this list are never validated against the SSL Common Name.
        * This feature does not work with the deployment server and client
          communication over SSL.
        * Optional.  Defaults to no alternate name checking

requireClientCert = true|false
        * Requires that any HTTPS client that connects to splunkd internal HTTPS server
        has a certificate that was signed by our CA (certificate authority).
        * Used by distributed search: Splunk indexing instances must be authenticated
        to connect to another splunk indexing instance.
        * Used by distributed deployment: the deployment server requires that 
        deployment clients are authenticated before allowing them to poll for new
        configurations/applications.
        * If true, a client can connect ONLY if a certificate created by our
        certificate authority was used on that client.
        * Default is false.

cipherSuite = &lt;cipher suite string&gt;
        * If set, Splunk uses the specified cipher string for the HTTP server.
        * If not set, Splunk uses the default cipher string
         provided by OpenSSL.  This is used to ensure that the server does not
         accept connections using weak encryption protocols.

ecdhCurveName = &lt;string&gt;
        * ECDH curve to use for ECDH key negotiation
        * We only support named curves specified by their SHORT name. 
        * The list of valid named curves by their short/long names
        * can be obtained by executing this command:
        * $SPLUNK_HOME/bin/splunk cmd openssl ecparam -list_curves
        * Default is empty string.
          
sslKeysfile = &lt;filename&gt;
        * Server certificate file. 
        * Certificates are auto-generated by splunkd upon starting Splunk.
        * You may replace the default cert with your own PEM format file.
        * Certs are stored in caPath (see below).
        * Default is server.pem.
        
sslKeysfilePassword = &lt;password&gt;
        * Server certificate password.
        * Default is password.

caCertFile = &lt;filename&gt;
        * Public key of the signing authority.
        * Default is cacert.pem.

caPath = &lt;path&gt;
        * Path where all these certs are stored.
        * Default is $SPLUNK_HOME/etc/auth.
        
certCreateScript = &lt;script name&gt;
        * Creation script for generating certs on startup 
          of Splunk.

sendStrictTransportSecurityHeader = true|false
        * If set to true, the REST interface will send a "Strict-Transport-Security"
          header with all responses to requests made over SSL.
        * This can help avoid a client being tricked later by a Man-In-The-Middle
          attack to accept a non-SSL request.  However, this requires a commitment that
          no non-SSL web hosts will ever be run on this hostname on any port.  For example,
          if splunkweb is in default non-SSL mode this can break the ability of browser
          to connect to it.  Enable with caution.
        * Defaults to false

allowSslCompression = true|false
        * If set to true, the server will allow clients to negotiate
          SSL-layer data compression.
        * Defaults to true.

allowSslRenegotiation = true|false
        * In the SSL protocol, a client may request renegotiation of the connection
          settings from time to time.
        * Setting this to false causes the server to reject all renegotiation
          attempts, breaking the connection.  This limits the amount of CPU a
          single TCP connection can use, but it can cause connectivity problems
          especially for long-lived connections.
        * Defaults to true.

##########################################################################################
# Splunkd HTTP server configuration
##########################################################################################

[httpServer]
    * Set stand-alone HTTP settings for Splunk under this stanza name.
    * Follow this stanza name with any number of the following attribute/value pairs.  
    * If you do not specify an entry for each attribute, Splunk uses the default value.

atomFeedStylesheet = &lt;string&gt;
    * Defines the stylesheet relative URL to apply to default Atom feeds.
    * Set to 'none' to stop writing out xsl-stylesheet directive.  
    * Defaults to /static/atom.xsl.

max-age = &lt;nonnegative integer&gt;
    * Set the maximum time (in seconds) to cache a static asset served off of the '/static' directory.
    * This value is passed along in the 'Cache-Control' HTTP header.
    * Defaults to 3600.
          
follow-symlinks = true|false
    * Toggle whether static file handler (serving the '/static' directory) follow filesystem 
      symlinks when serving files.  
    * Defaults to false.
          
disableDefaultPort = true|false
    * If true, turns off listening on the splunkd management port (8089 by default)
    * This setting is not recommended:
      * This is the general communication path to splunkd.  If it is disabled, there is
        no way to communicate with a running splunk.
      * This means many command line splunk invocations cannot function,
        splunkweb cannot function, the REST interface cannot function, etc.
      * If you choose to disable the port anyway, understand that you are selecting
        reduced Splunk functionality.
    * Default value is 'false'.

acceptFrom = &lt;network_acl&gt; ...
    * Lists a set of networks or addresses to accept data from.  These rules are separated by commas or spaces
    * Each rule can be in the following forms:
    *   1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")
    *   2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")
    *   3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")
    *   4. A single '*' which matches anything
    * Entries can also be prefixed with '!' to cause the rule to reject the
      connection.  Rules are applied in order, and the first one to match is
      used.  For example, "!10.1/16, *" will allow connections from everywhere
      except the 10.1.*.* network.
    * Defaults to "*" (accept from anywhere)

streamInWriteTimeout = &lt;positive number&gt;
        * When uploading data to http server, if http server is unable to write data to
        * receiver for configured streamInWriteTimeout seconds, it aborts write operation.
        * Defaults to 5 seconds.

max_content_length = &lt;int&gt;
        * Measured in bytes
        * HTTP requests over this size will rejected.
        * Exists to avoid allocating an unreasonable amount of memory from web requests
        * Defaulted to 838860800 or 800MB
        * In environments where indexers have enormous amounts of RAM, this
          number can be reasonably increased to handle large quantities of
          bundle data.

maxThreads = &lt;int&gt;
        * Number of threads that can be used by active HTTP transactions.
          This can be limited to constrain resource usage.
        * If set to 0 (the default) a limit will be automatically picked
          based on estimated server capacity.
        * If set to a negative number, no limit will be enforced.

maxSockets = &lt;int&gt;
        * Number of simultaneous HTTP connections that we accept simultaneously.
          This can be limited to constrain resource usage.
        * If set to 0 (the default) a limit will be automatically picked
          based on estimated server capacity.
        * If set to a negative number, no limit will be enforced.

forceHttp10 = auto|never|always
        * When set to "always", the REST HTTP server will not use some
          HTTP 1.1 features such as persistent connections or chunked
          transfer encoding.
        * When set to "auto" it will do this only if the client sent no
          User-Agent header, or if the user agent is known to have bugs
          in its HTTP/1.1 support.
        * When set to "never" it always will allow HTTP 1.1, even to
          clients it suspects may be buggy.
        * Defaults to "auto"

crossOriginSharingPolicy = &lt;origin_acl&gt; ...
        * List of HTTP Origins to return Access-Control-Allow-* (CORS) headers for
        * These headers tell browsers that we trust web applications at those sites
          to make requests to the REST interface
        * The origin is passed as a URL without a path component (for example
          "https://app.example.com:8000")
        * This setting can take a list of acceptable origins, separated
          by spaces and/or commas
        * Each origin can also contain wildcards for any part.  Examples:
            *://app.example.com:*  (either HTTP or HTTPS on any port)
            https://*.example.com  (any host under example.com, including example.com itself)
        * An address can be prefixed with a '!' to negate the match, with
          the first matching origin taking precedence.  For example,
          "!*://evil.example.com:* *://*.example.com:*" to not avoid
          matching one host in a domain
        * A single "*" can also be used to match all origins
        * By default the list is empty

x_frame_options_sameorigin = true|false
        * adds a X-Frame-Options header set to "SAMEORIGIN" to every response served by splunkd
        * Defaults to true

allowEmbedTokenAuth = true|false
        * If set to false, splunkd will not allow any access to artifacts
          that previously had been explicitly shared to anonymous users.
        * This effectively disables all use of the "embed" feature.
        * Defaults to true

cliLoginBanner = &lt;string&gt;
        * Sets a message which will be added to the HTTP reply headers
          of requests for authentication, and to the "server/info" endpoint
        * This will be printed by the Splunk CLI before it prompts
          for authentication credentials.  This can be used to print
          access policy information.
        * If this string starts with a '"' character, it is treated as a
          CSV-style list with each line comprising a line of the message.
          For example: "Line 1","Line 2","Line 3"
        * Defaults to empty (no message)

allowBasicAuth = true|false
        * Allows clients to make authenticated requests to the splunk
          server using "HTTP Basic" authentication in addition to the
          normal "authtoken" system
        * This is useful for programmatic access to REST endpoints and
          for accessing the REST API from a web browser.  It is not
          required for the UI or CLI.
        * Defaults to true

basicAuthRealm = &lt;string&gt;
        * When using "HTTP Basic" authenitcation, the 'realm' is a
          human-readable string describing the server.  Typically, a web
          browser will present this string as part of its dialog box when
          asking for the username and password.
        * This can be used to display a short message describing the
          server and/or its access policy.
        * Defaults to "/splunk"

allowCookieAuth = true|false
        * Allows clients to request an HTTP cookie from the /services/server/auth
          endpoint which can then be used to authenticate future requests
        * Defaults to true

cookieAuthHttpOnly = true|false
        * When using cookie based authentication, mark returned cookies
          with the "httponly" flag to tell the client not to allow javascript
          code to access its value
        * Defaults to true
        * NOTE: has no effect if allowCookieAuth=false

cookieAuthSecure = true|false
        * When using cookie based authentication, mark returned cookies
          with the "secure" flag to tell the client never to send it over
          an unencrypted HTTP channel
        * Defaults to true
        * NOTE: has no effect if allowCookieAuth=false OR the splunkd REST
          interface has SSL disabled

dedicatedIoThreads = &lt;int&gt;
        * If set to zero, HTTP I/O will be performed in the same thread
          that accepted the TCP connection.
        * If set set to a non-zero value, separate threads will be run
          to handle the HTTP I/O, including SSL encryption.
        * Defaults to "0"
        * Typically this does not need to be changed.  For most usage
          scenarios using the same the thread offers the best performance.

#########################################################################################
# Splunkd HTTPServer listener configuration
#########################################################################################

[httpServerListener:&lt;ip&gt;:&lt;port&gt;]
        * Enable the splunkd http server to listen on a network interface (NIC) specified by
        &lt;ip&gt; and a port number specified by &lt;port&gt;.  If you leave &lt;ip&gt; blank (but still include the ':'),
        splunkd will listen on the kernel picked NIC using port &lt;port&gt;.

ssl = true|false
        * Toggle whether this listening ip:port will use SSL or not.
        * Default value is 'true'.
          
listenOnIPv6 = no|yes|only
        * Toggle whether this listening ip:port will listen on IPv4, IPv6, or both
        * If not present, the setting in the [general] stanza will be used

acceptFrom = &lt;network_acl&gt; ...
    * Lists a set of networks or addresses to accept data from.  These rules are separated by commas or spaces
    * Each rule can be in the following forms:
    *   1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")
    *   2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")
    *   3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")
    *   4. A single '*' which matches anything
    * Entries can also be prefixed with '!' to cause the rule to reject the
      connection.  Rules are applied in order, and the first one to match is
      used.  For example, "!10.1/16, *" will allow connections from everywhere
      except the 10.1.*.* network.
    * Defaults to the setting in the [httpServer] stanza above

##########################################################################################
# Static file handler MIME-type map
##########################################################################################

[mimetype-extension-map]
    * Map filename extensions to MIME type for files served from the static file handler under
    this stanza name.
    
&lt;file-extension&gt; = &lt;MIME-type&gt;
    * Instructs the HTTP static file server to mark any files ending in 'file-extension' 
         with a header of 'Content-Type: &lt;MIME-type&gt;'.
    * Defaults to:
    
    [mimetype-extension-map]
    gif = image/gif
    htm = text/html
    jpg = image/jpg
    png = image/png
    txt = text/plain
    xml = text/xml
    xsl = text/xml
    
##########################################################################################
# Remote applications configuration (e.g. SplunkBase)
##########################################################################################

[applicationsManagement]
    * Set remote applications settings for Splunk under this stanza name.
    * Follow this stanza name with any number of the following attribute/value pairs.  
    * If you do not specify an entry for each attribute, Splunk uses the default value.

allowInternetAccess = true|false
    * Allow Splunk to access the remote applications repository.

url = &lt;URL&gt;
    * Applications repository.
    * Defaults to https://apps.splunk.com/api/apps

loginUrl = &lt;URL&gt;
    * Applications repository login.
    * Defaults to https://apps.splunk.com/api/account:login/

detailsUrl = &lt;URL&gt;
    * Base URL for application information, keyed off of app ID.
    * Defaults to https://apps.splunk.com/apps/id

useragent = &lt;splunk-version&gt;-&lt;splunk-build-num&gt;-&lt;platform&gt;
    * User-agent string to use when contacting applications repository.
    * &lt;platform&gt; includes information like operating system and CPU architecture.

updateHost = &lt;URL&gt;
    * Host section of URL to check for app updates, e.g. https://apps.splunk.com

updatePath = &lt;URL&gt;
    * Path section of URL to check for app updates, e.g. /api/apps:resolve/checkforupgrade

updateTimeout = &lt;time range string&gt;
    * The minimum amount of time Splunk will wait between checks for app updates
    * Examples include '24h' (24 hours), '3d' (3 days), '7200s' (7200 seconds, or two hours)
    * Defaults to '24h'

##########################################################################################
# Misc. configuration
##########################################################################################

[scripts]

initialNumberOfScriptProcesses = &lt;num&gt;
        * The number of pre-forked script processes that are launched
        when the system comes up.  These scripts are reused when script REST endpoints *and*
        search scripts are executed.  The idea is to eliminate the performance
        overhead of launching the script interpreter every time it is invoked.  These
        processes are put in a pool.  If the pool is completely busy when a script gets
        invoked, a new processes is fired up to handle the new invocation - but it 
        disappears when that invocation is finished.


##########################################################################################
# Disk usage settings (for the indexer, not for Splunk log files)
##########################################################################################

[diskUsage]

minFreeSpace = &lt;num&gt;
        * Specified in megabytes.
        * The default setting is 5000 (approx 5GB)
        * Specifies a safe amount of space that must exist for splunkd to continue operating.
        * Note that this affects search and indexing
        * This is how the searching is affected:
        * For search:
            * Before attempting to launch a search, splunk will require this
              amount of free space on the filesystem where the dispatch
              directory is stored, $SPLUNK_HOME/var/run/splunk/dispatch
            * Applied similarly to the search quota values in
              authorize.conf and limits.conf.
        * For indexing: 
            * Periodically, the indexer will check space on all partitions
              that contain splunk indexes as specified by indexes.conf.  Indexing
              will be paused and a ui banner + splunkd warning posted to indicate
              need to clear more disk space.

pollingFrequency = &lt;num&gt;
        * After every pollingFrequency events indexed, the disk usage is checked.
        * The default frequency is every 100000 events.

pollingTimerFrequency = &lt;num&gt;
        * After every pollingTimerFrequency seconds, the disk usage is checked
        * The default value is 10 seconds

##########################################################################################
# Queue settings
##########################################################################################
[queue]

maxSize = [&lt;integer&gt;|&lt;integer&gt;[KB|MB|GB]]
        * Specifies default capacity of a queue.
        * If specified as a lone integer (for example, maxSize=1000), maxSize indicates the maximum number of events allowed
          in the queue.
        * If specified as an integer followed by KB, MB, or GB (for example, maxSize=100MB), it indicates the maximum
          RAM allocated for queue.
        * The default is 500KB.

cntr_1_lookback_time = [&lt;integer&gt;[s|m]] 
        * The lookback counters are used to track the size and count (number of elements in the queue) variation of the queues using an 
          exponentially moving weighted average technique. Both size and count variation has 3 sets of counters each. The set
          of 3 counters is provided to be able to track short, medium and long term history of size/count variation. The user can
          customize the value of these counters or lookback time.
        * Specifies how far into history should the size/count variation be tracked for counter 1.
        * It needs to be specified via an integer followed by [s|m] which stands for seconds and minutes respectively.
        * The default value for counter 1 is set to 60 seconds.

cntr_2_lookback_time = [&lt;integer&gt;[s|m]]
        * See above for explanation and usage of the lookback counter.
        * Specifies how far into history should the size/count variation be tracked for counter 2.
        * The default value for counter 2 is set to 600 seconds.

cntr_3_lookback_time = [&lt;integer&gt;[s|m]]
        * See above for explanation and usage of the lookback counter..
        * Specifies how far into history should the size/count variation be tracked for counter 3.
        * The default value for counter 3 is set to 900 seconds.

sampling_interval = [&lt;integer&gt;[s|m]]
        * The lookback counters described above collects the size and count measurements for the queues.
          This specifies at what interval the measurement collection will happen. Note that for a particular
          queue all the counters sampling interval is same. 
        * It needs to be specified via an integer followed by [s|m] which stands for seconds and minutes respectively.
        * The default sampling_interval value is 1 second.

[queue=&lt;queueName&gt;]

maxSize = [&lt;integer&gt;|&lt;integer&gt;[KB|MB|GB]]
        * Specifies the capacity of a queue. It overrides the default capacity specified in [queue].
        * If specified as a lone integer (for example, maxSize=1000), maxSize indicates the maximum number of events allowed
          in the queue.
        * If specified as an integer followed by KB, MB, or GB (for example, maxSize=100MB), it indicates the maximum
          RAM allocated for queue.
        * The default is inherited from maxSize value specified in [queue]

cntr_1_lookback_time = [&lt;integer&gt;[s|m]] 
        * Same explanation as mentioned in [queue].
        * Specifies the lookback time for the specific queue for counter 1.
        * The default value is inherited from cntr_1_lookback_time value specified in [queue].

cntr_2_lookback_time = [&lt;integer&gt;[s|m]]
        * Specifies the lookback time for the specific queue for counter 2.
        * The default value is inherited from cntr_2_lookback_time value specified in [queue].

cntr_3_lookback_time = [&lt;integer&gt;[s|m]]
        * Specifies the lookback time for the specific queue for counter 3.
        * The default value is inherited from cntr_3_lookback_time value specified in [queue].

sampling_interval = [&lt;integer&gt;[s|m]]
        * Specifies the sampling interval for the specific queue.
        * The default value is inherited from sampling_interval value specified in [queue].

##########################################################################################
# PubSub server settings for the http endpoint.
##########################################################################################

[pubsubsvr-http]

disabled = true|false
    * If disabled, then http endpoint is not registered. Set this value to 'false' to 
        expose PubSub server on http.
    * Defaults to 'true'

stateIntervalInSecs = &lt;seconds&gt;
    * The number of seconds before a connection is flushed due to inactivity. The connection is not
        closed, only messages for that connection are flushed.
    * Defaults to 300 seconds (5 minutes).

##########################################################################################
# General file input settings.
##########################################################################################

[fileInput]

outputQueue = &lt;queue name&gt;
    * The queue that input methods should send their data to.  Most users will not need to
      change this value.
    * Defaults to parsingQueue.

##########################################################################################
# Settings controlling the behavior of 'splunk diag', the diagnostic tool
##########################################################################################

[diag]

# These settings provide defaults for invocations of the splunk diag command.
# Generally these can be further modified by command line flags to the diag command.

EXCLUDE-&lt;class&gt; = &lt;glob expression&gt;
    * Specifies a glob / shell pattern to be excluded from diags generated on
      this Splunk instance. 
        * Example: */etc/secret_app/local/*.conf
    * Further excludes can be added at the splunk diag command line, but there
      is no facility to disable configuration-based excludes at the command
      line.
    * There is one exclude by default, for the splunk.secret file.

# the following commands can be overridden entirely by their command-line equivalents.

components = &lt;comma separated list&gt;
    * Specifies which components of the diag should be gathered.
    * This allows the disabling and enabling, categorically, of entire portions
      of diag functionality.
    * All of these components are further subject to the exclude feature (see above), 
      and component-specific filters (see below).
    * Currently, with no configuration, all components except 'rest' are enabled by default.
    * Available components are:
        * index_files  &nbsp;: Files from the index that indicate their health
                          (Hosts|Sources|Sourcetypes.data and bucketManifests).
                          User data is not collected.
        * index_listing&nbsp;: Directory listings of the index contents are
                          gathered, in order to see filenames, directory names,
                          sizes, timestamps and the like.
        * etc          &nbsp;: The entire contents of the $SPLUNK_HOME/etc
                          directory.  In other words, the configuration files.
        * log          &nbsp;: The contents of $SPLUNK_HOME/var/log/... 
        * pool         &nbsp;: If search head pooling is enabled, the contents of the pool dir.
        * dispatch     &nbsp;: Search artifacts, without the actual results, 
                          In other words var/run/splunk/dispatch, but not the results or events files
        * searchpeers  &nbsp;: Directory listings of knowledge bundles replicated for distributed search
                          In other words: $SPLUNK_HOME/var/run/searchpeers
        * consensus    &nbsp;: Consensus protocol files produced by search head clustering
                          In other words: $SPLUNK_HOME/var/run/splunk/_raft
        * conf_replication_summary&nbsp;: Directory listing of configuration replication summaries produced by search head clustering
                          In other words: $SPLUNK_HOME/var/run/splunk/snapshot
        * rest         &nbsp;: The contents of a variety of splunkd endpoints
                          Includes server status messages (system banners), licenser banners, 
                            configured monitor inputs &amp;  tailing file status (progress reading input files). 
                          On cluster masters, also gathers master info, fixups, current peer list, 
                            clustered index info, current generation, &amp; buckets in bad stats
                          On cluster slaves, also gathers local buckets &amp; local slave info, and
                            the master information remotely from the configured master.
        * kvstore      &nbsp;: Directory listings of the KV Store data directory contents are
                          gathered, in order to see filenames, directory names,
                          sizes, and timestamps.
                          
    * The special value 'all' is also supported, enabling everything explicitly.
    * Further controlling the components from the command line:
        * The switch --collect replaces this list entirely.
            * Example: --collect log,etc
              This would set the componets to log and etc only, regardless of config
        * The switch --enable adds a specific component to this list.
            * Example: --enable pool
              This would ensure that pool data is collected, regardles of config
        * The switch --disable removes a specific component from this list.
            * Example: --disable pool
              This would ensure that pool data is *NOT* collected, regardles of config
    * Currently, the default is to collect all components.  
    * In the future there many be additional components which are not in the
      default set.
      * This may occur for new components that are expensive (large and/or slow) 
      * This may occur for new components that are preceived as sensitive

# Data filters; these further refine what is collected
# most of the existing ones are designed to limit the size and collection time
# to pleasant values.

# note that most values here use underscores '_' while the command line uses hyphens '-'

all_dumps = &lt;bool&gt;
    * This setting currently is irrelevant on Unix platforms.
    * Affects the 'log' component of diag. (dumps are written to the log dir on Windows)
    * Can be overridden with the --all-dumps command line flag.
    * Normally, Splunk diag will gather only three .DMP (crash dump) files on
      Windows to limit diag size.
    * If this is set to true, splunk diag will collect *all* .DMP files from
      the log directory.
    * Defaults to unset / false (equivalent).

index_files = [full|manifests]
    * Selects a detail level for the 'index_files' component.
    * Can be overridden with the --index-files command line flag.
    * 'manifests' limits the index file-content collection to just
      .bucketManifest files which give some information about Splunks idea of
      the general state of buckets in an index.
    * 'full' adds the collection of Hosts.data, Sources.data, and
      Sourcetypes.data which indicate the breakdown of count of items by those
      categories per-bucket, and the timespans of those category entries
        * 'full' can take quite some time on very large index sizes, especially
          when slower remote storage is involved.
    * Defaults to 'manifests'

index_listing = [full|light]
    * Selects a detail level for the 'index_listing' component.
    * Can be overridden with the --index-listing command line flag.
    * 'light' gets directory listings (ls, or dir) of the hot/warm and cold
      container directory locations of the indexes, as well as listings of each
      hot bucket.
    * 'full' gets a recursive directory listing of all the contents of every
      index location, which should mean all contents of all buckets.
      * 'full' may take significant time as well with very large bucket counts,
        espeically on slower storage.
    * Defaults to 'light'

etc_filesize_limit = &lt;non-negative integer in kilobytes&gt;
    * This filters the 'etc' component
    * Can be overridden with the --etc-filesize-limit command line flag
    * This value is specified in kilobytes.  
        * Example: 2000 - this would be approximately 2MB.
    * Files in the $SPLUNK_HOME/etc directory which are larger than this limit
      will not be collected in the diag.
    * Diag will produce a message stating that a file has been skipped for size
      to the console. (In practice we found these large files are often a
      surprise to the administrator and indicate problems).
    * If desired, this filter may be entirely disabled by setting the value to 0.
    * Defaults to 10000 or 10MB.

log_age = &lt;non-negative integer in days&gt;
    * This filters the 'log' component
    * Can be overridden with the --log-age command line flag
    * This value is specified in days
        * Example: 75 - this would be 75 days, or about 2.5 months.
    * If desired, this filter may be entirely disabled by setting the value to 0.
    * The idea of this default filter is that data older than this is rarely
      helpful in troubleshooting cases in any event.
    * Defaults to 60, or approximately 2 months.  


##########################################################################################
# License manager settings for configuring the license pool(s)
##########################################################################################

[license]
master_uri = [self|&lt;uri&gt;] 
    * An example of &lt;uri&gt;: &lt;scheme&gt;://&lt;hostname&gt;:&lt;port&gt;
active_group = Enterprise | Trial | Forwarder | Free
# these timeouts only matter if you have a master_uri set to remote master
connection_timeout = 30
    * Maximum time (in seconds) to wait before connection to master times out
send_timeout = 30
    * Maximum time (in seconds) to wait before sending data to master times out
receive_timeout = 30
    * Maximum time (in seconds) to wait before receiving data from master times out

squash_threshold = &lt;positive integer&gt;
    * Advanced setting.  Periodically the indexer must report to license manager the
    data indexed broken down by source, sourcetype, host, and index.  If the number of distinct
    (source,sourcetype,host,index) tuples grows over the squash_threshold, we squash the
    {host,source} values and only report a breakdown by {sourcetype,index}.  This is to
    prevent explosions in memory + license_usage.log lines.  Set this only
    after consulting a Splunk Support engineer.
    * Default: 2000

report_interval = &lt;nonnegative integer&gt;[s|m|h]
    * Selects a time period for reporting in license usage to the license master.
    * This value is intended for very large deployments (hundreds of indexers)
      where a large number of indexers may overwhelm the license server.
    * The maximum permitted interval is 1 hour, and the minimum permitted
      interval is 1 minute.
    * May be expressed as a positive number of seconds, minutes or hours.
    * If no time unit is provided, seconds will be assumed.
    * Defaults to 1 minute, or 1m.

strict_pool_quota = &lt;boolean&gt;
    * Toggles strict pool quota enforcement
    * If set to true, members of pools will receive warnings for a given day if usage exceeds pool size regardless of whether overall stack quota was exceeded
    * If set to false, members of pool will only receive warnings if both pool usage exceeds pool size AND overall stack usage exceeds stack size
    * Defaults to true

pool_suggestion = &lt;string&gt;
    * Defaults to empty, which means this feature is disabled
    * Suggest a pool to the master for this slave.
    * The master will use this suggestion if the master doesn't have an explicit rule mapping the slave to a given pool (ie...no slave list for the relevant license stack contains this slave explictly)
    * If the pool name doesn't match any existing pool, it will be ignored, no error will be generated
    * This setting is intended to give an alternative management option for pool/slave mappings.  When onboarding an indexer, it may be easier to manage the mapping on the indexer itself via this setting rather than having to update server.conf on master for every addition of new indexer
    * NOTE: if you have multiple stacks and a slave maps to multiple pools, this feature is limitted in only allowing a suggestion of a single pool;  This is not a common scenario however.

[lmpool:auto_generated_pool_forwarder]
    * This is the auto generated pool for the forwarder stack

description = &lt;textual description of this license pool&gt;
quota = MAX|&lt;maximum amount allowed by this license&gt;
    * MAX indicates the total capacity of the license. You may have only 1 pool with MAX size in a stack
    * The quota can also be specified as a specific size eg. 20MB, 1GB etc
slaves = *|&lt;slave list&gt;
    * An asterix(*) indicates that any slave can connect to this pool
    * You can also specifiy a comma separated slave guid list
stack_id = forwarder
    * the stack to which this pool belongs

[lmpool:auto_generated_pool_free]
    * This is the auto generated pool for the free stack
    * field descriptions are the same as that for the "lmpool:auto_generated_pool_forwarder"

[lmpool:auto_generated_pool_enterprise]
    * This is the auto generated pool for the enterprise stack
    * field descriptions are the same as that for the "lmpool:auto_generated_pool_forwarder"

[lmpool:auto_generated_pool_fixed-sourcetype_&lt;sha256 hash of srctypes&gt;]
    * This is the auto generated pool for the enterprise fixed srctype stack
    * field descriptions are the same as that for the "lmpool:auto_generated_pool_forwarder"

[lmpool:auto_generated_pool_download_trial]
    * This is the auto generated pool for the download trial stack
    * field descriptions are the same as that for the "lmpool:auto_generated_pool_forwarder"

#########################################################################################
#
# Search head pooling configuration
#
# Changes to a search head's pooling configuration must be made to:
#
#     $SPLUNK_HOME/etc/system/local/server.conf
#
# In other words, you may not deploy the [pooling] stanza via an app, either on
# local disk or on shared storage.
#
# This is because these values are read before the configuration system itself
# has been completely initialized. Take the value of "storage", for example.
# This value cannot be placed within an app on shared storage because Splunk
# must use this value to find shared storage in the first place!
#
##########################################################################################

[pooling]

state = [enabled|disabled]
    * Enables or disables search head pooling.
    * Defaults to disabled.

storage = &lt;path to shared storage&gt;
    * All members of a search head pool must have access to shared storage.
    * Splunk will store configurations and search artifacts here.
    * On *NIX, this should be an NFS mount.
    * On Windows, this should be a UNC path to a Samba/CIFS share.

app_update_triggers = true|false|silent
    * Should this search head run update triggers for apps modified by other
      search heads in the pool?
    * For more information about update triggers specifically, see the
      [triggers] stanza in $SPLUNK_HOME/etc/system/README/app.conf.spec.
    * If set to true, this search head will attempt to reload inputs, indexes,
      custom REST endpoints, etc. stored within apps that are installed,
      updated, enabled, or disabled by other search heads.
    * If set to false, this search head will not run any update triggers. Note
      that this search head will still detect configuration changes and app
      state changes made by other search heads. It simply will not reload any
      components within Splunk that might care about those changes, like input
      processors or the HTTP server.
    * Setting a value of "silent" is like setting a value of "true", with one
      difference: update triggers will never result in restart banner messages
      or restart warnings in the UI. Any need to restart will instead by
      signaled only by messages in splunkd.log.
    * Defaults to true.

lock.timeout = &lt;time range string&gt;
    * Timeout for acquiring file-based locks on configuration files.
    * Splunk will wait up to this amount of time before aborting a configuration write.
    * Defaults to '10s' (10 seconds).

lock.logging = true|false
    * When acquiring a file-based lock, log information into the locked file.
    * This information typically includes:
        * Which host is acquiring the lock
        * What that host intends to do while holding the lock
    * There is no maximum filesize or rolling policy for this logging. If you
      enable this setting, you must periodically truncate the locked file
      yourself to prevent unbounded growth.
    * The information logged to the locked file is intended for debugging
      purposes only. Splunk makes no guarantees regarding the contents of the
      file. It may, for example, write padding NULs to the file or truncate the
      file at any time.
    * Defaults to false.

# The following two intervals interelate; the longest possible time for a state
# change to travel from one search pool member to the rest should be
# approximately the sum of these two timers.
poll.interval.rebuild = &lt;time range string&gt;
    * Rebuild or refresh in-memory configuration data structures at most this often.
    * Defaults to '1m' (1 minute).

poll.interval.check = &lt;time range string&gt;
    * Check on-disk configuration files for changes at most this often.
    * Defaults to '1m' (1 minute).

poll.blacklist.&lt;name&gt; = &lt;regex&gt;
    * Do not check configuration files for changes if they match this regular expression.
    * Example: Do not check vim swap files for changes -- .swp$


##########################################################################################
# High availability clustering configuration
##########################################################################################

[clustering]

mode = [master|slave|searchhead|disabled]
    * Sets operational mode for this cluster node.
    * Only one master may exist per cluster.
    * Defaults to disabled.

master_uri = [&lt;uri&gt; | clustermaster:stanzaName1, clustermaster:stanzaName2]
    * Only valid for mode=slave or searchhead
    * uri of the cluster master that this slave or searchhead should connect to.
    * An example of &lt;uri&gt;: &lt;scheme&gt;://&lt;hostname&gt;:&lt;port&gt;
    * Only for mode=searchhead - If the searchhead is a part of multiple clusters,
    * the master uris can be specified by a comma separated list.

pass4SymmKey = &lt;string&gt;
    * Secret shared among the nodes in the cluster to prevent any
      arbitrary node from connecting to the cluster. If a slave or
      searchhead is not configured with the same secret as the master,
      it will not be able to communicate with the master.
    * Not set by default.
    * If it is not set in the clustering stanza, the key will be looked in
      the general stanza

service_interval = &lt;positive integer&gt;
    * Only valid for mode=master
    * Specifies, in seconds, how often the master runs its service
      loop. In its service loop, the master checks the state of the
      peers and the buckets in the cluster and also schedules
      corrective action, if possible, for buckets that are not in
      compliance with replication policies.
    * Defaults to 1

multisite = [true|false]
    * Turns on the multisite feature for this master.
    * Make sure you set site parameters on the peers when you turn this to true.
    * Defaults to false.

cxn_timeout = &lt;seconds&gt;
    * Lowlevel timeout for establishing connection between cluster nodes.
    * Defaults to 60s.

send_timeout = &lt;seconds&gt;
    * Lowlevel timeout for sending data between cluster nodes.
    * Defaults to 60s.

rcv_timeout = &lt;seconds&gt;
    * Lowlevel timeout for receiving data between cluster nodes.
    * Defaults to 60s.

rep_cxn_timeout = &lt;seconds&gt;
    * Lowlevel timeout for establishing connection for replicating data.
    * Defaults to 5s.

rep_send_timeout = &lt;seconds&gt;
    * Lowlevel timeout for sending replication slice data between cluster nodes.
    * This is a soft timeout. When this timeout is triggered on source peer, 
      it tries to determine if target is still alive. If it is still alive, 
      it reset the timeout for another rep_send_timeout interval and continues.
      If target has failed or cumulative timeout has exceeded rep_max_send_timeout,
      replication fails.
    * Defaults to 5s.

rep_rcv_timeout = &lt;seconds&gt;
    * Lowlevel timeout for receiving acknowledgement data from peers.
    * This is a soft timeout. When this timeout is triggered on source peer, 
      it tries to determine if target is still alive. If it is still alive, 
      it reset the timeout for another rep_send_timeout interval and continues.
      If target has failed or cumulative timeout has exceeded rep_max_rcv_timeout,
      replication fails.
    * Defaults to 10s.

search_files_retry_timeout = &lt;seconds&gt;
    * Timeout after which request for search files from a peer is aborted.
    * To make a bucket searchable, search specific files are copied from another 
      source peer with search files. If search files on source peers are undergoing
      chances, it asks requesting peer to retry after some time. If cumulative
      retry period exceeds specified timeout, the requesting peer aborts the request and
      requests search files from another peer in the cluster that may have search files.
    * Defaults to 600s.

rep_max_send_timeout = &lt;seconds&gt;
    * Maximum send timeout for sending replication slice data between cluster nodes.
    * On rep_send_timeout source peer determines if total send timeout has exceeded
      rep_max_send_timeout. If so, replication fails.
    * If cumulative rep_send_timeout exceeds rep_max_send_timeout, replication fails.
    * Defaults to 600s.

rep_max_rcv_timeout = &lt;seconds&gt;
    * Maximum cumulative receive timeout for receiving acknowledgement data from peers.
    * On rep_rcv_timeout source peer determines if total receive timeout has exceeded
      rep_max_rcv_timeout. If so, replication fails.
    * Defaults to 600s.

replication_factor = &lt;positive integer&gt;
    * Only valid for mode=master. 
    * Determines how many copies of rawdata are created in the cluster.
    * Use site_replication_factor instead of this in case multisite is turned on.
    * Must be greater than 0.
    * Defaults to 3

site_replication_factor = &lt;comma-separated string&gt;
    * Only valid for mode=master and is only used if multisite is true.
    * This specifies the per-site replication policy for any given
      bucket represented as a comma-separated list of per-site entries.
    * Currently specified globally and applies to buckets in all
      indexes.
    * Each entry is of the form &lt;site-id&gt;:&lt;positive integer&gt; which
      represents the number of copies to make in the specified site
    * Valid site-ids include two mandatory keywords and optionally
      specific site-ids from site1 to site63
    * The mandatory keywords are:
      - origin: Every bucket has a origin site which is the site of
      the peer that originally created this bucket. The notion of
      'origin' makes it possible to specify a policy that spans across
      multiple sites without having to enumerate it per-site.
      - total: The total number of copies we want for each bucket. 
    * When a site is the origin, it could potentially match both the
      origin and a specific site term. In that case, the max of the
      two is used as the count for that site.
    * The total must be greater than or equal to sum of all the other
      counts (including origin).
    * The difference between total and the sum of all the other counts
      is distributed across the remaining sites.
    * Example 1: site_replication_factor = origin:2, total:3
      Given a cluster of 3 sites, all indexing data, every site has 2
      copies of every bucket ingested in that site and one rawdata
      copy is put in one of the other 2 sites.
    * Example 2: site_replication_factor = origin:2, site3:1, total:3
      Given a cluster of 3 sites, 2 of them indexing data, every
      bucket has 2 copies in the origin site and one copy in site3. So
      site3 has one rawdata copy of buckets ingested in both site1 and
      site2 and those two sites have 2 copies of their own buckets.
    * Defaults to origin:2, total:3

search_factor = &lt;positive integer&gt;
    * Only valid for mode=master 
    * Determines how many buckets will have index structures pre-built.
    * Must be less than or equal to replication_factor and greater than 0.
    * Defaults to 2.

site_search_factor = &lt;comma-separated string&gt;
    * Only valid for mode=master and is only used if multisite is true.
    * This specifies the per-site policy for searchable copies for any
      given bucket represented as a comma-separated list of per-site
      entries.
    * This is similar to site_replication_factor. Please see that
      entry for more information on the syntax.
    * Defaults to origin:1, total:2

available_sites = &lt;comma-separated string&gt;
    * Only valid for mode=master and is only used if multisite is true.
    * This is a comma-separated list of all the sites in the cluster. 
    * Defaults to an empty string. So if multisite is turned on this needs
      to be explicitly set

heartbeat_timeout = &lt;positive integer&gt;
    * Only valid for mode=master
    * Determines when the master considers a slave down.  Once a slave
      is down, the master will initiate fixup steps to replicate
      buckets from the dead slave to its peers.
    * Defaults to 60s.

access_logging_for_heartbeats = &lt;bool&gt;
    * Only valid for mode=master
    * Enables/disables logging to splunkd_access.log for peer heartbeats
    * defaults to false (logging disabled)
    * NOTE: you do not have to restart master to set this config parameter.
      Simply run the cli command on master:

     &nbsp;% splunk edit cluster-config -access_logging_for_heartbeats &lt;true|false&gt;


restart_timeout = &lt;positive integer&gt;
    * Only valid for mode=master
    * This is the amount of time the master waits for a peer to come
      back when the peer is restarted (to avoid the overhead of
      trying to fixup the buckets that were on the peer).
    * Note that currently this only works if the peer is restarted vi the UI.

quiet_period = &lt;positive integer&gt;
    * Only valid for mode=master
    * This determines the amount of time for which the master is quiet
      right after it starts. During this period the master does not
      initiate any action but is instead waiting for the slaves to
      register themselves. At the end of this time period, it builds
      its view of the cluster based on the registered information and
      starts normal processing.
    * Defaults to 60s.
      
generation_poll_interval = &lt;positive integer&gt;
    * Only valid if mode=master or mode=searchhead
    * Determines how often the searchhead polls the master for generation information.
    * Defaults to 60s.

max_peer_build_load = &lt;integer&gt;
    * This is the maximum number of concurrent tasks to make buckets
      searchable that can be assigned to a peer.
    * Defaults to 2.

max_peer_rep_load = &lt;integer&gt;
    * This is the maximum number of concurrent non-streaming
      replications that a peer can take part in as a target.
    * Defaults to 5.

max_replication_errors = &lt;integer&gt;
    * Currently only valid for mode=slave
    * This is the maximum number of consecutive replication errors
      (currently only for hot bucket replication) from a source peer
      to a specific target peer. Until this limit is reached, the
      source continues to roll hot buckets on streaming failures to
      this target. After the limit is reached, the source will no
      longer roll hot buckets if streaming to this specific target
      fails. This is reset if at least one successful (hot bucket)
      replication occurs to this target from this source.
    * Defaults to 3.
    * The special value of 0 turns off this safeguard; so the source
      always rolls hot buckets on streaming error to any target.

searchable_targets = true|false
    * Only valid for mode=master
    * Tells the master to make some replication targets searchable
      even while the replication is going on. This only affects
      hot bucket replication for now.
    * Defaults to true

searchable_target_sync_timeout = &lt;integer&gt;
    * Only valid for mode=slave
    * If a hot bucket replication connection is inactive for this time
      (in seconds), a searchable target flushes out any pending search
      related in-memory files.
    * Note that regular syncing - when the data is flowing through
      regularly and the connection is not inactive - happens at a
      faster rate (default of 5 secs controlled by
      streamingTargetTsidxSyncPeriodMsec in indexes.conf).
    * The special value of 0 turns off this timeout behaviour.
    * Defaults to 60 (seconds)

target_wait_time = &lt;positive integer&gt;
    * Only valid for mode=master.
    * Specifies the time that the master waits for the target of a replication to
      register itself before it services the bucket again and potentially schedules
      another fixup.
    * Defaults to 150s

commit_retry_time = &lt;positive integer&gt;
    * Only valid for mode=master
    * Specifies the interval after which, if the last generation commit failed,
      the master forces a retry. A retry is usually automatically kicked off
      after the appropriate events. This is just a backup to make sure that the
       master does retry no matter what.
    * Defaults to 300s

percent_peers_to_restart = &lt;integer between 0-100&gt;
    * suggested percentage of maximum peers to restart for rolling-restart
    * actual percentage may vary due to lack of granularity for smaller peer sets
    * regardless of setting, a minimum of 1 peer will be restarted per round

auto_rebalance_primaries = &lt;bool&gt;
    * Only valid for mode=master
    * Specifies if the master should automatically rebalance bucket
      primaries on certain triggers. Currently the only defined
      trigger is when a peer registers with the master. When a peer
      registers, the master redistributes the bucket primaries so the
      cluster can make use of any copies in the incoming peer.
    * Defaults to true.

idle_connections_pool_size = &lt;int&gt;
    * Only valid for mode=master
    * Specifies how many idle http(s) connections we should keep alive to reuse.
      Reusing connections improves the time it takes to send messages to peers in
      the cluster.
    * -1 (default) corresponds to "auto", letting the master determine the number
      of connections to keep around based on the number of peers in the cluster.

use_batch_mask_changes = &lt;bool&gt;
    * Only valid for mode=master
    * Specifies if the master should process bucket mask changes in 
      batch or inidividually one by one.
    * Defaults to true.
    * Set to false when there are 6.1 peers in the cluster for backwards compatibility.

service_jobs_msec = &lt;positive integer&gt;
    * Only valid for mode=master
    * Max time in milliseconds cluster master spends in servicing finished jobs
      per service call. Increase this if metrics.log has very high current_size values.
    * Defaults to 100ms.

register_replication_address = &lt;IP address, or fully qualified machine/domain name&gt;
    * Only valid for mode=slave
    * This is the address on which a slave will be available for accepting
      replication data. This is useful in the cases where a slave host machine 
      has multiple interfaces and only one of them can be reached by another 
      splunkd instance
    
register_forwarder_address = &lt;IP address, or fully qualified machine/domain name&gt;
    * Only valid for mode=slave
    * This is the address on which a slave will be available for accepting
      data from forwarder.This is useful in the cases where a splunk host machine 
      has multiple interfaces and only one of them can be reached by another 
      splunkd instance.
 
register_search_address = &lt;IP address, or fully qualified machine/domain name&gt;
    * Only valid for mode=slave
    * This is the address on which a slave will be available as search head.
      This is useful in the cases where a splunk host machine has multiple 
      interfaces and only one of them can be reached by another splunkd instance.

executor_workers = &lt;positive integer&gt;
    * Only valid if mode=master or mode=slave
    * Number of threads that can be used by the clustering threadpool.
    * Defaults to 10. A value of 0 will default to 1.

heartbeat_period = &lt;non-zero positive integer&gt;
    * Only valid for mode=slave
    * Controls the frequency the slave attempts to send heartbeats

enableS2SHeartbeat = true|false
    * Only valid for mode=slave
    * Splunk will monitor each replication connection for presence of heartbeat, 
      and if the heartbeat is not seen for s2sHeartbeatTimeout seconds, it will 
      close the connection.
    * Defaults to true.

s2sHeartbeatTimeout = &lt;seconds&gt;
   * This specifies the global timeout value for monitoring heartbeats on replication connections.
   * Splunk will will close a replication connection if heartbeat is not seen for s2sHeartbeatTimeout seconds.
   * Defaults to 600 seconds (10 minutes). Replication source sends heartbeat every 30 second.

[clustermaster:stanza1]
   * Only valid for mode=searchhead when the searchhead is a part of multiple clusters.

master_uri = &lt;uri&gt;
    * Only valid for mode=searchhead when present in this stanza.
    * uri of the cluster master that this searchhead should connect to.

pass4SymmKey = &lt;string&gt;
    * Secret shared among the nodes in the cluster to prevent any
      arbitrary node from connecting to the cluster. If a searchhead
      is not configured with the same secret as the master,
      it will not be able to communicate with the master.
    * Not set by default.
    * If it is not present here, the key in the clustering stanza will be used. If it is not present 
      in the clustering stanza, the value in the general stanza will be used.

site = &lt;site-id&gt;
    * Specifies the site this searchhead belongs to for this particular master when multisite is enabled (see below).
    * Valid values for site-id include site1 to site63.

multisite = [true|false]
    * Turns on the multisite feature for this master_uri for the searchhead.
    * Make sure the master has the multisite feature turned on.
    * Make sure you specify the site in case this is set to true. If no configuration is found in
      the clustermaster stanza, we default to any value for site that might be defined in the [general]
      stanza. 
    * Defaults to false.

[replication_port://&lt;port&gt;]
    # Configure Splunk to listen on a given TCP port for replicated data from another cluster member.
    # If mode=slave is set in the [clustering] stanza at least one replication_port must be configured and not disabled.

disabled = true|false
    * Set to true to disable this replication port stanza.
    * Defaults to false.

listenOnIPv6 = no|yes|only
    * Toggle whether this listening port will listen on IPv4, IPv6, or both.
    * If not present, the setting in the [general] stanza will be used.

acceptFrom = &lt;network_acl&gt; ...
    * Lists a set of networks or addresses to accept connections from.  These rules are separated by commas or spaces
    * Each rule can be in the following forms:
    *   1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")
    *   2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")
    *   3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")
    *   4. A single '*' which matches anything
    * Entries can also be prefixed with '!' to cause the rule to reject the
      connection.  Rules are applied in order, and the first one to match is
      used.  For example, "!10.1/16, *" will allow connections from everywhere
      except the 10.1.*.* network.
    * Defaults to "*" (accept replication data from anywhere)

[replication_port-ssl://&lt;port&gt;]
    * This configuration is same as replication_port stanza above but uses SSL.

disabled = true|false
    * Set to true to disable this replication port stanza.
    * Defaults to false.

listenOnIPv6 = no|yes|only
    * Toggle whether this listening port will listen on IPv4, IPv6, or both.
    * If not present, the setting in the [general] stanza will be used.

acceptFrom = &lt;network_acl&gt; ...
    * This setting is same as setting in replication_port stanza defined above.

serverCert = &lt;path&gt;
    * Full path to file containing private key and server certificate.
    * There is no default value.
    
password = &lt;string&gt;
    * Server certificate password, if any.
    * There is no default value.

rootCA = &lt;string&gt;
    * The path to the file containing the SSL certificate for root certifying authority.
    * The file may also contain root and intermediate certificates, if required.
    * There is no default value.

cipherSuite = &lt;cipher suite string&gt;
    * If set, uses the specified cipher string for the SSL connection.
    * If not set, uses the default cipher string.
    * provided by OpenSSL.  This is used to ensure that the server does not
      accept connections using weak encryption protocols.

supportSSLV3Only = true|false
    * If true, it only accept connections from SSLv3 clients.
    * Default is false.

compressed = true|false
    * If true, it enables compression on SSL.
    * Default is true.

requireClientCert = true|false
    * Requires that any peer that connects to replication port has a certificate that
      can be validated by certificate authority specified in rootCA.
    * Default is false.

allowSslRenegotiation = true|false
    * In the SSL protocol, a client may request renegotiation of the connection
      settings from time to time.
    * Setting this to false causes the server to reject all renegotiation
      attempts, breaking the connection.  This limits the amount of CPU a
      single TCP connection can use, but it can cause connectivity problems
      especially for long-lived connections.
    * Defaults to true.


##########################################################################################
# Introspection settings
##########################################################################################

[introspection:generator:disk_objects]
    * For 'introspection_generator_addon', packaged with Splunk; provides the data ("i-data") consumed, and reported on, by 'introspection_viewer_app' (due to ship with a future release).
    * This stanza controls the collection of i-data about: indexes; bucket superdirectories (homePath, coldPath, ...); volumes; search dispatch artifacts.

acquireExtra_i_data = true | false
    * If true, extra Disk Objects i-data is emitted; you can gain more insight into your site, but at the cost of greater resource consumption both directly (the collection itself) and indirectly (increased disk and bandwidth utilization, to store the produced i-data).
    * Please consult documentation for list of regularly emitted Disk Objects i-data, and extra Disk Objects i-data, appropriate to your release.
    * Defaults to: false.

collectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of Disk Objects i-data collection; higher frequency (hence, smaller period) gives a more accurate picture, but at the cost of greater resource consumption both directly (the collection itself) and indirectly (increased disk and bandwidth utilization, to store the produced i-data).
    * Defaults to: 600 (10 minutes).


[introspection:generator:disk_objects__fishbucket]
	* This stanza controls the collection of i-data about: $SPLUNK_DB/fishbucket, where we persist per-input status of file-based inputs.
	* Inherits the values of 'acquireExtra_i_data' and 'collectionPeriodInSecs' attributes from the 'introspection:generator:disk_objects' stanza, but may be enabled/disabled independently of it.


[introspection:generator:disk_objects__partitions]
	* This stanza controls the collection of i-data about: disk partition space utilization.
	* Inherits the values of 'acquireExtra_i_data' and 'collectionPeriodInSecs' attributes from the 'introspection:generator:disk_objects' stanza, but may be enabled/disabled independently of it.


[introspection:generator:resource_usage]
    * For 'introspection_generator_addon', packaged with Splunk; provides the data ("i-data") consumed, and reported on, by 'introspection_viewer_app' (due to ship with a future release).
    * "Resource Usage" here refers to: CPU usage; scheduler overhead; main (physical) memory; virtual memory; pager overhead; swap; I/O; process creation (a.k.a. forking); file descriptors; TCP sockets; receive/transmit networking bandwidth.
    ** Resource Usage i-data is collected at both hostwide and per-process levels; the latter, only for processes associated with this SPLUNK_HOME.
    *** Per-process i-data for Splunk search processes will include additional, search-specific, information.

acquireExtra_i_data = true | false
    * If true, extra Resource Usage i-data is emitted; you can gain more insight into your site, but at the cost of greater resource consumption both directly (the collection itself) and indirectly (increased disk and bandwidth utilization, to store the produced i-data).
    * Please consult documentation for list of regularly emitted Resource Usage i-data, and extra Resource Usage i-data, appropriate to your release.
    * Defaults to: false.

collectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of Resource Usage i-data collection; higher frequency (hence, smaller period) gives a more accurate picture, but at the cost of greater resource consumption both directly (the collection itself) and indirectly (increased disk and bandwidth utilization, to store the produced i-data).
    * Defaults to: 600 (10 minutes) on UFs, 10 (1/6th of a minute) on non-UFs.

[introspection:generator:kvstore]
    * For 'introspection_generator_addon', packaged with Splunk; 
    * "KV Store" here refers to: statistics information about KV Store process.\

serverStatsCollectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of KV Store server status collection;
    * Defaults to: 27 seconds.

collectionStatsCollectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of KV Store db statistics collection;
    * Defaults to: 600 seconds.

profilingStatsCollectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of KV Store profiling data collection;
    * Defaults to: 5 seconds

rsStatsCollectionPeriodInSecs = &lt;positive integer&gt;
    * Controls frequency of KV Store replica set stats collection;
    * Defaults to: 60 seconds

##########################################################################################
# Settings used to control commands started by Splunk
##########################################################################################

[commands:user_configurable]

prefix = &lt;path&gt;
    * All non-internal commands started by splunkd will be prefixed with this string, allowing for "jailed" command execution.
    * Should be only one word.  In other words, commands are supported, but commands and arguments are not.
    * Applies to commands such as: search scripts, scripted inputs, SSL certificate generation scripts.  (Any commands that are user-configurable).
    * Does not apply to trusted/non-configurable command executions, such as: splunk search, splunk-optimize, gunzip.
    * Default is empty (no prefix).

##########################################################################################
# search head clustering configuration
##########################################################################################

[shclustering]
disabled = true|false
    * Disables or enables search head clustering on this instance.
    * Defaults to true; that is, disabled.
    * When enabled, the captain needs to be selected via a
      bootstrap mechanism. Once bootstrapped, further captain
      selections are made via a dynamic election mechanism.
    * When enabled, you will also need to specify the cluster member's own server
      address / management uri for identification purpose. This can be
      done in 2 ways: by specifying the mgmt_uri attribute individually on
      each member or by specifying pairs of 'GUID, mgmt-uri' strings in the 
      servers_list attribute.

mgmt_uri = [ mgmt-URI ]
    * The management uri is used to identify the cluster member's own address to itself.
    * Either mgmt_uri or servers_list is necessary.
    * mgmt_uri is simpler to author but is unique for each member.
    * servers_list is more involved, but can be copied as a config string to
      all members in the cluster.

servers_list = [ &lt;(GUID, mgmt-uri);&gt;+ ]
    * A semicolon separated list of instance GUIDs and management URIs.
    * Each member will use its GUID to identify its own management URI.

adhoc_searchhead = &lt;bool&gt;
    * This setting configures a member as an adhoc searchhead; i.e., the member will not
      run any scheduled jobs.
    * Use the setting captain_is_adhoc_searchhead to reduce compute load on the captain.
    * Defaults to false.

captain_is_adhoc_searchhead = &lt;bool&gt;
    * This setting prohibits the captain from running scheduled jobs. Captain will be dedicated to
      controlling the activities of the cluster, but can also run adhoc search jobs from clients.
    * Defaults to false.

replication_factor = &lt;positive integer&gt;
    * Determines how many copies of search artifacts are created in the cluster.
    * This must be set to the same value on all members.
    * Defaults to 3.

pass4SymmKey = &lt;string&gt;
    * Secret shared among the members in the search head cluster to prevent any
      arbitrary instance from connecting to the cluster. 
    * All members must use the same value.
    * If set in the [shclustering] stanza, it takes precedence over any setting in the [general] stanza.
    * Defaults to 'changeme' from the [general] stanza in the default server.conf.

async_replicate_on_proxy = &lt;bool&gt;
    * If the jobs/${sid}/results REST endpoint had to be proxied to a different member due to
      missing local replica, this attribute will automatically schedule an async replication to that 
      member when set to true.
    * Default is true.

master_dump_service_periods = &lt;int&gt;
    * if SHPMaster info is switched on in log.cfg, then captain statistics will be dumped in splunkd.log
      after the specified number of service periods. Purely a debugging aid.
    * Default is 500.

long_running_jobs_poll_period = &lt;int&gt;
    * Long running delegated jobs will be polled by the captain every "long_running_jobs_poll_period"
      seconds to ascertain whether they are still running, in order to account for potential node/member failure.
    * Default is 600, i.e. 10 minutes

scheduling_heuristic = &lt;string&gt;
    * This setting configures the job distribution heuristic on the captain. 
    * There are currently two supported strategies: 'round_robin' or 'scheduler_load_based'.
    * Default is 'scheduler_load_based'.

id = &lt;GUID&gt;
    * Unique identifier for this cluster as a whole, shared across all cluster members.
    * By default, Splunk will arrange for a unique value to be generated and shared across all members.

cxn_timeout = &lt;seconds&gt;
    * Low-level timeout for establishing connection between cluster members.
    * Defaults to 60s.

send_timeout = &lt;seconds&gt;
    * Low-level timeout for sending data between search head cluster members.
    * Defaults to 60s.

rcv_timeout = &lt;seconds&gt;
    * Low-level timeout for receiving data between search head cluster members.
    * Defaults to 60s.

cxn_timeout_raft = &lt;seconds&gt;
    * Low-level timeout for establishing connection between search head cluster members for the raft protocol.
    * Defaults to 2s.

send_timeout_raft = &lt;seconds&gt;
    * Low-level timeout for sending data between search head cluster members for the raft protocol.
    * Defaults to 5s.

rcv_timeout_raft = &lt;seconds&gt;
    * Low-level timeout for receiving data between search head cluster members for the raft protocol.
    * Defaults to 5s.

rep_cxn_timeout = &lt;seconds&gt;
    * Low-level timeout for establishing connection for replicating data.
    * Defaults to 5s.

rep_send_timeout = &lt;seconds&gt;
    * Low-level timeout for sending replication slice data between cluster members.
    * This is a soft timeout. When this timeout is triggered on source peer, 
      it tries to determine if target is still alive. If it is still alive, 
      it reset the timeout for another rep_send_timeout interval and continues.
      If target has failed or cumulative timeout has exceeded rep_max_send_timeout,
      replication fails.
    * Defaults to 5s.

rep_rcv_timeout = &lt;seconds&gt;
    * Low-level timeout for receiving acknowledgement data from members.
    * This is a soft timeout. When this timeout is triggered on source member, 
      it tries to determine if target is still alive. If it is still alive, 
      it reset the timeout for another rep_send_timeout interval and continues.
      If target has failed or cumulative timeout has exceeded rep_max_rcv_timeout,
      replication fails.
    * Defaults to 10s.

rep_max_send_timeout = &lt;seconds&gt;
    * Maximum send timeout for sending replication slice data between cluster members.
    * On rep_send_timeout source peer determines if total send timeout has exceeded
      rep_max_send_timeout. If so, replication fails.
    * If cumulative rep_send_timeout exceeds rep_max_send_timeout, replication fails.
    * Defaults to 600s.

rep_max_rcv_timeout = &lt;seconds&gt;
    * Maximum cumulative receive timeout for receiving acknowledgement data from members.
    * On rep_rcv_timeout source member determines if total receive timeout has exceeded
      rep_max_rcv_timeout. If so, replication fails.
    * Defaults to 600s.

log_heartbeat_append_entries = &lt;bool&gt;
    * Default is false.
    * Toggles the logging in splunkd_access.log of the low-level heartbeats between
    * members. These heartbeats are used to maintain the captain's authority over other members.

election_timeout_ms = &lt;positive_integer&gt;
    * The amount of time that a member will wait before trying to become the captain.
    * Half of this value is the heartbeat period.
    * A very low value of election_timeout_ms can lead to unnecessary captain elections.
   * The default is 60000ms, or 1 minute.

election_timeout_2_hb_ratio = &lt;positive_integer&gt;
    * The ratio between the election timeout and the heartbeat time.
    * A typical ratio between 5 - 20 is desirable. Default is 12 to keep the heartbeat time at 5s.
    * This ratio determines the number of heartbeat attempts that would fail
    * before a member starts to timeout and tries to become the captain.

heartbeat_timeout = &lt;positive integer&gt;
    * Determines when the captain considers a member down. Once a member
      is down, the captain will initiate fixup steps to replicate
      artifacts from the dead member to its peers.
    * Defaults to 60s.

access_logging_for_heartbeats = &lt;bool&gt;
    * Only valid on captain
    * Enables/disables logging to splunkd_access.log for member heartbeats
    * defaults to false (logging disabled)
    * NOTE: you do not have to restart captain to set this config parameter.
      Simply run the cli command on master:

     &nbsp;% splunk edit shcluster-config -access_logging_for_heartbeats &lt;true|false&gt;

restart_timeout = &lt;positive integer&gt;
    * This is the amount of time the captain waits for a member to come
      back when the instance is restarted (to avoid the overhead of
      trying to fixup the artifacts that were on the peer).

quiet_period = &lt;positive integer&gt;
    * This determines the amount of time for which a newly elected
      captain waits for members to join. During this period the
      captain does not initiate any fixups but instead waits for the
      members to register themselves. Job scheduling and conf
      replication still happen as usual during this time. At the end
      of this time period, the captain builds its view of the cluster
      based on the registered peers and starts normal
      processing.
    * Defaults to 60s.
      
max_peer_rep_load = &lt;integer&gt;
    * This is the maximum number of concurrent replications that a 
      member can take part in as a target.
    * Defaults to 5.

target_wait_time = &lt;positive integer&gt;
    * Specifies the time that the captain waits for the target of a replication to
      register itself before it services the artifact again and potentially schedules
      another fixup.
    * Defaults to 150s.

percent_peers_to_restart = &lt;integer between 0-100&gt;
    * The percentage of members to restart at one time during rolling restarts.
    * Actual percentage may vary due to lack of granularity for smaller peer sets
    * regardless of setting, a minimum of 1 peer will be restarted per round.
    * Do not set this attribute to a value greater than 20%. Otherwise, issues can arise
      during the captain election process.

register_replication_address = &lt;IP address, or fully qualified machine/domain name&gt;
    * This is the address on which a member will be available for accepting
      replication data. This is useful in the cases where a member host machine 
      has multiple interfaces and only one of them can be reached by another 
      splunkd instance
    
executor_workers = &lt;positive integer&gt;
    * Number of threads that can be used by the search head clustering threadpool.
    * Defaults to 10. A value of 0 will be interpreted as 1.

heartbeat_period = &lt;non-zero positive integer&gt;
    * Controls the frequency with which the member attempts to send heartbeats.

enableS2SHeartbeat = true|false
    * Splunk will monitor each replication connection for presence of heartbeat. 
      If the heartbeat is not seen for s2sHeartbeatTimeout seconds, it will 
      close the connection.
    * Defaults to true.

s2sHeartbeatTimeout = &lt;seconds&gt;
   * This specifies the global timeout value for monitoring heartbeats on replication connections.
   * Splunk will will close a replication connection if heartbeat is not seen for s2sHeartbeatTimeout seconds.
   * Defaults to 600 seconds (10 minutes). Replication source sends heartbeat every 30 second.

#proxying related
sid_proxying = &lt;bool&gt;
   * Enable or disable search artifact proxying. Changing this will impact the proxying of search results, 
   * and jobs feed will not be cluster-aware. Only for internal/expert use.
   * Defaults to true.

ss_proxying = &lt;bool&gt;
   * Enable or disable saved search proxying to captain. Changing this will impact the behavior of Searches and Reports Page.
   * Only for internal/expert use.
   * Defaults to true.

ra_proxying = &lt;bool&gt;
   * Enable or disable saved report acceleration summaries proxying to captain. Changing this will impact the 
   * behavior of report acceleration summaries page. Only for internal/expert use.
   * Defaults to true.

alert_proxying = &lt;bool&gt;
   * Enable or disable alerts proxying to captain. Changing this will impact the behavior of alerts, and essentially
   * make them not cluster-aware. Only for internal/expert use.
    * Defaults to true.

conf_replication_period = &lt;int&gt;
    * Controls how often a cluster member replicates configuration changes.
    * A value of 0 disables automatic replication of configuration changes.

conf_replication_max_pull_count = &lt;int&gt;
    * Controls the maximum number of configuration changes a member will replicate from the captain at one time.
    * A value of 0 disables any size limits.
    * Defaults to 1000.

conf_replication_max_push_count = &lt;int&gt;
    * Controls the maximum number of configuration changes a member will replicate to the captain at one time.
    * A value of 0 disables any size limits.
    * Defaults to 100.

conf_replication_include.&lt;conf_file_name&gt; = &lt;bool&gt;
    * Controls whether Splunk replicates changes to a particular type of *.conf file, along with any associated permissions in *.meta files.
    * Defaults to false.

conf_replication_summary.whitelist.&lt;name&gt; = &lt;whitelist_pattern&gt;
    * Whitelist files to be included in configuration replication summaries.

conf_replication_summary.blacklist.&lt;name&gt; = &lt;blacklist_pattern&gt;
    * Blacklist files to be excluded from configuration replication summaries.

conf_replication_summary.concerning_file_size = &lt;int&gt;
    * Any individual file within a configuration replication summary that is larger than this value (in MB) will trigger a splunkd.log warning message.
    * Defaults to 50.

conf_replication_summary.period = &lt;timespan&gt;
    * Controls how often configuration replication summaries are created.
    * Defaults to '1m' (1 minute).

conf_replication_purge.eligibile_count = &lt;int&gt;
    * Controls how many configuration changes must be present before any become eligible for purging.
    * In other words: controls the minimum number of configuration changes Splunk will remember for replication purposes.
    * Defaults to 20000.

conf_replication_purge.eligibile_age = &lt;timespan&gt;
    * Controls how old a configuration change must be before it is eligible for purging.
    * Defaults to '1d' (1 day).

conf_replication_purge.period = &lt;timespan&gt;
    * Controls how often configuration changes are purged.
    * Defaults to '1h' (1 hour).

conf_deploy_repository = &lt;path&gt;
    * Full path to directory containing configurations to deploy to cluster members.

conf_deploy_staging = &lt;path&gt;
    * Full path to directory where preprocessed configurations may be written before being deployed cluster members.

conf_deploy_concerning_file_size = &lt;int&gt;
    * Any individual file within &lt;conf_deploy_repository&gt; that is larger than this value (in MB) will trigger a splunkd.log warning message.
    * Defaults to: 50

conf_deploy_fetch_url = &lt;URL&gt;
    * Specifies the location of the deployer from which members fetch the configuration bundle.
    * This value must be set to a &lt;URL&gt; in order for the configuration bundle to be fetched.
    * Defaults to empty.

conf_deploy_fetch_mode = auto|replace|none
    * Controls configuration bundle fetching behavior when the member starts up.
    * When set to "replace", a member checks for a new configuration bundle on every startup.
    * When set to "none", a member does not fetch the configuration bundle on startup.
    * Regarding "auto":
        * If no configuration bundle has yet been fetched, "auto" is equivalent to "replace".
        * If the configuration bundle has already been fetched, "auto" is equivalent to "none".
    * Defaults to "replace".

[replication_port://&lt;port&gt;]
    # Configures the member to listen on a given TCP port for replicated data from another cluster member.
    * At least one replication_port must be configured and not disabled.

disabled = true|false
    * Set to true to disable this replication port stanza.
    * Defaults to false.

listenOnIPv6 = no|yes|only
    * Toggle whether this listening port will listen on IPv4, IPv6, or both.
    * If not present, the setting in the [general] stanza will be used.

acceptFrom = &lt;network_acl&gt; ...
    * Lists a set of networks or addresses to accept connections from. These rules are separated by commas or spaces.
    * Each rule can be in the following forms:
    *   1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")
    *   2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")
    *   3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")
    *   4. A single '*' which matches anything
    * Entries can also be prefixed with '!' to cause the rule to reject the
      connection.  Rules are applied in order, and the first one to match is
      used.  For example, "!10.1/16, *" will allow connections from everywhere
      except the 10.1.*.* network.
    * Defaults to "*" (accept replication data from anywhere)

[replication_port-ssl://&lt;port&gt;]
    * This configuration is same as replication_port stanza above but uses SSL.

disabled = true|false
    * Set to true to disable this replication port stanza.
    * Defaults to false.

listenOnIPv6 = no|yes|only
    * Toggle whether this listening port will listen on IPv4, IPv6, or both.
    * If not present, the setting in the [general] stanza will be used.

acceptFrom = &lt;network_acl&gt; ...
    * This setting is same as setting in replication_port stanza defined above.

serverCert = &lt;path&gt;
    * Full path to file containing private key and server certificate.
    * There is no default value.
    
password = &lt;string&gt;
    * Server certificate password, if any.
    * There is no default value.

rootCA = &lt;string&gt;
    * The path to the file containing the SSL certificate for root certifying authority.
    * The file may also contain root and intermediate certificates, if required.
    * There is no default value.

cipherSuite = &lt;cipher suite string&gt;
    * If set, uses the specified cipher string for the SSL connection.
    * If not set, uses the default cipher string.
    * provided by OpenSSL.  This is used to ensure that the server does not
      accept connections using weak encryption protocols.

supportSSLV3Only = true|false
    * If true, it only accept connections from SSLv3 clients.
    * Default is false.

compressed = true|false
    * If true, it enables compression on SSL.
    * Default is true.

requireClientCert = true|false
    * Requires that any peer that connects to replication port has a certificate that
      can be validated by certificate authority specified in rootCA.
    * Default is false.

allowSslRenegotiation = true|false
    * In the SSL protocol, a client may request renegotiation of the connection
      settings from time to time.
    * Setting this to false causes the server to reject all renegotiation
      attempts, breaking the connection.  This limits the amount of CPU a
      single TCP connection can use, but it can cause connectivity problems
      especially for long-lived connections.
    * Defaults to true.

##########################################################################################    
# KV Store configuration
##########################################################################################
[kvstore]

disabled = true|false
    * Set to true to disable this replication port stanza.
    * Defaults to false.

port = &lt;port&gt;
    * Port to connect to the KV Store server.
    * Defaults to 8191.

replicaset = &lt;replset&gt;
    * Replicaset name.
    * Defaults to splunkrs.

distributedLookupTimeout = &lt;seconds&gt;
    * Timeout in seconds used for distributed environment when a lookup is executed 
      from the indexer to the search head.
    * Defaults to 30 seconds.

shutdownTimeout = &lt;seconds&gt;
    * Time in seconds to wait for a clean shutdown of the KV Store. If this time is 
      reached after signaling for a shutdown, KV Store will be terminated forcibly.
    * Defaults to 10 seconds.

initAttempts = &lt;int&gt;
    * The maximum number of attempts to initialize the KV Store when starting splunkd.
    * Defaults to 300.

replication_host = &lt;host&gt;
    * The host name to access the KV Store.
    * This setting has no effect on a single Splunk instance.
    * In search head clustering, if you don't specify a value for "kvstore/replication_host",
      the host you specify for "shclustering/register_replication_address" is used for KV Store connection strings and replication.
    * In search head pooling, this host value is a requirement for using KV Store. 
    * This is the address on which a kvstore will be available for accepting
      remotely.

verbose = true|false
    * Set to true to enable verbose logging.
    * Defaults to false.

dbPath = &lt;path&gt;
    * Path where KV Store data is stored.
    * Changing this directory after initial startup does not move existing data. 
      The contents of the directory should be manually moved to the new location.
    * Defaults to $SPLUNK_DB/kvstore.

oplogSize = &lt;int&gt;
    * The size of the replication operation log in MB.
    * Defaults to 1000MB (1GB).
    * Once the KV Store has created the oplog for the first time, changing this setting 
      will NOT affect the size of the oplog. A full backup and restart of the KV Store 
      will be required.
    * Do not change this setting without first consulting with Splunk Support. 

replicationWriteTimeout = &lt;int&gt;
    * Timeout in seconds indicating when to save KV Store operations.
    * Used for replication environments (search head clustering or search head pooling).
    * Defaults to 1800 seconds (30 minutes).

caCertPath = &lt;filepath&gt;
    * Public key of the signing authority. 
    * If specified, it will be used in KV Store SSL connections and authentication.
    * Must be specified if FIPS is enabled (i.e. SPLUNK_FIPS=1), otherwise, KV Store will not be available.
    * Only used when FIPS is enabled.

sslKeysPath = &lt;filepath&gt;
    * A certificate file signed by the signing authority specified above by caCertPath. 
    * In search head clustering or search head pooling, the certificates at different members must share the same &acirc;&#128;&#152;subject'.
    * The Distinguished Name (DN) found in the certificate&acirc;&#128;&#153;s subject, must specify a non-empty value for at least one of the following attributes: Organization (O), the Organizational Unit (OU) or the Domain Component (DC).
    * Must be specified if FIPS is enabled (i.e. SPLUNK_FIPS=1), otherwise, KV Store will not be available.
    * Only used when FIPS is enabled.
        
sslKeysPassword = &lt;password&gt;
    * Password of the private key in the file specified by sslKeysPath above.    
    * Must be specified if FIPS is enabled (i.e. SPLUNK_FIPS=1), otherwise, KV Store will not be available. There is no default value.
    * Only used when FIPS is enabled.

sslCRLPath = &lt;filepath&gt;
    * Certificate Revocation List file.
    * Optional. Defaults to no Revocation List.
    * Only used when FIPS is enabled.

</font></code>
<h3> <a name="serverconf_server.conf.example"><span class="mw-headline" id="server.conf.example">server.conf.example</span></a></h3>
<code><font size="2">
#   Version 6.2.3 
#
# This file contains an example server.conf.  Use this file to configure SSL and HTTP server options.
#
# To use one or more of these configurations, copy the configuration block into
# server.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.
#
# To learn more about configuration files (including precedence) please see the documentation 
# located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles

# Allow users 8 hours before they time out
[general]
sessionTimeout=8h
pass4SymmKey = changeme

# Listen on IPv6 in addition to IPv4...
listenOnIPv6 = yes
# ...but make all outgoing TCP connections on IPv4 exclusively
connectUsingIpVersion = 4-only

# Turn on SSL:

[sslConfig]
enableSplunkdSSL = true
useClientSSLCompression = true
sslKeysfile = server.pem
sslKeysfilePassword = password
caCertFile = cacert.pem
caPath = $SPLUNK_HOME/etc/auth
certCreateScript = genMyServerCert.sh



######## SSO Example ########
# This example trusts all logins from the splunk web server and localhost
# Note that a proxy to the splunk web server should exist to enforce authentication
[general]
trustedIP = 127.0.0.1



##########################################################################################
# Set this node to be a cluster master.
##########################################################################################

[clustering]
mode = master
replication_factor = 3
pass4SymmKey = someSecret
search_factor = 2


##########################################################################################
# Set this node to be a slave to cluster master "SplunkMaster01" on port 8089.
##########################################################################################

[clustering]
mode = slave
master_uri = https://SplunkMaster01.example.com:8089
pass4SymmKey = someSecret

##########################################################################################
# Set this node to be a searchhead to cluster master "SplunkMaster01" on port 8089.
##########################################################################################
[clustering]
mode = searchhead
master_uri = https://SplunkMaster01.example.com:8089
pass4SymmKey = someSecret

##########################################################################################################
# Set this node to be a searchhead to multiple cluster masters - 
# "SplunkMaster01" with pass4SymmKey set to 'someSecret and "SplunkMaster02" with no pass4SymmKey set here.
##########################################################################################################
[clustering]
mode = searchhead
master_uri = clustermaster:east, clustermaster:west

[clustermaster:east]
master_uri=https://SplunkMaster01.example.com:8089
pass4SymmKey=someSecret

[clustermaster:west]
master_uri=https://SplunkMaster02.example.com:8089

</font></code>

<a name="serverclassconf"></a><h2> <a name="serverclassconf_serverclass.conf"><span class="mw-headline" id="serverclass.conf">serverclass.conf</span></a></h2>
<p>The following are the spec and example files for serverclass.conf.
</p>
<h3> <a name="serverclassconf_serverclass.conf.spec"><span class="mw-headline" id="serverclass.conf.spec">serverclass.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values for defining server classes to which <br># deployment clients can belong. These attributes and values specify what content a given server<br># class member will receive from the deployment server. <br>#<br># For examples, see serverclass.conf.example. You must reload deployment server ("splunk reload<br># deploy-server"), or restart splunkd, for changes to this file to take effect.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br><br>#***************************************************************************<br># Configure the server classes that are used by a deployment server instance.<br>#<br># Server classes are essentially categories. &nbsp;They use filters to control what<br># clients they apply to, contain a set of applications, and may define<br># deployment server behavior for the management of those applications. &nbsp;The<br># filters can be based on DNS name, IP address, build number of client<br># machines, platform, and the so-called clientName.<br># If a target machine matches the filter, then the apps and configuration<br># content that make up the server class will be deployed to it.<br><br># Property Inheritance<br>#<br># Stanzas in serverclass.conf go from general to more specific, in the following order:<br># [global] -&gt; [serverClass:&lt;name&gt;] -&gt; [serverClass:&lt;scname&gt;:app:&lt;appname&gt;]<br>#<br># Some properties defined at a general level (say [global]) can be<br># overridden by a more specific stanza as it applies to them. All overridable<br># properties are marked as such.<br><br><br><br>###########################################<br>########### FIRST LEVEL: global ###########<br>###########################################<br><br># Global stanza that defines properties for all server classes.<br>[global]<br><br>disabled = true|false<br>&nbsp;&nbsp;&nbsp;&nbsp;* Toggles deployment server component off and on.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set to true to disable.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false.<br><br>excludeFromUpdate = &lt;path&gt;[,&lt;path&gt;]...<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies paths to one or more top-level files or directories (and their<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contents) to exclude from being touched during app update. &nbsp;Note that<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;each comma-separated entry MUST be prefixed by "$app_root$/" (otherwise a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;warning will be generated).<br><br>repositoryLocation = &lt;path&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The repository of applications on the server machine.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $SPLUNK_HOME/etc/deployment-apps<br><br>targetRepositoryLocation = &lt;path&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The location on the deployment client where to install the apps defined for this Deployment Server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If this value is unset, or set to empty, the repositoryLocation path is used.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Useful only with complex (for example, tiered) deployment strategies.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $SPLUNK_HOME/etc/apps, the live configuration directory for a Splunk instance.<br><br>tmpFolder = &lt;path&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Working folder used by deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $SPLUNK_HOME/var/run/tmp<br><br>continueMatching = true | false<br>&nbsp;&nbsp;&nbsp;&nbsp;* Controls how configuration is layered across classes and server-specific settings.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If true, configuration lookups continue matching server classes, beyond the first match.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If false, only the first match will be used.<br>&nbsp;&nbsp;&nbsp;&nbsp;* A serverClass can override this property and stop the matching.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Matching is done in the order in which server classes are defined.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to true<br><br>endpoint = &lt;URL template string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The endpoint from which content can be downloaded by a deployment client. The deployment client knows how to substitute values for variables in the URL.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Any custom URL can also be supplied here, as long as it uses the specified variables.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Need not be specified unless you have a very specific need, for example: To acquire deployment application files from a third-party Web server, for extremely large environments.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to $deploymentServerUri$/services/streams/deployment?name=$serverClassName$:$appName$<br><br>filterType = whitelist | blacklist<br>&nbsp;&nbsp;&nbsp;&nbsp;* The whitelist setting indicates a filtering strategy that pulls in a subset:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items are not considered to match the stanza by default.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items that match any whitelist entry, and do not match any blacklist entry are considered to match the stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items that match any blacklist entry are not considered to match the stanza, regardless of whitelist.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The blacklist setting indicates a filtering strategy that rules out a subset:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items are considered to match the stanza by default.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items that match any blacklist entry, and do not match any whitelist entry are considered to not match the stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Items that match any whitelist entry are considered to match the stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;* More briefly:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* whitelist: default no-match -&gt; whitelists enable -&gt; blacklists disable<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* blacklist: default match -&gt; blacklists disable-&gt; whitelists enable<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level, and the serverClass:app level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to whitelist<br><br>whitelist.&lt;n&gt; = &lt;clientName&gt; | &lt;IP address&gt; | &lt;hostname&gt;<br>blacklist.&lt;n&gt; = &lt;clientName&gt; | &lt;IP address&gt; | &lt;hostname&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* 'n' is a number starting at 0, and increasing by 1. Splunk stops evaluating a filter at the first break in sequence of 'n' values.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The value of this attribute is matched against several things in order:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Any clientName specified by the client in its deploymentclient.conf file<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The IP address of the connected client<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The hostname of the connected client, as provided by reverse DNS lookup<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The hostname of the client, as provided by the client<br>&nbsp;&nbsp;&nbsp;&nbsp;* All of these can be used with wildcards. &nbsp;* will match any sequence of characters. &nbsp;For example:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Match a network range: 10.1.1.*<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Match a domain: *.splunk.com<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level, and the serverClass:app level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* There are no whitelist or blacklist entries by default.<br>&nbsp;&nbsp;&nbsp;&nbsp;* These patterns are PCRE regular expressions, with the following aids for easier entry:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can specify simply '.' to mean '\.'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can specify simply '*' to mean '.*'<br>&nbsp;&nbsp;&nbsp;&nbsp;* Matches are always case-insensitive; you do not need to specify the '(?i)' prefix.<br><br># Note: Overriding one type of filter (whitelist/blacklist) causes the other to<br># be overridden (and hence not inherited from parent) too.<br><br># Example with filterType=whitelist:<br># &nbsp;&nbsp;&nbsp;&nbsp;whitelist.0=*.splunk.com<br># &nbsp;&nbsp;&nbsp;&nbsp;blacklist.0=printer.splunk.com<br># &nbsp;&nbsp;&nbsp;&nbsp;blacklist.1=scanner.splunk.com<br># This will cause all hosts in splunk.com, except 'printer' and 'scanner', to match this server class.<br><br># Example with filterType=blacklist:<br># &nbsp;&nbsp;&nbsp;&nbsp;blacklist.0=*<br># &nbsp;&nbsp;&nbsp;&nbsp;whitelist.0=*.web.splunk.com<br># &nbsp;&nbsp;&nbsp;&nbsp;whitelist.1=*.linux.splunk.com<br># This will cause only the 'web' and 'linux' hosts to match the server class. No other hosts will match.<br><br># Deployment client machine types (hardware type of respective host machines) can also be used to match DCs.<br># This filter will be used only if match of a client could not be decided using the whitelist/blacklist filters.<br># The value of each machine type is designated by the hardware platform itself; a few common ones are:<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;linux-x86_64, windows-intel, linux-i686, freebsd-i386, darwin-i386, sunos-sun4u.<br># The method for finding it varies by platform; once a deployment client is connected to the DS, however, you can<br># determine the value of DC's machine type with this Splunk CLI command on the DS:<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;code&gt;./splunk list deploy-clients&lt;/code&gt;<br># The &lt;code&gt;utsname&lt;/code&gt; values in the output are the respective DCs' machine types.<br><br>machineTypesFilter = &lt;comma-separated list&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Not used unless specified.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Boolean OR logic is employed: a match against any element in the list constitutes a match.<br>&nbsp;&nbsp;&nbsp;&nbsp;* This filter is used in boolean AND logic with white/blacklist filters.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Only clients which match the white/blacklist AND which match this maachineTypesFilter<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will be included.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* In other words, the match is an intersection of the matches for the white/blacklist and<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the matches for MachineTypesFilter.<br>&nbsp;&nbsp;&nbsp;&nbsp;* This filter can be overridden at the serverClass and serverClass:app levels.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Unset by default.<br>&nbsp;&nbsp;&nbsp;&nbsp;* These patterns are PCRE regular expressions, with the following aids for easier entry:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can specify simply '.' to mean '\.'<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* You can specify simply '*' to mean '.*'<br>&nbsp;&nbsp;&nbsp;&nbsp;* Matches are always case-insensitive; you do not need to specify the '(?i)' prefix.<br><br>restartSplunkWeb = true | false<br>&nbsp;&nbsp;&nbsp;&nbsp;* If true, restarts SplunkWeb on the client when a member app or a directly configured app is updated.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level and the serverClass:app level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br><br>restartSplunkd = true | false<br>&nbsp;&nbsp;&nbsp;&nbsp;* If true, restarts splunkd on the client when a member app or a directly configured app is updated.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level and the serverClass:app level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to false<br><br>stateOnClient = enabled | disabled | noop<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to "enabled", sets the application state to enabled on the client, regardless of state on the deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to "disabled", set the application state to disabled on the client, regardless of state on the deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to "noop", the state on the client will be the same as on the deployment server.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Can be overridden at the serverClass level and the serverClass:app level.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to enabled.<br><br><br><br>#################################################<br>########### SECOND LEVEL: serverClass ###########<br>#################################################<br><br>[serverClass:&lt;serverClassName&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* This stanza defines a server class. A server class is a collection of applications; an application may belong to multiple server classes.<br>&nbsp;&nbsp;&nbsp;&nbsp;* serverClassName is a unique name that is assigned to this server class.<br>&nbsp;&nbsp;&nbsp;&nbsp;* A server class can override all inheritable properties in the [global] stanza.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* A server class name may only contain: letters, numbers, space, underscore, dash, dot, tilde, and the '@' symbol. &nbsp;It is case-sensitive.<br><br># NOTE:<br># The keys listed below are all described in detail in the<br># [global] section above. They can be used with serverClass stanza to<br># override the global setting<br>continueMatching = true | false<br>endpoint = &lt;URL template string&gt;<br>excludeFromUpdate = &lt;path&gt;[,&lt;path&gt;]...<br>filterType = whitelist | blacklist<br>whitelist.&lt;n&gt; = &lt;clientName&gt; | &lt;IP address&gt; | &lt;hostname&gt;<br>blacklist.&lt;n&gt; = &lt;clientName&gt; | &lt;IP address&gt; | &lt;hostname&gt;<br>machineTypesFilter = &lt;comma-separated list&gt;<br>restartSplunkWeb = true | false<br>restartSplunkd = true | false<br>stateOnClient = enabled | disabled | noop<br>repositoryLocation = &lt;path&gt;<br><br><br><br>########################################<br>########### THIRD LEVEL: app ###########<br>########################################<br><br>[serverClass:&lt;server class name&gt;:app:&lt;app name&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* This stanza maps an application (which must already exist in repositoryLocation) to the specified server class.<br>&nbsp;&nbsp;&nbsp;&nbsp;* server class name - the server class to which this content should be added. <br>&nbsp;&nbsp;&nbsp;&nbsp;* app name can be '*' or the name of an app:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The value '*' refers to all content in the repositoryLocation, adding it to this serverClass. '*' stanza cannot be mixed with named stanzas, for a given server class.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The name of an app explicitly adds the app to a server class. Typically apps are named by the folders that contain them.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* An application name, if it is not the special '*' sign explained directly above, may only contain: letters, numbers, space, underscore, dash, dot, tilde, and the '@' symbol. &nbsp;It is case-sensitive.<br><br>appFile=&lt;file name&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* In cases where the app name is different from the file or directory name, you can use this parameter to specify the file name. Supported formats are: directories, .tar files, and .tgz files.<br><br># May override higher-level settings.<br>excludeFromUpdate = &lt;path&gt;[,&lt;path&gt;]...<br><br></font></code>
<h3> <a name="serverclassconf_serverclass.conf.example"><span class="mw-headline" id="serverclass.conf.example">serverclass.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># Example 1<br># Matches all clients and includes all apps in the server class<br><br>[global]<br>whitelist.0=*<br># whitelist matches all clients.<br>[serverClass:AllApps]<br>[serverClass:AllApps:app:*]<br># a server class that encapsulates all apps in the repositoryLocation<br><br><br># Example 2<br># Assign server classes based on dns names.<br><br>[global]<br><br>[serverClass:AppsForOps]<br>whitelist.0=*.ops.yourcompany.com<br>[serverClass:AppsForOps:app:unix]<br>[serverClass:AppsForOps:app:SplunkLightForwarder]<br><br>[serverClass:AppsForDesktops]<br>filterType=blacklist<br># blacklist everybody except the Windows desktop machines.<br>blacklist.0=* <br>whitelist.0=*.desktops.yourcompany.com<br>[serverClass:AppsForDesktops:app:SplunkDesktop]<br><br><br># Example 3<br># Deploy server class based on machine types<br><br>[global]<br><br>[serverClass:AppsByMachineType]<br># Ensure this server class is matched by all clients. It is IMPORTANT to have a general filter here, and a more specific filter<br># at the app level. An app is matched _only_ if the server class it is contained in was successfully matched!<br>whitelist.0=*<br><br>[serverClass:AppsByMachineType:app:SplunkDesktop]<br># Deploy this app only to Windows boxes.<br>machineTypesFilter=windows-*<br><br>[serverClass:AppsByMachineType:app:unix]<br># Deploy this app only to unix boxes - 32/64 bit.<br>machineTypesFilter=linux-i686, linux-x86_64<br><br># Example 4<br># Specify app update exclusion list.<br><br>[global]<br><br># The local/ subdirectory within every app will not be touched upon update.<br>excludeFromUpdate=$app_root$/local<br><br>[serverClass:MyApps]<br><br>[serverClass:MyApps:app:SpecialCaseApp]<br># For the SpecialCaseApp, both the local/ and lookups/ subdirectories will not<br># be touched upon update.<br>excludeFromUpdate=$app_root$/local,$app_root$/lookups<br><br></font></code>

<a name="serverclassseedxmlconf"></a><h2> <a name="serverclassseedxmlconf_serverclass.seed.xml.conf"><span class="mw-headline" id="serverclass.seed.xml.conf">serverclass.seed.xml.conf</span></a></h2>
<p>The following are the spec and example files for serverclass.seed.xml.conf.
</p>
<h3> <a name="serverclassseedxmlconf_serverclass.seed.xml.conf.spec"><span class="mw-headline" id="serverclass.seed.xml.conf.spec">serverclass.seed.xml.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br><br>&lt;!--<br># This configuration is used by deploymentClient to seed a Splunk installation with applications, at startup time.<br># This file should be located in the workingDir folder defined by deploymentclient.conf.<br>#<br># An interesting fact - the DS -&gt; DC communication on the wire also uses this XML format.<br>--&gt;<br>&lt;?xml version="1.0"?&gt;<br>&lt;deployment name="somename"&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- <br>&nbsp;&nbsp;&nbsp;&nbsp;# The endpoint from which all apps can be downloaded. &nbsp;This value can be overridden by serviceClass or ap declarations below.<br>&nbsp;&nbsp;&nbsp;&nbsp;# In addition, deploymentclient.conf can control how this property is used by deploymentClient - see deploymentclient.conf.spec.<br>&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;$deploymentServerUri$/services/streams/deployment?name=$serviceClassName$:$appName$&lt;/endpoint&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!--<br>&nbsp;&nbsp;&nbsp;&nbsp;# The location on the deploymentClient where all applications will be installed. This value can be overridden by serviceClass or <br>&nbsp;&nbsp;&nbsp;&nbsp;# app declarations below.<br>&nbsp;&nbsp;&nbsp;&nbsp;# In addition, deploymentclient.conf can control how this property is used by deploymentClient - see deploymentclient.conf.spec.<br>&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;repositoryLocation&gt;$SPLUNK_HOME/etc/apps&lt;/repositoryLocation&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;serviceClass name="serviceClassName"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!--<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# The order in which this service class is processed.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;order&gt;N&lt;/order&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!--<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# DeploymentClients can also override these values using serverRepositoryLocationPolicy and serverEndpointPolicy.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;repositoryLocation&gt;$SPLUNK_HOME/etc/myapps&lt;/repositoryLocation&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;splunk.com/spacecake/$serviceClassName$/$appName$.tgz&lt;/endpoint&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!--<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Please See serverclass.conf.spec for how these properties are used.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;continueMatching&gt;true&lt;/continueMatching&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;restartSplunkWeb&gt;false&lt;/restartSplunkWeb&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;restartSplunkd&gt;false&lt;/restartSplunkd&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;stateOnClient&gt;enabled&lt;/stateOnClient&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="appName1"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!--<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Applications can override the endpoint property.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;splunk.com/spacecake/$appName$&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="appName2"/&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/serviceClass&gt;<br>&lt;/deployment&gt;<br><br><br></font></code>
<h3> <a name="serverclassseedxmlconf_serverclass.seed.xml.conf.example"><span class="mw-headline" id="serverclass.seed.xml.conf.example">serverclass.seed.xml.conf.example</span></a></h3>
<code><font size="2"><br>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>&lt;deployment name="root"&gt;<br>&nbsp;&nbsp;&lt;serverClass name="spacecake_apps"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_0"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;repositoryLocation&gt;$SPLUNK_HOME/etc/myapps&lt;/repositoryLocation&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- Download app_0 from the given location --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;splunk.com/spacecake/apps/app_0.tgz&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_1"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;repositoryLocation&gt;$SPLUNK_HOME/etc/myapps&lt;/repositoryLocation&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- Download app_1 from the given location --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;splunk.com/spacecake/apps/app_1.tgz&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&lt;/serverClass&gt;<br>&nbsp;&nbsp;&lt;serverClass name="foobar_apps"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- construct url for each location based on the scheme below and download each app --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;foobar.com:5556/services/streams/deployment?name=$serverClassName$_$appName$.bundle&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_0"/&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_1"/&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_2"/&gt;<br>&nbsp;&nbsp;&lt;/serverClass&gt;<br>&nbsp;&nbsp;&lt;serverClass name="local_apps"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;foo&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_0"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- app present in local filesystem --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;file:/home/johndoe/splunk/ds/service_class_2_app_0.bundle&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_1"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- app present in local filesystem --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;file:/home/johndoe/splunk/ds/service_class_2_app_1.bundle&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;app name="app_2"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- app present in local filesystem --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;endpoint&gt;file:/home/johndoe/splunk/ds/service_class_2_app_2.bundle&lt;/endpoint&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/app&gt;<br>&nbsp;&nbsp;&lt;/serverClass&gt;<br>&lt;/deployment&gt;<br><br><br></font></code>

<a name="setup.xmlconf"></a><h2> <a name="setup.xmlconf_setup.xml.conf"><span class="mw-headline" id="setup.xml.conf">setup.xml.conf</span></a></h2>
<p>The following are the spec and example files for setup.xml.conf.
</p>
<h3> <a name="setup.xmlconf_setup.xml.conf.spec"><span class="mw-headline" id="setup.xml.conf.spec">setup.xml.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br>#<br><br>&lt;!-- <br>This file describes the setup XML config and provides some examples.<br><br>setup.xml provides a Setup Screen that you provide to users to specify configurations<br>for an app. The Setup Screen is available when the user first runs the app or from the <br>Splunk Manager: Splunk &gt; Manager &gt; Apps &gt; Actions &gt; Set up<br><br>Place setup.xml in the app's default directory:<br><br>&nbsp;&nbsp;$SPLUNK_HOME/etc/apps/&lt;app&gt;/default/setup.xml<br><br>The basic unit of work is an &lt;input&gt;, which is targeted to a triplet <br>(endpoint, entity, field) and other information used to model the data. For example <br>data type, validation information, name/label, etc.<br><br>The (endpoint, entity, field attributes) identifies an object where the input is <br>read/written to, for example:<br><br>&nbsp;&nbsp;&nbsp;endpoint=saved/searches <br>&nbsp;&nbsp;&nbsp;entity=MySavedSearch <br>&nbsp;&nbsp;&nbsp;field=cron_schedule<br><br>The endpoint/entities addressing is relative to the app being configured. Endpoint/entity can <br>be inherited from the outer blocks (see below how blocks work).<br><br>Inputs are grouped together within a &lt;block&gt; element:<br><br>(1) blocks provide an iteration concept when the referenced REST entity is a regex<br><br>(2) blocks allow you to group similar configuration items<br><br>(3) blocks can contain &lt;text&gt; elements to provide descriptive text to the user. <br><br>(4) blocks can be used to create a new entry rather than edit an already existing one, set the <br>&nbsp;&nbsp;&nbsp;&nbsp;entity name to "_new". NOTE: make sure to add the required field 'name' as <br>&nbsp;&nbsp;&nbsp;&nbsp;an input.<br><br>(5) blocks cannot be nested<br><br>See examples below.<br><br><br>Block Node attributes:<br><br>endpoint - The REST endpoint relative to "https://hostname:port/servicesNS/nobody/&lt;app-name&gt;/"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of entities/object the block/input addresses. Generally, an endpoint maps to a <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splunk configuration file.<br><br>entity &nbsp;&nbsp;- An object at the endpoint. Generally, this maps to a stanza name in a configuration file.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE: entity names should be URI encoded.<br><br>mode &nbsp;&nbsp;&nbsp;&nbsp;- (bulk | iter) used if the entity attribute is a regular expression:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o iter - (default value for mode) Iterate over all matching entities and provide a <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separate input field for each.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o bulk - Update all matching entities with the same value.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE: splunk interprets '*' as the regex '.*'<br><br>eai_search - a search to filter entities returned by an endpoint. If not specified the following <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;search is used: eai:acl.app="" OR eai:acl.app="&lt;current-app&gt;" This search matches <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only objects defined in the app which the setup page is being used for.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE: if objects from another app are allowed to be configured, any changes to those <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;objects will be stored in the current app.<br><br>enabled &nbsp;&nbsp;- (true | false | in-windows | in-unix) whether this block is enabled or not<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o true &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- (default) this block is enabled<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o false &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- block disabled<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o in-windows &nbsp;&nbsp;&nbsp;- block is enabled only in windows installations<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o in-unix &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- block is enabled in non-windows installations &nbsp;&nbsp;<br><br>Input Node Attributes:<br><br>endpoint &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- see description above (inherited from block)<br><br>entity &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- see description above (inherited from block)<br><br>field &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &lt;string&gt; the field which is being configured<br><br>old_style_disable - &lt;bool&gt; whether to perform entity disabling by submitting the edited entity with the following <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;field set: disabled=1. (This is only relevant for inputs whose field=disabled|enabled). <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults to false.<br><br>Nodes within an &lt;input&gt; element can display the name of the entity and field values within the entity<br>on the setup screen. Specify $name$ to display the name of the entity. Use $&lt;field_name&gt;$ to specify<br>the value of a specified field.<br><br>&nbsp;--&gt;<br><br>&lt;setup&gt;<br>&nbsp;&nbsp;&lt;block title="Basic stuff" endpoint="saved/searches/" entity="foobar"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;text&gt; some description here &lt;/text&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="is_scheduled"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable Schedule for $name$&lt;/label&gt; &nbsp;&nbsp;&lt;!-- this will be rendered as "Enable Schedule for foobar" --&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="cron_scheduled"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Cron Schedule&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;text&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="actions"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Select Active Actions&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;list&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;!-- bulk update &nbsp;--&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input entity="*" field="is_scheduled" mode="bulk"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable Schedule For All&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;&lt;!-- iterative update in this block --&gt;<br>&nbsp;&nbsp;&lt;block title="Configure search" endpoint="saved/eventypes/" entity="*" mode="iter"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="search"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;$name$ search&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;string&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="disabled"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;disable $name$&lt;/label&gt; &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&lt;block title="Create a new eventtype" endpoint="saved/eventtypes/" entity="_new"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input target="name"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Name&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;text&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input target="search"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Search&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;text&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt; &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&lt;block title="Add Account Info" endpoint="storage/passwords" entity="_new"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="name"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Username&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;text&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input field="password"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Password&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;password&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;&lt;!-- &nbsp;example config for "Windows setup" --&gt; &nbsp;<br>&nbsp;&nbsp;&lt;block title="Collect local event logs" endpoint="admin/win-eventlogs/" eai_search="" &gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;text&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splunk for Windows needs at least your local event logs to demonstrate how to search them. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You can always add more event logs after the initial setup in Splunk Manager.<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/text&gt;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input entity="System" field="enabled" old_style_disable="true"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable $name$&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input entity="Security" field="enabled" &nbsp;old_style_disable="true"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable $name$&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input entity="Application" field="enabled" &nbsp;old_style_disable="true"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable $name$&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt; &nbsp;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;&lt;block title="Monitor Windows update logs" endpoint="data/inputs/monitor"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;text&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you monitor the Windows update flat-file log, Splunk for Windows can show your patch history.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You can also monitor other logs if you have them, such as IIS or DHCP logs, from Data Inputs in Splunk Manager<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/text&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;input entity="%24WINDIR%5CWindowsUpdate.log" field="enabled"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;label&gt;Enable $name$&lt;/label&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;bool&lt;/type&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;/input&gt;<br>&nbsp;&nbsp;&lt;/block&gt;<br>&lt;/setup&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br><br><br><br></font></code>
<h3> <a name="setup.xmlconf_setup.xml.conf.example"><span class="mw-headline" id="setup.xml.conf.example">setup.xml.conf.example</span></a></h3>
<code><font size="2"><br>No example<br></font></code>

<a name="source-classifierconf"></a><h2> <a name="source-classifierconf_source-classifier.conf"><span class="mw-headline" id="source-classifier.conf">source-classifier.conf</span></a></h2>
<p>The following are the spec and example files for source-classifier.conf.
</p>
<h3> <a name="source-classifierconf_source-classifier.conf.spec"><span class="mw-headline" id="source-classifier.conf.spec">source-classifier.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains all possible options for configuring settings for the file classifier<br># in source-classifier.conf.<br>#<br># There is a source-classifier.conf in $SPLUNK_HOME/etc/system/default/ To set custom <br># configurations, place a source-classifier.conf in $SPLUNK_HOME/etc/system/local/. <br># For examples, see source-classifier.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br>ignored_model_keywords = &lt;space-separated list of terms&gt; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Terms to ignore when generating a sourcetype model. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* To prevent sourcetype "bundles/learned/*-model.xml" files from containing sensitive<br>&nbsp;&nbsp;&nbsp;&nbsp;terms (e.g. "bobslaptop") that occur very frequently in your data<br>&nbsp;&nbsp;&nbsp;&nbsp;files, add those terms to ignored_model_keywords.<br><br>ignored_filename_keywords = &lt;space-separated list of terms&gt; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Terms to ignore when comparing a new sourcename against a known sourcename, for the purpose of <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classifying a source.<br><br><br></font></code>
<h3> <a name="source-classifierconf_source-classifier.conf.example"><span class="mw-headline" id="source-classifier.conf.example">source-classifier.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example source-classifier.conf. &nbsp;Use this file to configure classification<br># of sources into sourcetypes.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># source-classifier.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># terms to ignore when generating sourcetype model to prevent model from containing servernames, <br>ignored_model_keywords = sun mon tue tues wed thurs fri sat sunday monday tuesday wednesday thursday friday saturday jan feb mar apr may jun jul aug sep oct nov dec january february march april may june july august september october november december 2003 2004 2005 2006 2007 2008 2009 am pm ut utc gmt cet cest cetdst met mest metdst mez mesz eet eest eetdst wet west wetdst msk msd ist jst kst hkt ast adt est edt cst cdt mst mdt pst pdt cast cadt east eadt wast wadt<br><br># terms to ignore when comparing a sourcename against a known sourcename<br>ignored_filename_keywords = log logs com common event events little main message messages queue server splunk <br><br></font></code>

<a name="sourcetype%20metadataconf"></a><h2> <a name="sourcetype%20metadataconf_sourcetype_metadata.conf"><span class="mw-headline" id="sourcetype_metadata.conf">sourcetype_metadata.conf</span></a></h2>
<p>The following are the spec and example files for sourcetype_metadata.conf.
</p>
<h3> <a name="sourcetype%20metadataconf_sourcetype_metadata.conf.spec"><span class="mw-headline" id="sourcetype_metadata.conf.spec">sourcetype_metadata.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2<br>#<br># This file contains possible attribute/value pairs for sourcetype metadata.<br>#<br># There is a default sourcetype_metadata.conf in $SPLUNK_HOME/etc/system/default. To set custom<br># configurations, place a sourcetype_metadata.conf in $SPLUNK_HOME/etc/system/local/. To set custom configuration for an app, place<br># sourcetype_metadata.conf in $SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/. <br># For examples, see sourcetype_metadata.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;sourcetype&gt;]<br>* This stanza enables properties for a given &lt;sourcetype&gt;.<br>* Follow the stanza name with any number of the following attribute/value pairs.<br><br>category = &lt;string&gt;<br>* Category of the sourcetype.<br>* Can use one of the predefined categories, or a custom one.<br>* Predefined categories are: category_structured, category_web, category_application, category_network_security<br>category_voip, category_database, category_email, category_linux, category_miscellaneous, category_custom, category_company<br><br>description = &lt;string&gt;<br>* Description of the sourcetype. &nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br></font></code>
<h3> <a name="sourcetype%20metadataconf_sourcetype_metadata.conf.example"><span class="mw-headline" id="sourcetype_metadata.conf.example">sourcetype_metadata.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2 <br>#<br># This is an example sourcetype_metadata.conf. &nbsp;Use this file to define category and descripton for sourcetype.<br>#<br># To use one or more of these configurations, copy the configuration block into sourcetype_metadata.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br># <br># First example is an example of assigning one of defined categories to a custom sourcetype. &nbsp;Second example<br># uses a custom category and description<br><br>[custom_sourcetype_1]<br>category = category_database<br>description = Custom sourcetype for database logs<br><br>[custom_sourcetype_2]<br>category = My Sourcetypes<br>description = Custom sourcetype for apache logs<br><br></font></code>

<a name="sourcetypesconf"></a><h2> <a name="sourcetypesconf_sourcetypes.conf"><span class="mw-headline" id="sourcetypes.conf">sourcetypes.conf</span></a></h2>
<p>The following are the spec and example files for sourcetypes.conf.
</p>
<h3> <a name="sourcetypesconf_sourcetypes.conf.spec"><span class="mw-headline" id="sourcetypes.conf.spec">sourcetypes.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;Version 6.2.3 <br>#<br># NOTE: sourcetypes.conf is a machine-generated file that stores the document models used by the <br># file classifier for creating source types.<br><br># Generally, you should not edit sourcetypes.conf, as most attributes are machine generated.<br># However, there are two attributes which you can change.<br>#<br># There is a sourcetypes.conf in $SPLUNK_HOME/etc/system/default/ To set custom <br># configurations, place a sourcetypes..conf in $SPLUNK_HOME/etc/system/local/. <br># For examples, see sourcetypes.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>_sourcetype = &lt;value&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the sourcetype for the model.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Change this to change the model's sourcetype.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Future sources that match the model will receive a sourcetype of this new name.<br><br><br>_source = &lt;value&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the source (filename) for the model.<br><br></font></code>
<h3> <a name="sourcetypesconf_sourcetypes.conf.example"><span class="mw-headline" id="sourcetypes.conf.example">sourcetypes.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains an example sourcetypes.conf. &nbsp;Use this file to configure sourcetype models.<br>#<br># NOTE: sourcetypes.conf is a machine-generated file that stores the document models used by the <br># file classifier for creating source types.<br>#<br># Generally, you should not edit sourcetypes.conf, as most attributes are machine generated.<br># However, there are two attributes which you can change.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># sourcetypes.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>#<br># This is an example of a machine-generated sourcetype models for a fictitious sourcetype cadcamlog.<br>#<br><br>[/Users/bob/logs/bnf.x5_Thu_Dec_13_15:59:06_2007_171714722]<br>_source = /Users/bob/logs/bnf.x5<br>_sourcetype = cadcamlog<br>L----------- = 0.096899<br>L-t&lt;_EQ&gt; = 0.016473<br><br></font></code>

<a name="splunk-launchconf"></a><h2> <a name="splunk-launchconf_splunk-launch.conf"><span class="mw-headline" id="splunk-launch.conf">splunk-launch.conf</span></a></h2>
<p>The following are the spec and example files for splunk-launch.conf.
</p>
<h3> <a name="splunk-launchconf_splunk-launch.conf.spec"><span class="mw-headline" id="splunk-launch.conf.spec">splunk-launch.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br><br># splunk-launch.conf contains values used at startup time, by the splunk command <br># and by windows services.<br># <br><br># Note: this conf file is different from most splunk conf files. &nbsp;There is only<br># one in the whole system, located at $SPLUNK_HOME/etc/splunk-launch.conf; further,<br># there are no stanzas, explicit or implicit. &nbsp;Finally, any<br># splunk-launch.conf files in etc/apps/... or etc/users/... will be ignored.<br><br><br># Lines beginning with a # are considered comments and are ignored.<br><br>#*******<br># Environment variables<br>#<br># Primarily, this file simply sets environment variables to be used by Splunk<br># programs.<br>#<br># These environment variables are the same type of system environment variables<br># that can be set, on unix, using:<br># &nbsp;&nbsp;bourne shells:<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ export ENV_VAR=value<br># &nbsp;&nbsp;c-shells:<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;% setenv ENV_VAR value<br>#<br># or at a windows command prompt:<br># &nbsp;&nbsp;C:\&gt; SET ENV_VAR=value<br>#*******<br><br>&lt;environment_variable&gt;=&lt;value&gt;<br><br>* Any desired environment variable can be set to any value.<br>&nbsp;&nbsp;Whitespace is trimmed from around both the key and value.<br>* Environment variables set here will be available to all splunk processes,<br>&nbsp;&nbsp;barring operating system limitations.<br><br><br>#*******<br># Specific Splunk environment settings<br>#<br># These settings are primarily treated as environment variables, though some<br># have some additional logic (defaulting). <br>#<br># There is no need to explicitly set any of these values in typical environments.<br>#*******<br><br>SPLUNK_HOME=&lt;pathname&gt;<br>* The comment in the auto-generated splunk-launch.conf is informational, not a<br>&nbsp;&nbsp;live setting, and does not need to be uncommented.<br>* Fully qualified path to the Splunk install directory.<br>* If unset, Splunk automatically determines the location of SPLUNK_HOME based<br>&nbsp;&nbsp;on the location of splunk-launch.conf<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifically, the parent of the directory containing splunk-launch.conf<br>* Defaults to unset.<br><br>SPLUNK_DB=&lt;pathname&gt;<br>* The comment in the auto-generated splunk-launch.conf is informational, not a<br>&nbsp;&nbsp;live setting, and does not need to be uncommented.<br>* Fully qualified path to the directory containing the splunk index directories.<br>* Primarily used by paths expressed in indexes.conf<br>* The comment in the autogenerated splunk-launch.conf is informational, not a<br>&nbsp;&nbsp;live setting, and does not need to be uncommented.<br>* If unset, becomes $SPLUNK_HOME/var/lib/splunk (unix) or&nbsp;%SPLUNK_HOME%\var\lib\splunk (windows)<br>* Defaults to unset.<br><br>SPLUNK_BINDIP=&lt;ip address&gt;<br>* Specifies an interface that splunkd and splunkweb should bind to, as opposed<br>&nbsp;&nbsp;to binding to the default for the local operating system.<br>* If unset, Splunk makes no specific request to the operating system when binding to<br>&nbsp;&nbsp;ports/opening a listening socket. &nbsp;This means it effectively binds to '*'; i.e. <br>&nbsp;&nbsp;an unspecified bind. &nbsp;The exact result of this is controlled by operating<br>&nbsp;&nbsp;system behavior and configuration.<br>* NOTE: When using this setting you must update mgmtHostPort in web.conf to<br>&nbsp;&nbsp;match, or the command line and splunkweb will not know how to<br>&nbsp;&nbsp;reach splunkd.<br>* For splunkd, this sets both the management port and the receiving ports<br>&nbsp;&nbsp;(from forwarders).<br>* Useful for a host with multiple IP addresses, either to enable<br>&nbsp;&nbsp;access or restrict access; though firewalling is typically a superior method<br>&nbsp;&nbsp;of restriction.<br>* Overrides the Splunkweb-specific web.conf/[settings]/server.socket_host param;<br>&nbsp;&nbsp;the latter is preferred when SplunkWeb behavior is the focus.<br>* Defaults to unset. <br><br>SPLUNK_IGNORE_SELINUX=true<br>* If unset (not present), Splunk on Linux will abort startup if it detects<br>&nbsp;&nbsp;it is running in an SELinux environment. &nbsp;This is because in<br>&nbsp;&nbsp;shipping/distribution-provided SELinux environments, Splunk will not be<br>&nbsp;&nbsp;permitted to work, and Splunk will not be able to identify clearly why.<br>* This setting is useful in environments where you have configured SELinux to<br>&nbsp;&nbsp;enable Splunk to work.<br>* If set to any value, Splunk will launch, despite the presence of SELinux.<br>* Defaults to unset.<br><br>SPLUNK_OS_USER = &lt;string&gt; | &lt;nonnegative integer&gt;<br>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The OS user whose privileges Splunk will adopt when running, if this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameter is set.<br>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example: SPLUNK_OS_USER=fnietzsche, but a root login is used to start<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;splunkd. Immediately upon starting, splunkd abandons root's privileges,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and acquires fnietzsche's privileges; any files created by splunkd (index<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data, logs, etc.) will be consequently owned by fnietzsche. &nbsp;So when<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;splunkd is started next time by fnietzsche, files will be readable.<br>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When 'splunk enable boot-start -user &lt;U&gt;' is invoked, SPLUNK_OS_USER<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is set to &lt;U&gt; as a side effect.<br>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Under UNIX, username or apposite numeric UID are both acceptable;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;under Windows, only a username.<br><br>#*******<br># Service/server names.<br>#<br># These settings are considered internal, and altering them is not supported.<br># <br># Under Windows, they influence the expected name of the service; on UNIX they<br># influence the reported name of the appropriate server or daemon process.<br>#<br># If you want to run multiple instances of Splunk as *services* under Windows,<br># you will need to change the names below for 2nd, 3rd, ..., instances. &nbsp;That is<br># because the 1st instance has taken up service names 'Splunkd' and 'Splunkweb',<br># and you may not have multiple services with same name.<br>#*******<br><br>SPLUNK_SERVER_NAME=&lt;name&gt;<br>* Names the splunkd server/service.<br>* Defaults to splunkd (UNIX), or Splunkd (Windows).<br><br>SPLUNK_WEB_NAME=&lt;name&gt;<br>* Names the Python app server / web server/service.<br>* Defaults to splunkweb (UNIX), or Splunkweb (Windows).<br><br>MONGOC_DISABLE_SHM=&lt;1|0&gt;<br>* This parameter is to specify whether kvstore will export counters to shared memory.<br>* Defaults to 1.<br><br></font></code>
<h3> <a name="splunk-launchconf_splunk-launch.conf.example"><span class="mw-headline" id="splunk-launch.conf.example">splunk-launch.conf.example</span></a></h3>
<code><font size="2"><br>No example<br></font></code>

<a name="tagsconf"></a><h2> <a name="tagsconf_tags.conf"><span class="mw-headline" id="tags.conf">tags.conf</span></a></h2>
<p>The following are the spec and example files for tags.conf.
</p>
<h3> <a name="tagsconf_tags.conf.spec"><span class="mw-headline" id="tags.conf.spec">tags.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring tags. &nbsp;Set any number of tags <br># for indexed or extracted fields.<br>#<br># There is no tags.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a tags.conf in $SPLUNK_HOME/etc/system/local/. For help, see tags.conf.example. <br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[&lt;fieldname&gt;=&lt;value&gt;] <br>&nbsp;&nbsp;&nbsp;&nbsp;* The field name and value to which the tags in the stanza apply ( eg host=localhost ).<br>&nbsp;&nbsp;&nbsp;&nbsp;* A tags.conf file can contain multiple stanzas. It is recommended that the value be URL encoded to avoid <br>&nbsp;&nbsp;&nbsp;&nbsp;* config file parsing errors especially if the field value contains the following characters: \n, =, []<br>&nbsp;&nbsp;&nbsp;&nbsp;* Each stanza can refer to only one field=value<br>&nbsp;<br>&lt;tag1&gt; = &lt;enabled|disabled&gt;<br>&lt;tag2&gt; = &lt;enabled|disabled&gt;<br>&lt;tag3&gt; = &lt;enabled|disabled&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Set whether each &lt;tag&gt; for this specific &lt;fieldname&gt;&lt;value&gt; is enabled or disabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;* While you can have multiple tags in a stanza (meaning that multiple tags are assigned to <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the same field/value combination), only one tag is allowed per stanza line. In other words, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you can't have a list of tags on one line of the stanza.<br>* WARNING: Do not quote the &lt;tag&gt; value: foo=enabled, not "foo"=enabled.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br><br><br><br></font></code>
<h3> <a name="tagsconf_tags.conf.example"><span class="mw-headline" id="tags.conf.example">tags.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example tags.conf. &nbsp;Use this file to define tags for fields.<br>#<br># To use one or more of these configurations, copy the configuration block into tags.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br># <br># This first example presents a situation where the field is "host" and the three hostnames for which tags are being defined <br># are "hostswitch," "emailbox," and "devmachine." Each hostname has two tags applied to it, one per line. Note also that<br># the "building1" tag has been applied to two hostname values (emailbox and devmachine).<br><br>[host=hostswitch]<br>pci = enabled<br>cardholder-dest = enabled<br><br>[host=emailbox]<br>email = enabled<br>building1 = enabled<br><br>[host=devmachine]<br>development = enabled<br>building1 = enabled<br><br>[src_ip=192.168.1.1]<br>firewall = enabled<br><br>[seekPtr=1cb58000]<br>EOF = enabled<br>NOT_EOF = disabled<br><br></font></code>

<a name="tenantsconf"></a><h2> <a name="tenantsconf_tenants.conf"><span class="mw-headline" id="tenants.conf">tenants.conf</span></a></h2>
<p>The following are the spec and example files for tenants.conf.
</p>
<h3> <a name="tenantsconf_tenants.conf.spec"><span class="mw-headline" id="tenants.conf.spec">tenants.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.0.3<br><br>########## ########## ########## ########## ########## ########## ########## ########## #########<br># This .conf file is DEPRECATED as of Splunk 6.0; support for it and the feature it configures, #<br># Multi-Tenanted Deployment Server, will be removed altogether in the next major release. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#<br>########## ########## ########## ########## ########## ########## ########## ########## #########<br><br>#<br># Use tenants.conf to redirect incoming requests from deployment clients to another deployment <br># server or servers. This is typically used for offloading load on your splunkd's HTTP server. <br># This is not a typical configuration for deployment server. There is no default tenants.conf<br>#<br># There is no need to create/edit tenants.conf* unless you have worked with Splunk Professional <br># Services to design a custom deployment that includes explicit involvement of tenants.conf. <br>#<br># To set custom configurations, place a pubsub.conf in $SPLUNK_HOME/etc/system/local/. <br># For examples, see pubsub.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#***************************************************************************<br># Configure tenants (DeploymentServer instances) within the same Splunk server.<br># <br># Multiple instances of deployment servers can be configured within the same Splunk instance <br># using this configuration file.<br># If this file is missing, a default DeploymentServer for tenant='default'is configured <br># by the system, if there exists serverclass.conf or default-serverclass.conf.<br><br>############# It is possible to redirect deployment clients to the appropriate instance of deployment server? ##############<br># by using a whitelist/blacklist mechanism, similar to the one in serverclass.conf.<br><br>######## How does it all work? ############<br># A DeploymentClient does a handshake with TenantService to determine which DeploymentServer<br># it should be talking to. The TenantService will use this configuration to redirect a <br># client to the appropriate deployment server (represented by phoneHomeTopic).<br><br>######## How is multi-tenant configuration stored? #########<br># Server class configuration for each tenant (except for 'default' tenant) MUST be made available in:<br># &lt;tenantName&gt;-serverclass.conf, and MUST include at least 1 serverclass.<br>#<br># Server class configuration for the 'default' tenant MAY be made available in:<br># &lt;tenantName&gt;-serverclass.conf; it MAY also reside in 'serverclass.conf' (note the missing<br># &lt;tenantName&gt; prefix) in this case. &nbsp;The 'default' tenant MAY be associated with 0 serverclasses.<br><br>[tenant:&lt;tenantName&gt;]<br><br>filterType = whitelist|blacklist<br>&nbsp;&nbsp;&nbsp;&nbsp;* defaults to whitelist<br><br>whitelist.&lt;n&gt; = &lt;IP address or hostname or clientName&gt;<br>blacklist.&lt;n&gt; = &lt;IP address of hostname of clientName&gt;<br><br>&nbsp;&nbsp;&nbsp;&nbsp;* 'n' is a number starting at 0, and increasing by 1. Stop looking at the filter when 'n' breaks.<br>&nbsp;&nbsp;&nbsp;&nbsp;* IP address of deployment client. &nbsp;Can also use wildcards, such as 10.1.1.*<br>&nbsp;&nbsp;&nbsp;&nbsp;* hostname of deployment client. &nbsp;Can also use wildcards as *.splunk.com.<br>&nbsp;&nbsp;&nbsp;&nbsp;* clientName- a logical or 'tag' name that can be assigned to each deployment client in deploymentclient.conf. &nbsp;Takes precedence (over IP/hostname) when matching a client to a filter.<br><br># Internal.<br>phoneHomeTopic=deploymentServer/phoneHome/$tenantName$<br>&nbsp;&nbsp;&nbsp;&nbsp;* some unique suffix. Default is to use the tenant name. Make sure this value is unique.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Override this value only when you wish to script and roll your own deployment server.<br><br>token = &lt;token-id for phone home communication&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* defaults to 'default' <br><br>########## ########## ########## ########## ########## ########## ########## ########## #########<br># This .conf file is DEPRECATED as of Splunk 6.0; support for it and the feature it configures, #<br># Multi-Tenanted Deployment Server, will be removed altogether in the next major release. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#<br>########## ########## ########## ########## ########## ########## ########## ########## #########<br><br></font></code>
<h3> <a name="tenantsconf_tenants.conf.example"><span class="mw-headline" id="tenants.conf.example">tenants.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.0.3 <br><br>########## ########## ########## ########## ########## ########## ########## ########## #########<br># This .conf file is DEPRECATED as of Splunk 6.0; support for it and the feature it configures, #<br># Multi-Tenanted Deployment Server, will be removed altogether in the next major release. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#<br>########## ########## ########## ########## ########## ########## ########## ########## #########<br><br># Define two tenants - dept1 and dept2.<br># DS Configuration for dept1 will be in a matching dept1-serverclass.conf<br># DS Configuration for dept2 will be in a matching dept2-serverclass.conf<br><br>[tenant:dept1]<br>whitelist.0=*.dept1.splunk.com<br>token=dept1<br><br>[tenant:dept2]<br>whitelist.0=*.dept2.splunk.com<br>token=dept2<br><br></font></code>

<a name="timesconf"></a><h2> <a name="timesconf_times.conf"><span class="mw-headline" id="times.conf">times.conf</span></a></h2>
<p>The following are the spec and example files for times.conf.
</p>
<h3> <a name="timesconf_times.conf.spec"><span class="mw-headline" id="times.conf.spec">times.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for creating custom time <br># ranges.<br>#<br># To set custom configurations, place a times.conf in $SPLUNK_HOME/etc/system/local/. <br># For help, see times.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;timerange_name&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* The token to be used when accessing time ranges via the API or command line<br>&nbsp;&nbsp;&nbsp;&nbsp;* A times.conf file can contain multiple stanzas. <br><br>label = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The textual description used by the UI to reference this time range<br>&nbsp;&nbsp;&nbsp;&nbsp;* Required<br><br>header_label = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The textual description used by the UI when displaying search results in<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this time range.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Optional. &nbsp;If omitted, the &lt;timerange_name&gt; is used instead.<br><br>earliest_time = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The string that represents the time of the earliest event to return, inclusive.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The time can be expressed with a relative time identifier or in epoch time.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Optional. &nbsp;If omitted, no earliest time bound is used.<br><br>latest_time = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The string that represents the time of the earliest event to return, inclusive.<br>&nbsp;&nbsp;&nbsp;&nbsp;* The time can be expressed with a relative time identifier or in epoch time.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Optional. &nbsp;If omitted, no latest time bound is used. &nbsp;NOTE: events that<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;occur in the future (relative to the server timezone) may be returned.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>order = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The key on which all custom time ranges are sorted, ascending.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The default time range selector in the UI will merge and sort all time<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ranges according to the 'order' key, and then alphabetically.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional. &nbsp;Default value is 0.<br><br>sub_menu = &lt;submenu name&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* if present, the time range is to be shown in the given submenu instead <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of in the main menu. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* the value for this key must be the label key of an existing stanza name, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and that stanza name must have an is_sub_menu = True key<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Optional. If omitted the given time option will display in the main menu. &nbsp;&nbsp;&nbsp;<br><br>is_sub_menu = &lt;boolean&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If True, the given item is only the 'opener' element for a submenu. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* stanzas containing this key can still be assigned an order value to set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the placement within the main menu, but can not themselves have <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;latest_time nor earliest_time keys.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br></font></code>
<h3> <a name="timesconf_times.conf.example"><span class="mw-headline" id="times.conf.example">times.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example times.conf. &nbsp;Use this file to create custom time ranges<br># that can be used while interacting with the search system.<br>#<br># To use one or more of these configurations, copy the configuration block into times.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># Note: These are examples. &nbsp;Replace the values with your own customizations.<br><br><br># The stanza name is an alphanumeric string (no spaces) that uniquely identifies<br># a time range.<br>[this_business_week]<br><br># Define the label used in the time range control<br>label = This business week<br><br># Define the label to be used in display headers. If omitted the 'label' key will be used<br># with the first letter lowercased.<br>header_label = during this business week<br>earliest_time = +1d@w1<br>latest_time = +6d@w6<br><br># Define the ordering sequence of this time range. &nbsp;All time ranges are sorted<br># numerically, ascending. If the time range is in a sub menu and not in the main <br># menu, this will determine the position within the sub menu. <br>order = 110<br><br><br># a time range that only has a bound on the earliest time<br>#<br>[last_3_hours]<br>label = Last 3 hours<br>header_label = in the last 3 hours<br>earliest_time = -3h<br>order = 30<br><br><br># Use epoch time notation to define the time bounds for the Fall Semester 2013,<br># where earliest_time is 9/4/13 00:00:00 and latest_time is 12/13/13 00:00:00.<br># <br>[Fall_2013]<br>label = Fall Semester 2013<br>earliest_time = 1378278000<br>latest_time = 1386921600<br><br><br># two time ranges that should appear in a sub menu instead of in the main menu. <br># the order values here determine relative ordering within the submenu.<br>#<br>[yesterday]<br>label = Yesterday<br>earliest_time = -1d@d<br>latest_time = @d<br>order = 10<br>sub_menu = Other options<br><br>[day_before_yesterday]<br>label = Day before yesterday<br>header_label = from the day before yesterday<br>earliest_time = -2d@d<br>latest_time = -1d@d<br>order = 20<br>sub_menu = Other options<br><br><br>#<br># The sub menu item that should contain the previous two time ranges.<br># the order key here determines the submenu opener's placement within the main menu.<br>#<br>[other]<br>label = Other options<br>order = 202<br><br></font></code>

<a name="transactiontypesconf"></a><h2> <a name="transactiontypesconf_transactiontypes.conf"><span class="mw-headline" id="transactiontypes.conf">transactiontypes.conf</span></a></h2>
<p>The following are the spec and example files for transactiontypes.conf.
</p>
<h3> <a name="transactiontypesconf_transactiontypes.conf.spec"><span class="mw-headline" id="transactiontypes.conf.spec">transactiontypes.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains all possible attributes and value pairs for a transactiontypes.conf<br># file. &nbsp;Use this file to configure transaction searches and their properties.<br>#<br># There is a transactiontypes.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom <br># configurations, place a transactiontypes.conf in $SPLUNK_HOME/etc/system/local/. You must restart <br># Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;TRANSACTIONTYPE&gt;]<br>* Create any number of transaction types, each represented by a stanza name and any number of the <br>&nbsp;&nbsp;following attribute/value pairs.<br>* Use the stanza name, [&lt;TRANSACTIONTYPE&gt;], to search for the transaction in Splunk Web.<br>* If you do not specify an entry for each of the following attributes, Splunk uses the default <br>&nbsp;&nbsp;value.<br><br>maxspan = [&lt;integer&gt; s|m|h|d|-1]<br>* Set the maximum time span for the transaction.<br>* Can be in seconds, minutes, hours, or days, or -1 for an unlimited timespan.<br>&nbsp;* For example: &nbsp;5s, 6m, 12h or 30d.<br>* Defaults to: maxspan=-1<br><br>maxpause = [&lt;integer&gt; s|m|h|d|-1]<br>* Set the maximum pause between the events in a transaction.<br>* Can be in seconds, minutes, hours, or days, or -1 for an unlimited pause.<br>&nbsp;* For example: &nbsp;5s, 6m, 12h or 30d.<br>* Defaults to: maxpause=-1<br><br>maxevents = &lt;integer&gt;<br>* The maximum number of events in a transaction. This constraint is disabled if the value is a <br>&nbsp;&nbsp;negative integer.<br>* Defaults to: maxevents=1000<br><br>fields = &lt;comma-separated list of fields&gt;<br>* If set, each event must have the same field(s) to be considered part of the same transaction.<br>&nbsp;* For example: fields=host,cookie<br>* Defaults to: ""<br><br>connected=[true|false]<br>* Relevant only if fields (see above) is not empty. Controls whether an event that is not <br>&nbsp;&nbsp;inconsistent and not consistent with the fields of a transaction opens a new transaction <br>&nbsp;&nbsp;(connected=true) or is added to the transaction. <br>* An event can be not inconsistent and not field-consistent if it contains fields required by the <br>&nbsp;&nbsp;transaction but none of these fields has been instantiated in the transaction (by a previous <br>&nbsp;&nbsp;event addition).<br>* Defaults to: connected=true<br><br>startswith=&lt;transam-filter-string&gt;<br>* A search or eval filtering expression which, if satisfied by an event, marks the beginning of a <br>&nbsp;&nbsp;new transaction.<br>* For example:<br>&nbsp;&nbsp;* startswith="login"<br>&nbsp;&nbsp;* startswith=(username=foobar)<br>&nbsp;&nbsp;* startswith=eval(speed_field &lt; max_speed_field)<br>&nbsp;&nbsp;* startswith=eval(speed_field &lt; max_speed_field/12)<br>* Defaults to: ""<br><br>endswith=&lt;transam-filter-string&gt;<br>* A search or eval filtering expression which, if satisfied by an event, marks the end of a <br>&nbsp;&nbsp;transaction.<br>* For example:<br>&nbsp;&nbsp;* endswith="logout"<br>&nbsp;&nbsp;* endswith=(username=foobar)<br>&nbsp;&nbsp;* endswith=eval(speed_field &gt; max_speed_field)<br>&nbsp;&nbsp;* endswith=eval(speed_field &gt; max_speed_field/12)<br>* Defaults to: ""<br><br>* For startswith/endswith &lt;transam-filter-string&gt; has the following syntax: <br>* syntax: &nbsp;&nbsp;"&lt;search-expression&gt;" | (&lt;quoted-search-expression&gt;) | eval(&lt;eval-expression&gt;)<br>* Where: <br>&nbsp;&nbsp;* &lt;search-expression&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is a valid search expression that does not contain quotes<br>&nbsp;&nbsp;* &lt;quoted-search-expression&gt; is a valid search expression that contains quotes<br>&nbsp;&nbsp;* &lt;eval-expression&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is a valid eval expression that evaluates to a boolean. For example,<br>&nbsp;&nbsp;&nbsp;&nbsp;startswith=eval(foo&lt;bar*2) will match events where foo is less than 2 x bar.<br>* Examples:<br>&nbsp;* "&lt;search expression&gt;": &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startswith="foo bar"<br>&nbsp;* &lt;quoted-search-expression&gt;: &nbsp;startswith=(name="mildred")<br>&nbsp;* &lt;quoted-search-expression&gt;: &nbsp;startswith=("search literal")<br>&nbsp;* eval(&lt;eval-expression&gt;): &nbsp;&nbsp;&nbsp;&nbsp;startswith=eval(distance/time &lt; max_speed)<br><br>### memory constraint options ###<br><br>maxopentxn=&lt;int&gt;<br>* Specifies the maximum number of not yet closed transactions to keep in the open pool. When this <br>&nbsp;&nbsp;limit is surpassed, Splunk begins evicting transactions using LRU (least-recently-used memory <br>&nbsp;&nbsp;cache algorithm) policy.<br>* The default value of this attribute is read from the transactions stanza in limits.conf.<br><br>maxopenevents=&lt;int&gt;<br>* Specifies the maximum number of events (can be) part of open transactions. When this limit is <br>&nbsp;&nbsp;surpassed, Splunk begins evicting transactions using LRU (least-recently-used memory cache <br>&nbsp;&nbsp;algorithm) policy.<br>* The default value of this attribute is read from the transactions stanza in limits.conf.<br><br>keepevicted=&lt;bool&gt;<br>* Whether to output evicted transactions. Evicted transactions can be distinguished from <br>&nbsp;&nbsp;non-evicted transactions by checking the value of the 'evicted' field, which is set to '1' for <br>&nbsp;&nbsp;evicted transactions.<br>* Defaults to: keepevicted=false<br><br>### multivalue rendering options ###<br><br>mvlist=&lt;bool&gt;|&lt;field-list&gt;<br>* Field controlling whether the multivalued fields of the transaction are (1) a list of the original <br>* events ordered in arrival order or (2) a set of unique field values ordered lexigraphically. If a <br>* comma/space delimited list of fields is provided only those fields are rendered as lists<br>* Defaults to: mvlist=f<br><br>delim=&lt;string&gt;<br>* A string used to delimit the original event values in the transaction event fields.<br>* Defaults to: delim=" "<br><br>nullstr=&lt;string&gt;<br>* The string value to use when rendering missing field values as part of mv fields in a <br>&nbsp;&nbsp;transaction.<br>* This option applies only to fields that are rendered as lists.<br>* Defaults to: nullstr=NULL<br><br>### values only used by the searchtxn search command ###<br><br>search=&lt;string&gt;<br>* A search string used to more efficiently seed transactions of this type.<br>* The value should be as specific as possible, to limit the number of events that must be retrieved <br>&nbsp;&nbsp;to find transactions.<br>* Example: sourcetype="sendmaill_sendmail"<br>* Defaults to "*" (all events)<br><br></font></code>
<h3> <a name="transactiontypesconf_transactiontypes.conf.example"><span class="mw-headline" id="transactiontypes.conf.example">transactiontypes.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This is an example transactiontypes.conf. &nbsp;Use this file as a template to configure transactions types.<br>#<br># To use one or more of these configurations, copy the configuration block into transactiontypes.conf <br># in $SPLUNK_HOME/etc/system/local/.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[default]<br>maxspan = 5m<br>maxpause = 2s<br>match = closest<br><br>[purchase]<br>maxspan &nbsp;= 10m<br>maxpause = 5m<br>fields &nbsp;= userid<br><br><br></font></code>

<a name="transformsconf"></a><h2> <a name="transformsconf_transforms.conf"><span class="mw-headline" id="transforms.conf">transforms.conf</span></a></h2>
<p>The following are the spec and example files for transforms.conf.
</p>
<h3> <a name="transformsconf_transforms.conf.spec"><span class="mw-headline" id="transforms.conf.spec">transforms.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains attributes and values that you can use to configure data transformations.<br># and event signing in transforms.conf.<br>#<br># Transforms.conf is commonly used for:<br># * Configuring regex-based host and source type overrides. <br># * Anonymizing certain types of sensitive incoming data, such as credit card or social <br># &nbsp;&nbsp;security numbers. <br># * Routing specific events to a particular index, when you have multiple indexes. <br># * Creating new index-time field extractions. NOTE: We do not recommend adding to the set of <br># &nbsp;&nbsp;fields that are extracted at index time unless it is absolutely necessary because there<br># &nbsp;&nbsp;are negative performance implications.<br># * Creating advanced search-time field extractions that involve one or more of the following:<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Reuse of the same field-extracting regular expression across multiple sources, <br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;source types, or hosts.<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Application of more than one regex to the same source, source type, or host.<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Using a regex to extract one or more values from the values of another field.<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Delimiter-based field extractions (they involve field-value pairs that are <br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separated by commas, colons, semicolons, bars, or something similar).<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Extraction of multiple values for the same field (multivalued field extraction).<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Extraction of fields with names that begin with numbers or underscores.<br>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: Less complex search-time field extractions can be set up entirely in props.conf.<br># * Setting up lookup tables that look up fields from external sources.<br>#<br># All of the above actions require corresponding settings in props.conf.<br>#<br># You can find more information on these topics by searching the Splunk documentation <br># (http://docs.splunk.com/Documentation)<br>#<br># There is a transforms.conf file in $SPLUNK_HOME/etc/system/default/. To set custom <br># configurations, place a transforms.conf $SPLUNK_HOME/etc/system/local/. For examples, see the <br># transforms.conf.example file.<br>#<br># You can enable configurations changes made to transforms.conf by typing the following search <br># string in Splunk Web:<br>#<br># | extract reload=t <br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;unique_transform_stanza_name&gt;]<br>* Name your stanza. Use this name when you configure field extractions, lookup tables, and event <br>&nbsp;&nbsp;routing in props.conf. For example, if you are setting up an advanced search-time field <br>&nbsp;&nbsp;extraction, in props.conf you would add REPORT-&lt;class&gt; = &lt;unique_transform_stanza_name&gt; under <br>&nbsp;&nbsp;the [&lt;spec&gt;] stanza that corresponds with a stanza you've created in transforms.conf.<br>* Follow this stanza name with any number of the following attribute/value pairs, as appropriate<br>&nbsp;&nbsp;for what you intend to do with the transform. &nbsp;<br>* If you do not specify an entry for each attribute, Splunk uses the default value.<br><br>REGEX = &lt;regular expression&gt;<br>* Enter a regular expression to operate on your data. <br>* NOTE: This attribute is valid for both index-time and search-time field extraction.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* REGEX is required for all search-time transforms unless you are setting up a <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;delimiter-based field extraction, in which case you use DELIMS (see the DELIMS attribute <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description, below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* REGEX is required for all index-time transforms.<br>* REGEX and the FORMAT attribute:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Name-capturing groups in the REGEX are extracted directly to fields. This means that you<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do not need to specify the FORMAT attribute for simple field extraction cases (see the <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description of FORMAT, below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the REGEX extracts both the field name and its corresponding field value, you can use <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the following special capturing groups if you want to skip specifying the mapping in <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FORMAT: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_KEY_&lt;string&gt;, _VAL_&lt;string&gt;. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* For example, the following are equivalent:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Using FORMAT:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* REGEX &nbsp;= ([a-z]+)=([a-z]+)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* FORMAT = $1::$2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Without using FORMAT<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* REGEX &nbsp;= (?&lt;_KEY_1&gt;[a-z]+)=(?&lt;_VAL_1&gt;[a-z]+)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When using either of the above formats, in a search-time extraction, the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regex will continue to match against the source text, extracting as many<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fields as can be identified in the source text.<br>* Defaults to an empty string.<br><br>FORMAT = &lt;string&gt;<br>* NOTE: This option is valid for both index-time and search-time field extraction. However, FORMAT <br>&nbsp;&nbsp;behaves differently depending on whether the extraction is performed at index time or <br>&nbsp;&nbsp;search time.<br>* This attribute specifies the format of the event, including any field names or values you want <br>&nbsp;&nbsp;to add.<br>* FORMAT for index-time extractions:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Use $n (for example $1, $2, etc) to specify the output of each REGEX match. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If REGEX does not have n groups, the matching fails. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The special identifier $0 represents what was in the DEST_KEY before the REGEX was performed.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* At index time only, you can use FORMAT to create concatenated fields:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* FORMAT = ipaddress::$1.$2.$3.$4<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* When you create concatenated fields with FORMAT, "$" is the only special character. It is <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;treated as a prefix for regex-capturing groups only if it is followed by a number and only <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if the number applies to an existing capturing group. So if REGEX has only one capturing <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group and its value is "bar", then:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* "FORMAT = foo$1" yields "foobar"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* "FORMAT = foo$bar" yields "foo$bar"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* "FORMAT = foo$1234" yields "foo$1234"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* "FORMAT = foo$1\$2" yields "foobar\$2"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* At index-time, FORMAT defaults to &lt;stanza-name&gt;::$1<br>* FORMAT for search-time extractions:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The format of this field as used during search time extractions is as follows:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* FORMAT = &lt;field-name&gt;::&lt;field-value&gt;( &lt;field-name&gt;::&lt;field-value&gt;)* <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* where:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* field-name &nbsp;= [&lt;string&gt;|$&lt;extracting-group-number&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* field-value = [&lt;string&gt;|$&lt;extracting-group-number&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Search-time extraction examples:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 1. FORMAT = first::$1 second::$2 third::other-value<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 2. FORMAT = $1::$2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If the key-name of a FORMAT setting is varying, for example $1 in the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;example 2 just above, then the regex will continue to match against<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the source key to extract as many matches as are present in the text.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* NOTE: You cannot create concatenated fields with FORMAT at search time. That <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality is only available at index time.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* At search-time, FORMAT defaults to an empty string.<br><br>CLONE_SOURCETYPE = &lt;string&gt;<br>* Specifies the sourcetype of a cloned data stream.<br>* The value string must be nonempty.<br>* The value string should be different from the sourcetype of the original data stream.<br>&nbsp;&nbsp;If it is the same, no cloning happens and a warning will be logged.<br><br>LOOKAHEAD = &lt;integer&gt;<br>* NOTE: This option is valid for all index time transforms, such as index-time<br>&nbsp;&nbsp;field creation, or DEST_KEY modifications.<br>* Optional. Specifies how many characters to search into an event.<br>* Defaults to 4096. You may want to increase this value if you have event line lengths that <br>&nbsp;&nbsp;exceed 4096 characters (before linebreaking).<br><br>WRITE_META = [true|false]<br>* NOTE: This attribute is only valid for index-time field extractions.<br>* Automatically writes REGEX to metadata.<br>* Required for all index-time field extractions except for those where DEST_KEY = _meta (see <br>&nbsp;&nbsp;the description of the DEST_KEY attribute, below)<br>* Use instead of DEST_KEY = _meta.<br>* Defaults to false.<br><br>DEST_KEY = &lt;KEY&gt;<br>* NOTE: This attribute is only valid for index-time field extractions.<br>* Specifies where Splunk stores the expanded FORMAT results in accordance with the REGEX match.<br>* Required for index-time field extractions where WRITE_META = false or is not set.<br>* For index-time extractions, DEST_KEY can be set to a number of values<br>&nbsp;&nbsp;mentioned in the KEYS section at the bottom of this file.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If DEST_KEY = _meta (not recommended) you should also add $0 to the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;start of your FORMAT attribute. &nbsp;$0 represents the DEST_KEY value<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;before Splunk performs the REGEX (in other words, _meta).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The $0 value is in no way derived *from* the REGEX match. (It<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;does not represent a captured group.)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* KEY names are case-sensitive, and should be used exactly as they appear in the KEYs list at<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the bottom of this file. (For example, you would say DEST_KEY = MetaData:Host, *not* <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DEST_KEY = metadata:host .)<br><br>DEFAULT_VALUE = &lt;string&gt;<br>* NOTE: This attribute is only valid for index-time field extractions.<br>* Optional. Splunk writes the DEFAULT_VALUE to DEST_KEY if the REGEX fails.<br>* Defaults to empty.<br><br>SOURCE_KEY = &lt;string&gt;<br>* NOTE: This attribute is valid for both index-time and search-time field extractions.<br>* Optional. Defines the KEY that Splunk applies the REGEX to. <br>* For search time extractions, you can use this attribute to extract one or more values from <br>&nbsp;&nbsp;the values of another field. You can use any field that is available at the time of the <br>&nbsp;&nbsp;execution of this field extraction.<br>* For index-time extractions use the KEYs described at the bottom of this file. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* KEYs are case-sensitive, and should be used exactly as they appear in the KEYs list at<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the bottom of this file. (For example, you would say SOURCE_KEY = MetaData:Host, *not* <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SOURCE_KEY = metadata:host .)<br>* If &lt;string&gt; starts with "field:" or "fields:" the meaning is changed. Instead of <br>&nbsp;&nbsp;looking up a KEY, it instead looks up an already indexed field. &nbsp;For example, if<br>&nbsp;&nbsp;a CSV field name "price" was indexed then "SOURCE_KEY = field:price" causes the REGEX<br>&nbsp;&nbsp;to match against the contents of that field. &nbsp;It's also possible to list multiple<br>&nbsp;&nbsp;fields here with "SOURCE_KEY = fields:name1,name2,name3" which causes MATCH to be<br>&nbsp;&nbsp;run against a string comprising of all three values, separated by space characters.<br>* SOURCE_KEY is typically used in conjunction with REPEAT_MATCH in index-time field <br>&nbsp;&nbsp;transforms.<br>* Defaults to _raw, which means it is applied to the raw, unprocessed text of all events.<br><br>REPEAT_MATCH = [true|false]<br>* NOTE: This attribute is only valid for index-time field extractions.<br>* Optional. When set to true Splunk runs the REGEX multiple times on the SOURCE_KEY. <br>* REPEAT_MATCH starts wherever the last match stopped, and continues until no more matches are <br>&nbsp;&nbsp;found. Useful for situations where an unknown number of REGEX matches are expected per<br>&nbsp;&nbsp;event.<br>* Defaults to false.<br><br>DELIMS = &lt;quoted string list&gt;<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* IMPORTANT: If a value may contain an embedded unescaped double quote character, <br>&nbsp;&nbsp;such as "foo"bar", use REGEX, not DELIMS. An escaped double quote (\") is ok.<br>* Optional. Used in place of REGEX when dealing with delimiter-based field extractions, <br>&nbsp;&nbsp;where field values (or field/value pairs) are separated by delimiters such as colons, <br>&nbsp;&nbsp;spaces, line breaks, and so on.<br>* Sets delimiter characters, first to separate data into field/value pairs, and then to <br>&nbsp;&nbsp;separate field from value.<br>* Each individual character in the delimiter string is used as a delimiter to split the event.<br>* Delimiters must be quoted with " " (use \ to escape).<br>* When the event contains full delimiter-separated field/value pairs, you enter two sets of <br>&nbsp;&nbsp;quoted characters for DELIMS: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The first set of quoted delimiters extracts the field/value pairs.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* The second set of quoted delimiters separates the field name from its corresponding<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value.<br>* When the event only contains delimiter-separated values (no field names) you use just one set<br>&nbsp;&nbsp;of quoted delimiters to separate the field values. Then you use the FIELDS attribute to<br>&nbsp;&nbsp;apply field names to the extracted values (see FIELDS, below).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Alternately, Splunk reads even tokens as field names and odd tokens as field values.<br>* Splunk consumes consecutive delimiter characters unless you specify a list of field names.<br>* The following example of DELIMS usage applies to an event where field/value pairs are <br>&nbsp;&nbsp;separated by '|' symbols and the field names are separated from their corresponding values <br>&nbsp;&nbsp;by '=' symbols:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[pipe_eq]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DELIMS = "|", "="<br>* Defaults to "". &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;<br>FIELDS = &lt;quoted string list&gt;<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* Used in conjunction with DELIMS when you are performing delimiter-based field extraction <br>&nbsp;&nbsp;and only have field values to extract. <br>* FIELDS enables you to provide field names for the extracted field values, in list format <br>&nbsp;&nbsp;according to the order in which the values are extracted.<br>* NOTE: If field names contain spaces or commas they must be quoted with " " (to escape, <br>&nbsp;&nbsp;use \).<br>* The following example is a delimiter-based field extraction where three field values appear<br>&nbsp;&nbsp;in an event. They are separated by a comma and then a space.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[commalist]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DELIMS = ", "<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FIELDS = field1, field2, field3<br>* Defaults to "".<br><br>MV_ADD = [true|false]<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* Optional. Controls what the extractor does when it finds a field which already exists.<br>* If set to true, the extractor makes the field a multivalued field and appends the <br>* newly found value, otherwise the newly found value is discarded.<br>* Defaults to false<br><br>CLEAN_KEYS = [true|false]<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* Optional. Controls whether Splunk "cleans" the keys (field names) it extracts at search time. <br>&nbsp;&nbsp;"Key cleaning" is the practice of replacing any non-alphanumeric characters (characters other<br>&nbsp;&nbsp;than those falling between the a-z, A-Z, or 0-9 ranges) in field names with underscores, as <br>&nbsp;&nbsp;well as the stripping of leading underscores and 0-9 characters from field names.<br>* Add CLEAN_KEYS = false to your transform if you need to extract field names that include <br>&nbsp;&nbsp;non-alphanumeric characters, or which begin with underscores or 0-9 characters.<br>* Defaults to true.<br><br>KEEP_EMPTY_VALS = [true|false]<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* Optional. Controls whether Splunk keeps field/value pairs when the value is an empty string.<br>* This option does not apply to field/value pairs that are generated by Splunk's autokv <br>&nbsp;&nbsp;extraction. Autokv ignores field/value pairs with empty values.<br>* Defaults to false.<br><br>CAN_OPTIMIZE = [true|false]<br>* NOTE: This attribute is only valid for search-time field extractions.<br>* Optional. Controls whether Splunk can optimize this extraction out (another way of saying<br>&nbsp;&nbsp;the extraction is disabled). <br>* You might use this if you're running searches under a Search Mode setting that disables field <br>&nbsp;&nbsp;discovery--it ensures that Splunk *always* discovers specific fields.<br>* Splunk only disables an extraction if it can determine that none of the fields identified by <br>&nbsp;&nbsp;the extraction will ever be needed for the successful evaluation of a search. <br>* NOTE: This option should be rarely set to false.<br>* Defaults to true.<br><br><br>#*******<br># Lookup tables<br>#*******<br># NOTE: Lookup tables are used ONLY during search time<br><br>filename = &lt;string&gt;<br>* Name of static lookup file. &nbsp;<br>* File should be in $SPLUNK_HOME/etc/&lt;app_name&gt;/lookups/ for some &lt;app_name&gt;, or in <br>&nbsp;&nbsp;$SPLUNK_HOME/etc/system/lookups/<br>* If file is in multiple 'lookups' directories, no layering is done. &nbsp;<br>* Standard conf file precedence is used to disambiguate.<br>* Defaults to empty string.<br><br>collection = &lt;string&gt;<br>* Name of the collection to use for this lookup. &nbsp;<br>* Collection should be defined in $SPLUNK_HOME/etc/&lt;app_name&gt;/collections.conf &nbsp;<br>&nbsp;&nbsp;for some &lt;app_name&gt;<br>* If collection is in multiple collections.conf file, no layering is done. &nbsp;<br>* Standard conf file precedence is used to disambiguate.<br>* Defaults to empty string (in which case the name of the stanza is used).<br><br>max_matches = &lt;integer&gt;<br>* The maximum number of possible matches for each input lookup value (range 1 - 1000).<br>* If the lookup is non-temporal (not time-bounded, meaning the time_field attribute is <br>&nbsp;&nbsp;not specified), Splunk uses the first &lt;integer&gt; entries, in file order. &nbsp;&nbsp;<br>* If the lookup is temporal, Splunk uses the first &lt;integer&gt; entries in descending time order.<br>&nbsp;&nbsp;In other words, up &lt;max_matches&gt; lookup entries will be allowed to match, and<br>&nbsp;&nbsp;if more than this many the ones nearest to the lookup value will be used.<br>* Default = 1000 if the lookup is not temporal, default = 1 if it is temporal.<br><br>min_matches = &lt;integer&gt;<br>* Minimum number of possible matches for each input lookup value.<br>* Default = 0 for both temporal and non-temporal lookups, which means that Splunk outputs <br>&nbsp;&nbsp;nothing if it cannot find any matches.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* However, if min_matches &gt; 0, and Splunk get less than min_matches, then Splunk provides <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the default_match value provided (see below).<br><br>default_match = &lt;string&gt;<br>* If min_matches &gt; 0 and Splunk has less than min_matches for any given input, it provides <br>&nbsp;&nbsp;this default_match value one or more times until the min_matches threshold is reached.<br>* Defaults to empty string. &nbsp;<br><br>case_sensitive_match = &lt;bool&gt;<br>* If set to false, case insensitive matching will be performed for all fields in a lookup <br>&nbsp;&nbsp;table<br>* Defaults to true (case sensitive matching)<br><br>match_type = &lt;string&gt;<br>* A comma and space-delimited list of &lt;match_type&gt;(&lt;field_name&gt;) specification to allow for <br>&nbsp;&nbsp;non-exact matching<br>* The available match_type values are WILDCARD, CIDR, and EXACT. &nbsp;EXACT is the default and <br>&nbsp;&nbsp;does not need to be specified. &nbsp;Only fields that should use WILDCARD or CIDR matching should <br>&nbsp;&nbsp;be specified in this list<br><br>external_cmd = &lt;string&gt;<br>* Provides the command and arguments to invoke to perform a lookup. Use this for external <br>&nbsp;&nbsp;(or "scripted") lookups, where you interface with with an external script rather than a <br>&nbsp;&nbsp;lookup table.<br>* This string is parsed like a shell command.<br>* The first argument is expected to be a python script (or executable file) located in <br>&nbsp;&nbsp;$SPLUNK_HOME/etc/&lt;app_name&gt;/bin (or ../etc/searchscripts).<br>* Presence of this field indicates that the lookup is external and command based.<br>* Defaults to empty string.<br><br>fields_list = &lt;string&gt;<br>* A comma- and space-delimited list of all fields that are supported by the external command.<br><br>external_type = [python|executable|kvstore]<br>* Type of external command. &nbsp;<br>* "python" a python script<br>* "executable" a binary executable<br>* Defaults to "python".<br><br>time_field = &lt;string&gt;<br>* Used for temporal (time bounded) lookups. Specifies the name of the field in the lookup <br>&nbsp;&nbsp;table that represents the timestamp.<br>* Defaults to an empty string, meaning that lookups are not temporal by default.<br><br>time_format = &lt;string&gt;<br>* For temporal lookups this specifies the 'strptime' format of the timestamp field.<br>* You can include subseconds but Splunk will ignore them.<br>* Defaults to&nbsp;%s.%Q or seconds from unix epoch in UTC an optional milliseconds.<br><br>max_offset_secs = &lt;integer&gt;<br>* For temporal lookups, this is the maximum time (in seconds) that the event timestamp can be <br>&nbsp;&nbsp;later than the lookup entry time for a match to occur.<br>* Default is 2000000000 (no maximum, effectively).<br><br>min_offset_secs = &lt;integer&gt;<br>* For temporal lookups, this is the minimum time (in seconds) that the event timestamp can be <br>&nbsp;&nbsp;later than the lookup entry timestamp for a match to occur.<br>* Defaults to 0.<br><br>batch_index_query = &lt;bool&gt;<br>* For large file based lookups, this determines whether queries can be grouped to improve <br>&nbsp;&nbsp;search performance.<br>* Default is unspecified here, but defaults to true (at global level in limits.conf)<br><br>allow_caching = &lt;bool&gt;<br>* Allow output from lookup scripts to be cached<br>* Default is true<br><br>max_ext_batch = &lt;integer&gt;<br>* The maximum size of external batch (range 1 - 1000).<br>* Only used with kvstore.<br>* Default = 300.<br><br>#*******<br># KEYS:<br>#*******<br>* NOTE: Keys are case-sensitive. Use the following keys exactly as they appear.<br><br>queue&nbsp;: Specify which queue to send the event to (can be nullQueue, indexQueue).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* indexQueue is the usual destination for events going through the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transform-handling processor.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* nullQueue is a destination which will cause the events to be dropped<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;entirely.<br>_raw &nbsp;: The raw text of the event.<br>_meta&nbsp;: A space-separated list of metadata for an event.<br>_time&nbsp;: The timestamp of the event, in seconds since 1/1/1970 UTC.<br><br>MetaData:Host &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: The host associated with the event.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The value must be prefixed by "host::"<br><br>_MetaData:Index &nbsp;&nbsp;&nbsp;&nbsp;: The index where the event should be stored.<br><br>MetaData:Source &nbsp;&nbsp;&nbsp;&nbsp;: The source associated with the event.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The value must be prefixed by "source::"<br><br>MetaData:Sourcetype&nbsp;: The sourcetype of the event.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The value must be prefixed by "sourcetype::"<br><br>_TCP_ROUTING &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: Comma separated list of tcpout group names (from outputs.conf)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults to groups present in 'defaultGroup' for [tcpout].<br><br>_SYSLOG_ROUTING &nbsp;&nbsp;&nbsp;&nbsp;: Comma separated list of syslog-stanza &nbsp;names (from outputs.conf)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults to groups present in 'defaultGroup' for [syslog].<br><br>* NOTE: Any KEY (field name) prefixed by '_' is not indexed by Splunk, in general.<br><br><br>[accepted_keys]<br><br>&lt;name&gt; = &lt;key&gt;<br><br>* Modifies Splunk's list of key names it considers valid when automatically<br>&nbsp;&nbsp;checking your transforms for use of undocumented SOURCE_KEY or DEST_KEY<br>&nbsp;&nbsp;values in index-time transformations.<br>* By adding entries to [accepted_keys], you can tell Splunk that a key that is<br>&nbsp;&nbsp;not documented is a key you intend to work for reasons that are valid in your<br>&nbsp;&nbsp;environment / app / etc.<br>* The 'name' element is simply used to disambiguate entries, similar to -class<br>&nbsp;&nbsp;entries in props.conf. &nbsp;The name can be anything of your chosing, including a<br>&nbsp;&nbsp;descriptive name for why you use the key.<br>* The entire stanza defaults to not being present, causing all keys not<br>&nbsp;&nbsp;documented just above to be flagged.<br><br></font></code>
<h3> <a name="transformsconf_transforms.conf.example"><span class="mw-headline" id="transforms.conf.example">transforms.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example transforms.conf. &nbsp;Use this file to create regexes and rules for transforms.<br># Use this file in tandem with props.conf.<br>#<br># To use one or more of these configurations, copy the configuration block into transforms.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># Note: These are examples. &nbsp;Replace the values with your own customizations.<br><br><br># Indexed field:<br><br>[netscreen-error]<br>REGEX = &nbsp;device_id=\[w+\](?&lt;err_code&gt;[^:]+)<br>FORMAT = err_code::$1<br>WRITE_META = true<br><br><br># Override host:<br><br>[hostoverride]<br>DEST_KEY = MetaData:Host<br>REGEX = \s(\w*)$<br>FORMAT = host::$1<br><br><br># Extracted fields:<br><br>[netscreen-error-field]<br>REGEX = device_id=\[w+\](?&lt;err_code&gt;[^:]+)<br>FORMAT = err_code::$1<br><br><br># Static lookup table<br><br>[mylookuptable]<br>filename = mytable.csv<br><br># one to one lookup<br># guarantees that we output a single lookup value for each input value, if no match exists, <br># we use the value of "default_match", which by default is "NONE"<br>[mylook]<br>filename = mytable.csv<br>max_matches = 1<br>min_matches = 1<br>default_match = nothing<br><br># external command lookup table<br><br>[myexternaltable]<br>external_cmd = testadapter.py blah<br>fields_list = foo bar<br><br># Temporal based static lookup table<br><br>[staticwtime]<br>filename = mytable.csv<br>time_field = timestamp<br>time_format =&nbsp;%d/%m/%y&nbsp;%H:%M:%S<br><br><br># Mask sensitive data:<br><br>[session-anonymizer]<br>REGEX = (?m)^(.*)SessionId=\w+(\w{4}[&amp;"].*)$<br>FORMAT = $1SessionId=########$2<br>DEST_KEY = _raw<br><br><br># Route to an alternate index:<br><br>[AppRedirect]<br>REGEX = Application<br>DEST_KEY = _MetaData:Index<br>FORMAT = Verbose<br><br><br># Extract comma-delimited values into fields:<br><br>[extract_csv]<br>DELIMS = ","<br>FIELDS = "field1", "field2", "field3"<br><br># This example assigns the extracted values from _raw to field1, field2 and field3 (in order of <br># extraction). If more than three values are extracted the values without a matching field name <br># are ignored.<br><br>[pipe_eq]<br>DELIMS = "|", "="<br><br># The above example extracts key-value pairs which are separated by '|'<br># while the key is delimited from value by '='.<br><br><br>[multiple_delims]<br>DELIMS = "|;", "=:"<br><br># The above example extracts key-value pairs which are separated by '|' or ';'.<br># while the key is delimited from value by '=' or ':'. <br><br><br>###### BASIC MODULAR REGULAR EXPRESSIONS DEFINITION START ###########<br># When adding a new basic modular regex PLEASE add a comment that lists <br># the fields that it extracts (named capturing groups), or whether it <br># provides a placeholder for the group name as:<br># Extracts: field1, field2....<br>#<br><br>[all_lazy]<br>REGEX = .*?<br><br>[all]<br>REGEX = .*<br><br>[nspaces]<br># matches one or more NON space characters<br>REGEX = \S+<br><br>[alphas]<br># matches a string containing only letters a-zA-Z<br>REGEX = [a-zA-Z]+<br><br>[alnums]<br># matches a string containing letters + digits<br>REGEX = [a-zA-Z0-9]+<br><br>[qstring]<br>#matches a quoted "string" - extracts an unnamed variable - name MUST be provided as in [[qstring:name]]<br># Extracts: empty-name-group (needs name)<br>REGEX = "(?&lt;&gt;[^"]*+)"<br><br>[sbstring]<br>#matches a string enclosed in [] - extracts an unnamed variable - name MUST be provided as in [[sbstring:name]]<br># Extracts: empty-name-group (needs name)<br>REGEX = \[(?&lt;&gt;[^\]]*+)\]<br><br>[digits]<br>REGEX = \d+<br><br>[int]<br># matches an integer or a hex number<br>REGEX = 0x[a-fA-F0-9]+|\d+<br><br>[float]<br># matches a float (or an int)<br>REGEX = \d*\.\d+|[[int]]<br><br>[octet] <br># this would match only numbers from 0-255 (one octet in an ip)<br>REGEX = (?:2(?:5[0-5]|[0-4][0-9])|[0-1][0-9][0-9]|[0-9][0-9]?)<br><br>[ipv4]<br># matches a valid IPv4 optionally followed by&nbsp;:port_num the octets in the ip would also be validated 0-255 range<br># Extracts: ip, port<br>REGEX = (?&lt;ip&gt;[[octet]](?:\.[[octet]]){3})(?::[[int:port]])?<br><br>[simple_url]<br># matches a url of the form proto://domain.tld/uri<br># Extracts: url, domain<br>REGEX = (?&lt;url&gt;\w++://(?&lt;domain&gt;[a-zA-Z0-9\-.:]++)(?:/[^\s"]*)?)<br><br>[url]<br># matches a url of the form proto://domain.tld/uri<br># Extracts: url, proto, domain, uri<br>REGEX = (?&lt;url&gt;[[alphas:proto]]://(?&lt;domain&gt;[a-zA-Z0-9\-.:]++)(?&lt;uri&gt;/[^\s"]*)?)<br><br>[simple_uri]<br># matches a uri of the form /path/to/resource?query &nbsp;<br># Extracts: uri, uri_path, uri_query<br>REGEX = (?&lt;uri&gt;(?&lt;uri_path&gt;[^\s\?"]++)(?:\\?(?&lt;uri_query&gt;[^\s"]+))?)<br><br>[uri]<br># uri &nbsp;= path optionally followed by query [/this/path/file.js?query=part&amp;other=var]<br># path = root part followed by file &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[/root/part/file.part]<br># Extracts: uri, uri_path, uri_root, uri_file, uri_query, uri_domain (optional if in proxy mode)<br>REGEX = (?&lt;uri&gt;(?:\w++://(?&lt;uri_domain&gt;[^/\s]++))?(?&lt;uri_path&gt;(?&lt;uri_root&gt;/+(?:[^\s\?;=/]*+/+)*)(?&lt;uri_file&gt;[^\s\?;=?/]*+))(?:\?(?&lt;uri_query&gt;[^\s"]+))?)<br><br><br>###### BASIC MODULAR REGULAR EXPRESSIONS DEFINITION END ###########<br><br></font></code>

<a name="ui-prefsconf"></a><h2> <a name="ui-prefsconf_ui-prefs.conf"><span class="mw-headline" id="ui-prefs.conf">ui-prefs.conf</span></a></h2>
<p>The following are the spec and example files for ui-prefs.conf.
</p>
<h3> <a name="ui-prefsconf_ui-prefs.conf.spec"><span class="mw-headline" id="ui-prefs.conf.spec">ui-prefs.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute/value pairs for ui preferences for a view.<br>#<br># There is a default ui-prefs.conf in $SPLUNK_HOME/etc/system/default. To set custom<br># configurations, place a ui-prefs.conf in $SPLUNK_HOME/etc/system/local/. To set custom configuration for an app, place<br># ui-prefs.conf in $SPLUNK_HOME/etc/apps/&lt;app_name&gt;/local/. <br># For examples, see ui-prefs.conf.example. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation<br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>[&lt;stanza name&gt;]<br>* Stanza name is the name of the xml view file<br><br>dispatch.earliest_time =<br>dispatch.latest_time =<br><br># Pref only options<br>display.prefs.autoOpenSearchAssistant = 0 | 1<br>display.prefs.timeline.height = &lt;string&gt;<br>display.prefs.timeline.minimized = 0 | 1<br>display.prefs.timeline.minimalMode = 0 | 1<br>display.prefs.aclFilter = [none|app|owner]<br>display.prefs.listMode = [tiles|table]<br>display.prefs.searchContext = &lt;string&gt;<br>display.prefs.events.count = [10|20|50]<br>display.prefs.statistics.count = [10|20|50|100]<br>display.prefs.fieldCoverage = [0|.01|.50|.90|1]<br>display.prefs.enableMetaData = 0 | 1<br>display.prefs.showDataSummary = 0 | 1<br><br>#******<br># Display Formatting Options<br>#******<br><br># General options<br>display.general.enablePreview = 0 | 1<br><br># Event options<br># TODO: uncomment the fields when we are ready to merge the values<br>display.events.fields = &lt;string&gt;<br>display.events.type = [raw|list|table]<br>display.events.rowNumbers = 0 | 1<br>display.events.maxLines = [0|5|10|20|50|100|200]<br>display.events.raw.drilldown = [inner|outer|full|none]<br>display.events.list.drilldown = [inner|outer|full|none]<br>display.events.list.wrap = 0 | 1<br>display.events.table.drilldown = 0 | 1<br>display.events.table.wrap = 0 | 1<br><br># Statistics options<br>display.statistics.rowNumbers = 0 | 1<br>display.statistics.wrap = 0 | 1<br>display.statistics.drilldown = [row|cell|none]<br><br># Visualization options<br>display.visualizations.type = [charting|singlevalue]<br>display.visualizations.chartHeight = &lt;int&gt;<br>display.visualizations.charting.chart = [line|area|column|bar|pie|scatter|radialGauge|fillerGauge|markerGauge]<br>display.visualizations.charting.chart.style = [minimal|shiny]<br>display.visualizations.charting.legend.labelStyle.overflowMode = [ellipsisEnd|ellipsisMiddle|ellipsisStart]<br><br># Patterns options<br>display.page.search.patterns.sensitivity = &lt;float&gt;<br><br># Page options<br>display.page.search.mode = [fast|smart|verbose]<br>display.page.search.timeline.format = [hidden|compact|full]<br>display.page.search.timeline.scale = [linear|log]<br>display.page.search.showFields = 0 | 1<br>display.page.home.showGettingStarted = 0 | 1<br><br></font></code>
<h3> <a name="ui-prefsconf_ui-prefs.conf.example"><span class="mw-headline" id="ui-prefs.conf.example">ui-prefs.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains example of ui preferences for a view.<br>#<br># To use one or more of these configurations, copy the configuration block into<br># ui-prefs.conf in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br>#<br># The following ui preferences will default timerange picker on the search page from All time to Today <br># We will store this ui-prefs.conf in $SPLUNK_HOME/etc/apps/search/local/ to only update search view of search app. <br>[search]<br>dispatch.earliest_time = @d<br>dispatch.latest_time = now<br><br></font></code>

<a name="user-prefsconf"></a><h2> <a name="user-prefsconf_user-prefs.conf"><span class="mw-headline" id="user-prefs.conf">user-prefs.conf</span></a></h2>
<p>The following are the spec and example files for user-prefs.conf.
</p>
<h3> <a name="user-prefsconf_user-prefs.conf.spec"><span class="mw-headline" id="user-prefs.conf.spec">user-prefs.conf.spec</span></a></h3>
<code><font size="2"><br># This file describes some of the settings that are used, and<br># can be configured on a per-user basis for use by the Splunk Web UI.<br><br># Settings in this file are requested with user and application scope<br># of the relevant user, and the user-prefs app. &nbsp;<br><br># Additionally, settings by the same name which are available in the<br># roles the user belongs to will be used at lower precedence.<br><br># This means interactive setting of these values will cause the values<br># to be updated in<br># $SPLUNK_HOME/etc/users/user-prefs/&lt;username&gt;/local/user-prefs.conf<br># where &lt;username&gt; is the username for the user altering their<br># preferences.<br><br># It also means that values in another app will never be used unless<br># they are exported globally (to system scope) or to the user-prefs<br># app.<br><br># In practice, providing values in other apps isn't very interesting,<br># since values from the authorize.conf roles settings are more<br># typically sensible ways to defaults for values in user-prefs.<br><br>[general]<br><br>default_namespace = &lt;app name&gt;<br>* Specifies the app that the user will see initially upon login to the<br>&nbsp;&nbsp;Splunk Web User Interface.<br>* This uses the "short name" of the app, such as launcher, or search,<br>&nbsp;&nbsp;which is synonymous with the app directory name.<br>* Splunk defaults this to 'launcher' via the default authorize.conf<br><br>tz = &lt;timezone&gt;<br>* Specifies the per-user timezone to use<br>* If unset, the timezone of the Splunk Server or Search Head is used.<br>* Only canonical timezone names such as America/Los_Angeles should be<br>&nbsp;&nbsp;used (for best results use the Splunk UI).<br>* Defaults to unset.<br><br><br>[default]<br># Additional settings exist, but are entirely UI managed.<br>&lt;setting&gt; = &lt;value&gt;<br><br><br></font></code>
<h3> <a name="user-prefsconf_user-prefs.conf.example"><span class="mw-headline" id="user-prefs.conf.example">user-prefs.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example user-prefs.conf. &nbsp;Use this file to configure settings on a per-user <br># basis for use by the Splunk Web UI.<br>#<br># To use one or more of these configurations, copy the configuration block into user-prefs.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># Note: These are examples. &nbsp;Replace the values with your own customizations.<br><br><br># EXAMPLE: Setting the default timezone to GMT for all Power and User role members. <br><br>[role_power]<br>tz = GMT <br><br>[role_user]<br>tz = GMT <br><br></font></code>

<a name="user-seedconf"></a><h2> <a name="user-seedconf_user-seed.conf"><span class="mw-headline" id="user-seed.conf">user-seed.conf</span></a></h2>
<p>The following are the spec and example files for user-seed.conf.
</p>
<h3> <a name="user-seedconf_user-seed.conf.spec"><span class="mw-headline" id="user-seed.conf.spec">user-seed.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># Specification for user-seed.conf. &nbsp;Allows configuration of Splunk's initial username and password.<br># Currently, only one user can be configured with user-seed.conf.<br>#<br># To override the default username and password, place user-seed.conf in <br># $SPLUNK_HOME/etc/system/local. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[user_info]<br>USERNAME = &lt;string&gt; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Username you want to associate with a password.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is Admin.<br>PASSWORD = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Password you wish to set for that user.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Default is changeme.<br><br></font></code>
<h3> <a name="user-seedconf_user-seed.conf.example"><span class="mw-headline" id="user-seed.conf.example">user-seed.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example user-seed.conf. &nbsp;Use this file to create an initial login.<br>#<br># NOTE: To change the default start up login and password, this file must be in <br># $SPLUNK_HOME/etc/system/default/ prior to starting Splunk for the first time.<br>#<br># To use this configuration, copy the configuration block into user-seed.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br>[user_info]<br>USERNAME = admin<br>PASSWORD = myowndefaultPass<br><br></font></code>

<a name="viewstatesconf"></a><h2> <a name="viewstatesconf_viewstates.conf"><span class="mw-headline" id="viewstates.conf">viewstates.conf</span></a></h2>
<p>The following are the spec and example files for viewstates.conf.
</p>
<h3> <a name="viewstatesconf_viewstates.conf.spec"><span class="mw-headline" id="viewstates.conf.spec">viewstates.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file explains how to format viewstates.<br>#<br># To use this configuration, copy the configuration block into viewstates.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br><br>[&lt;view_name&gt;:&lt;viewstate_id&gt;]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Auto-generated persistence stanza label that corresponds to UI views<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;view_name&gt; is the URI name (not label) of the view to persist<br>&nbsp;&nbsp;&nbsp;&nbsp;* if &lt;view_name&gt; = "*", then this viewstate is considered to be 'global'<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;viewstate_id&gt; is the unique identifier assigned to this set of parameters<br>&nbsp;&nbsp;&nbsp;&nbsp;* &lt;viewstate_id&gt; = '_current' is a reserved name for normal view 'sticky state'<br>&nbsp;&nbsp;&nbsp;&nbsp;* &lt;viewstate_id&gt; = '_empty' is a reserved name for no persistence, i.e., all defaults<br><br>&lt;module_id&gt;.&lt;setting_name&gt; = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;module_id&gt; is the runtime id of the UI module requesting persistence<br>&nbsp;&nbsp;&nbsp;&nbsp;* The &lt;setting_name&gt; is the setting designated by &lt;module_id&gt; to persist<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br><br></font></code>
<h3> <a name="viewstatesconf_viewstates.conf.example"><span class="mw-headline" id="viewstates.conf.example">viewstates.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example viewstates.conf.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>[charting:g3b5fa7l]<br>ChartTypeFormatter_0_7_0.default = area<br>Count_0_6_0.count = 10<br>LegendFormatter_0_13_0.default = right<br>LineMarkerFormatter_0_10_0.default = false<br>NullValueFormatter_0_12_0.default = gaps<br><br>[*:g3jck9ey]<br>Count_0_7_1.count = 20<br>DataOverlay_0_12_0.dataOverlayMode = none<br>DataOverlay_1_13_0.dataOverlayMode = none<br>FieldPicker_0_6_1.fields = host sourcetype source date_hour date_mday date_minute date_month<br>FieldPicker_0_6_1.sidebarDisplay = True<br>FlashTimeline_0_5_0.annotationSearch = search index=twink<br>FlashTimeline_0_5_0.enableAnnotations = true<br>FlashTimeline_0_5_0.minimized = false<br>MaxLines_0_13_0.maxLines = 10<br>RowNumbers_0_12_0.displayRowNumbers = true<br>RowNumbers_1_11_0.displayRowNumbers = true<br>RowNumbers_2_12_0.displayRowNumbers = true<br>Segmentation_0_14_0.segmentation = full<br>SoftWrap_0_11_0.enable = true<br><br>[dashboard:_current]<br>TimeRangePicker_0_1_0.selected = All time<br><br></font></code>

<a name="webconf"></a><h2> <a name="webconf_web.conf"><span class="mw-headline" id="web.conf">web.conf</span></a></h2>
<p>The following are the spec and example files for web.conf.
</p>
<h3> <a name="webconf_web.conf.spec"><span class="mw-headline" id="web.conf.spec">web.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attributes and values you can use to configure Splunk's web interface.<br>#<br># There is a web.conf in $SPLUNK_HOME/etc/system/default/. &nbsp;To set custom configurations, <br># place a web.conf in $SPLUNK_HOME/etc/system/local/. &nbsp;For examples, see web.conf.example.<br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br>[settings]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set general SplunkWeb configuration options under this stanza name.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Follow this stanza name with any number of the following attribute/value pairs. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If you do not specify an entry for each attribute, Splunk will use the default value.<br><br>startwebserver = [0 | 1]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set whether or not to start SplunkWeb.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 0 disables SplunkWeb, 1 enables it.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1.<br><br>httpport = &lt;port_number&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Must be present for SplunkWeb to start.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If omitted or 0 the server will NOT start an http listener.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If using SSL, set to the HTTPS port number.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 8000.<br><br>mgmtHostPort = &lt;IP:port&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Location of splunkd.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Don't include http[s]:// -- just the IP address.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 127.0.0.1:8089.<br><br>appServerPorts = &lt;one or more port numbers&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Port number(s) for the python-based application server to listen on.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This port is bound only on the loopback interface -- it is not<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exposed to the network at large.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* If set to "0", this will prevent the application server from<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;being run from splunkd. &nbsp;Instead, splunkweb will be started as<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a separate python-based service which directly listens to the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'httpport'. &nbsp;This is how Splunk 6.1.X and earlier behaved.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Generally, you should only set one port number here. &nbsp;For most<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;deployments a single application server won't be a performance<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bottleneck. &nbsp;However you can provide a comma-separated list of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;port numbers here and splunkd will start a load-balanced<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;application server on each one.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* It is recommended that this be set to a non-zero value. &nbsp;Setting<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this to "0" should only be done if you experience a compatibility<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;problem. &nbsp;The new separate application server configuration is faster<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and supports more configuration options. &nbsp;Also, setting this to<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"0" may cause problems when using the Search Head Clustering feature<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(see the [shclustering] stanza in server.conf)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 8065.<br><br>splunkdConnectionTimeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Number of seconds to wait before timing out when communicating with splunkd<br>&nbsp;&nbsp;&nbsp;&nbsp;* Must be at least 30<br>&nbsp;&nbsp;&nbsp;&nbsp;* Values smaller than 30 will be ignored, resulting in the use of the default value<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 30<br><br>enableSplunkWebSSL = [True | False]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Toggle between http or https.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Set to true to enable https and SSL.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to False.<br>&nbsp;&nbsp;&nbsp;<br>privKeyPath = etc/auth/splunkweb/privkey.pem<br>&nbsp;&nbsp;&nbsp;&nbsp;* The path to the file containing the web server's SSL certificate's private key<br>&nbsp;&nbsp;&nbsp;&nbsp;* Relative paths are interpreted as relative to $SPLUNK_HOME<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Relative paths may not refer outside of $SPLUNK_HOME (eg. no ../somewhere)<br>&nbsp;&nbsp;&nbsp;&nbsp;* An absolute path can also be specified to an external key<br>&nbsp;&nbsp;&nbsp;&nbsp;* See also enableSplunkWebSSL and caCertPath<br><br>caCertPath = etc/auth/splunkweb/cert.pem<br>&nbsp;&nbsp;&nbsp;* The path to the file containing the SSL certificate for the splunk web server<br>&nbsp;&nbsp;&nbsp;* The file may also contain root and intermediate certificates, if required<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;They should be listed sequentially in the order:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ Server's SSL certificate ]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ One or more intermediate certificates, if required ]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ Root certificate, if required ]<br>&nbsp;&nbsp;&nbsp;* Relative paths are interpreted as relative to $SPLUNK_HOME<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Relative paths may not refer outside of $SPLUNK_HOME (eg. no ../somewhere)<br>&nbsp;&nbsp;&nbsp;* An absolute path can also be specified to an external certificate<br>&nbsp;&nbsp;&nbsp;* See also enableSplunkWebSSL and privKeyPath<br><br>serviceFormPostURL = http://docs.splunk.com/Documentation/Splunk<br>&nbsp;&nbsp;&nbsp;* This attribute is deprecated since 5.0.3<br><br>userRegistrationURL = https://www.splunk.com/page/sign_up<br>updateCheckerBaseURL = http://quickdraw.Splunk.com/js/<br>docsCheckerBaseURL = http://quickdraw.splunk.com/help<br>&nbsp;&nbsp;&nbsp;* These are various Splunk.com urls that are configurable. <br>&nbsp;&nbsp;&nbsp;* Setting updateCheckerBaseURL to 0 will stop the SplunkWeb from pinging Splunk.com <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for new versions of itself. <br><br>enable_insecure_login = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if the GET-based /account/insecurelogin endpoint is enabled<br>&nbsp;&nbsp;&nbsp;* Provides an alternate GET-based authentication mechanism<br>&nbsp;&nbsp;&nbsp;* If True, the /account/insecurelogin?username=USERNAME&amp;password=PASSWD is available<br>&nbsp;&nbsp;&nbsp;* If False, only the main /account/login endpoint is available<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>login_content = &lt;content_string&gt;<br>&nbsp;&nbsp;&nbsp;* Add custom content to the login page<br>&nbsp;&nbsp;&nbsp;* Supports any text including html <br>&nbsp;&nbsp;&nbsp;<br>sslVersions = &lt;list of ssl versions string&gt;<br>&nbsp;&nbsp;&nbsp;* Comma-separated list of SSL versions to support<br>&nbsp;&nbsp;&nbsp;* The versions available are "ssl2", "ssl3", "tls1.0", "tls1.1", and "tls1.2"<br>&nbsp;&nbsp;&nbsp;* The special version "*" selects all supported versions. &nbsp;The version "tls"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;selects all versions tls1.0 or newer<br>&nbsp;&nbsp;&nbsp;* If a version is prefixed with "-" it is removed from the list<br>&nbsp;&nbsp;&nbsp;* When appServerPorts=0 only supported values are &nbsp;"all", "ssl3, tls" and "tls"<br>&nbsp;&nbsp;&nbsp;* When configured in FIPS mode ssl2 and ssl3 are always disabled regardless of this configuration<br>&nbsp;&nbsp;&nbsp;* Defaults to "ssl3, tls". &nbsp;(anything newer than SSLv2)<br>&nbsp;&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br><br>supportSSLV3Only = [True | False]<br>&nbsp;&nbsp;&nbsp;* When appServerPorts is set to a non-zero value value (the default mode),<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this setting is DEPRECATED. &nbsp;SSLv2 is now disabled by default in this mode.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The exact set of SSL versions allowed is now configurable via the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sslVersions" setting above.<br>&nbsp;&nbsp;&nbsp;* When appServerPorts is set to zero, this controls wither we disallow SSLv2<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;connections.<br><br>cipherSuite = &lt;cipher suite string&gt;<br>&nbsp;&nbsp;&nbsp;* If set, uses the specified cipher string for the HTTP server.<br>&nbsp;&nbsp;&nbsp;* If not set, uses the default cipher string<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;provided by OpenSSL. &nbsp;This is used to ensure that the server does not <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accept connections using weak encryption protocols.<br>&nbsp;&nbsp;&nbsp;<br>ecdhCurveName = &lt;string&gt;<br>&nbsp;&nbsp;&nbsp;* ECDH curve to use for ECDH key negotiation<br>&nbsp;&nbsp;&nbsp;* We only support named curves specified by their SHORT name. <br>&nbsp;&nbsp;&nbsp;* (see struct ASN1_OBJECT in asn1.h)<br>&nbsp;&nbsp;&nbsp;* The list of valid named curves by their short/long names<br>&nbsp;&nbsp;&nbsp;* can be obtained by executing this command:<br>&nbsp;&nbsp;&nbsp;* $SPLUNK_HOME/bin/splunk cmd openssl ecparam -list_curves<br>&nbsp;&nbsp;&nbsp;* Default is empty string.<br><br>root_endpoint = &lt;URI_prefix_string&gt;<br>&nbsp;&nbsp;&nbsp;* defines the root URI path on which the appserver will listen<br>&nbsp;&nbsp;&nbsp;* default setting is '/'<br>&nbsp;&nbsp;&nbsp;* Ex: if you want to proxy the splunk UI at http://splunk:8000/splunkui, then set root_endpoint = /splunkui<br><br>static_endpoint = &lt;URI_prefix_string&gt;<br>&nbsp;&nbsp;&nbsp;* path to static content<br>&nbsp;&nbsp;&nbsp;* The path here is automatically appended to root_endpoint defined above<br>&nbsp;&nbsp;&nbsp;* default is /static<br><br>static_dir = &lt;relative_filesystem_path&gt;<br>&nbsp;&nbsp;&nbsp;* The directory that actually holds the static content<br>&nbsp;&nbsp;&nbsp;* This can be an absolute url if you want to put it elsewhere<br>&nbsp;&nbsp;&nbsp;* Default is share/splunk/search_mrsparkle/exposed<br><br>rss_endpoint = &lt;URI_prefix_string&gt;<br>&nbsp;&nbsp;&nbsp;* path to static rss content<br>&nbsp;&nbsp;&nbsp;* The path here is automatically appended to root_endpoint defined above<br>&nbsp;&nbsp;&nbsp;* default is /rss<br><br>embed_uri = &lt;URI&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* optional URI scheme/host/port prefix for embedded content<br>&nbsp;&nbsp;&nbsp;&nbsp;* This presents an optional strategy for exposing embedded shared <br>&nbsp;&nbsp;&nbsp;&nbsp;* content that does not require authentication in reverse proxy/SSO environment.<br>&nbsp;&nbsp;&nbsp;&nbsp;* default is empty and will resolve to the client window.location.protocol + "//" + window.location.host <br><br>embed_footer = &lt;html_string&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* chunk of html to define the footer for an embedded report.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to "splunk&gt;" but can be changed to whatever the user would like. <br><br>tools.staticdir.generate_indexes = [1 | 0]<br>&nbsp;&nbsp;&nbsp;* Indicates if the webserver will serve a directory listing for static directories<br>&nbsp;&nbsp;&nbsp;* Defaults to 0 (false)<br><br>template_dir = &lt;relative_filesystem_path&gt;<br>&nbsp;&nbsp;&nbsp;* base path to mako templates<br>&nbsp;&nbsp;&nbsp;* Defaults to share/splunk/search_mrsparkle/templates<br><br>module_dir = &lt;relative_filesystem_path&gt;<br>&nbsp;&nbsp;&nbsp;* base path to UI module assets<br>&nbsp;&nbsp;&nbsp;* Defaults to share/splunk/search_mrsparkle/modules<br>&nbsp;&nbsp;&nbsp;<br>enable_gzip = [True | False]<br>&nbsp;&nbsp;&nbsp;* Determines if webserver applies gzip compression to responses<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>use_future_expires = [True | False]<br>&nbsp;&nbsp;&nbsp;* Determines if the Expires header of /static files is set to a far-future date<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>flash_major_version = &lt;integer&gt;<br>flash_minor_version = &lt;integer&gt;<br>flash_revision_version = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the minimum Flash plugin version requirements<br>&nbsp;&nbsp;&nbsp;* Flash support, broken into three parts.<br>&nbsp;&nbsp;&nbsp;* We currently require a min baseline of Shockwave Flash 9.0 r124<br><br>simple_xml_force_flash_charting = [True | False]<br>&nbsp;&nbsp;&nbsp;* Specifies whether or not to force the use of FlashChart when rendering simple xml into view xml<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>override_JSON_MIME_type_with_text_plain = [True | False]<br>&nbsp;&nbsp;&nbsp;* Specifies whether or not to override the MIME type for JSON data served up by splunkweb endpoints with content-type="text/plain; charset=UTF-8"<br>&nbsp;&nbsp;&nbsp;* If True, splunkweb endpoints (other than proxy) that serve JSON data will serve as "text/plain; charset=UTF-8"<br>&nbsp;&nbsp;&nbsp;* If False, splunkweb endpoints that serve JSON data will serve as "application/json; charset=UTF-8"<br><br>enable_proxy_write = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if the /splunkd proxy endpoint allows POST operations<br>&nbsp;&nbsp;&nbsp;* If True, both GET and POST operations are proxied through to splunkd<br>&nbsp;&nbsp;&nbsp;* If False, only GET operations are proxied through to splunkd<br>&nbsp;&nbsp;&nbsp;* Setting this to False will prevent many client-side packages (such as the Splunk JavaScript SDK) from working correctly<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>js_logger_mode = [None | Firebug | Server]<br>&nbsp;&nbsp;&nbsp;* JavaScript Logger mode<br>&nbsp;&nbsp;&nbsp;* Available modes: None, Firebug, Server<br>&nbsp;&nbsp;&nbsp;* Mode None: Does not log anything<br>&nbsp;&nbsp;&nbsp;* Mode Firebug: Use firebug by default if it exists or defer to the older less promiscuous version of firebug lite<br>&nbsp;&nbsp;&nbsp;* Mode Server: Log to a defined server endpoint<br>&nbsp;&nbsp;&nbsp;* See js/logger.js Splunk.Logger.Mode for mode implementation details and if you would like to author your own<br>&nbsp;&nbsp;&nbsp;* Defaults to None<br><br>js_logger_mode_server_end_point = &lt;URI_relative_path&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the server endpoint to post javascript log messages<br>&nbsp;&nbsp;&nbsp;* Used when js_logger_mode = Server<br>&nbsp;&nbsp;&nbsp;* Defaults to util/log/js<br><br>js_logger_mode_server_poll_buffer = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifieds the interval in milliseconds to check, post and cleanse the javascript log buffer<br>&nbsp;&nbsp;&nbsp;* Defaults to 1000<br><br>js_logger_mode_server_max_buffer = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the maximum size threshold to post and cleanse the javascript log buffer<br>&nbsp;&nbsp;&nbsp;* Defaults to 100<br><br>ui_inactivity_timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the length of time lapsed (in minutes) for notification when there is no user interface clicking, mouseover, scrolling or resizing.<br>&nbsp;&nbsp;&nbsp;* Notifies client side pollers to stop, resulting in sessions expiring at the tools.sessions.timeout value.<br>&nbsp;&nbsp;&nbsp;* If less than 1, results in no timeout notification ever being triggered (Sessions will stay alive for as long as the browser is open).<br>&nbsp;&nbsp;&nbsp;* Defaults to 60 minutes<br><br>js_no_cache = [True | False]<br>&nbsp;&nbsp;&nbsp;* Toggle js cache control<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>cacheBytesLimit = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* When appServerPorts is set to a non-zero value, splunkd can keep a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;small cache of static assets in memory. &nbsp;When the total size of the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;objects in cache grows larger than this, we begin the process of<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ageing entries out.<br>&nbsp;&nbsp;&nbsp;* Defaults to 4194304 (i.e. 4 Megabytes)<br>&nbsp;&nbsp;&nbsp;* If set to zero, this cache is completely disabled<br><br>cacheEntriesLimit = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* When appServerPorts is set to a non-zero value, splunkd can keep a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;small cache of static assets in memory. &nbsp;When the number of the objects<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in cache grows larger than this, we begin the process of ageing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;entries out.<br>&nbsp;&nbsp;&nbsp;* Defaults to 16384<br>&nbsp;&nbsp;&nbsp;* If set to zero, this cache is completely disabled<br><br>staticCompressionLevel = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* When appServerPorts is set to a non-zero value, splunkd can keep a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;small cache of static assets in memory. &nbsp;These are stored<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;compressed and can usually be served directly to the web browser<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in compressed format.<br>&nbsp;&nbsp;&nbsp;* This level can be a number between 1 and 9. &nbsp;Lower numbers use less<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU time to compress objects, but the resulting compressed objects<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will be larger.<br>&nbsp;&nbsp;&nbsp;* Defaults to 9. &nbsp;Usually not much CPU time is spent compressing these<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;objects so there is not much benefit to decreasing this.<br><br>enable_autocomplete_login = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indictes if the main login page allows browsers to autocomplete the username<br>&nbsp;&nbsp;&nbsp;* If True, browsers may display an autocomplete drop down in the username field<br>&nbsp;&nbsp;&nbsp;* If False, browsers are instructed not to show autocomplete drop down in the username field<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>verifyCookiesWorkDuringLogin = [True | False]<br>&nbsp;&nbsp;&nbsp;* Normally, the login page will make an attempt to see if cookies work<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;properly in the user's browser before allowing them to log in. &nbsp;If<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this is set to False, this check is skipped.<br>&nbsp;&nbsp;&nbsp;* Defaults to True. &nbsp;It is recommended that this be left on.<br>&nbsp;&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br><br>minify_js = [True | False]<br>&nbsp;&nbsp;&nbsp;* indicates whether the static JS files for modules are consolidated and minified<br>&nbsp;&nbsp;&nbsp;* enabling improves client-side performance by reducing the number of HTTP requests and the size of HTTP responses<br><br>minify_css = [True | False]<br>&nbsp;&nbsp;&nbsp;* indicates whether the static CSS files for modules are consolidated and minified<br>&nbsp;&nbsp;&nbsp;* enabling improves client-side performance by reducing the number of HTTP requests and the size of HTTP responses<br>&nbsp;&nbsp;&nbsp;* due to browser limitations, disabling this when using IE9 and earlier may result in display problems.<br><br>trap_module_exceptions = [True | False]<br>&nbsp;&nbsp;&nbsp;* Toggle whether the JS for individual modules is wrapped in a try/catch<br>&nbsp;&nbsp;&nbsp;* If True, syntax errors in individual modules will not cause the UI to hang,<br>&nbsp;&nbsp;&nbsp;* other than when using the module in question<br>&nbsp;&nbsp;&nbsp;* Set this to False when developing apps.<br><br>enable_pivot_adhoc_acceleration = [True | False]<br>&nbsp;&nbsp;&nbsp;* Toggle whether the pivot interface will use its own ad-hoc acceleration when a data model is not accelerated.<br>&nbsp;&nbsp;&nbsp;* If True, this ad-hoc acceleration will be used to make reporting in pivot faster and more responsive.<br>&nbsp;&nbsp;&nbsp;* In situations where data is not stored in time order or where the majority of events are far in the past, disabling<br>&nbsp;&nbsp;&nbsp;* this behavior can improve the pivot experience.<br>&nbsp;&nbsp;&nbsp;* DEPRECATED in version 6.1 and later, use pivot_adhoc_acceleration_mode instead<br><br>pivot_adhoc_acceleration_mode = [Elastic | AllTime | None]<br>&nbsp;&nbsp;&nbsp;* Specify the type of ad-hoc acceleration used by the pivot interface when a data model is not accelerated.<br>&nbsp;&nbsp;&nbsp;* If Elastic, the pivot interface will only accelerate the time range specified for reporting, and will dynamically<br>&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;adjust when this time range is changed.<br>&nbsp;&nbsp;&nbsp;* If AllTime, the pivot interface will accelerate the relevant data over all time. &nbsp;This will make the interface<br>&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;more responsive to time-range changes but places a larger load on system resources.<br>&nbsp;&nbsp;&nbsp;* If None, the pivot interface will not use any acceleration. &nbsp;This means any change to the report will require<br>&nbsp;&nbsp;&nbsp;* &nbsp;&nbsp;restarting the search.<br>&nbsp;&nbsp;&nbsp;* Defaults to Elastic<br><br>jschart_test_mode = [True | False]<br>&nbsp;&nbsp;&nbsp;* Toggle whether JSChart module runs in Test Mode<br>&nbsp;&nbsp;&nbsp;* If True, JSChart module attaches HTML classes to chart elements for introspection<br>&nbsp;&nbsp;&nbsp;* This will negatively impact performance, so should be disabled unless actively in use.<br><br>#<br># JSChart data truncation configuration<br># To avoid negatively impacting browser performance, the JSChart library places a limit on the number of points that<br># will be plotted by an individual chart. &nbsp;This limit can be configured here either across all browsers or specifically<br># per-browser. &nbsp;An empty or zero value will disable the limit entirely.<br>#<br><br>jschart_truncation_limit = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Cross-broswer truncation limit, if defined takes precedence over the browser-specific limits below<br><br>jschart_truncation_limit.chrome = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Chrome only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.firefox = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Firefox only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.safari = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Safari only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.ie11 = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Internet Explorer 11 only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.ie10 = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Internet Explorer 10 only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.ie9 = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Internet Explorer 9 only<br>&nbsp;&nbsp;&nbsp;* Defaults to 20000<br>jschart_truncation_limit.ie8 = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Internet Explorer 8 only<br>&nbsp;&nbsp;&nbsp;* Defaults to 2000<br>jschart_truncation_limit.ie7 = &lt;int&gt;<br>&nbsp;&nbsp;&nbsp;* Chart truncation limit for Internet Explorer 7 only<br>&nbsp;&nbsp;&nbsp;* Defaults to 2000<br><br>max_view_cache_size = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum number of views to cache in the appserver.<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 300.<br><br>pdfgen_is_available = [0 | 1]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether Integrated PDF Generation is available on this search head<br>&nbsp;&nbsp;&nbsp;&nbsp;* This is used to bypass an extra call to splunkd <br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 1 on platforms where node is supported, defaults to 0 otherwise<br><br>version_label_format = &lt;printf_string&gt;<br>&nbsp;&nbsp;&nbsp;* internal config<br>&nbsp;&nbsp;&nbsp;* used to override the version reported by the UI to *.splunk.com resources<br>&nbsp;&nbsp;&nbsp;* defaults to:&nbsp;%s<br><br>auto_refresh_views = [0 | 1]<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies whether the following actions cause the appserver to ask splunkd to reload views from disk.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Logging in via the UI<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Switching apps<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Clicking the Splunk logo<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 0.<br>#<br># Header options<br>#<br>x_frame_options_sameorigin = [True | False]<br>&nbsp;&nbsp;&nbsp;&nbsp;* adds a X-Frame-Options header set to "SAMEORIGIN" to every response served by cherrypy<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>#<br># SSO<br>#<br><br>remoteUser = &lt;http_header_string&gt;<br>&nbsp;&nbsp;&nbsp;* Remote user HTTP header sent by the authenticating proxy server.<br>&nbsp;&nbsp;&nbsp;* This header should be set to the authenticated user.<br>&nbsp;&nbsp;&nbsp;* Defaults to 'REMOTE_USER'.<br>&nbsp;&nbsp;&nbsp;* Caution: There is a potential security concern regarding Splunk's treatment of HTTP headers.<br>&nbsp;&nbsp;&nbsp;* Your proxy provides the selected username as an HTTP header as specified above.<br>&nbsp;&nbsp;&nbsp;* If the browser or other http agent were to specify the value of this<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;header, probably any proxy would overwrite it, or in the case that the<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;username cannot be determined, refuse to pass along the request or set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it blank.<br>&nbsp;&nbsp;&nbsp;* However, Splunk (cherrypy) will normalize headers containing the dash,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and the underscore to the same value. &nbsp;For example USER-NAME and<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;USER_NAME will be treated as the same in SplunkWeb.<br>&nbsp;&nbsp;&nbsp;* This means that if the browser provides REMOTE-USER and splunk accepts<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;REMOTE_USER, theoretically the browser could dictate the username.<br>&nbsp;&nbsp;&nbsp;* In practice, however, in all our testing, the proxy adds its headers<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;last, which causes them to take precedence, making the problem moot.<br>&nbsp;&nbsp;&nbsp;* See also the 'remoteUserMatchExact' setting which can enforce more exact<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;header matching when running with appServerPorts enabled.<br><br>remoteUserMatchExact = [0 | 1]<br>&nbsp;&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;&nbsp;* When matching the remoteUser header, consider dashes and underscores<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distinct (so "Remote-User" and "Remote_User" will be considered different<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;headers<br>&nbsp;&nbsp;&nbsp;* Defaults to "0" for compatibility with older versions of Splunk, but<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;setting to "1" is a good idea when setting up SSO with appServerPorts enabled<br><br>SSOMode = [permissive | strict]<br>&nbsp;&nbsp;&nbsp;* Allows SSO to behave in either permissive or strict mode.<br>&nbsp;&nbsp;&nbsp;* Permissive: Requests to Splunk Web that originate from an untrusted IP address <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are redirected to a login page where they can log into Splunk without using SSO.<br>&nbsp;&nbsp;&nbsp;* Strict: All requests to splunkweb will be restricted to those originating<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from a trusted IP except those to endpoints not requiring authentication. <br>&nbsp;&nbsp;&nbsp;* Defaults to "strict"<br><br>trustedIP = &lt;ip_address&gt;<br>&nbsp;&nbsp;&nbsp;* Trusted IP. &nbsp;This is the IP address of the authenticating proxy.<br>&nbsp;&nbsp;&nbsp;* Splunkweb verifies it is receiving data from the proxy host for all<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SSO requests.<br>&nbsp;&nbsp;&nbsp;* Uncomment and set to a valid IP address to enable SSO.<br>&nbsp;&nbsp;&nbsp;* Disabled by default. &nbsp;Normal value is '127.0.0.1'<br>&nbsp;&nbsp;&nbsp;* If appServerPorts is set to a non-zero value, this setting can accept a<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;richer set of configurations, using the same format as the "acceptFrom"<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;setting.<br><br>allowSsoWithoutChangingServerConf = [0 | 1]<br>&nbsp;&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;&nbsp;* Usually when configuring SSO, a trustedIP needs to be set both here<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in web.conf and also in server.conf. &nbsp;If this is set to "1" then we<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will enable web-based SSO without a trustedIP in server.conf<br>&nbsp;&nbsp;&nbsp;* Defaults to 0<br><br>testing_endpoint = &lt;relative_uri_path&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the root URI path on which to serve splunkweb unit and <br>&nbsp;&nbsp;&nbsp;* integration testing resources.<br>&nbsp;&nbsp;&nbsp;* Development only setting<br>&nbsp;&nbsp;&nbsp;* Defaults to '/testing'<br>&nbsp;&nbsp;&nbsp;<br>testing_dir = &lt;relative_file_path&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the path relative to $SPLUNK_HOME that contains the testing<br>&nbsp;&nbsp;&nbsp;* files to be served at endpoint defined by 'testing_endpoint'.<br>&nbsp;&nbsp;&nbsp;* Development only setting<br>&nbsp;&nbsp;&nbsp;* Defaults to 'share/splunk/testing'<br><br># Results export server config<br><br>export_timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* An integral value that specifies the timeout when exporting results.<br>&nbsp;&nbsp;&nbsp;* The splunk server will wait till the given timeout period before closing<br>&nbsp;&nbsp;&nbsp;* the connection with splunkd. <br>&nbsp;&nbsp;<br><br>#<br># cherrypy HTTP server config<br>#<br><br>server.thread_pool = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the minimum number of threads the appserver is allowed to maintain<br>&nbsp;&nbsp;&nbsp;* Defaults to 20<br><br>server.thread_pool_max = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the maximum number of threads the appserver is allowed to maintain<br>&nbsp;&nbsp;&nbsp;* Defaults to -1 (unlimited)<br><br>server.thread_pool_min_spare = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the minimum number of spare threads the appserver keeps idle<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 5<br><br>server.thread_pool_max_spare = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum number of spare threads the appserver keeps idle<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 10<br>&nbsp;&nbsp;&nbsp;<br>server.socket_host = &lt;ip_address&gt;<br>&nbsp;&nbsp;&nbsp;* Host values may be any IPv4 or IPv6 address, or any valid hostname.<br>&nbsp;&nbsp;&nbsp;* The string 'localhost' is a synonym for '127.0.0.1' (or '::1', if<br>&nbsp;&nbsp;&nbsp;* your hosts file prefers IPv6). The string '0.0.0.0' is a special<br>&nbsp;&nbsp;&nbsp;* IPv4 entry meaning "any active interface" (INADDR_ANY), and '::'<br>&nbsp;&nbsp;&nbsp;* is the similar IN6ADDR_ANY for IPv6. <br>&nbsp;&nbsp;&nbsp;* Defaults to 0.0.0.0 if listenOnIPv6 is set to no, else&nbsp;::<br><br>server.socket_timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* The timeout in seconds for accepted connections between the browser and splunkweb<br>&nbsp;&nbsp;&nbsp;* Defaults to 10<br><br>listenOnIPv6 = &lt;no | yes | only&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* By default, splunkweb will listen for incoming connections using <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IPv4 only<br>&nbsp;&nbsp;&nbsp;&nbsp;* To enable IPv6 support in splunkweb, set this to "yes". &nbsp;Splunkweb<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will simultaneously listen for connections on both IPv4 and IPv6<br>&nbsp;&nbsp;&nbsp;&nbsp;* To disable IPv4 entirely, set this to "only", which will cause splunkweb<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to exclusively accept connections over IPv6. &nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;* You will also want to set server.socket_host (use "::" instead of "0.0.0.0")<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if you wish to listen on an IPv6 address<br><br>max_upload_size = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the hard maximum size of uploaded files in MB<br>&nbsp;&nbsp;&nbsp;* Defaults to 500<br><br>log.access_file = &lt;filename&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the HTTP access log filename<br>&nbsp;&nbsp;&nbsp;* Stored in default Splunk /var/log directory<br>&nbsp;&nbsp;&nbsp;* Defaults to web_access.log<br><br>log.access_maxsize = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum size the web_access.log file should be allowed to grow to (in bytes)<br>&nbsp;&nbsp;&nbsp;&nbsp;* Comment out or set to 0 for unlimited file size<br>&nbsp;&nbsp;&nbsp;&nbsp;* File will be rotated to web_access.log.0 after max file size is reached<br>&nbsp;&nbsp;&nbsp;&nbsp;* See log.access_maxfiles to limit the number of backup files created<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to unlimited file size<br><br>log.access_maxfiles = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum number of backup files to keep after the web_access.log file has reached its maximum size<br>&nbsp;&nbsp;&nbsp;&nbsp;* Warning: setting this to very high numbers (eg. 10000) may impact performance during log rotations<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 5 if access_maxsize is set<br><br>log.error_maxsize = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum size the web_service.log file should be allowed to grow to (in bytes)<br>&nbsp;&nbsp;&nbsp;&nbsp;* Comment out or set to 0 for unlimited file size<br>&nbsp;&nbsp;&nbsp;&nbsp;* File will be rotated to web_service.log.0 after max file size is reached<br>&nbsp;&nbsp;&nbsp;&nbsp;* See log.error_maxfiles to limit the number of backup files created<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to unlimited file size<br><br>log.error_maxfiles = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;* Specifies the maximum number of backup files to keep after the web_service.log file has reached its maximum size<br>&nbsp;&nbsp;&nbsp;&nbsp;* Warning: setting this to very high numbers (eg. 10000) may impact performance during log rotations<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to 5 if access_maxsize is set<br><br>log.screen = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if runtime output is displayed inside an interactive tty<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br>&nbsp;&nbsp;&nbsp;<br>request.show_tracebacks = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if a an exception traceback is displayed to the user on fatal exceptions<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>engine.autoreload_on = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if the appserver will auto-restart if it detects a python file has changed<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>tools.sessions.on = True<br>&nbsp;&nbsp;&nbsp;&nbsp;* Indicates if user session support is enabled<br>&nbsp;&nbsp;&nbsp;&nbsp;* Should always be True<br><br>tools.sessions.timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the number of minutes of inactivity before a user session is expired<br>&nbsp;&nbsp;&nbsp;* The countdown is effectively reset by browser activity minute until<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ui_inactivity_timeout inactivity timeout is reached.<br>&nbsp;&nbsp;&nbsp;* Use a value of 2 or higher, as a value of 1 will race with the browser<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;refresh, producing unpredictable behavior. <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Low values aren't very useful though except for testing.)<br>&nbsp;&nbsp;&nbsp;* Defaults to 60<br><br>tools.sessions.restart_persist = [True | False]<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to False then the session cookie will be deleted from the browser<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when the browser quits<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to True - Sessions persist across browser restarts<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(assuming the tools.sessions.timeout limit hasn't been reached)<br><br>tools.sessions.httponly = [True | False]<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to True then the session cookie will be made unavailable<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to running javascript scripts, increasing session security<br>&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>tools.sessions.secure = [True | False]<br>&nbsp;&nbsp;&nbsp;&nbsp;* If set to True and Splunkweb is configured to server requests using HTTPS<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(see the enableSplunkWebSSL setting) then the browser will only transmit <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the session cookie over HTTPS connections, increasing session security<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>response.timeout = &lt;integer&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the number of seconds to wait for the server to complete a response<br>&nbsp;&nbsp;&nbsp;* Some requests such as uploading large files can take a long time<br>&nbsp;&nbsp;&nbsp;* Defaults to 7200<br><br>tools.sessions.storage_type = [file]<br>tools.sessions.storage_path = &lt;filepath&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the session information storage mechanisms<br>&nbsp;&nbsp;&nbsp;* Comment out the next two lines to use RAM based sessions instead<br>&nbsp;&nbsp;&nbsp;* Use an absolute path to store sessions outside of the splunk tree<br>&nbsp;&nbsp;&nbsp;* Defaults to storage_type=file, storage_path=var/run/splunk<br><br>tools.decode.on = [True | False]<br>&nbsp;&nbsp;&nbsp;* Indicates if all strings that come into Cherrpy controller methods are decoded as unicode (assumes UTF-8 encoding).<br>&nbsp;&nbsp;&nbsp;* WARNING: Disabling this will likely break the application, as all incoming strings are assumed<br>&nbsp;&nbsp;&nbsp;* to be unicode.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>tools.encode.on = [True | False]<br>&nbsp;&nbsp;&nbsp;* Encodes all controller method response strings into UTF-8 str objects in Python.<br>&nbsp;&nbsp;&nbsp;* WARNING: Disabling this will likely cause high byte character encoding to fail.<br>&nbsp;&nbsp;&nbsp;* Defaults to True<br><br>tools.encode.encoding = &lt;codec&gt;<br>&nbsp;&nbsp;&nbsp;* Force all outgoing characters to be encoded into UTF-8.<br>&nbsp;&nbsp;&nbsp;* This only works with tools.encode.on set to True.<br>&nbsp;&nbsp;&nbsp;* By setting this to utf-8, Cherrypy's default behavior of observing the Accept-Charset header<br>&nbsp;&nbsp;&nbsp;* is overwritten and forces utf-8 output. Only change this if you know a particular browser<br>&nbsp;&nbsp;&nbsp;* installation must receive some other character encoding (Latin-1 iso-8859-1, etc)<br>&nbsp;&nbsp;&nbsp;* WARNING: Change this at your own risk.<br>&nbsp;&nbsp;&nbsp;* Defaults to utf08<br><br>tools.proxy.on = [True | False]<br>&nbsp;&nbsp;&nbsp;* Used for running Apache as a proxy for Splunk UI, typically for SSO configuration. See http://tools.cherrypy.org/wiki/BehindApache for more information.<br>&nbsp;&nbsp;&nbsp;* For Apache 1.x proxies only. Set this attribute to "true". This configuration instructs CherryPy (the Splunk Web HTTP server) to look for an incoming X-Forwarded-Host header and to use the value of that header to construct canonical redirect URLs that include the proper host name. For more information, refer to the CherryPy documentation on running behind an Apache proxy. This setting is only necessary for Apache 1.1 proxies. For all other proxies, the setting must be "false", which is the default.<br>&nbsp;&nbsp;&nbsp;* Defaults to False<br><br>tools.proxy.base = &lt;scheme&gt;://&lt;URL&gt;<br>&nbsp;&nbsp;&nbsp;* Used for setting the proxy base url in Splunk<br>&nbsp;&nbsp;&nbsp;* Defaults to an empty value<br><br>pid_path = &lt;filepath&gt;<br>&nbsp;&nbsp;&nbsp;* Specifies the path to the PID file<br>&nbsp;&nbsp;&nbsp;* Equals precisely and only var/run/splunk/splunkweb.pid<br>&nbsp;&nbsp;&nbsp;* NOTE: Do not change this parameter.<br><br>enabled_decomposers = &lt;intention&gt; [, &lt;intention&gt;]...<br>&nbsp;&nbsp;&nbsp;* Added in Splunk 4.2 as a short term workaround measure for apps which happen to still require search decomposition, which is deprecated with 4.2.<br>&nbsp;&nbsp;&nbsp;* Search decomposition will be entirely removed in a future release.<br>&nbsp;&nbsp;&nbsp;* Comma separated list of allowed intentions.<br>&nbsp;&nbsp;&nbsp;* Modifies search decomposition, which is a splunk-web internal behavior.<br>&nbsp;&nbsp;&nbsp;* Can be controlled on a per-app basis.<br>&nbsp;&nbsp;&nbsp;* If set to the empty string, no search decomposition occurs, which causes some usability problems with report builder.<br>&nbsp;&nbsp;&nbsp;* The current possible values are: addcommand, stats, addterm, addtermgt, addtermlt, setfields, excludefields, audit, sort, plot <br>&nbsp;&nbsp;&nbsp;* Default is 'plot', leaving only the plot intention enabled.<br><br>simple_xml_module_render = [True | False]<br>&nbsp;&nbsp;* If True, simple xml dashboards and forms will render using the module system<br>&nbsp;&nbsp;* Defaults to False<br><br>simple_xml_perf_debug = [True | False]<br>&nbsp;&nbsp;* If True, simple xml dashboards will log some performance metrics to the browser console<br>&nbsp;&nbsp;* Defaults to False<br><br>job_min_polling_interval = &lt;integer&gt;<br>&nbsp;&nbsp;* Minimum polling interval for job in miliseconds (ms)<br>&nbsp;&nbsp;* The default value is 100<br>&nbsp;&nbsp;* This is the intial time wait for fetching results<br>&nbsp;&nbsp;* The poll period increases gradually from min interval to max interval when search is in queued state or parsing state (and not running state) for a some time.<br>&nbsp;&nbsp;* Set this value between 100 to job_max_polling_interval<br><br>job_max_polling_interval = &lt;integer&gt;<br>&nbsp;&nbsp;* Maximum polling interval for job in miliseconds (ms)<br>&nbsp;&nbsp;* The default value is 1000<br>&nbsp;&nbsp;* This is the maximum time wait for fetching results<br>&nbsp;&nbsp;* The recommended maximum value is 3000<br><br>acceptFrom = &lt;network_acl&gt; ...<br>&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* Lists a set of networks or addresses to accept connections from. &nbsp;These rules are separated by commas or spaces<br>&nbsp;&nbsp;* Each rule can be in the following forms:<br>&nbsp;&nbsp;* &nbsp;&nbsp;1. A single IPv4 or IPv6 address (examples: "10.1.2.3", "fe80::4a3")<br>&nbsp;&nbsp;* &nbsp;&nbsp;2. A CIDR block of addresses (examples: "10/8", "fe80:1234/32")<br>&nbsp;&nbsp;* &nbsp;&nbsp;3. A DNS name, possibly with a '*' used as a wildcard (examples: "myhost.example.com", "*.splunk.com")<br>&nbsp;&nbsp;* &nbsp;&nbsp;4. A single '*' which matches anything<br>&nbsp;&nbsp;* Entries can also be prefixed with '!' to cause the rule to reject the<br>&nbsp;&nbsp;&nbsp;&nbsp;connection. &nbsp;Rules are applied in order, and the first one to match is<br>&nbsp;&nbsp;&nbsp;&nbsp;used. &nbsp;For example, "!10.1/16, *" will allow connections from everywhere<br>&nbsp;&nbsp;&nbsp;&nbsp;except the 10.1.*.* network.<br>&nbsp;&nbsp;* Defaults to "*" (accept from anywhere)<br><br>maxThreads = &lt;int&gt;<br>&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* Number of threads that can be used by active HTTP transactions.<br>&nbsp;&nbsp;&nbsp;&nbsp;This can be limited to constrain resource usage.<br>&nbsp;&nbsp;* If set to 0 (the default) a limit will be automatically picked<br>&nbsp;&nbsp;&nbsp;&nbsp;based on estimated server capacity.<br>&nbsp;&nbsp;* If set to a negative number, no limit will be enforced.<br><br>maxSockets = &lt;int&gt;<br>&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* Number of simultaneous HTTP connections that we accept simultaneously.<br>&nbsp;&nbsp;&nbsp;&nbsp;This can be limited to constrain resource usage.<br>&nbsp;&nbsp;* If set to 0 (the default) a limit will be automatically picked<br>&nbsp;&nbsp;&nbsp;&nbsp;based on estimated server capacity.<br>&nbsp;&nbsp;* If set to a negative number, no limit will be enforced.<br><br>forceHttp10 = auto|never|always<br>&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* When set to "always", the REST HTTP server will not use some<br>&nbsp;&nbsp;&nbsp;&nbsp;HTTP 1.1 features such as persistent connections or chunked<br>&nbsp;&nbsp;&nbsp;&nbsp;transfer encoding.<br>&nbsp;&nbsp;* When set to "auto" it will do this only if the client sent no<br>&nbsp;&nbsp;&nbsp;&nbsp;User-Agent header, or if the user agent is known to have bugs<br>&nbsp;&nbsp;&nbsp;&nbsp;in its HTTP/1.1 support.<br>&nbsp;&nbsp;* When set to "never" it always will allow HTTP 1.1, even to<br>&nbsp;&nbsp;&nbsp;&nbsp;clients it suspects may be buggy.<br>&nbsp;&nbsp;* Defaults to "auto"<br><br>crossOriginSharingPolicy = &lt;origin_acl&gt; ...<br>&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* List of HTTP Origins to return Access-Control-Allow-* (CORS) headers for<br>&nbsp;&nbsp;* These headers tell browsers that we trust web applications at those sites<br>&nbsp;&nbsp;&nbsp;&nbsp;to make requests to the REST interface<br>&nbsp;&nbsp;* The origin is passed as a URL without a path component (for example<br>&nbsp;&nbsp;&nbsp;&nbsp;"https://app.example.com:8000")<br>&nbsp;&nbsp;* This setting can take a list of acceptable origins, separated<br>&nbsp;&nbsp;&nbsp;&nbsp;by spaces and/or commas<br>&nbsp;&nbsp;* Each origin can also contain wildcards for any part. &nbsp;Examples:<br>&nbsp;&nbsp;&nbsp;&nbsp;*://app.example.com:* &nbsp;(either HTTP or HTTPS on any port)<br>&nbsp;&nbsp;&nbsp;&nbsp;https://*.example.com &nbsp;(any host under example.com, including example.com itself)<br>&nbsp;&nbsp;* An address can be prefixed with a '!' to negate the match, with<br>&nbsp;&nbsp;&nbsp;&nbsp;the first matching origin taking precedence. &nbsp;For example,<br>&nbsp;&nbsp;&nbsp;&nbsp;"!*://evil.example.com:* *://*.example.com:*" to not avoid<br>&nbsp;&nbsp;&nbsp;&nbsp;matching one host in a domain<br>&nbsp;&nbsp;* A single "*" can also be used to match all origins<br>&nbsp;&nbsp;* By default the list is empty<br><br>allowSslCompression = true|false<br>&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set<br>&nbsp;&nbsp;&nbsp;&nbsp;to a non-zero value. &nbsp;When appServerPorts is zero or missing, this setting<br>&nbsp;&nbsp;&nbsp;&nbsp;will always act as if it is set to "true"<br>&nbsp;&nbsp;* If set to true, the server will allow clients to negotiate<br>&nbsp;&nbsp;&nbsp;&nbsp;SSL-layer data compression.<br>&nbsp;&nbsp;* Defaults to false. &nbsp;The HTTP layer has its own compression layer<br>&nbsp;&nbsp;&nbsp;&nbsp;which is usually sufficient.<br><br>allowSslRenegotiation = true|false<br>&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* In the SSL protocol, a client may request renegotiation of the connection<br>&nbsp;&nbsp;&nbsp;&nbsp;settings from time to time.<br>&nbsp;&nbsp;* Setting this to false causes the server to reject all renegotiation<br>&nbsp;&nbsp;&nbsp;&nbsp;attempts, breaking the connection. &nbsp;This limits the amount of CPU a<br>&nbsp;&nbsp;&nbsp;&nbsp;single TCP connection can use, but it can cause connectivity problems<br>&nbsp;&nbsp;&nbsp;&nbsp;especially for long-lived connections.<br>&nbsp;&nbsp;* Defaults to true<br><br>sendStrictTransportSecurityHeader = true|false<br>&nbsp;&nbsp;* IMPORTANT: this setting only takes effect when appServerPorts is set to a non-zero value<br>&nbsp;&nbsp;* If set to true, the REST interface will send a "Strict-Transport-Security"<br>&nbsp;&nbsp;&nbsp;&nbsp;header with all responses to requests made over SSL.<br>&nbsp;&nbsp;* This can help avoid a client being tricked later by a Man-In-The-Middle<br>&nbsp;&nbsp;&nbsp;&nbsp;attack to accept a non-SSL request. &nbsp;However, this requires a commitment that<br>&nbsp;&nbsp;&nbsp;&nbsp;no non-SSL web hosts will ever be run on this hostname on any port. &nbsp;For example,<br>&nbsp;&nbsp;&nbsp;&nbsp;if splunkweb is in default non-SSL mode this can break the ability of browser<br>&nbsp;&nbsp;&nbsp;&nbsp;to connect to it. &nbsp;Enable with caution.<br>&nbsp;&nbsp;* Defaults to false<br><br>dedicatedIoThreads = &lt;int&gt;<br>&nbsp;&nbsp;* If set to zero, HTTP I/O will be performed in the same thread<br>&nbsp;&nbsp;&nbsp;&nbsp;that accepted the TCP connection.<br>&nbsp;&nbsp;* If set set to a non-zero value, separate threads will be run<br>&nbsp;&nbsp;&nbsp;&nbsp;to handle the HTTP I/O, including SSL encryption.<br>&nbsp;&nbsp;* Defaults to "0"<br>&nbsp;&nbsp;* Typically this does not need to be changed. &nbsp;For most usage<br>&nbsp;&nbsp;&nbsp;&nbsp;scenarios using the same the thread offers the best performance.<br>&nbsp;&nbsp;* NOTE: this setting only takes effect when appServerPorts is set to a non-zero value<br><br>[framework]<br>* Put App Framework settings here<br>django_enable = [True | False]<br>&nbsp;&nbsp;* Specifies whether Django should be enabled or not<br>&nbsp;&nbsp;* Defaults to True<br>&nbsp;&nbsp;* Django will not start unless an app requires it<br>&nbsp;&nbsp;<br>django_path = &lt;filepath&gt;<br>&nbsp;&nbsp;* Specifies the root path to the new App Framework files, relative to $SPLUNK_HOME<br>&nbsp;&nbsp;* Defaults to etc/apps/framework<br>&nbsp;&nbsp;<br>django_force_enable = [True | False]<br>&nbsp;&nbsp;* Specifies whether to force Django to start, even if no app requires it<br>&nbsp;&nbsp;* Defaults to False<br><br><br>#<br># custom cherrypy endpoints<br>#<br><br>[endpoint:&lt;python_module_name&gt;]<br>&nbsp;&nbsp;&nbsp;* registers a custom python CherryPy endpoint<br>&nbsp;&nbsp;&nbsp;* the expected file must be located at: $SPLUNK_HOME/etc/apps/&lt;APP_NAME&gt;/appserver/controllers/&lt;PYTHON_NODULE_NAME&gt;.py<br>&nbsp;&nbsp;&nbsp;* this module's methods will be exposed at /custom/&lt;APP_NAME&gt;/&lt;PYTHON_NODULE_NAME&gt;/&lt;METHOD_NAME&gt;<br><br>#<br># exposed splunkd REST endpoints<br>#<br>[expose:&lt;unique_name&gt;]<br>&nbsp;&nbsp;&nbsp;* Registers a splunkd-based endpoint that should be made available to the UI under the "/splunkd" and "/splunkd/__raw" hierarchies<br>&nbsp;&nbsp;&nbsp;* The name of the stanza doesn't matter as long as it starts with "expose:" &nbsp;Each stanza name must be unique, however<br><br>pattern = &lt;url_pattern&gt;<br>&nbsp;&nbsp;&nbsp;* Pattern to match under the splunkd /services hierarchy. &nbsp;For instance, "a/b/c" would match URIs "/services/a/b/c" and "/servicesNS/*/*/a/b/c"<br>&nbsp;&nbsp;&nbsp;* The pattern should not include leading or trailing slashes<br>&nbsp;&nbsp;&nbsp;* Inside the pattern an element of "*" will match a single path element. &nbsp;For example, "a/*/c" would match "a/b/c" but not "a/1/2/c"<br>&nbsp;&nbsp;&nbsp;* A path element of "**" will match any number of elements. &nbsp;For example, "a/**/c" would match both "a/1/c" and "a/1/2/3/c"<br>&nbsp;&nbsp;&nbsp;* A path element can end with a "*" to match a prefix. &nbsp;For example, "a/elem-*/b" would match "a/elem-123/c"<br><br>methods = &lt;method_lists&gt;<br>&nbsp;&nbsp;&nbsp;* Comma separated list of methods to allow from the web browser (example: "GET,POST,DELETE")<br>&nbsp;&nbsp;&nbsp;* If not included, defaults to "GET"<br><br>oidEnabled = [0 | 1]<br>&nbsp;&nbsp;&nbsp;* If set to 1 indicates that the endpoint is capable of taking an embed-id as a query parameter<br>&nbsp;&nbsp;&nbsp;* Defaults to 0<br>&nbsp;&nbsp;&nbsp;* This is only needed for some internal splunk endpoints, you probably should not specify this for app-supplied endpoints<br><br>skipCSRFProtection = [0 | 1]<br>&nbsp;&nbsp;&nbsp;* If set to 1, tells splunkweb that it is safe to post to this endpoint without applying CSRF protection<br>&nbsp;&nbsp;&nbsp;* Defaults to 0<br>&nbsp;&nbsp;&nbsp;* This should only be set on the login endpoint (which already contains sufficient auth credentials to avoid CSRF problems)<br><br></font></code>
<h3> <a name="webconf_web.conf.example"><span class="mw-headline" id="web.conf.example">web.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example web.conf. &nbsp;Use this file to configure data web settings.<br>#<br># To use one or more of these configurations, copy the configuration block into web.conf <br># in $SPLUNK_HOME/etc/system/local/. You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br><br><br># This stanza heading must precede any changes.<br>[settings]<br><br># Change the default port number:<br>httpport = 12800<br># Also run the python application server on a non-default port:<br>appServerPorts = 12801<br><br># Turn on SSL:<br>enableSplunkWebSSL = true<br># absolute paths may be used here.<br>privKeyPath = /home/user/certs/myprivatekey.pem<br>caCertPath = /home/user/certs/mycacert.pem<br># NOTE: non-absolute paths are relative to $SPLUNK_HOME<br><br></font></code>

<a name="wmiconf"></a><h2> <a name="wmiconf_wmi.conf"><span class="mw-headline" id="wmi.conf">wmi.conf</span></a></h2>
<p>The following are the spec and example files for wmi.conf.
</p>
<h3> <a name="wmiconf_wmi.conf.spec"><span class="mw-headline" id="wmi.conf.spec">wmi.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This file contains possible attribute/value pairs for configuring <br># Windows Management Instrumentation (WMI) access from Splunk. &nbsp;<br>#<br># There is a wmi.conf in $SPLUNK_HOME\etc\system\default\. &nbsp;To set <br># custom configurations, place a wmi.conf in <br># $SPLUNK_HOME\etc\system\local\. For examples, see wmi.conf.example.<br>#<br># You must restart Splunk to enable configurations.<br># <br># To learn more about configuration files (including precedence) please see the<br># documentation located at <br># http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br>###################################################################<br>#----GLOBAL SETTINGS-----<br>###################################################################<br><br>[settings]<br>* The settings stanza specifies various runtime parameters.<br>* The entire stanza and every parameter within it is optional. <br>* If the stanza is missing, Splunk assumes system defaults.<br><br>initial_backoff = &lt;integer&gt;<br>* How long, in seconds, to wait before retrying the connection to<br>the WMI provider after the first connection error.<br>* If connection errors continue, the wait time doubles until it reaches<br>the integer specified in max_backoff.<br>* Defaults to 5.<br><br>max_backoff = &lt;integer&gt;<br>* The maximum time, in seconds, to attempt to reconnect to the<br>WMI provider.<br>* Defaults to 20.<br><br>max_retries_at_max_backoff = &lt;integer&gt;<br>* Once max_backoff is reached, tells Splunk how many times to attempt<br>to reconnect to the WMI provider. &nbsp;<br>* Splunk will try to reconnect every max_backoff seconds.<br>* If reconnection fails after max_retries, give up forever (until restart).<br>* Defaults to 2.<br><br>checkpoint_sync_interval = &lt;integer&gt;<br>* The minimum wait time, in seconds, for state data (event log checkpoint)<br>to be written to disk.<br>* Defaults to 2.<br><br>###################################################################<br>#----INPUT-SPECIFIC SETTINGS-----<br>###################################################################<br><br>[WMI:$NAME]<br>* There are two types of WMI stanzas:<br>&nbsp;* Event log: for pulling event logs. You must set the <br>&nbsp;&nbsp;&nbsp;event_log_file attribute.<br>&nbsp;* WQL: for issuing raw Windows Query Language (WQL) requests. You <br>&nbsp;&nbsp;&nbsp;must set the wql attribute.<br>&nbsp;* Do not use both the event_log_file or the wql attributes. &nbsp;Use <br>&nbsp;&nbsp;&nbsp;one or the other.<br><br>server = &lt;comma-separated strings&gt;<br>* A comma-separated list of servers from which to get data.<br>* If not present, defaults to the local machine.<br><br>interval = &lt;integer&gt;<br>* How often, in seconds, to poll for new data.<br>* This attribute is required, and the input will not run if the attribute is<br>&nbsp;&nbsp;not present.<br>* There is no default.<br><br>disabled = [0|1]<br>* Specifies whether the input is enabled or not.<br>* 1 to disable the input, 0 to enable it.<br>* Defaults to 0 (enabled).<br><br>hostname = &lt;host&gt;<br>* All results generated by this stanza will appear to have arrived from<br>&nbsp;&nbsp;the string specified here.<br>* This attribute is optional. &nbsp;<br>* If it is not present, the input will detect the host automatically.<br><br>current_only = [0|1]<br>* Changes the characteristics and interaction of WMI-based event collections.<br>* When current_only is set to 1:<br>&nbsp;&nbsp;* For event log stanzas, this will only capture events that occur<br>&nbsp;&nbsp;&nbsp;&nbsp;while Splunk is running.<br>&nbsp;&nbsp;* For WQL stanzas, event notification queries are expected. &nbsp;The <br>&nbsp;&nbsp;&nbsp;&nbsp;queried class must support sending events. &nbsp;Failure to supply <br>&nbsp;&nbsp;&nbsp;&nbsp;the correct event notification query structure will cause<br>&nbsp;&nbsp;&nbsp;&nbsp;WMI to return a syntax error.<br>&nbsp;&nbsp;* An example event notification query that watches for process creation:<br>&nbsp;&nbsp;&nbsp;&nbsp;* SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TargetInstance ISA 'Win32_Process'.<br>* When current_only is set to 0:<br>&nbsp;&nbsp;* For event log stanzas, all the events from the checkpoint are <br>&nbsp;&nbsp;&nbsp;&nbsp;gathered. If there is no checkpoint, all events starting from<br>&nbsp;&nbsp;&nbsp;&nbsp;the oldest events are retrieved.<br>&nbsp;&nbsp;* For WQL stanzas, the query is executed and results are retrieved.<br>&nbsp;&nbsp;&nbsp;&nbsp;The query is a non-notification query.<br>&nbsp;&nbsp;* For example<br>&nbsp;&nbsp;&nbsp;&nbsp;* Select * Win32_Process where caption = "explorer.exe"<br>* Defaults to 0.<br><br>index = &lt;string&gt;<br>* Specifies the index that this input should send the data to.<br>* This attribute is optional.<br>* When defined, "index=" is automatically prepended to &lt;string&gt;.<br>* Defaults to "index=main" (or whatever you have set as your default index).<br><br>#####<br># Event log-specific attributes:<br>#####<br><br>event_log_file = &lt;Application, System, etc&gt;<br>* Tells Splunk to expect event log data for this stanza, and specifies<br>&nbsp;&nbsp;the event log channels you want Splunk to monitor.<br>* Use this instead of WQL to specify sources.<br>* Specify one or more event log channels to poll. &nbsp;Multiple event log<br>&nbsp;&nbsp;channels must be separated by commas.<br>* There is no default.<br><br>disable_hostname_normalization = [0|1]<br>* If set to true, hostname normalization is disabled<br>* If absent or set to false, the hostname for 'localhost' will be converted to&nbsp;%COMPUTERNAME%.<br>* 'localhost' refers to the following list of strings: localhost, 127.0.0.1,&nbsp;::1, the name of the DNS domain for the local computer,<br>* the fully qualified DNS name, the NetBIOS name, the DNS host name of the local computer<br><br>#####<br># WQL-specific attributes:<br>#####<br><br>wql = &lt;string&gt;<br>* Tells Splunk to expect data from a WMI provider for this stanza, and<br>&nbsp;&nbsp;specifies the WQL query you want Splunk to make to gather that data.<br>* Use this if you are not using the event_log_file attribute.<br>* Ensure that your WQL queries are syntactically and structurally correct<br>&nbsp;&nbsp;when using this option.<br>* For example, <br>&nbsp;&nbsp;SELECT * FROM Win32_PerfFormattedData_PerfProc_Process WHERE Name = "splunkd".<br>* If you wish to use event notification queries, you must also set the<br>&nbsp;&nbsp;"current_only" attribute to 1 within the stanza, and your query must be <br>&nbsp;&nbsp;appropriately structured for event notification (meaning it must contain<br>&nbsp;&nbsp;one or more of the GROUP, WITHIN or HAVING clauses.)<br>* For example, <br>&nbsp;&nbsp;SELECT * FROM __InstanceCreationEvent WITHIN 1 WHERE TargetInstance ISA<br>&nbsp;&nbsp;'Win32_Process'<br>* There is no default.<br><br>namespace = &lt;string&gt;<br>* The namespace where the WMI provider resides.<br>* The namespace spec can either be relative (root\cimv2) or absolute <br>&nbsp;&nbsp;(\\server\root\cimv2).<br>* If the server attribute is present, you cannot specify an absolute namespace.<br>* Defaults to root\cimv2.<br><br><br></font></code>
<h3> <a name="wmiconf_wmi.conf.example"><span class="mw-headline" id="wmi.conf.example">wmi.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example wmi.conf. &nbsp;These settings are used to control inputs from<br># WMI providers. Refer to wmi.conf.spec and the documentation at splunk.com for<br># more information about this file. <br>#<br># To use one or more of these configurations, copy the configuration block into<br># wmi.conf in $SPLUNK_HOME\etc\system\local\. &nbsp;You must restart Splunk to <br># enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the<br># documentation located at<br># http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># This stanza specifies runtime parameters. <br><br>[settings]<br>initial_backoff = 5<br>max_backoff = 20<br>max_retries_at_max_backoff = 2<br>checkpoint_sync_interval = 2<br><br># Pull events from the Application, System and Security event logs from the<br># local system every 10 seconds. Store the events in the "wmi_eventlog" <br># Splunk index.<br><br>[WMI:LocalApplication]<br>interval = 10<br>event_log_file = Application<br>disabled = 0<br>index = wmi_eventlog<br><br>[WMI:LocalSystem]<br>interval = 10<br>event_log_file = System<br>disabled = 0<br>index = wmi_eventlog<br><br>[WMI:LocalSecurity]<br>interval = 10<br>event_log_file = Security<br>disabled = 0<br>index = wmi_eventlog<br><br># Gather disk and memory performance metrics from the local system every second.<br># Store event in the "wmi_perfmon" Splunk index.<br><br>[WMI:LocalPhysicalDisk]<br>interval = 1<br>wql = select Name, DiskBytesPerSec, PercentDiskReadTime, PercentDiskWriteTime, PercentDiskTime from Win32_PerfFormattedData_PerfDisk_PhysicalDisk<br>disabled = 0<br>index = wmi_perfmon<br><br>[WMI:LocalMainMemory]<br>interval = 10<br>wql = select CommittedBytes, AvailableBytes, PercentCommittedBytesInUse, Caption from Win32_PerfFormattedData_PerfOS_Memory<br>disabled = 0<br>index = wmi_perfmon<br><br># Collect all process-related performance metrics for the splunkd process, <br># every second. &nbsp;Store those events in the "wmi_perfmon" index.<br>[WMI:LocalSplunkdProcess]<br>interval = 1<br>wql = select * from Win32_PerfFormattedData_PerfProc_Process where Name = "splunkd"<br>disabled = 0<br>index = wmi_perfmon<br><br># Listen from three event log channels, capturing log events that occur only<br># while Splunk is running, every 10 seconds. &nbsp;Gather data from three remote<br># servers srv1, srv2 and srv3.<br><br>[WMI:TailApplicationLogs]<br>interval = 10<br>event_log_file = Application, Security, System<br>server = srv1, srv2, srv3<br>disabled = 0<br>current_only = 1<br><br># Listen for process-creation events on a remote machine, once a second.<br><br>[WMI:ProcessCreation]<br>interval = 1<br>server = remote-machine<br>wql = select * from __InstanceCreationEvent within 1 where TargetInstance isa 'Win32_Process'<br>disabled = 0<br>current_only = 1<br><br># Receive events whenever someone connects or removes a USB device on<br># the computer, once a second.<br><br>[WMI:USBChanges]<br>interval = 1<br>wql = select * from __InstanceOperationEvent within 1 where TargetInstance ISA 'Win32_PnPEntity' and TargetInstance.Description='USB Mass Storage Device'<br>disabled = 0<br>current_only = 1<br><br><br></font></code>

<a name="workflow%20actionsconf"></a><h2> <a name="workflow%20actionsconf_workflow_actions.conf"><span class="mw-headline" id="workflow_actions.conf">workflow_actions.conf</span></a></h2>
<p>The following are the spec and example files for workflow_actions.conf.
</p>
<h3> <a name="workflow%20actionsconf_workflow_actions.conf.spec"><span class="mw-headline" id="workflow_actions.conf.spec">workflow_actions.conf.spec</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3<br>#<br># This file contains possible attribute/value pairs for configuring workflow actions in Splunk.<br>#<br># There is a workflow_actions.conf in $SPLUNK_HOME/etc/apps/search/default/.<br># To set custom configurations, place a workflow_actions.conf in either $SPLUNK_HOME/etc/system/local/<br># or add a workflow_actions.conf file to your app's local/ directory. For examples, see <br># workflow_actions.conf.example. &nbsp;You must restart Splunk to enable configurations, unless editing<br># them through the Splunk manager.<br># <br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># GLOBAL SETTINGS<br># Use the [default] stanza to define any global settings.<br># &nbsp;&nbsp;&nbsp;&nbsp;* You can also define global settings outside of any stanza, at the top of the file.<br># &nbsp;&nbsp;&nbsp;&nbsp;* Each conf file should have at most one default stanza. If there are multiple default<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stanzas, attributes are combined. In the case of multiple definitions of the same<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attribute, the last definition in the file wins.<br># &nbsp;&nbsp;&nbsp;&nbsp;* If an attribute is defined at both the global level and in a specific stanza, the<br># &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value in the specific stanza takes precedence.<br><br>#########################################################################################<br># General required settings:<br># These apply to all workflow action types.<br>#########################################################################################<br><br>type = &lt;string&gt;<br>* The type of the workflow action.<br>* If not set, Splunk skips this workflow action.<br><br>label = &lt;string&gt;<br>* The label to display in the workflow action menu.<br>* If not set, Splunk skips this workflow action.<br><br>#########################################################################################<br># General optional settings:<br># These settings are not required but are available for all workflow actions.<br>#########################################################################################<br><br>fields = &lt;comma or space separated list&gt;<br>* The fields required to be present on the event in order for the workflow action to be applied.<br>* When "display_location" is set to "both" or "field_menu", the workflow action will be applied to the menu's corresponding to the specified fields.<br>* If fields is undefined or set to *, the workflow action is applied to all field menus.<br>* If the * character is used in a field name, it is assumed to act as a "globber". For example host* would match the fields hostname, hostip, etc.<br>* Acceptable values are any valid field name, any field name including the * character, or * (e.g. *_ip).<br>* Defaults to *<br><br>eventtypes = &lt;comma or space separated list&gt;<br>* The eventtypes required to be present on the event in order for the workflow action to be applied.<br>* Acceptable values are any valid eventtype name, or any eventtype name plus the * character (e.g. host*).<br><br>display_location = &lt;string&gt;<br>* Dictates whether to display the workflow action in the event menu, the field menus or in both locations.<br>* Accepts field_menu, event_menu, or both.<br>* Defaults to both.<br><br>disabled = [True | False]<br>* Dictates whether the workflow action is currently disabled<br>* Defaults to False<br><br>#########################################################################################<br># Using field names to insert values into workflow action settings<br>#########################################################################################<br><br># Several settings detailed below allow for the substitution of field values using a special<br># variable syntax, where the field's name is enclosed in dollar signs. &nbsp;For example, $_raw$,<br># $hostip$, etc.<br># <br># The settings, label, link.uri, link.postargs, and search.search_string all accept the value of any<br># valid field to be substituted into the final string.<br># <br># For example, you might construct a Google search using an error message field called error_msg like so:<br># link.uri = http://www.google.com/search?q=$error_msg$.<br># <br># Some special variables exist to make constructing the settings simpler.<br><br>$@field_name$<br>* Allows for the name of the current field being clicked on to be used in a field action.<br>* Useful when constructing searches or links that apply to all fields.<br>* NOT AVAILABLE FOR EVENT MENUS<br><br>$@field_value$<br>* Allows for the value of the current field being clicked on to be used in a field action.<br>* Useful when constructing searches or links that apply to all fields.<br>* NOT AVAILABLE FOR EVENT MENUS<br><br>$@sid$<br>* The sid of the current search job.<br><br>$@offset$<br>* The offset of the event being clicked on in the list of search events.<br><br>$@namespace$<br>* The name of the application from which the search was run.<br><br>$@latest_time$<br>* The latest time the event occurred. &nbsp;This is used to disambiguate similar events from one another. It is not often available for all fields.<br><br><br>#########################################################################################<br># Field action types<br>#########################################################################################<br><br>#########################################################################################<br># Link type:<br># Allows for the construction of GET and POST requests via links to external resources. <br>#########################################################################################<br><br>link.uri = &lt;string&gt;<br>* The URI for the resource to link to.<br>* Accepts field values in the form $&lt;field name&gt;$, (e.g $_raw$).<br>* All inserted values are URI encoded.<br>* Required<br><br>link.target = &lt;string&gt;<br>* Determines if clicking the link opens a new window, or redirects the current window to the resource defined in link.uri.<br>* Accepts: "blank" (opens a new window), "self" (opens in the same window)<br>* Defaults to "blank"<br><br>link.method = &lt;string&gt;<br>* Determines if clicking the link should generate a GET request or a POST request to the resource defined in link.uri.<br>* Accepts: "get" or "post".<br>* Defaults to "get".<br><br>link.postargs.&lt;int&gt;.&lt;key/value&gt; = &lt;value&gt;<br>* Only available when link.method = post.<br>* Defined as a list of key / value pairs like such that foo=bar becomes:<br>&nbsp;&nbsp;link.postargs.1.key = "foo"<br>&nbsp;&nbsp;link.postargs.1.value = "bar"<br>* Allows for a conf compatible method of defining multiple identical keys (e.g.):<br>&nbsp;&nbsp;link.postargs.1.key = "foo"<br>&nbsp;&nbsp;link.postargs.1.value = "bar"<br>&nbsp;&nbsp;link.postargs.2.key = "foo"<br>&nbsp;&nbsp;link.postargs.2.value = "boo"<br>&nbsp;&nbsp;...<br>* All values are html form encoded appropriately.<br><br>#########################################################################################<br># Search type:<br># Allows for the construction of a new search to run in a specified view. <br>#########################################################################################<br><br>search.search_string = &lt;string&gt;<br>* The search string to construct.<br>* Accepts field values in the form $&lt;field name&gt;$, (e.g. $_raw$).<br>* Does NOT attempt to determine if the inserted field values may brake quoting or other search language escaping.<br>* Required<br><br>search.app = &lt;string&gt;<br>* The name of the Splunk application in which to perform the constructed search.<br>* By default this is set to the current app.<br><br>search.view = &lt;string&gt;<br>* The name of the view in which to preform the constructed search.<br>* By default this is set to the current view.<br><br>search.target = &lt;string&gt;<br>* Accepts: blank, self.<br>* Works in the same way as link.target. See link.target for more info.<br><br>search.earliest = &lt;time&gt;<br>* Accepts absolute and Splunk relative times (e.g. -10h).<br>* Determines the earliest time to search from.<br><br>search.latest = &lt;time&gt;<br>* Accepts absolute and Splunk relative times (e.g. -10h).<br>* Determines the latest time to search to.<br><br>search.preserve_timerange = &lt;boolean&gt;<br>* Ignored if either the search.earliest or search.latest values are set.<br>* When true, the time range from the original search which produced the events list will be used.<br>* Defaults to false.<br><br></font></code>
<h3> <a name="workflow%20actionsconf_workflow_actions.conf.example"><span class="mw-headline" id="workflow_actions.conf.example">workflow_actions.conf.example</span></a></h3>
<code><font size="2"><br># &nbsp;&nbsp;Version 6.2.3 <br>#<br># This is an example workflow_actions.conf. &nbsp;These settings are used to create workflow actions accessible in an event viewer.<br># Refer to workflow_actions.conf.spec and the documentation at splunk.com for more information about this file. <br>#<br># To use one or more of these configurations, copy the configuration block into workflow_actions.conf <br># in $SPLUNK_HOME/etc/system/local/, or into your application's local/ folder.<br># You must restart Splunk to enable configurations.<br>#<br># To learn more about configuration files (including precedence) please see the documentation <br># located at http://docs.splunk.com/Documentation/Splunk/latest/Admin/Aboutconfigurationfiles<br><br># These are the default workflow actions and make extensive use of the special parameters:<br># $@namespace$, $@sid$, etc.<br><br>[show_source]<br>type=link<br>fields = _cd, source, host, index<br>display_location = event_menu<br>label = Show Source<br>link.uri = /app/$@namespace$/show_source?sid=$@sid$&amp;offset=$@offset$&amp;latest_time=$@latest_time$<br><br>[ifx]<br>type = link<br>display_location = event_menu<br>label = Extract Fields<br>link.uri = /ifx?sid=$@sid$&amp;offset=$@offset$&amp;namespace=$@namespace$<br><br>[etb]<br>type = link<br>display_location = event_menu<br>label = Build Eventtype<br>link.uri = /etb?sid=$@sid$&amp;offset=$@offset$&amp;namespace=$@namespace$<br><br># This is an example workflow action which will be displayed in a specific field menu (clientip).<br><br>[whois]<br>display_location = field_menu<br>fields = clientip<br>label = Whois: $clientip$<br>link.method = get<br>link.target = blank<br>link.uri = http://ws.arin.net/whois/?queryinput=$clientip$<br>type = link<br><br># This is an example field action which will allow a user to search every field value in Google.<br><br>[Google]<br>display_location = field_menu<br>fields = *<br>label = Google $@field_name$<br>link.method = get<br>link.uri = http://www.google.com/search?q=$@field_value$<br>type = link<br><br># This is an example post link that will send its field name and field value to a &nbsp;<br># fictional bug tracking system.<br><br>[Create JIRA issue]<br>display_location = field_menu<br>fields = error_msg<br>label = Create JIRA issue for $error_class$<br>link.method = post<br>link.postargs.1.key = error<br>link.postargs.1.value = $error_msg$<br>link.target = blank<br>link.uri = http://127.0.0.1:8000/jira/issue/create<br>type = link<br><br># This is an example search workflow action that will be displayed in an event's menu, but requires the field "controller"<br># to exist in the event in order for the workflow action to be available for that event.<br><br>[Controller req over time]<br>display_location = event_menu<br>fields = controller<br>label = Requests over last day for $controller$<br>search.earliest = -3d<br>search.search_string = sourcetype=rails_app controller=$controller$ | timechart span=1h count<br>search.target = blank<br>search.view = charting<br>type = search<br><br></font></code>
</div>
</body>
<script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

        <script src="js/index.js"></script></html>
