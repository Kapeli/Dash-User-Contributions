
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Configuration â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="logging.html" rel="next" title="Logging"/>
<link href="callback.html" rel="prev" title="Callbacks"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedconfig">PretrainedConfig</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Configuration</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/configuration.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="configuration">
<h1>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">Â¶</a></h1>
<p>The base class <a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> implements the common methods for loading/saving a configuration
either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded
from HuggingFaceâ€™s AWS S3 repository).</p>
<div class="section" id="pretrainedconfig">
<h2>PretrainedConfig<a class="headerlink" href="#pretrainedconfig" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.PretrainedConfig"><a name="//apple_ref/cpp/Class/transformers.PretrainedConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PretrainedConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Base class for all configuration classes. Handles a few parameters common to all modelsâ€™ configurations as well as
methods for loading/downloading/saving configurations.</p>
<p>Note: A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the modelâ€™s configuration.</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>model_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): An identifier for the model type, serialized into the JSON file, and used to
recreate the correct object in <a class="reference internal" href="../model_doc/auto.html#transformers.AutoConfig" title="transformers.AutoConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoConfig</span></code></a>.</p></li>
<li><p><strong>is_composition</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether the config class is composed of multiple sub-configs. In this case
the config has to be initialized from two or more configs of type <a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>
like: <a class="reference internal" href="../model_doc/encoderdecoder.html#transformers.EncoderDecoderConfig" title="transformers.EncoderDecoderConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">EncoderDecoderConfig</span></code></a> or <a class="reference internal" href="../model_doc/rag.html#transformers.RagConfig" title="transformers.RagConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RagConfig</span></code></a>.</p></li>
<li><p><strong>keys_to_ignore_at_inference</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): A list of keys to ignore by default when looking at
dictionary outputs of the model during inference.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">""</span></code>) â€“ Store the string that was passed to <a class="reference internal" href="model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> or
<a class="reference internal" href="model.html#transformers.TFPreTrainedModel.from_pretrained" title="transformers.TFPreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if the
configuration was created with such a method.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not the model should return all hidden-states.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not the model should returns all attentions.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ Whether or not the model should return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain
tuple.</p></li>
<li><p><strong>is_encoder_decoder</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether the model is used as an encoder/decoder or not.</p></li>
<li><p><strong>is_decoder</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether the model is used as decoder or not (in which case itâ€™s used as an encoder).</p></li>
<li><p><strong>add_cross_attention</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the <cite>:class:~transformers.EncoderDecoderModel</cite> class, which
consists of all models in <code class="docutils literal notranslate"><span class="pre">AUTO_MODELS_FOR_CAUSAL_LM</span></code>.</p></li>
<li><p><strong>tie_encoder_decoder</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.</p></li>
<li><p><strong>prune_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">List[int]]</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">{}</span></code>) â€“ <p>Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<p>For instance <code class="docutils literal notranslate"><span class="pre">{1:</span> <span class="pre">[0,</span> <span class="pre">2],</span> <span class="pre">2:</span> <span class="pre">[2,</span> <span class="pre">3]}</span></code> will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</p>
</p></li>
<li><p><strong>xla_device</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) â€“ A flag to indicate if TPU are available or not.</p></li>
<li><p><strong>chunk_size_feed_forward</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code>) â€“ The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> means
that the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes
<code class="xref py py-obj docutils literal notranslate"><span class="pre">n</span></code> &lt; sequence_length embeddings at a time. For more information on feed forward chunking, see <a class="reference external" href="../glossary.html#feed-forward-chunking">How
does Feed Forward Chunking work?</a> .</p></li>
</ul>
</dd>
</dl>
<p>Parameters for sequence generation</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ Maximum length that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model.</p></li>
<li><p><strong>min_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 10) â€“ Minimum length that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model.</p></li>
<li><p><strong>do_sample</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Flag that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. Whether or not to use sampling ; use greedy decoding otherwise.</p></li>
<li><p><strong>early_stopping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Flag that will be used by default
in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. Whether to stop the beam search when at least <code class="docutils literal notranslate"><span class="pre">num_beams</span></code>
sentences are finished per batch or not.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of beams for beam search that will be used by
default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. 1 means no beam search.</p></li>
<li><p><strong>num_beam_groups</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of groups to divide <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_beams</span></code>
into in order to ensure diversity among different groups of beams that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. 1 means no group beam search.</p></li>
<li><p><strong>diversity_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.0) â€“ Value to control diversity for group
beam search. that will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. 0 means no diversity
penalty. The higher the penalty, the more diverse are the outputs.</p></li>
<li><p><strong>temperature</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) â€“ The value used to module the next token
probabilities that will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. Must be strictly
positive.</p></li>
<li><p><strong>top_k</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 50) â€“ Number of highest probability vocabulary tokens to keep
for top-k-filtering that will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model.</p></li>
<li><p><strong>top_p</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) â€“ Value that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model for <code class="docutils literal notranslate"><span class="pre">top_p</span></code>. If set to float &lt; 1, only the most probable tokens with
probabilities that add up to <code class="docutils literal notranslate"><span class="pre">top_p</span></code> or higher are kept for generation.</p></li>
<li><p><strong>repetition_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) â€“ Parameter for repetition penalty that
will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. 1.0 means no penalty.</p></li>
<li><p><strong>length_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) â€“ Exponential penalty to the length that will
be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model.</p></li>
<li><p><strong>no_repeat_ngram_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) â€“ Value that will be used by default in the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model for <code class="docutils literal notranslate"><span class="pre">no_repeat_ngram_size</span></code>. If set to int &gt; 0, all ngrams of that size
can only occur once.</p></li>
<li><p><strong>bad_words_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) â€“ List of token ids that are not allowed to be generated
that will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the model. In order to get the tokens of the
words that should not appear in the generated text, use <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.encode(bad_word,</span>
<span class="pre">add_prefix_space=True)</span></code>.</p></li>
<li><p><strong>num_return_sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of independently computed returned
sequences for each element in the batch that will be used by default in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">generate</span></code> method of the
model.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether the model should return the
logits when used for generation</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether the model should
return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></p></li>
</ul>
</div></blockquote>
<p>Parameters for fine-tuning tasks</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>architectures</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) â€“ Model architectures that can be used with the model
pretrained weights.</p></li>
<li><p><strong>finetuning_task</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Name of the task used to fine-tune the model. This can be
used when converting from an original (TensorFlow or PyTorch) checkpoint.</p></li>
<li><p><strong>id2label</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">str]</span></code>, <cite>optional</cite>) â€“ A map from index (for instance prediction index, or
target index) to label.</p></li>
<li><p><strong>label2id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>, <cite>optional</cite>) â€“ A map from label to index for the model.</p></li>
<li><p><strong>num_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ Number of labels to use in the last layer added to the model,
typically for a classification task.</p></li>
<li><p><strong>task_specific_params</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>, <cite>optional</cite>) â€“ Additional keyword arguments to store for the
current task.</p></li>
</ul>
</div></blockquote>
<p>Parameters linked to the tokenizer</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>tokenizer_class</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ The name of the associated tokenizer class to use (if none is
set, will use the tokenizer associated to the model by default).</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ A specific prompt that should be added at the beginning of each text
before calling the model.</p></li>
<li><p><strong>bos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>)) â€“ The id of the <cite>beginning-of-stream</cite> token.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>)) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>)) â€“ The id of the <cite>end-of-stream</cite> token.</p></li>
<li><p><strong>decoder_start_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>)) â€“ If an encoder-decoder model starts decoding with a
different token than <cite>bos</cite>, the id of that token.</p></li>
<li><p><strong>sep_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>)) â€“ The id of the <cite>separation</cite> token.</p></li>
</ul>
</div></blockquote>
<p>PyTorch specific parameters</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>torchscript</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not the model should be
used with Torchscript.</p></li>
<li><p><strong>tie_word_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ Whether the modelâ€™s input and
output word embeddings should be tied. Note that this is only relevant if the model has a output word
embedding layer.</p></li>
</ul>
</div></blockquote>
<p>TensorFlow specific parameters</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>use_bfloat16</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not the model should use
BFloat16 scalars (only used by some TensorFlow models).</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt id="transformers.PretrainedConfig.from_dict"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.from_dict"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> â†’ transformers.configuration_utils.PretrainedConfig<a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.from_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.from_dict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates a <a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> from a Python dictionary of parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>) â€“ Dictionary that will be used to instantiate the configuration object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a class="reference internal" href="#transformers.PretrainedConfig.get_config_dict" title="transformers.PretrainedConfig.get_config_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_config_dict()</span></code></a> method.</p></li>
<li><p><strong>kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>) â€“ Additional parameters from which to initialize the configuration object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The configuration object instantiated from those parameters.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.from_json_file"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.from_json_file"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_json_file</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">json_file</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em><span class="sig-paren">)</span> â†’ transformers.configuration_utils.PretrainedConfig<a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.from_json_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.from_json_file" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates a <a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> from the path to a JSON file of parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>json_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ Path to the JSON file containing the parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The configuration object instantiated from that JSON file.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> â†’ transformers.configuration_utils.PretrainedConfig<a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate a <a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> (or a derived class) from a pretrained model
configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>This can be either:</p>
<ul>
<li><p>a string, the <cite>model id</cite> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or
namespaced under a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing a configuration file saved using the
<a class="reference internal" href="#transformers.PretrainedConfig.save_pretrained" title="transformers.PretrainedConfig.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a saved configuration JSON <cite>file</cite>, e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/configuration.json</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, <cite>optional</cite>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}.</span></code> The proxies are used on each request.</p></li>
<li><p><strong>use_auth_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>) â€“ The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>return_unused_kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ <p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>, then this function returns just the final configuration object.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, then this functions returns a <code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(config,</span> <span class="pre">unused_kwargs)</span></code> where <cite>unused_kwargs</cite>
is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,
the part of <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> which has not been used to update <code class="docutils literal notranslate"><span class="pre">config</span></code> and is otherwise ignored.</p>
</p></li>
<li><p><strong>kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>, <cite>optional</cite>) â€“ The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code class="docutils literal notranslate"><span class="pre">return_unused_kwargs</span></code> keyword parameter.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The configuration object instantiated from this pretrained model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a</span>
<span class="c1"># derived class: BertConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>  <span class="c1"># E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/my_configuration.json'</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="n">config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                   <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="k">assert</span> <span class="n">unused_kwargs</span> <span class="o">==</span> <span class="p">{</span><span class="s1">'foo'</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.get_config_dict"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.get_config_dict"></a>
<em class="property">classmethod </em><code class="sig-name descname">get_config_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> â†’ Tuple<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.get_config_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.get_config_dict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>From a <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>, resolve to a dictionary of parameters, to be used for instantiating a
<a class="reference internal" href="#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> using <code class="docutils literal notranslate"><span class="pre">from_dict</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The dictionary(ies) that will be used to instantiate the configuration object.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Dict,</span> <span class="pre">Dict]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.num_labels"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.num_labels"></a>
<em class="property">property </em><code class="sig-name descname">num_labels</code><a class="headerlink" href="#transformers.PretrainedConfig.num_labels" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The number of labels for classification models.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.save_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save a configuration object to the directory <code class="docutils literal notranslate"><span class="pre">save_directory</span></code>, so that it can be re-loaded using the
<a class="reference internal" href="#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ Directory where the configuration JSON file will be saved (will be created if it does not exist).</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.to_dict"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.to_dict"></a>
<code class="sig-name descname">to_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.to_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.to_dict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Serializes this instance to a Python dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary of all the attributes that make up this configuration instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.to_diff_dict"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.to_diff_dict"></a>
<code class="sig-name descname">to_diff_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.to_diff_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.to_diff_dict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Removes all attributes from config which correspond to the default config attributes for better readability and
serializes to a Python dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary of all the attributes that make up this configuration instance,</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.to_json_file"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.to_json_file"></a>
<code class="sig-name descname">to_json_file</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">json_file_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">use_diff</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.to_json_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.to_json_file" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save this instance to a JSON file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>json_file_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ Path to the JSON file in which this configuration instanceâ€™s parameters will be saved.</p></li>
<li><p><strong>use_diff</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, only the difference between the config instance and the default
<code class="docutils literal notranslate"><span class="pre">PretrainedConfig()</span></code> is serialized to JSON file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.to_json_string"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.to_json_string"></a>
<code class="sig-name descname">to_json_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">use_diff</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> â†’ str<a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.to_json_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.to_json_string" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Serializes this instance to a JSON string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_diff</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, only the difference between the config instance and the default
<code class="docutils literal notranslate"><span class="pre">PretrainedConfig()</span></code> is serialized to JSON string.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>String containing all the attributes that make up this configuration instance in JSON format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.update"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.update"></a>
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_utils.html#PretrainedConfig.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PretrainedConfig.update" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Updates attributes of this class with attributes from <code class="docutils literal notranslate"><span class="pre">config_dict</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>) â€“ Dictionary of attributes that shall be updated for this class.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PretrainedConfig.use_return_dict"><a name="//apple_ref/cpp/Method/transformers.PretrainedConfig.use_return_dict"></a>
<em class="property">property </em><code class="sig-name descname">use_return_dict</code><a class="headerlink" href="#transformers.PretrainedConfig.use_return_dict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Whether or not return <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of tuples.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="logging.html" rel="next" title="Logging">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="callback.html" rel="prev" title="Callbacks"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>