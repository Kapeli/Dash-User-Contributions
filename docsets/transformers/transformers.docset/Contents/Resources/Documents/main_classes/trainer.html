
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Trainer — transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../model_doc/albert.html" rel="next" title="ALBERT"/>
<link href="tokenizer.html" rel="prev" title="Tokenizer"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using 🤗 Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">🤗 Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trainer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#seq2seqtrainer">Seq2SeqTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tftrainer">TFTrainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainingarguments">TrainingArguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#seq2seqtrainingarguments">Seq2SeqTrainingArguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tftrainingarguments">TFTrainingArguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainer-integrations">Trainer Integrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installation-notes">Installation Notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fairscale">FairScale</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deepspeed">DeepSpeed</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deployment-with-multiple-gpus">Deployment with multiple GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deployment-with-one-gpu">Deployment with one GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuration">Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shared-configuration">Shared Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#zero">ZeRO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimizer">Optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduler">Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#automatic-mixed-precision">Automatic Mixed Precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gradient-clipping">Gradient Clipping</a></li>
<li class="toctree-l4"><a class="reference internal" href="#notes">Notes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#main-deepspeed-resources">Main DeepSpeed Resources</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> »</li>
<li>Trainer</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/trainer.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="trainer">
<h1>Trainer<a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and <a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a> classes provide an API for feature-complete
training in most standard use cases. It’s used in most of the <a class="reference internal" href="../examples.html"><span class="doc">example scripts</span></a>.</p>
<p>Before instantiating your <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>/<a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a>, create a
<a class="reference internal" href="#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a>/<a class="reference internal" href="#transformers.TFTrainingArguments" title="transformers.TFTrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainingArguments</span></code></a> to access all the points of
customization during training.</p>
<p>The API supports distributed training on multiple GPUs/TPUs, mixed precision through <a class="reference external" href="https://github.com/NVIDIA/apex">NVIDIA Apex</a> for PyTorch and <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.mixed_precision</span></code> for TensorFlow.</p>
<p>Both <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and <a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a> contain the basic training loop supporting the
previous features. To inject custom behavior you can subclass them and override the following methods:</p>
<ul class="simple">
<li><p><strong>get_train_dataloader</strong>/<strong>get_train_tfdataset</strong> – Creates the training DataLoader (PyTorch) or TF Dataset.</p></li>
<li><p><strong>get_eval_dataloader</strong>/<strong>get_eval_tfdataset</strong> – Creates the evaluation DataLoader (PyTorch) or TF Dataset.</p></li>
<li><p><strong>get_test_dataloader</strong>/<strong>get_test_tfdataset</strong> – Creates the test DataLoader (PyTorch) or TF Dataset.</p></li>
<li><p><strong>log</strong> – Logs information on the various objects watching training.</p></li>
<li><p><strong>create_optimizer_and_scheduler</strong> – Setups the optimizer and learning rate scheduler if they were not passed at
init.</p></li>
<li><p><strong>compute_loss</strong> - Computes the loss on a batch of training inputs.</p></li>
<li><p><strong>training_step</strong> – Performs a training step.</p></li>
<li><p><strong>prediction_step</strong> – Performs an evaluation/test step.</p></li>
<li><p><strong>run_model</strong> (TensorFlow only) – Basic pass through the model.</p></li>
<li><p><strong>evaluate</strong> – Runs an evaluation loop and returns metrics.</p></li>
<li><p><strong>predict</strong> – Returns predictions (with metrics if labels are available) on a test set.</p></li>
</ul>
<p>Here is an example of how to customize <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> using a custom loss function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="k">class</span> <span class="nc">MyTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">my_custom_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Another way to customize the training loop behavior for the PyTorch <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> is to use
<a class="reference internal" href="callback.html"><span class="doc">callbacks</span></a> that can inspect the training loop state (for progress reporting, logging on TensorBoard or
other ML platforms…) and take decisions (like early stopping).</p>
<div class="section" id="id1">
<h2>Trainer<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.Trainer"><a name="//apple_ref/cpp/Class/transformers.Trainer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">Trainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>transformers.modeling_utils.PreTrainedModel<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.training_args.TrainingArguments<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">data_collator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>DataCollator<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase">transformers.tokenization_utils_base.PreTrainedTokenizerBase</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model_init</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">]</span><span class="p">, </span>transformers.modeling_utils.PreTrainedModel<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compute_metrics</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>transformers.trainer_utils.EvalPrediction<span class="p">]</span><span class="p">, </span>Dict<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.trainer_callback.TrainerCallback<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">optimizers</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.optim.optimizer.Optimizer<span class="p">, </span>torch.optim.lr_scheduler.LambdaLR<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(None, None)</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, <cite>optional</cite>) – <p>The model to train, evaluate or use for predictions. If not provided, a <code class="docutils literal notranslate"><span class="pre">model_init</span></code> must be passed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> is optimized to work with the <a class="reference internal" href="model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>
provided by the library. You can still use your own models defined as <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> as long as
they work the same way as the 🤗 Transformers models.</p>
</div>
</p></li>
<li><p><strong>args</strong> (<a class="reference internal" href="#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a>, <cite>optional</cite>) – The arguments to tweak for training. Will default to a basic instance of
<a class="reference internal" href="#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a> with the <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> set to a directory named <cite>tmp_trainer</cite> in
the current directory if not provided.</p></li>
<li><p><strong>data_collator</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">DataCollator</span></code>, <cite>optional</cite>) – The function to use to form a batch from a list of elements of <code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataset</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_dataset</span></code>.
Will default to <code class="xref py py-func docutils literal notranslate"><span class="pre">default_data_collator()</span></code> if no <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> is provided, an instance of
<code class="xref py py-func docutils literal notranslate"><span class="pre">DataCollatorWithPadding()</span></code> otherwise.</p></li>
<li><p><strong>train_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code>, <cite>optional</cite>) – The dataset to use for training. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not accepted by the
<code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed.</p></li>
<li><p><strong>eval_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code>, <cite>optional</cite>) – The dataset to use for evaluation. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not accepted by the
<code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed.</p></li>
<li><p><strong>tokenizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code>, <cite>optional</cite>) – The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the
maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an
interrupted training or reuse the fine-tuned model.</p></li>
<li><p><strong>model_init</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[[],</span> <span class="pre">PreTrainedModel]</span></code>, <cite>optional</cite>) – <p>A function that instantiates the model to be used. If provided, each call to
<a class="reference internal" href="#transformers.Trainer.train" title="transformers.Trainer.train"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train()</span></code></a> will start from a new instance of the model as given by this function.</p>
<p>The function may have zero argument, or a single one containing the optuna/Ray Tune trial object, to be
able to choose different architectures according to hyper parameters (such as layer count, sizes of inner
layers, dropout probabilities etc).</p>
</p></li>
<li><p><strong>compute_metrics</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[[EvalPrediction],</span> <span class="pre">Dict]</span></code>, <cite>optional</cite>) – The function that will be used to compute metrics at evaluation. Must take a
<a class="reference internal" href="../internal/trainer_utils.html#transformers.EvalPrediction" title="transformers.EvalPrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">EvalPrediction</span></code></a> and return a dictionary string to metric values.</p></li>
<li><p><strong>callbacks</strong> (List of <a class="reference internal" href="callback.html#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a>, <cite>optional</cite>) – <p>A list of callbacks to customize the training loop. Will add those to the list of default callbacks
detailed in <a class="reference internal" href="callback.html"><span class="doc">here</span></a>.</p>
<p>If you want to remove one of the default callbacks used, use the <a class="reference internal" href="#transformers.Trainer.remove_callback" title="transformers.Trainer.remove_callback"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Trainer.remove_callback()</span></code></a> method.</p>
</p></li>
<li><p><strong>optimizers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.optim.Optimizer,</span> <span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code>, <cite>optional</cite>) – A tuple
containing the optimizer and the scheduler to use. Will default to an instance of
<a class="reference internal" href="optimizer_schedules.html#transformers.AdamW" title="transformers.AdamW"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdamW</span></code></a> on your model and a scheduler given by
<a class="reference internal" href="optimizer_schedules.html#transformers.get_linear_schedule_with_warmup" title="transformers.get_linear_schedule_with_warmup"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_linear_schedule_with_warmup()</span></code></a> controlled by <code class="xref py py-obj docutils literal notranslate"><span class="pre">args</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Important attributes:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>model</strong> – Always points to the core model. If using a transformers model, it will be a
<a class="reference internal" href="model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> subclass.</p></li>
<li><p><strong>model_wrapped</strong> – Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under <code class="docutils literal notranslate"><span class="pre">DeepSpeed</span></code>,
the inner model is wrapped in <code class="docutils literal notranslate"><span class="pre">DeepSpeed</span></code> and then again in <code class="docutils literal notranslate"><span class="pre">torch.nn.DistributedDataParallel</span></code>. If the
inner model hasn’t been wrapped, then <code class="docutils literal notranslate"><span class="pre">self.model_wrapped</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">self.model</span></code>.</p></li>
<li><p><strong>is_model_parallel</strong> – Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt id="transformers.Trainer.add_callback"><a name="//apple_ref/cpp/Method/transformers.Trainer.add_callback"></a>
<code class="sig-name descname">add_callback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">callback</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.add_callback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.add_callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a callback to the current list of <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>callback</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code> class or an instance of a <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>.
In the first case, will instantiate a member of that class.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.compute_loss"><a name="//apple_ref/cpp/Method/transformers.Trainer.compute_loss"></a>
<code class="sig-name descname">compute_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.compute_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.compute_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>How the loss is computed by Trainer. By default, all models return the loss in the first element.</p>
<p>Subclass and override for custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.create_optimizer_and_scheduler"><a name="//apple_ref/cpp/Method/transformers.Trainer.create_optimizer_and_scheduler"></a>
<code class="sig-name descname">create_optimizer_and_scheduler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_training_steps</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.create_optimizer_and_scheduler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.create_optimizer_and_scheduler" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optimizer and the learning rate scheduler.</p>
<p>We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer’s init through <code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code>, or subclass and override this method in a subclass.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.evaluate"><a name="//apple_ref/cpp/Method/transformers.Trainer.evaluate"></a>
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric_key_prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'eval'</span></em><span class="sig-paren">)</span> → Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Run evaluation and returns metrics.</p>
<p>The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init <code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_metrics</span></code> argument).</p>
<p>You can also subclass and override this method to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eval_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – Pass a dataset if you wish to override <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.eval_dataset</span></code>. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>,
columns not accepted by the <code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. It must implement the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code> method.</p></li>
<li><p><strong>ignore_keys</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Lst[str]</span></code>, <cite>optional</cite>) – A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p></li>
<li><p><strong>metric_key_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval"</span></code>) – An optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named
“eval_bleu” if the prefix is “eval” (default)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.floating_point_ops"><a name="//apple_ref/cpp/Method/transformers.Trainer.floating_point_ops"></a>
<code class="sig-name descname">floating_point_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.floating_point_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.floating_point_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>For models that inherit from <a class="reference internal" href="model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>, uses that method to compute the number of
floating point operations for every backward + forward pass. If using another model, either implement such a
method in the model or subclass and override this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[torch.Tensor,</span> <span class="pre">Any]]</span></code>) – The inputs and targets of the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of floating-point operations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.get_eval_dataloader"><a name="//apple_ref/cpp/Method/transformers.Trainer.get_eval_dataloader"></a>
<code class="sig-name descname">get_eval_dataloader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → torch.utils.data.dataloader.DataLoader<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.get_eval_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.get_eval_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the evaluation <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code>, <cite>optional</cite>) – If provided, will override <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.eval_dataset</span></code>. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not
accepted by the <code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. It must implement <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.get_test_dataloader"><a name="//apple_ref/cpp/Method/transformers.Trainer.get_test_dataloader"></a>
<code class="sig-name descname">get_test_dataloader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">test_dataset</span><span class="p">:</span> <span class="n">torch.utils.data.dataset.Dataset</span></em><span class="sig-paren">)</span> → torch.utils.data.dataloader.DataLoader<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.get_test_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.get_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the test <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>test_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code>, <cite>optional</cite>) – The test dataset to use. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not accepted by the
<code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. It must implement <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.get_train_dataloader"><a name="//apple_ref/cpp/Method/transformers.Trainer.get_train_dataloader"></a>
<code class="sig-name descname">get_train_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → torch.utils.data.dataloader.DataLoader<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.get_train_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.get_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the training <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
<p>Will use no sampler if <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.train_dataset</span></code> does not implement <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code>, a random sampler (adapted
to distributed training if necessary) otherwise.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.hyperparameter_search"><a name="//apple_ref/cpp/Method/transformers.Trainer.hyperparameter_search"></a>
<code class="sig-name descname">hyperparameter_search</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hp_space</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>optuna.Trial<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compute_objective</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">, </span>float<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_trials</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">20</span></em>, <em class="sig-param"><span class="n">direction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'minimize'</span></em>, <em class="sig-param"><span class="n">backend</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>transformers.trainer_utils.HPSearchBackend<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hp_name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>optuna.Trial<span class="p">]</span><span class="p">, </span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.BestRun<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.hyperparameter_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.hyperparameter_search" title="Permalink to this definition">¶</a></dt>
<dd><p>Launch an hyperparameter search using <code class="docutils literal notranslate"><span class="pre">optuna</span></code> or <code class="docutils literal notranslate"><span class="pre">Ray</span> <span class="pre">Tune</span></code>. The optimized quantity is determined by
<code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_objectie</span></code>, which defaults to a function returning the evaluation loss when no metric is provided,
the sum of all metrics otherwise.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>To use this method, you need to have provided a <code class="docutils literal notranslate"><span class="pre">model_init</span></code> when initializing your
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>: we need to reinitialize the model at each new run. This is incompatible
with the <code class="docutils literal notranslate"><span class="pre">optimizers</span></code> argument, so you need to subclass <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and override the
method <a class="reference internal" href="#transformers.Trainer.create_optimizer_and_scheduler" title="transformers.Trainer.create_optimizer_and_scheduler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">create_optimizer_and_scheduler()</span></code></a> for custom optimizer/scheduler.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hp_space</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[["optuna.Trial"],</span> <span class="pre">Dict[str,</span> <span class="pre">float]]</span></code>, <cite>optional</cite>) – A function that defines the hyperparameter search space. Will default to
<code class="xref py py-func docutils literal notranslate"><span class="pre">default_hp_space_optuna()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">default_hp_space_ray()</span></code> depending on your backend.</p></li>
<li><p><strong>compute_objective</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[[Dict[str,</span> <span class="pre">float]],</span> <span class="pre">float]</span></code>, <cite>optional</cite>) – A function computing the objective to minimize or maximize from the metrics returned by the
<a class="reference internal" href="#transformers.Trainer.evaluate" title="transformers.Trainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a> method. Will default to <code class="xref py py-func docutils literal notranslate"><span class="pre">default_compute_objective()</span></code>.</p></li>
<li><p><strong>n_trials</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 100) – The number of trial runs to test.</p></li>
<li><p><strong>direction</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"minimize"</span></code>) – Whether to optimize greater or lower objects. Can be <code class="xref py py-obj docutils literal notranslate"><span class="pre">"minimize"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"maximize"</span></code>, you should
pick <code class="xref py py-obj docutils literal notranslate"><span class="pre">"minimize"</span></code> when optimizing the validation loss, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"maximize"</span></code> when optimizing one or
several metrics.</p></li>
<li><p><strong>backend</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">HPSearchBackend</span></code>, <cite>optional</cite>) – The backend to use for hyperparameter search. Will default to optuna or Ray Tune, depending on which
one is installed. If both are installed, will default to optuna.</p></li>
<li><p><strong>kwargs</strong> – <p>Additional keyword arguments passed along to <code class="xref py py-obj docutils literal notranslate"><span class="pre">optuna.create_study</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">ray.tune.run</span></code>. For
more information see:</p>
<ul>
<li><p>the documentation of <a class="reference external" href="https://optuna.readthedocs.io/en/stable/reference/alias_generated/optuna.create_study.html#optuna.create_study">optuna.create_study</a></p></li>
<li><p>the documentation of <a class="reference external" href="https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run">tune.run</a></p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>All the information about the best run.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.trainer_utils.BestRun</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.is_local_process_zero"><a name="//apple_ref/cpp/Method/transformers.Trainer.is_local_process_zero"></a>
<code class="sig-name descname">is_local_process_zero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → bool<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.is_local_process_zero"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.is_local_process_zero" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several
machines) main process.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.is_world_process_zero"><a name="//apple_ref/cpp/Method/transformers.Trainer.is_world_process_zero"></a>
<code class="sig-name descname">is_world_process_zero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → bool<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.is_world_process_zero"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.is_world_process_zero" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether or not this process is the global main process (when training in a distributed fashion on several
machines, this is only going to be <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> for one process).</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.log"><a name="//apple_ref/cpp/Method/transformers.Trainer.log"></a>
<code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logs</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log <code class="xref py py-obj docutils literal notranslate"><span class="pre">logs</span></code> on the various objects watching training.</p>
<p>Subclass and override this method to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>logs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>) – The values to log.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.num_examples"><a name="//apple_ref/cpp/Method/transformers.Trainer.num_examples"></a>
<code class="sig-name descname">num_examples</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataloader</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em><span class="sig-paren">)</span> → int<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.num_examples"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.num_examples" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper to get number of samples in a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> by accessing its dataset.</p>
<p>Will raise an exception if the underlying dataset dese not implement method <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code></p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.pop_callback"><a name="//apple_ref/cpp/Method/transformers.Trainer.pop_callback"></a>
<code class="sig-name descname">pop_callback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">callback</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.pop_callback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.pop_callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove a callback from the current list of <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code> and returns it.</p>
<p>If the callback is not found, returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> (and no error is raised).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>callback</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code> class or an instance of a <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>.
In the first case, will pop the first member of that class found in the list of callbacks.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The callback removed, if found.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.predict"><a name="//apple_ref/cpp/Method/transformers.Trainer.predict"></a>
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">test_dataset</span><span class="p">:</span> <span class="n">torch.utils.data.dataset.Dataset</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric_key_prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'eval'</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.PredictionOutput<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Run prediction and returns predictions and potential metrics.</p>
<p>Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in <a class="reference internal" href="#transformers.Trainer.evaluate" title="transformers.Trainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dataset</span></code>) – Dataset to run the predictions on. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not accepted by the
<code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. Has to implement the method <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code></p></li>
<li><p><strong>ignore_keys</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Lst[str]</span></code>, <cite>optional</cite>) – A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p></li>
<li><p><strong>metric_key_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval"</span></code>) – An optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named
“eval_bleu” if the prefix is “eval” (default)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your predictions or labels have different sequence length (for instance because you’re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.</p>
</div>
<p>Returns: <cite>NamedTuple</cite> A namedtuple with the following keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>predictions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): The predictions on <code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataset</span></code>.</p></li>
<li><p>label_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>, <cite>optional</cite>): The labels (if the dataset contained some).</p></li>
<li><p>metrics (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>, <cite>optional</cite>): The potential dictionary of metrics (if the dataset
contained labels).</p></li>
</ul>
</div></blockquote>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.prediction_loop"><a name="//apple_ref/cpp/Method/transformers.Trainer.prediction_loop"></a>
<code class="sig-name descname">prediction_loop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataloader</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em>, <em class="sig-param"><span class="n">description</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">prediction_loss_only</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric_key_prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'eval'</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.PredictionOutput<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.prediction_loop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.prediction_loop" title="Permalink to this definition">¶</a></dt>
<dd><p>Prediction/evaluation loop, shared by <a class="reference internal" href="#transformers.Trainer.evaluate" title="transformers.Trainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Trainer.evaluate()</span></code></a> and <a class="reference internal" href="#transformers.Trainer.predict" title="transformers.Trainer.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Trainer.predict()</span></code></a>.</p>
<p>Works both with or without labels.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.prediction_step"><a name="//apple_ref/cpp/Method/transformers.Trainer.prediction_step"></a>
<code class="sig-name descname">prediction_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">prediction_loss_only</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → Tuple<span class="p">[</span>Optional<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>Optional<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Optional<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.prediction_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.prediction_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform an evaluation step on <code class="xref py py-obj docutils literal notranslate"><span class="pre">model</span></code> using obj:<cite>inputs</cite>.</p>
<p>Subclass and override to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – The model to evaluate.</p></li>
<li><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[torch.Tensor,</span> <span class="pre">Any]]</span></code>) – <p>The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code>. Check your model’s documentation for all accepted arguments.</p>
</p></li>
<li><p><strong>prediction_loss_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – Whether or not to return the loss only.</p></li>
<li><p><strong>ignore_keys</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Lst[str]</span></code>, <cite>optional</cite>) – A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple with the loss, logits and
labels (each being optional).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.remove_callback"><a name="//apple_ref/cpp/Method/transformers.Trainer.remove_callback"></a>
<code class="sig-name descname">remove_callback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">callback</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.remove_callback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.remove_callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove a callback from the current list of <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>callback</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code> class or an instance of a <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code>.
In the first case, will remove the first member of that class found in the list of callbacks.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.save_model"><a name="//apple_ref/cpp/Method/transformers.Trainer.save_model"></a>
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Will save the model, so you can reload it using <code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained()</span></code>.</p>
<p>Will only save from the world_master process (unless in TPUs).</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.train"><a name="//apple_ref/cpp/Method/transformers.Trainer.train"></a>
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_path</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trial</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>optuna.Trial<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Main training entry point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Local path to the model if the model to train has been instantiated from a local path. If present,
training will resume from the optimizer/scheduler states loaded here.</p></li>
<li><p><strong>trial</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">optuna.Trial</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>, <cite>optional</cite>) – The trial run or the hyperparameter dictionary for hyperparameter search.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Trainer.training_step"><a name="//apple_ref/cpp/Method/transformers.Trainer.training_step"></a>
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/transformers/trainer.html#Trainer.training_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Trainer.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a training step on a batch of inputs.</p>
<p>Subclass and override to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – The model to train.</p></li>
<li><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[torch.Tensor,</span> <span class="pre">Any]]</span></code>) – <p>The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span></code>. Check your model’s documentation for all accepted arguments.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tensor with training loss on this batch.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="seq2seqtrainer">
<h2>Seq2SeqTrainer<a class="headerlink" href="#seq2seqtrainer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.Seq2SeqTrainer"><a name="//apple_ref/cpp/Class/transformers.Seq2SeqTrainer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">Seq2SeqTrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>transformers.modeling_utils.PreTrainedModel<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.training_args.TrainingArguments<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">data_collator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>DataCollator<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase">transformers.tokenization_utils_base.PreTrainedTokenizerBase</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model_init</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">]</span><span class="p">, </span>transformers.modeling_utils.PreTrainedModel<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compute_metrics</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>transformers.trainer_utils.EvalPrediction<span class="p">]</span><span class="p">, </span>Dict<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>transformers.trainer_callback.TrainerCallback<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">optimizers</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.optim.optimizer.Optimizer<span class="p">, </span>torch.optim.lr_scheduler.LambdaLR<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(None, None)</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_seq2seq.html#Seq2SeqTrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Seq2SeqTrainer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="transformers.Seq2SeqTrainer.evaluate"><a name="//apple_ref/cpp/Method/transformers.Seq2SeqTrainer.evaluate"></a>
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.data.dataset.Dataset<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric_key_prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'eval'</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beams</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><a class="reference internal" href="../_modules/transformers/trainer_seq2seq.html#Seq2SeqTrainer.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Seq2SeqTrainer.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Run evaluation and returns metrics.</p>
<p>The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init <code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_metrics</span></code> argument).</p>
<p>You can also subclass and override this method to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eval_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – Pass a dataset if you wish to override <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.eval_dataset</span></code>. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>,
columns not accepted by the <code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. It must implement the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code> method.</p></li>
<li><p><strong>ignore_keys</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) – A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p></li>
<li><p><strong>metric_key_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval"</span></code>) – An optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named
“eval_bleu” if the prefix is <code class="docutils literal notranslate"><span class="pre">"eval"</span></code> (default)</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The maximum target length to use when predicting with the generate method.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of beams for beam search that will be used when predicting with the generate method. 1 means no
beam search.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.Seq2SeqTrainer.predict"><a name="//apple_ref/cpp/Method/transformers.Seq2SeqTrainer.predict"></a>
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">test_dataset</span><span class="p">:</span> <span class="n">torch.utils.data.dataset.Dataset</span></em>, <em class="sig-param"><span class="n">ignore_keys</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">metric_key_prefix</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'eval'</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beams</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.PredictionOutput<a class="reference internal" href="../_modules/transformers/trainer_seq2seq.html#Seq2SeqTrainer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Seq2SeqTrainer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Run prediction and returns predictions and potential metrics.</p>
<p>Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in <a class="reference internal" href="#transformers.Seq2SeqTrainer.evaluate" title="transformers.Seq2SeqTrainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_dataset</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dataset</span></code>) – Dataset to run the predictions on. If it is an <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, columns not accepted by the
<code class="docutils literal notranslate"><span class="pre">model.forward()</span></code> method are automatically removed. Has to implement the method <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__</span></code></p></li>
<li><p><strong>ignore_keys</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) – A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.</p></li>
<li><p><strong>metric_key_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval"</span></code>) – An optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named
“eval_bleu” if the prefix is <code class="docutils literal notranslate"><span class="pre">"eval"</span></code> (default)</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – The maximum target length to use when predicting with the generate method.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of beams for beam search that will be used when predicting with the generate method. 1 means no
beam search.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your predictions or labels have different sequence lengths (for instance because you’re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.</p>
</div>
<p>Returns: <cite>NamedTuple</cite> A namedtuple with the following keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>predictions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): The predictions on <code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataset</span></code>.</p></li>
<li><p>label_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>, <cite>optional</cite>): The labels (if the dataset contained some).</p></li>
<li><p>metrics (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>, <cite>optional</cite>): The potential dictionary of metrics (if the dataset
contained labels).</p></li>
</ul>
</div></blockquote>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tftrainer">
<h2>TFTrainer<a class="headerlink" href="#tftrainer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFTrainer"><a name="//apple_ref/cpp/Class/transformers.TFTrainer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFTrainer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">transformers.modeling_tf_utils.TFPreTrainedModel</span></em>, <em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args_tf.TFTrainingArguments</span></em>, <em class="sig-param"><span class="n">train_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.data.ops.dataset_ops.DatasetV2<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.data.ops.dataset_ops.DatasetV2<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compute_metrics</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>transformers.trainer_utils.EvalPrediction<span class="p">]</span><span class="p">, </span>Dict<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tb_writer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.ops.summary_ops_v2.SummaryWriter<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">optimizers</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2<span class="p">, </span>tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(None, None)</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>TFTrainer is a simple but feature-complete training and eval loop for TensorFlow, optimized for 🤗 Transformers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="model.html#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>) – The model to train, evaluate or use for predictions.</p></li>
<li><p><strong>args</strong> (<a class="reference internal" href="#transformers.TFTrainingArguments" title="transformers.TFTrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainingArguments</span></code></a>) – The arguments to tweak training.</p></li>
<li><p><strong>train_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – The dataset to use for training. The dataset should yield tuples of <code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> where
<code class="docutils literal notranslate"><span class="pre">features</span></code> is a dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a tensor, the loss
is calculated by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">labels=labels)</span></code>. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict, such as
when using a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code>.</p></li>
<li><p><strong>eval_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – The dataset to use for evaluation. The dataset should yield tuples of <code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> where
<code class="docutils literal notranslate"><span class="pre">features</span></code> is a dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a tensor, the loss
is calculated by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">labels=labels)</span></code>. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict, such as
when using a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code>.</p></li>
<li><p><strong>compute_metrics</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[[EvalPrediction],</span> <span class="pre">Dict]</span></code>, <cite>optional</cite>) – The function that will be used to compute metrics at evaluation. Must take a
<a class="reference internal" href="../internal/trainer_utils.html#transformers.EvalPrediction" title="transformers.EvalPrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">EvalPrediction</span></code></a> and return a dictionary string to metric values.</p></li>
<li><p><strong>tb_writer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.summary.SummaryWriter</span></code>, <cite>optional</cite>) – Object to write to TensorBoard.</p></li>
<li><p><strong>optimizers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[tf.keras.optimizers.Optimizer,</span> <span class="pre">tf.keras.optimizers.schedules.LearningRateSchedule]</span></code>, <cite>optional</cite>) – A tuple containing the optimizer and the scheduler to use. The optimizer default to an instance of
<code class="xref py py-class docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">args.weight_decay_rate</span></code> is 0 else an instance of
<a class="reference internal" href="optimizer_schedules.html#transformers.AdamWeightDecay" title="transformers.AdamWeightDecay"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdamWeightDecay</span></code></a>. The scheduler will default to an instance of
<code class="xref py py-class docutils literal notranslate"><span class="pre">tf.keras.optimizers.schedules.PolynomialDecay</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">args.num_warmup_steps</span></code> is 0 else an
instance of <a class="reference internal" href="optimizer_schedules.html#transformers.WarmUp" title="transformers.WarmUp"><code class="xref py py-class docutils literal notranslate"><span class="pre">WarmUp</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TFTrainer.create_optimizer_and_scheduler"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.create_optimizer_and_scheduler"></a>
<code class="sig-name descname">create_optimizer_and_scheduler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_training_steps</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.create_optimizer_and_scheduler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.create_optimizer_and_scheduler" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optimizer and the learning rate scheduler.</p>
<p>We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
TFTrainer’s init through <code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code>, or subclass and override this method.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.evaluate"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.evaluate"></a>
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.data.ops.dataset_ops.DatasetV2<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Run evaluation and returns metrics.</p>
<p>The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init <code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_metrics</span></code> argument).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – Pass a dataset if you wish to override <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.eval_dataset</span></code>. The dataset should yield tuples of
<code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> where <code class="docutils literal notranslate"><span class="pre">features</span></code> is a dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If
<code class="docutils literal notranslate"><span class="pre">labels</span></code> is a tensor, the loss is calculated by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span>
<span class="pre">labels=labels)</span></code>. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict, such as when using a QuestionAnswering head model with
multiple targets, the loss is instead calculated by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.get_eval_tfdataset"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.get_eval_tfdataset"></a>
<code class="sig-name descname">get_eval_tfdataset</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.data.ops.dataset_ops.DatasetV2<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → tensorflow.python.data.ops.dataset_ops.DatasetV2<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.get_eval_tfdataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.get_eval_tfdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the evaluation <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>, <cite>optional</cite>) – If provided, will override <cite>self.eval_dataset</cite>. The dataset should yield tuples of <code class="docutils literal notranslate"><span class="pre">(features,</span>
<span class="pre">labels)</span></code> where <code class="docutils literal notranslate"><span class="pre">features</span></code> is a dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is
a tensor, the loss is calculated by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">labels=labels)</span></code>. If
<code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict, such as when using a QuestionAnswering head model with multiple targets, the loss
is instead calculated by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code>.</p>
</dd>
</dl>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.get_test_tfdataset"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.get_test_tfdataset"></a>
<code class="sig-name descname">get_test_tfdataset</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">test_dataset</span><span class="p">:</span> <span class="n">tensorflow.python.data.ops.dataset_ops.DatasetV2</span></em><span class="sig-paren">)</span> → tensorflow.python.data.ops.dataset_ops.DatasetV2<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.get_test_tfdataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.get_test_tfdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a test <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>test_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>) – The dataset to use. The dataset should yield tuples of <code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> where <code class="docutils literal notranslate"><span class="pre">features</span></code> is a
dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a tensor, the loss is calculated
by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">labels=labels)</span></code>. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict, such as when using
a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code>.</p>
</dd>
</dl>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.get_train_tfdataset"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.get_train_tfdataset"></a>
<code class="sig-name descname">get_train_tfdataset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → tensorflow.python.data.ops.dataset_ops.DatasetV2<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.get_train_tfdataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.get_train_tfdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the training <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.log"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.log"></a>
<code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logs</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log <code class="xref py py-obj docutils literal notranslate"><span class="pre">logs</span></code> on the various objects watching training.</p>
<p>Subclass and override this method to inject custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>logs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>) – The values to log.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.predict"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.predict"></a>
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">test_dataset</span><span class="p">:</span> <span class="n">tensorflow.python.data.ops.dataset_ops.DatasetV2</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.PredictionOutput<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Run prediction and returns predictions and potential metrics.</p>
<p>Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in <a class="reference internal" href="#transformers.TFTrainer.evaluate" title="transformers.TFTrainer.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>test_dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>) – Dataset to run the predictions on. The dataset should yield tuples of <code class="docutils literal notranslate"><span class="pre">(features,</span> <span class="pre">labels)</span></code> where
<code class="docutils literal notranslate"><span class="pre">features</span></code> is a dict of input features and <code class="docutils literal notranslate"><span class="pre">labels</span></code> is the labels. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a tensor, the
loss is calculated by the model by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">labels=labels)</span></code>. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is a dict,
such as when using a QuestionAnswering head model with multiple targets, the loss is instead calculated
by calling <code class="docutils literal notranslate"><span class="pre">model(features,</span> <span class="pre">**labels)</span></code></p>
</dd>
</dl>
<p>Returns: <cite>NamedTuple</cite> A namedtuple with the following keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>predictions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): The predictions on <code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataset</span></code>.</p></li>
<li><p>label_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>, <cite>optional</cite>): The labels (if the dataset contained some).</p></li>
<li><p>metrics (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>, <cite>optional</cite>): The potential dictionary of metrics (if the dataset
contained labels).</p></li>
</ul>
</div></blockquote>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.prediction_loop"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.prediction_loop"></a>
<code class="sig-name descname">prediction_loop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span><span class="p">:</span> <span class="n">tensorflow.python.data.ops.dataset_ops.DatasetV2</span></em>, <em class="sig-param"><span class="n">steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_examples</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">description</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">prediction_loss_only</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> → transformers.trainer_utils.PredictionOutput<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.prediction_loop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.prediction_loop" title="Permalink to this definition">¶</a></dt>
<dd><p>Prediction/evaluation loop, shared by <a class="reference internal" href="#transformers.TFTrainer.evaluate" title="transformers.TFTrainer.evaluate"><code class="xref py py-func docutils literal notranslate"><span class="pre">evaluate()</span></code></a> and
<a class="reference internal" href="#transformers.TFTrainer.predict" title="transformers.TFTrainer.predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">predict()</span></code></a>.</p>
<p>Works both with or without labels.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.prediction_step"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.prediction_step"></a>
<code class="sig-name descname">prediction_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">nb_instances_in_global_batch</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> → tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.prediction_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.prediction_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the prediction on features and update the loss with labels.</p>
<p>Subclass and override to inject some custom behavior.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.run_model"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.run_model"></a>
<code class="sig-name descname">run_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span></em>, <em class="sig-param"><span class="n">labels</span></em>, <em class="sig-param"><span class="n">training</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.run_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.run_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the loss of the given features and labels pair.</p>
<p>Subclass and override this method if you want to inject some custom behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code>) – A batch of input features.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code>) – A batch of labels.</p></li>
<li><p><strong>training</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) – Whether or not to run the model in training mode.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loss and logits.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A tuple of two <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.save_model"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.save_model"></a>
<code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output_dir</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Will save the model, so you can reload it using <code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained()</span></code>.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.setup_comet"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.setup_comet"></a>
<code class="sig-name descname">setup_comet</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.setup_comet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.setup_comet" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optional Comet.ml integration.</p>
<dl class="simple">
<dt>Environment:</dt><dd><dl class="simple">
<dt>COMET_MODE:</dt><dd><p>(Optional): str - “OFFLINE”, “ONLINE”, or “DISABLED”</p>
</dd>
<dt>COMET_PROJECT_NAME:</dt><dd><p>(Optional): str - Comet.ml project name for experiments</p>
</dd>
<dt>COMET_OFFLINE_DIRECTORY:</dt><dd><p>(Optional): str - folder to use for saving offline experiments when <cite>COMET_MODE</cite> is “OFFLINE”</p>
</dd>
</dl>
</dd>
</dl>
<p>For a number of configurable items in the environment, see <a class="reference external" href="https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables">here</a></p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.setup_wandb"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.setup_wandb"></a>
<code class="sig-name descname">setup_wandb</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.setup_wandb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.setup_wandb" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optional Weights &amp; Biases (<cite>wandb</cite>) integration.</p>
<p>One can subclass and override this method to customize the setup if needed. Find more information <a class="reference external" href="https://docs.wandb.com/huggingface">here</a>. You can also override the following environment variables:</p>
<dl class="simple">
<dt>Environment:</dt><dd><dl class="simple">
<dt>WANDB_PROJECT:</dt><dd><p>(Optional): str - “huggingface” by default, set this to a custom string to store results in a different
project.</p>
</dd>
<dt>WANDB_DISABLED:</dt><dd><p>(Optional): boolean - defaults to false, set to “true” to disable wandb entirely.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.train"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.train"></a>
<code class="sig-name descname">train</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train method to train the model.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainer.training_step"><a name="//apple_ref/cpp/Method/transformers.TFTrainer.training_step"></a>
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">features</span></em>, <em class="sig-param"><span class="n">labels</span></em>, <em class="sig-param"><span class="n">nb_instances_in_global_batch</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_tf.html#TFTrainer.training_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainer.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a training step on features and labels.</p>
<p>Subclass and override to inject some custom behavior.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="trainingarguments">
<h2>TrainingArguments<a class="headerlink" href="#trainingarguments" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TrainingArguments"><a name="//apple_ref/cpp/Class/transformers.TrainingArguments"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TrainingArguments</code><span class="sig-paren">(</span><em class="sig-param">output_dir: str</em>, <em class="sig-param">overwrite_output_dir: bool = False</em>, <em class="sig-param">do_train: bool = False</em>, <em class="sig-param">do_eval: Optional[bool] = None</em>, <em class="sig-param">do_predict: bool = False</em>, <em class="sig-param">evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no'</em>, <em class="sig-param">prediction_loss_only: bool = False</em>, <em class="sig-param">per_device_train_batch_size: int = 8</em>, <em class="sig-param">per_device_eval_batch_size: int = 8</em>, <em class="sig-param">per_gpu_train_batch_size: Optional[int] = None</em>, <em class="sig-param">per_gpu_eval_batch_size: Optional[int] = None</em>, <em class="sig-param">gradient_accumulation_steps: int = 1</em>, <em class="sig-param">eval_accumulation_steps: Optional[int] = None</em>, <em class="sig-param">learning_rate: float = 5e-05</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">adam_beta1: float = 0.9</em>, <em class="sig-param">adam_beta2: float = 0.999</em>, <em class="sig-param">adam_epsilon: float = 1e-08</em>, <em class="sig-param">max_grad_norm: float = 1.0</em>, <em class="sig-param">num_train_epochs: float = 3.0</em>, <em class="sig-param">max_steps: int = -1</em>, <em class="sig-param">lr_scheduler_type: transformers.trainer_utils.SchedulerType = 'linear'</em>, <em class="sig-param">warmup_steps: int = 0</em>, <em class="sig-param">logging_dir: Optional[str] = &lt;factory&gt;</em>, <em class="sig-param">logging_first_step: bool = False</em>, <em class="sig-param">logging_steps: int = 500</em>, <em class="sig-param">save_steps: int = 500</em>, <em class="sig-param">save_total_limit: Optional[int] = None</em>, <em class="sig-param">no_cuda: bool = False</em>, <em class="sig-param">seed: int = 42</em>, <em class="sig-param">fp16: bool = False</em>, <em class="sig-param">fp16_opt_level: str = 'O1'</em>, <em class="sig-param">fp16_backend: str = 'auto'</em>, <em class="sig-param">local_rank: int = -1</em>, <em class="sig-param">tpu_num_cores: Optional[int] = None</em>, <em class="sig-param">tpu_metrics_debug: bool = False</em>, <em class="sig-param">debug: bool = False</em>, <em class="sig-param">dataloader_drop_last: bool = False</em>, <em class="sig-param">eval_steps: Optional[int] = None</em>, <em class="sig-param">dataloader_num_workers: int = 0</em>, <em class="sig-param">past_index: int = -1</em>, <em class="sig-param">run_name: Optional[str] = None</em>, <em class="sig-param">disable_tqdm: Optional[bool] = None</em>, <em class="sig-param">remove_unused_columns: Optional[bool] = True</em>, <em class="sig-param">label_names: Optional[List[str]] = None</em>, <em class="sig-param">load_best_model_at_end: Optional[bool] = False</em>, <em class="sig-param">metric_for_best_model: Optional[str] = None</em>, <em class="sig-param">greater_is_better: Optional[bool] = None</em>, <em class="sig-param">ignore_data_skip: bool = False</em>, <em class="sig-param">sharded_ddp: bool = False</em>, <em class="sig-param">deepspeed: Optional[str] = None</em>, <em class="sig-param">label_smoothing_factor: float = 0.0</em>, <em class="sig-param">adafactor: bool = False</em>, <em class="sig-param">group_by_length: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/training_args.html#TrainingArguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainingArguments" title="Permalink to this definition">¶</a></dt>
<dd><p>TrainingArguments is the subset of the arguments we use in our example scripts <strong>which relate to the training loop
itself</strong>.</p>
<p>Using <a class="reference internal" href="../internal/trainer_utils.html#transformers.HfArgumentParser" title="transformers.HfArgumentParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">HfArgumentParser</span></code></a> we can turn this class into <a class="reference external" href="https://docs.python.org/3/library/argparse.html#module-argparse">argparse</a> arguments that can be specified on the command
line.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The output directory where the model predictions and checkpoints will be written.</p></li>
<li><p><strong>overwrite_output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, overwrite the content of the output directory. Use this to continue training if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code> points to a checkpoint directory.</p></li>
<li><p><strong>do_train</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run training or not. This argument is not directly used by <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s
intended to be used by your training/evaluation scripts instead. See the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more details.</p></li>
<li><p><strong>do_eval</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether to run evaluation on the validation set or not. Will be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_strategy</span></code> is different from <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>do_predict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run predictions on the test set or not. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>evaluation_strategy</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">EvaluationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>) – <p>The evaluation strategy to adopt during training. Possible values are:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>: No evaluation is done during training.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"steps"</span></code>: Evaluation is done (and logged) every <code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_steps</span></code>.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"epoch"</span></code>: Evaluation is done at the end of each epoch.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>prediction_loss_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) – When performing evaluation and generating predictions, only returns the loss.</p></li>
<li><p><strong>per_device_train_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for training.</p></li>
<li><p><strong>per_device_eval_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for evaluation.</p></li>
<li><p><strong>gradient_accumulation_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) – <p>Number of updates steps to accumulate the gradients for, before performing a backward/update pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using gradient accumulation, one step is counted as one step with backward pass. Therefore,
logging, evaluation, save will be conducted every <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span> <span class="pre">*</span> <span class="pre">xxx_step</span></code> training
examples.</p>
</div>
</p></li>
<li><p><strong>eval_accumulation_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If
left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but
requires more memory).</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 5e-5) – The initial learning rate for Adam.</p></li>
<li><p><strong>weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) – The weight decay to apply (if not zero).</p></li>
<li><p><strong>adam_beta1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.9) – The beta1 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_beta2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.999) – The beta2 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-8) – The epsilon hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>max_grad_norm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) – Maximum gradient norm (for gradient clipping).</p></li>
<li><p><strong>num_train_epochs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 3.0) – Total number of training epochs to perform (if not an integer, will perform the decimal part percents of
the last epoch before stopping training).</p></li>
<li><p><strong>max_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – If set to a positive number, the total number of training steps to perform. Overrides
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_train_epochs</span></code>.</p></li>
<li><p><strong>lr_scheduler_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="optimizer_schedules.html#transformers.SchedulerType" title="transformers.SchedulerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchedulerType</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"linear"</span></code>) – The scheduler type to use. See the documentation of <a class="reference internal" href="optimizer_schedules.html#transformers.SchedulerType" title="transformers.SchedulerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchedulerType</span></code></a> for all possible
values.</p></li>
<li><p><strong>warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Number of steps used for a linear warmup from 0 to <code class="xref py py-obj docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p></li>
<li><p><strong>logging_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> log directory. Will default to
<cite>runs/**CURRENT_DATETIME_HOSTNAME**</cite>.</p></li>
<li><p><strong>logging_first_step</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to log and evaluate the first <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code> or not.</p></li>
<li><p><strong>logging_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of update steps between two logs.</p></li>
<li><p><strong>save_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of updates steps before two checkpoint saves.</p></li>
<li><p><strong>save_total_limit</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code>.</p></li>
<li><p><strong>no_cuda</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to not use CUDA even when it is available or not.</p></li>
<li><p><strong>seed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 42) – Random seed for initialization.</p></li>
<li><p><strong>fp16</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to use 16-bit (mixed) precision training (through NVIDIA Apex) instead of 32-bit training.</p></li>
<li><p><strong>fp16_opt_level</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to ‘O1’) – For <code class="xref py py-obj docutils literal notranslate"><span class="pre">fp16</span></code> training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details
on the <a class="reference external" href="https://nvidia.github.io/apex/amp.html">Apex documentation</a>.</p></li>
<li><p><strong>fp16_backend</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code>) – The backend to use for mixed precision training. Must be one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"amp"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"apex"</span></code>. <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code> will use AMP or APEX depending on the PyTorch version detected, while the
other choices will force the requested backend.</p></li>
<li><p><strong>local_rank</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – Rank of the process during distributed training.</p></li>
<li><p><strong>tpu_num_cores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – When training on TPU, the number of TPU cores (automatically passed by launcher script).</p></li>
<li><p><strong>debug</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – When training on TPU, whether to print debug metrics or not.</p></li>
<li><p><strong>dataloader_drop_last</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
or not.</p></li>
<li><p><strong>eval_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of update steps between two evaluations if <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_strategy="steps"</span></code>. Will default to the
same value as <code class="xref py py-obj docutils literal notranslate"><span class="pre">logging_steps</span></code> if not set.</p></li>
<li><p><strong>dataloader_num_workers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the
main process.</p></li>
<li><p><strong>past_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – Some models like <a class="reference internal" href="../model_doc/transformerxl.html"><span class="doc">TransformerXL</span></a> or :doc`XLNet &lt;../model_doc/xlnet&gt;` can
make use of the past hidden states for their predictions. If this argument is set to a positive int, the
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will use the corresponding output (usually index 2) as the past state and feed it to the model
at the next training step under the keyword argument <code class="docutils literal notranslate"><span class="pre">mems</span></code>.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – A descriptor for the run. Typically used for <a class="reference external" href="https://www.wandb.com/">wandb</a> logging.</p></li>
<li><p><strong>disable_tqdm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to disable the tqdm progress bars and table of metrics produced by
<code class="xref py py-class docutils literal notranslate"><span class="pre">NotebookTrainingTracker</span></code> in Jupyter Notebooks. Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>
if the logging level is set to warn or lower (default), <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></li>
<li><p><strong>remove_unused_columns</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – <p>If using <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code> datasets, whether or not to automatically remove the columns unused by the
model forward method.</p>
<p>(Note that this behavior is not implemented for <a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a> yet.)</p>
</p></li>
<li><p><strong>label_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) – <p>The list of keys in your dictionary of inputs that correspond to the labels.</p>
<p>Will eventually default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">["labels"]</span></code> except if the model used is one of the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">XxxForQuestionAnswering</span></code> in which case it will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">["start_positions",</span>
<span class="pre">"end_positions"]</span></code>.</p>
</p></li>
<li><p><strong>load_best_model_at_end</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not to load the best model found during training at the end of training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the parameters <code class="xref py py-obj docutils literal notranslate"><span class="pre">save_steps</span></code> will be ignored and the model will be saved
after each evaluation.</p>
</div>
</p></li>
<li><p><strong>metric_for_best_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <p>Use in conjunction with <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end</span></code> to specify the metric to use to compare two different
models. Must be the name of a metric returned by the evaluation with or without the prefix <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_"</span></code>.
Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> if unspecified and <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end=True</span></code> (to use the evaluation
loss).</p>
<p>If you set this value, <code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_is_better</span></code> will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>. Don’t forget to set it to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> if your metric is better when lower.</p>
</p></li>
<li><p><strong>greater_is_better</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – <p>Use in conjunction with <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> to specify if better
models should have a greater metric or not. Will default to:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> is set to a value that isn’t <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_loss"</span></code>.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> is not set, or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_loss"</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>ignore_skip_data</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – When resuming training, whether or not to skip the epochs and batches to get the data loading at the same
stage as in the previous training. If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the training will begin faster (as that skipping
step can take a long time) but will not yield the same results as the interrupted training would have.</p></li>
<li><p><strong>sharded_ddp</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Use Sharded DDP training from <a class="reference external" href="https://github.com/facebookresearch/fairscale">FairScale</a> (in distributed
training only). This is an experimental feature.</p></li>
<li><p><strong>deepspeed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Use <a class="reference external" href="https://github.com/microsoft/deepspeed">Deepspeed</a>. This is an experimental feature and its API may
evolve in the future. The value is the location of its json config file (usually <code class="docutils literal notranslate"><span class="pre">ds_config.json</span></code>).</p></li>
<li><p><strong>label_smoothing_factor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.0) – The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded
labels are changed from 0s and 1s to <code class="xref py py-obj docutils literal notranslate"><span class="pre">label_smoothing_factor/num_labels</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span>
<span class="pre">label_smoothing_factor</span> <span class="pre">+</span> <span class="pre">label_smoothing_factor/num_labels</span></code> respectively.</p></li>
<li><p><strong>adafactor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to use the <a class="reference internal" href="optimizer_schedules.html#transformers.Adafactor" title="transformers.Adafactor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adafactor</span></code></a> optimizer instead of
<a class="reference internal" href="optimizer_schedules.html#transformers.AdamW" title="transformers.AdamW"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdamW</span></code></a>.</p></li>
<li><p><strong>group_by_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to group together samples of roughly the same legnth in the training dataset (to minimize
padding applied and be more efficient). Only useful if applying dynamic padding.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.device"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.device"></a>
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#transformers.TrainingArguments.device" title="Permalink to this definition">¶</a></dt>
<dd><p>The device used by this process.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.eval_batch_size"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.eval_batch_size"></a>
<em class="property">property </em><code class="sig-name descname">eval_batch_size</code><a class="headerlink" href="#transformers.TrainingArguments.eval_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual batch size for evaluation (may differ from <code class="xref py py-obj docutils literal notranslate"><span class="pre">per_gpu_eval_batch_size</span></code> in distributed training).</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.n_gpu"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.n_gpu"></a>
<em class="property">property </em><code class="sig-name descname">n_gpu</code><a class="headerlink" href="#transformers.TrainingArguments.n_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of GPUs used by this process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This will only be greater than one when you have multiple GPUs available but are not using distributed
training. For distributed training, it will always be 1.</p>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.parallel_mode"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.parallel_mode"></a>
<em class="property">property </em><code class="sig-name descname">parallel_mode</code><a class="headerlink" href="#transformers.TrainingArguments.parallel_mode" title="Permalink to this definition">¶</a></dt>
<dd><p>The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ParallelMode.NOT_PARALLEL</span></code>: no parallelism (CPU or one GPU).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ParallelMode.NOT_DISTRIBUTED</span></code>: several GPUs in one single process (uses <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code>).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ParallelMode.DISTRIBUTED</span></code>: several GPUs, each ahving its own process (uses
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.DistributedDataParallel</span></code>).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ParallelMode.TPU</span></code>: several TPU cores.</p></li>
</ul>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.to_dict"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.to_dict"></a>
<code class="sig-name descname">to_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/training_args.html#TrainingArguments.to_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainingArguments.to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Serializes this instance while replace <cite>Enum</cite> by their values (for JSON serialization support).</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.to_json_string"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.to_json_string"></a>
<code class="sig-name descname">to_json_string</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/training_args.html#TrainingArguments.to_json_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainingArguments.to_json_string" title="Permalink to this definition">¶</a></dt>
<dd><p>Serializes this instance to a JSON string.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.to_sanitized_dict"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.to_sanitized_dict"></a>
<code class="sig-name descname">to_sanitized_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../_modules/transformers/training_args.html#TrainingArguments.to_sanitized_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainingArguments.to_sanitized_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Sanitized serialization to use with TensorBoard’s hparams</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainingArguments.train_batch_size"><a name="//apple_ref/cpp/Method/transformers.TrainingArguments.train_batch_size"></a>
<em class="property">property </em><code class="sig-name descname">train_batch_size</code><a class="headerlink" href="#transformers.TrainingArguments.train_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual batch size for training (may differ from <code class="xref py py-obj docutils literal notranslate"><span class="pre">per_gpu_train_batch_size</span></code> in distributed training).</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="seq2seqtrainingarguments">
<h2>Seq2SeqTrainingArguments<a class="headerlink" href="#seq2seqtrainingarguments" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.Seq2SeqTrainingArguments"><a name="//apple_ref/cpp/Class/transformers.Seq2SeqTrainingArguments"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">Seq2SeqTrainingArguments</code><span class="sig-paren">(</span><em class="sig-param">output_dir: str</em>, <em class="sig-param">overwrite_output_dir: bool = False</em>, <em class="sig-param">do_train: bool = False</em>, <em class="sig-param">do_eval: Optional[bool] = None</em>, <em class="sig-param">do_predict: bool = False</em>, <em class="sig-param">evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no'</em>, <em class="sig-param">prediction_loss_only: bool = False</em>, <em class="sig-param">per_device_train_batch_size: int = 8</em>, <em class="sig-param">per_device_eval_batch_size: int = 8</em>, <em class="sig-param">per_gpu_train_batch_size: Optional[int] = None</em>, <em class="sig-param">per_gpu_eval_batch_size: Optional[int] = None</em>, <em class="sig-param">gradient_accumulation_steps: int = 1</em>, <em class="sig-param">eval_accumulation_steps: Optional[int] = None</em>, <em class="sig-param">learning_rate: float = 5e-05</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">adam_beta1: float = 0.9</em>, <em class="sig-param">adam_beta2: float = 0.999</em>, <em class="sig-param">adam_epsilon: float = 1e-08</em>, <em class="sig-param">max_grad_norm: float = 1.0</em>, <em class="sig-param">num_train_epochs: float = 3.0</em>, <em class="sig-param">max_steps: int = -1</em>, <em class="sig-param">lr_scheduler_type: transformers.trainer_utils.SchedulerType = 'linear'</em>, <em class="sig-param">warmup_steps: int = 0</em>, <em class="sig-param">logging_dir: Optional[str] = &lt;factory&gt;</em>, <em class="sig-param">logging_first_step: bool = False</em>, <em class="sig-param">logging_steps: int = 500</em>, <em class="sig-param">save_steps: int = 500</em>, <em class="sig-param">save_total_limit: Optional[int] = None</em>, <em class="sig-param">no_cuda: bool = False</em>, <em class="sig-param">seed: int = 42</em>, <em class="sig-param">fp16: bool = False</em>, <em class="sig-param">fp16_opt_level: str = 'O1'</em>, <em class="sig-param">fp16_backend: str = 'auto'</em>, <em class="sig-param">local_rank: int = -1</em>, <em class="sig-param">tpu_num_cores: Optional[int] = None</em>, <em class="sig-param">tpu_metrics_debug: bool = False</em>, <em class="sig-param">debug: bool = False</em>, <em class="sig-param">dataloader_drop_last: bool = False</em>, <em class="sig-param">eval_steps: Optional[int] = None</em>, <em class="sig-param">dataloader_num_workers: int = 0</em>, <em class="sig-param">past_index: int = -1</em>, <em class="sig-param">run_name: Optional[str] = None</em>, <em class="sig-param">disable_tqdm: Optional[bool] = None</em>, <em class="sig-param">remove_unused_columns: Optional[bool] = True</em>, <em class="sig-param">label_names: Optional[List[str]] = None</em>, <em class="sig-param">load_best_model_at_end: Optional[bool] = False</em>, <em class="sig-param">metric_for_best_model: Optional[str] = None</em>, <em class="sig-param">greater_is_better: Optional[bool] = None</em>, <em class="sig-param">ignore_data_skip: bool = False</em>, <em class="sig-param">sharded_ddp: bool = False</em>, <em class="sig-param">deepspeed: Optional[str] = None</em>, <em class="sig-param">label_smoothing_factor: float = 0.0</em>, <em class="sig-param">adafactor: bool = False</em>, <em class="sig-param">group_by_length: bool = False</em>, <em class="sig-param">sortish_sampler: bool = False</em>, <em class="sig-param">predict_with_generate: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/training_args_seq2seq.html#Seq2SeqTrainingArguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Seq2SeqTrainingArguments" title="Permalink to this definition">¶</a></dt>
<dd><p>TrainingArguments is the subset of the arguments we use in our example scripts <strong>which relate to the training loop
itself</strong>.</p>
<p>Using <a class="reference internal" href="../internal/trainer_utils.html#transformers.HfArgumentParser" title="transformers.HfArgumentParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">HfArgumentParser</span></code></a> we can turn this class into <a class="reference external" href="https://docs.python.org/3/library/argparse.html#module-argparse">argparse</a> arguments that can be specified on the command
line.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The output directory where the model predictions and checkpoints will be written.</p></li>
<li><p><strong>overwrite_output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, overwrite the content of the output directory. Use this to continue training if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code> points to a checkpoint directory.</p></li>
<li><p><strong>do_train</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run training or not. This argument is not directly used by <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s
intended to be used by your training/evaluation scripts instead. See the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more details.</p></li>
<li><p><strong>do_eval</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether to run evaluation on the validation set or not. Will be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_strategy</span></code> is different from <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>do_predict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run predictions on the test set or not. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>evaluation_strategy</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">EvaluationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>) – <p>The evaluation strategy to adopt during training. Possible values are:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>: No evaluation is done during training.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"steps"</span></code>: Evaluation is done (and logged) every <code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_steps</span></code>.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"epoch"</span></code>: Evaluation is done at the end of each epoch.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>prediction_loss_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) – When performing evaluation and generating predictions, only returns the loss.</p></li>
<li><p><strong>per_device_train_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for training.</p></li>
<li><p><strong>per_device_eval_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for evaluation.</p></li>
<li><p><strong>gradient_accumulation_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) – <p>Number of updates steps to accumulate the gradients for, before performing a backward/update pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using gradient accumulation, one step is counted as one step with backward pass. Therefore,
logging, evaluation, save will be conducted every <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span> <span class="pre">*</span> <span class="pre">xxx_step</span></code> training
examples.</p>
</div>
</p></li>
<li><p><strong>eval_accumulation_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If
left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but
requires more memory).</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 5e-5) – The initial learning rate for Adam.</p></li>
<li><p><strong>weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) – The weight decay to apply (if not zero).</p></li>
<li><p><strong>adam_beta1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.9) – The beta1 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_beta2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.999) – The beta2 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-8) – The epsilon hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>max_grad_norm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) – Maximum gradient norm (for gradient clipping).</p></li>
<li><p><strong>num_train_epochs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 3.0) – Total number of training epochs to perform (if not an integer, will perform the decimal part percents of
the last epoch before stopping training).</p></li>
<li><p><strong>max_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – If set to a positive number, the total number of training steps to perform. Overrides
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_train_epochs</span></code>.</p></li>
<li><p><strong>lr_scheduler_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="optimizer_schedules.html#transformers.SchedulerType" title="transformers.SchedulerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchedulerType</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"linear"</span></code>) – The scheduler type to use. See the documentation of <a class="reference internal" href="optimizer_schedules.html#transformers.SchedulerType" title="transformers.SchedulerType"><code class="xref py py-class docutils literal notranslate"><span class="pre">SchedulerType</span></code></a> for all possible
values.</p></li>
<li><p><strong>warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Number of steps used for a linear warmup from 0 to <code class="xref py py-obj docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p></li>
<li><p><strong>logging_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> log directory. Will default to
<cite>runs/**CURRENT_DATETIME_HOSTNAME**</cite>.</p></li>
<li><p><strong>logging_first_step</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to log and evaluate the first <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code> or not.</p></li>
<li><p><strong>logging_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of update steps between two logs.</p></li>
<li><p><strong>save_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of updates steps before two checkpoint saves.</p></li>
<li><p><strong>save_total_limit</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code>.</p></li>
<li><p><strong>no_cuda</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to not use CUDA even when it is available or not.</p></li>
<li><p><strong>seed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 42) – Random seed for initialization.</p></li>
<li><p><strong>fp16</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to use 16-bit (mixed) precision training (through NVIDIA Apex) instead of 32-bit training.</p></li>
<li><p><strong>fp16_opt_level</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to ‘O1’) – For <code class="xref py py-obj docutils literal notranslate"><span class="pre">fp16</span></code> training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details
on the <a class="reference external" href="https://nvidia.github.io/apex/amp.html">Apex documentation</a>.</p></li>
<li><p><strong>fp16_backend</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code>) – The backend to use for mixed precision training. Must be one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"amp"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"apex"</span></code>. <code class="xref py py-obj docutils literal notranslate"><span class="pre">"auto"</span></code> will use AMP or APEX depending on the PyTorch version detected, while the
other choices will force the requested backend.</p></li>
<li><p><strong>local_rank</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – Rank of the process during distributed training.</p></li>
<li><p><strong>tpu_num_cores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – When training on TPU, the number of TPU cores (automatically passed by launcher script).</p></li>
<li><p><strong>debug</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – When training on TPU, whether to print debug metrics or not.</p></li>
<li><p><strong>dataloader_drop_last</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
or not.</p></li>
<li><p><strong>eval_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of update steps between two evaluations if <code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_strategy="steps"</span></code>. Will default to the
same value as <code class="xref py py-obj docutils literal notranslate"><span class="pre">logging_steps</span></code> if not set.</p></li>
<li><p><strong>dataloader_num_workers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the
main process.</p></li>
<li><p><strong>past_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – Some models like <a class="reference internal" href="../model_doc/transformerxl.html"><span class="doc">TransformerXL</span></a> or :doc`XLNet &lt;../model_doc/xlnet&gt;` can
make use of the past hidden states for their predictions. If this argument is set to a positive int, the
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will use the corresponding output (usually index 2) as the past state and feed it to the model
at the next training step under the keyword argument <code class="docutils literal notranslate"><span class="pre">mems</span></code>.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <p>A descriptor for the run. Typically used for <a class="reference external" href="https://www.wandb.com/">wandb</a> logging.</p>
</p></li>
<li><p><strong>disable_tqdm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to disable the tqdm progress bars and table of metrics produced by
<code class="xref py py-class docutils literal notranslate"><span class="pre">NotebookTrainingTracker</span></code> in Jupyter Notebooks. Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>
if the logging level is set to warn or lower (default), <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></li>
<li><p><strong>remove_unused_columns</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – <p>If using <code class="xref py py-obj docutils literal notranslate"><span class="pre">datasets.Dataset</span></code> datasets, whether or not to automatically remove the columns unused by the
model forward method.</p>
<p>(Note that this behavior is not implemented for <a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a> yet.)</p>
</p></li>
<li><p><strong>label_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) – <p>The list of keys in your dictionary of inputs that correspond to the labels.</p>
<p>Will eventually default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">["labels"]</span></code> except if the model used is one of the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">XxxForQuestionAnswering</span></code> in which case it will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">["start_positions",</span>
<span class="pre">"end_positions"]</span></code>.</p>
</p></li>
<li><p><strong>load_best_model_at_end</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not to load the best model found during training at the end of training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the parameters <code class="xref py py-obj docutils literal notranslate"><span class="pre">save_steps</span></code> will be ignored and the model will be saved
after each evaluation.</p>
</div>
</p></li>
<li><p><strong>metric_for_best_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <p>Use in conjunction with <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end</span></code> to specify the metric to use to compare two different
models. Must be the name of a metric returned by the evaluation with or without the prefix <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_"</span></code>.
Will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> if unspecified and <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end=True</span></code> (to use the evaluation
loss).</p>
<p>If you set this value, <code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_is_better</span></code> will default to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>. Don’t forget to set it to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> if your metric is better when lower.</p>
</p></li>
<li><p><strong>greater_is_better</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – <p>Use in conjunction with <code class="xref py py-obj docutils literal notranslate"><span class="pre">load_best_model_at_end</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> to specify if better
models should have a greater metric or not. Will default to:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> is set to a value that isn’t <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_loss"</span></code>.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> if <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> is not set, or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"loss"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"eval_loss"</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>ignore_skip_data</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – When resuming training, whether or not to skip the epochs and batches to get the data loading at the same
stage as in the previous training. If set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the training will begin faster (as that skipping
step can take a long time) but will not yield the same results as the interrupted training would have.</p></li>
<li><p><strong>sharded_ddp</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Use Sharded DDP training from <a class="reference external" href="https://github.com/facebookresearch/fairscale">FairScale</a> (in distributed
training only). This is an experimental feature.</p></li>
<li><p><strong>deepspeed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – Use <a class="reference external" href="https://github.com/microsoft/deepspeed">Deepspeed</a>. This is an experimental feature and its API may
evolve in the future. The value is the location of its json config file (usually <code class="docutils literal notranslate"><span class="pre">ds_config.json</span></code>).</p></li>
<li><p><strong>label_smoothing_factor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.0) – The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded
labels are changed from 0s and 1s to <code class="xref py py-obj docutils literal notranslate"><span class="pre">label_smoothing_factor/num_labels</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span>
<span class="pre">label_smoothing_factor</span> <span class="pre">+</span> <span class="pre">label_smoothing_factor/num_labels</span></code> respectively.</p></li>
<li><p><strong>adafactor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to use the <a class="reference internal" href="optimizer_schedules.html#transformers.Adafactor" title="transformers.Adafactor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adafactor</span></code></a> optimizer instead of
<a class="reference internal" href="optimizer_schedules.html#transformers.AdamW" title="transformers.AdamW"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdamW</span></code></a>.</p></li>
<li><p><strong>group_by_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether or not to group together samples of roughly the same legnth in the training dataset (to minimize
padding applied and be more efficient). Only useful if applying dynamic padding.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>sortish_sampler (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether to use a <cite>sortish sampler</cite> or not. Only possible if the underlying datasets are <cite>Seq2SeqDataset</cite> for
now but will become generally available in the near future.</p>
<p>It sorts the inputs according to lengths in order to minimize the padding size, with a bit of randomness for
the training set.</p>
</dd>
<dt>predict_with_generate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether to use generate to calculate generative metrics (ROUGE, BLEU).</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tftrainingarguments">
<h2>TFTrainingArguments<a class="headerlink" href="#tftrainingarguments" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFTrainingArguments"><a name="//apple_ref/cpp/Class/transformers.TFTrainingArguments"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFTrainingArguments</code><span class="sig-paren">(</span><em class="sig-param">output_dir: str</em>, <em class="sig-param">overwrite_output_dir: bool = False</em>, <em class="sig-param">do_train: bool = False</em>, <em class="sig-param">do_eval: Optional[bool] = None</em>, <em class="sig-param">do_predict: bool = False</em>, <em class="sig-param">evaluation_strategy: transformers.trainer_utils.EvaluationStrategy = 'no'</em>, <em class="sig-param">prediction_loss_only: bool = False</em>, <em class="sig-param">per_device_train_batch_size: int = 8</em>, <em class="sig-param">per_device_eval_batch_size: int = 8</em>, <em class="sig-param">per_gpu_train_batch_size: Optional[int] = None</em>, <em class="sig-param">per_gpu_eval_batch_size: Optional[int] = None</em>, <em class="sig-param">gradient_accumulation_steps: int = 1</em>, <em class="sig-param">eval_accumulation_steps: Optional[int] = None</em>, <em class="sig-param">learning_rate: float = 5e-05</em>, <em class="sig-param">weight_decay: float = 0.0</em>, <em class="sig-param">adam_beta1: float = 0.9</em>, <em class="sig-param">adam_beta2: float = 0.999</em>, <em class="sig-param">adam_epsilon: float = 1e-08</em>, <em class="sig-param">max_grad_norm: float = 1.0</em>, <em class="sig-param">num_train_epochs: float = 3.0</em>, <em class="sig-param">max_steps: int = -1</em>, <em class="sig-param">lr_scheduler_type: transformers.trainer_utils.SchedulerType = 'linear'</em>, <em class="sig-param">warmup_steps: int = 0</em>, <em class="sig-param">logging_dir: Optional[str] = &lt;factory&gt;</em>, <em class="sig-param">logging_first_step: bool = False</em>, <em class="sig-param">logging_steps: int = 500</em>, <em class="sig-param">save_steps: int = 500</em>, <em class="sig-param">save_total_limit: Optional[int] = None</em>, <em class="sig-param">no_cuda: bool = False</em>, <em class="sig-param">seed: int = 42</em>, <em class="sig-param">fp16: bool = False</em>, <em class="sig-param">fp16_opt_level: str = 'O1'</em>, <em class="sig-param">fp16_backend: str = 'auto'</em>, <em class="sig-param">local_rank: int = -1</em>, <em class="sig-param">tpu_num_cores: Optional[int] = None</em>, <em class="sig-param">tpu_metrics_debug: bool = False</em>, <em class="sig-param">debug: bool = False</em>, <em class="sig-param">dataloader_drop_last: bool = False</em>, <em class="sig-param">eval_steps: Optional[int] = None</em>, <em class="sig-param">dataloader_num_workers: int = 0</em>, <em class="sig-param">past_index: int = -1</em>, <em class="sig-param">run_name: Optional[str] = None</em>, <em class="sig-param">disable_tqdm: Optional[bool] = None</em>, <em class="sig-param">remove_unused_columns: Optional[bool] = True</em>, <em class="sig-param">label_names: Optional[List[str]] = None</em>, <em class="sig-param">load_best_model_at_end: Optional[bool] = False</em>, <em class="sig-param">metric_for_best_model: Optional[str] = None</em>, <em class="sig-param">greater_is_better: Optional[bool] = None</em>, <em class="sig-param">ignore_data_skip: bool = False</em>, <em class="sig-param">sharded_ddp: bool = False</em>, <em class="sig-param">deepspeed: Optional[str] = None</em>, <em class="sig-param">label_smoothing_factor: float = 0.0</em>, <em class="sig-param">adafactor: bool = False</em>, <em class="sig-param">group_by_length: bool = False</em>, <em class="sig-param">tpu_name: Optional[str] = None</em>, <em class="sig-param">poly_power: float = 1.0</em>, <em class="sig-param">xla: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/training_args_tf.html#TFTrainingArguments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFTrainingArguments" title="Permalink to this definition">¶</a></dt>
<dd><p>TrainingArguments is the subset of the arguments we use in our example scripts <strong>which relate to the training loop
itself</strong>.</p>
<p>Using <a class="reference internal" href="../internal/trainer_utils.html#transformers.HfArgumentParser" title="transformers.HfArgumentParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">HfArgumentParser</span></code></a> we can turn this class into <a class="reference external" href="https://docs.python.org/3/library/argparse.html#module-argparse">argparse</a> arguments that can be specified on the command
line.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) – The output directory where the model predictions and checkpoints will be written.</p></li>
<li><p><strong>overwrite_output_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, overwrite the content of the output directory. Use this to continue training if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code> points to a checkpoint directory.</p></li>
<li><p><strong>do_train</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run training or not. This argument is not directly used by <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s
intended to be used by your training/evaluation scripts instead. See the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more details.</p></li>
<li><p><strong>do_eval</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether to run evaluation on the validation set or not. Will be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluation_strategy</span></code> is different from <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>do_predict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to run predictions on the test set or not. This argument is not directly used by
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, it’s intended to be used by your training/evaluation scripts instead. See
the <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples">example scripts</a> for more
details.</p></li>
<li><p><strong>evaluation_strategy</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-class docutils literal notranslate"><span class="pre">EvaluationStrategy</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>) – <p>The evaluation strategy to adopt during training. Possible values are:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"no"</span></code>: No evaluation is done during training.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"steps"</span></code>: Evaluation is done (and logged) every <code class="xref py py-obj docutils literal notranslate"><span class="pre">eval_steps</span></code>.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"epoch"</span></code>: Evaluation is done at the end of each epoch.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>per_device_train_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for training.</p></li>
<li><p><strong>per_device_eval_batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – The batch size per GPU/TPU core/CPU for evaluation.</p></li>
<li><p><strong>gradient_accumulation_steps</strong> – <p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1):
Number of updates steps to accumulate the gradients for, before performing a backward/update pass.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using gradient accumulation, one step is counted as one step with backward pass. Therefore,
logging, evaluation, save will be conducted every <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span> <span class="pre">*</span> <span class="pre">xxx_step</span></code> training
examples.</p>
</div>
</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 5e-5) – The initial learning rate for Adam.</p></li>
<li><p><strong>weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) – The weight decay to apply (if not zero).</p></li>
<li><p><strong>adam_beta1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.9) – The beta1 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_beta2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.999) – The beta2 hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>adam_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-8) – The epsilon hyperparameter for the Adam optimizer.</p></li>
<li><p><strong>max_grad_norm</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) – Maximum gradient norm (for gradient clipping).</p></li>
<li><p><strong>num_train_epochs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 3.0) – Total number of training epochs to perform.</p></li>
<li><p><strong>max_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – If set to a positive number, the total number of training steps to perform. Overrides
<code class="xref py py-obj docutils literal notranslate"><span class="pre">num_train_epochs</span></code>.</p></li>
<li><p><strong>warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Number of steps used for a linear warmup from 0 to <code class="xref py py-obj docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p></li>
<li><p><strong>logging_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – <a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> log directory. Will default to
<cite>runs/**CURRENT_DATETIME_HOSTNAME**</cite>.</p></li>
<li><p><strong>logging_first_step</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to log and evaluate the first <code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code> or not.</p></li>
<li><p><strong>logging_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of update steps between two logs.</p></li>
<li><p><strong>save_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 500) – Number of updates steps before two checkpoint saves.</p></li>
<li><p><strong>save_total_limit</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dir</span></code>.</p></li>
<li><p><strong>no_cuda</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to not use CUDA even when it is available or not.</p></li>
<li><p><strong>seed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 42) – Random seed for initialization.</p></li>
<li><p><strong>fp16</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to use 16-bit (mixed) precision training (through NVIDIA Apex) instead of 32-bit training.</p></li>
<li><p><strong>fp16_opt_level</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to ‘O1’) – For <code class="xref py py-obj docutils literal notranslate"><span class="pre">fp16</span></code> training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details
on the <a class="reference external" href="https://nvidia.github.io/apex/amp.html">Apex documentation</a>.</p></li>
<li><p><strong>local_rank</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – During distributed training, the rank of the process.</p></li>
<li><p><strong>tpu_num_cores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – When training on TPU, the number of TPU cores (automatically passed by launcher script).</p></li>
<li><p><strong>debug</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to activate the trace to record computation graphs and profiling information or not.</p></li>
<li><p><strong>dataloader_drop_last</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
or not.</p></li>
<li><p><strong>eval_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1000) – Number of update steps before two evaluations.</p></li>
<li><p><strong>past_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) – Some models like <a class="reference internal" href="../model_doc/transformerxl.html"><span class="doc">TransformerXL</span></a> or :doc`XLNet &lt;../model_doc/xlnet&gt;` can
make use of the past hidden states for their predictions. If this argument is set to a positive int, the
<code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will use the corresponding output (usually index 2) as the past state and feed it to the model
at the next training step under the keyword argument <code class="docutils literal notranslate"><span class="pre">mems</span></code>.</p></li>
<li><p><strong>tpu_name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – The name of the TPU the process is running on.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – A descriptor for the run. Notably used for wandb logging.</p></li>
<li><p><strong>xla</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether to activate the XLA compilation or not.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TFTrainingArguments.eval_batch_size"><a name="//apple_ref/cpp/Method/transformers.TFTrainingArguments.eval_batch_size"></a>
<em class="property">property </em><code class="sig-name descname">eval_batch_size</code><a class="headerlink" href="#transformers.TFTrainingArguments.eval_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual batch size for evaluation (may differ from <code class="xref py py-obj docutils literal notranslate"><span class="pre">per_gpu_eval_batch_size</span></code> in distributed training).</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainingArguments.n_gpu"><a name="//apple_ref/cpp/Method/transformers.TFTrainingArguments.n_gpu"></a>
<em class="property">property </em><code class="sig-name descname">n_gpu</code><a class="headerlink" href="#transformers.TFTrainingArguments.n_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of replicas (CPUs, GPUs or TPU cores) used in this training.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainingArguments.n_replicas"><a name="//apple_ref/cpp/Method/transformers.TFTrainingArguments.n_replicas"></a>
<em class="property">property </em><code class="sig-name descname">n_replicas</code><a class="headerlink" href="#transformers.TFTrainingArguments.n_replicas" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of replicas (CPUs, GPUs or TPU cores) used in this training.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainingArguments.strategy"><a name="//apple_ref/cpp/Method/transformers.TFTrainingArguments.strategy"></a>
<em class="property">property </em><code class="sig-name descname">strategy</code><a class="headerlink" href="#transformers.TFTrainingArguments.strategy" title="Permalink to this definition">¶</a></dt>
<dd><p>The strategy used for distributed training.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFTrainingArguments.train_batch_size"><a name="//apple_ref/cpp/Method/transformers.TFTrainingArguments.train_batch_size"></a>
<em class="property">property </em><code class="sig-name descname">train_batch_size</code><a class="headerlink" href="#transformers.TFTrainingArguments.train_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual batch size for training (may differ from <code class="xref py py-obj docutils literal notranslate"><span class="pre">per_gpu_train_batch_size</span></code> in distributed training).</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="trainer-integrations">
<h2>Trainer Integrations<a class="headerlink" href="#trainer-integrations" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> has been extended to support libraries that may dramatically improve your training
time and fit much bigger models.</p>
<p>Currently it supports third party solutions, <a class="reference external" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> and <a class="reference external" href="https://github.com/facebookresearch/fairscale/">FairScale</a>, which implement parts of the paper <a class="reference external" href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations
Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He</a>.</p>
<p>This provided support is new and experimental as of this writing.</p>
<div class="section" id="installation-notes">
<h3>Installation Notes<a class="headerlink" href="#installation-notes" title="Permalink to this headline">¶</a></h3>
<p>As of this writing, both FairScale and Deepspeed require compilation of CUDA C++ code, before they can be used.</p>
<p>While all installation issues should be dealt with through the corresponding GitHub Issues of <a class="reference external" href="https://github.com/facebookresearch/fairscale/issues">FairScale</a> and <a class="reference external" href="https://github.com/microsoft/DeepSpeed/issues">Deepspeed</a>, there are a few common issues that one may encounter while building
any PyTorch extension that needs to build CUDA extensions.</p>
<p>Therefore, if you encounter a CUDA-related build issue while doing one of the following or both:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install fairscale
pip install deepspeed
</pre></div>
</div>
<p>please, read the following notes first.</p>
<p>In these notes we give examples for what to do when <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> has been built with CUDA <code class="docutils literal notranslate"><span class="pre">10.2</span></code>. If your situation is
different remember to adjust the version number to the one you are after.</p>
<p><strong>Possible problem #1:</strong></p>
<p>While, Pytorch comes with its own CUDA toolkit, to build these two projects you must have an identical version of CUDA
installed system-wide.</p>
<p>For example, if you installed <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> with <code class="docutils literal notranslate"><span class="pre">cudatoolkit==10.2</span></code> in the Python environment, you also need to have
CUDA <code class="docutils literal notranslate"><span class="pre">10.2</span></code> installed system-wide.</p>
<p>The exact location may vary from system to system, but <code class="docutils literal notranslate"><span class="pre">/usr/local/cuda-10.2</span></code> is the most common location on many
Unix systems. When CUDA is correctly set up and added to the <code class="docutils literal notranslate"><span class="pre">PATH</span></code> environment variable, one can find the
installation location by doing:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>which nvcc
</pre></div>
</div>
<p>If you don’t have CUDA installed system-wide, install it first. You will find the instructions by using your favorite
search engine. For example, if you’re on Ubuntu you may want to search for: <a class="reference external" href="https://www.google.com/search?q=ubuntu+cuda+10.2+install">ubuntu cuda 10.2 install</a>.</p>
<p><strong>Possible problem #2:</strong></p>
<p>Another possible common problem is that you may have more than one CUDA toolkit installed system-wide. For example you
may have:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/local/cuda-10.2
/usr/local/cuda-11.0
</pre></div>
</div>
<p>Now, in this situation you need to make sure that your <code class="docutils literal notranslate"><span class="pre">PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variables contain
the correct paths to the desired CUDA version. Typically, package installers will set these to contain whatever the
last version was installed. If you encounter the problem, where the package build fails because it can’t find the right
CUDA version despite you having it installed system-wide, it means that you need to adjust the 2 aforementioned
environment variables.</p>
<p>First, you may look at their contents:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span> <span class="nv">$PATH</span>
<span class="nb">echo</span> <span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>so you get an idea of what is inside.</p>
<p>It’s possible that <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> is empty.</p>
<p><code class="docutils literal notranslate"><span class="pre">PATH</span></code> lists the locations of where executables can be found and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> is for where shared libraries
are to looked for. In both cases, earlier entries have priority over the later ones. <code class="docutils literal notranslate"><span class="pre">:</span></code> is used to separate multiple
entries.</p>
<p>Now, to tell the build program where to find the specific CUDA toolkit, insert the desired paths to be listed first by
doing:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda-10.2/bin:<span class="nv">$PATH</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-10.2/lib64:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>Note that we aren’t overwriting the existing values, but prepending instead.</p>
<p>Of course, adjust the version number, the full path if need be. Check that the directories you assign actually do
exist. <code class="docutils literal notranslate"><span class="pre">lib64</span></code> sub-directory is where the various CUDA <code class="docutils literal notranslate"><span class="pre">.so</span></code> objects, like <code class="docutils literal notranslate"><span class="pre">libcudart.so</span></code> reside, it’s unlikely
that your system will have it named differently, but if it is adjust it to reflect your reality.</p>
<p><strong>Possible problem #3:</strong></p>
<p>Some older CUDA versions may refuse to build with newer compilers. For example, you my have <code class="docutils literal notranslate"><span class="pre">gcc-9</span></code> but it wants
<code class="docutils literal notranslate"><span class="pre">gcc-7</span></code>.</p>
<p>There are various ways to go about it.</p>
<p>If you can install the latest CUDA toolkit it typically should support the newer compiler.</p>
<p>Alternatively, you could install the lower version of the compiler in addition to the one you already have, or you may
already have it but it’s not the default one, so the build system can’t see it. If you have <code class="docutils literal notranslate"><span class="pre">gcc-7</span></code> installed but the
build system complains it can’t find it, the following might do the trick:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc
sudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++
</pre></div>
</div>
<p>Here, we are making a symlink to <code class="docutils literal notranslate"><span class="pre">gcc-7</span></code> from <code class="docutils literal notranslate"><span class="pre">/usr/local/cuda-10.2/bin/gcc</span></code> and since
<code class="docutils literal notranslate"><span class="pre">/usr/local/cuda-10.2/bin/</span></code> should be in the <code class="docutils literal notranslate"><span class="pre">PATH</span></code> environment variable (see the previous problem’s solution), it
should find <code class="docutils literal notranslate"><span class="pre">gcc-7</span></code> (and <code class="docutils literal notranslate"><span class="pre">g++7</span></code>) and then the build will succeed.</p>
<p>As always make sure to edit the paths in the example to match your situation.</p>
<p><strong>If still unsuccessful:</strong></p>
<p>If after addressing these you still encounter build issues, please, proceed with the GitHub Issue of <a class="reference external" href="https://github.com/facebookresearch/fairscale/issues">FairScale</a> and <a class="reference external" href="https://github.com/microsoft/DeepSpeed/issues">Deepspeed</a>, depending on the project you have the problem with.</p>
</div>
<div class="section" id="fairscale">
<h3>FairScale<a class="headerlink" href="#fairscale" title="Permalink to this headline">¶</a></h3>
<p>By integrating <a class="reference external" href="https://github.com/facebookresearch/fairscale/">FairScale</a> the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>
provides support for the following features from <a class="reference external" href="https://arxiv.org/abs/1910.02054">the ZeRO paper</a>:</p>
<ol class="arabic simple">
<li><p>Optimizer State Sharding</p></li>
<li><p>Gradient Sharding</p></li>
</ol>
<p>You will need at least two GPUs to use this feature.</p>
<p>To deploy this feature:</p>
<ol class="arabic">
<li><p>Install the library via pypi:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install fairscale
</pre></div>
</div>
<p>or find more details on <a class="reference external" href="https://github.com/facebookresearch/fairscale/#installation">the FairScale’s GitHub page</a>.</p>
</li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">--sharded_ddp</span></code> to the command line arguments, and make sure you have added the distributed launcher <code class="docutils literal notranslate"><span class="pre">-m</span>
<span class="pre">torch.distributed.launch</span> <span class="pre">--nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE</span></code> if you haven’t been using it already.</p></li>
</ol>
<p>For example here is how you could use it for <code class="docutils literal notranslate"><span class="pre">finetune_trainer.py</span></code> with 2 GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/seq2seq
python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">2</span> ./finetune_trainer.py <span class="se">\</span>
--model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro <span class="se">\</span>
--output_dir output_dir --overwrite_output_dir <span class="se">\</span>
--do_train --n_train <span class="m">500</span> --num_train_epochs <span class="m">1</span> <span class="se">\</span>
--per_device_train_batch_size <span class="m">1</span>  --freeze_embeds <span class="se">\</span>
--src_lang en_XX --tgt_lang ro_RO --task translation <span class="se">\</span>
--fp16 --sharded_ddp
</pre></div>
</div>
<p>Notes:</p>
<ul class="simple">
<li><p>This feature requires distributed training (so multiple GPUs).</p></li>
<li><p>It is not implemented for TPUs.</p></li>
<li><p>It works with <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> too, to make things even faster.</p></li>
<li><p>One of the main benefits of enabling <code class="docutils literal notranslate"><span class="pre">--sharded_ddp</span></code> is that it uses a lot less GPU memory, so you should be able
to use significantly larger batch sizes using the same hardware (e.g. 3x and even bigger) which should lead to
significantly shorter training time.</p></li>
</ul>
</div>
<div class="section" id="deepspeed">
<h3>DeepSpeed<a class="headerlink" href="#deepspeed" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> implements everything described in the <a class="reference external" href="https://arxiv.org/abs/1910.02054">ZeRO paper</a>, except ZeRO’s stage 3. “Parameter Partitioning (Pos+g+p)”. Currently it provides
full support for:</p>
<ol class="arabic simple">
<li><p>Optimizer State Partitioning (ZeRO stage 1)</p></li>
<li><p>Add Gradient Partitioning (ZeRO stage 2)</p></li>
</ol>
<div class="section" id="installation">
<h4>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h4>
<p>Install the library via pypi:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install deepspeed
</pre></div>
</div>
<p>or find more details on <a class="reference external" href="https://github.com/microsoft/deepspeed#installation">the DeepSpeed’s GitHub page</a>.</p>
</div>
<div class="section" id="deployment-with-multiple-gpus">
<h4>Deployment with multiple GPUs<a class="headerlink" href="#deployment-with-multiple-gpus" title="Permalink to this headline">¶</a></h4>
<p>To deploy this feature with multiple GPUs adjust the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> command line arguments as
following:</p>
<ol class="arabic simple">
<li><p>replace <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">torch.distributed.launch</span></code> with <code class="docutils literal notranslate"><span class="pre">deepspeed</span></code>.</p></li>
<li><p>add a new argument <code class="docutils literal notranslate"><span class="pre">--deepspeed</span> <span class="pre">ds_config.json</span></code>, where <code class="docutils literal notranslate"><span class="pre">ds_config.json</span></code> is the DeepSpeed configuration file as
documented <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/">here</a>. The file naming is up to you.</p></li>
</ol>
<p>Therefore, if your original command line looked as following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">2</span> your_program.py &lt;normal cl args&gt;
</pre></div>
</div>
<p>Now it should be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>deepspeed --num_gpus<span class="o">=</span><span class="m">2</span> your_program.py &lt;normal cl args&gt; --deepspeed ds_config.json
</pre></div>
</div>
<p>Unlike, <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> where you have to specify how many GPUs to use with <code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code>, with the
<code class="docutils literal notranslate"><span class="pre">deepspeed</span></code> launcher you don’t have to use the corresponding <code class="docutils literal notranslate"><span class="pre">--num_gpus</span></code> if you want all of your GPUs used. The
full details on how to configure various nodes and GPUs can be found <a class="reference external" href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">here</a>.</p>
<p>Here is an example of running <code class="docutils literal notranslate"><span class="pre">finetune_trainer.py</span></code> under DeepSpeed deploying all available GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/seq2seq
deepspeed ./finetune_trainer.py --deepspeed ds_config.json <span class="se">\</span>
--model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro <span class="se">\</span>
--output_dir output_dir --overwrite_output_dir <span class="se">\</span>
--do_train --n_train <span class="m">500</span> --num_train_epochs <span class="m">1</span> <span class="se">\</span>
--per_device_train_batch_size <span class="m">1</span>  --freeze_embeds <span class="se">\</span>
--src_lang en_XX --tgt_lang ro_RO --task translation
</pre></div>
</div>
<p>Note that in the DeepSpeed documentation you are likely to see <code class="docutils literal notranslate"><span class="pre">--deepspeed</span> <span class="pre">--deepspeed_config</span> <span class="pre">ds_config.json</span></code> - i.e.
two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal
with, we combined the two into a single argument.</p>
<p>For some practical usage examples, please, see this <a class="reference external" href="https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400">post</a>.</p>
</div>
<div class="section" id="deployment-with-one-gpu">
<h4>Deployment with one GPU<a class="headerlink" href="#deployment-with-one-gpu" title="Permalink to this headline">¶</a></h4>
<p>To deploy DeepSpeed with one GPU adjust the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> command line arguments as following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/seq2seq
deepspeed --num_gpus<span class="o">=</span><span class="m">1</span> ./finetune_trainer.py --deepspeed ds_config.json <span class="se">\</span>
--model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro <span class="se">\</span>
--output_dir output_dir --overwrite_output_dir <span class="se">\</span>
--do_train --n_train <span class="m">500</span> --num_train_epochs <span class="m">1</span> <span class="se">\</span>
--per_device_train_batch_size <span class="m">1</span>  --freeze_embeds <span class="se">\</span>
--src_lang en_XX --tgt_lang ro_RO --task translation
</pre></div>
</div>
<p>This is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU. By default,
DeepSpeed deploys all GPUs it can see. If you have only 1 GPU to start with, then you don’t need this argument. The
following <a class="reference external" href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">documentation</a> discusses the
launcher options.</p>
<p>Why would you want to use DeepSpeed with just one GPU?</p>
<ol class="arabic simple">
<li><p>It has a ZeRO-offload feature which can delegate some computations and memory to the host’s CPU and RAM, and thus
leave more GPU resources for model’s needs - e.g. larger batch size, or enabling a fitting of a very big model which
normally won’t fit.</p></li>
<li><p>It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit
bigger models and data batches.</p></li>
</ol>
<p>While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU
with DeepSpeed is to have at least the following configuration in the configuration file:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="nt">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
     <span class="nt">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
     <span class="nt">"allgather_partitions"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
     <span class="nt">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>
     <span class="nt">"reduce_scatter"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
     <span class="nt">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>
     <span class="nt">"overlap_comm"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
     <span class="nt">"contiguous_gradients"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
     <span class="nt">"cpu_offload"</span><span class="p">:</span> <span class="kc">true</span>
  <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<p>which enables <code class="docutils literal notranslate"><span class="pre">cpu_offload</span></code> and some other important features. You may experiment with the buffer sizes, you will
find more details in the discussion below.</p>
<p>For a practical usage example of this type of deployment, please, see this <a class="reference external" href="https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685">post</a>.</p>
</div>
<div class="section" id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h4>
<p>For the complete guide to the DeepSpeed configuration options that can be used in its configuration file please refer
to the <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/">following documentation</a>.</p>
<p>While you always have to supply the DeepSpeed configuration file, you can configure the DeepSpeed integration in
several ways:</p>
<ol class="arabic simple">
<li><p>Supply most of the configuration inside the file, and just use a few required command line arguments. This is the
recommended way as it puts most of the configuration params in one place.</p></li>
<li><p>Supply just the ZeRO configuration params inside the file, and configure the rest using the normal
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> command line arguments.</p></li>
<li><p>Any variation of the first two ways.</p></li>
</ol>
<p>To get an idea of what DeepSpeed configuration file looks like, here is one that activates ZeRO stage 2 features,
enables FP16, uses AdamW optimizer and WarmupLR scheduler:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">"fp16"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"enabled"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
        <span class="nt">"loss_scale"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="nt">"loss_scale_window"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="nt">"hysteresis"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="nt">"min_loss_scale"</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">},</span>

   <span class="nt">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
       <span class="nt">"allgather_partitions"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"overlap_comm"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_scatter"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"contiguous_gradients"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"cpu_offload"</span><span class="p">:</span> <span class="kc">true</span>
   <span class="p">},</span>

   <span class="nt">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
     <span class="nt">"type"</span><span class="p">:</span> <span class="s2">"AdamW"</span><span class="p">,</span>
     <span class="nt">"params"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"lr"</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
       <span class="nt">"betas"</span><span class="p">:</span> <span class="p">[</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.999</span> <span class="p">],</span>
       <span class="nt">"eps"</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span>
       <span class="nt">"weight_decay"</span><span class="p">:</span> <span class="mf">3e-7</span>
     <span class="p">}</span>
   <span class="p">},</span>
   <span class="nt">"zero_allow_untested_optimizer"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>

   <span class="nt">"scheduler"</span><span class="p">:</span> <span class="p">{</span>
     <span class="nt">"type"</span><span class="p">:</span> <span class="s2">"WarmupLR"</span><span class="p">,</span>
     <span class="nt">"params"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"warmup_min_lr"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
       <span class="nt">"warmup_max_lr"</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
       <span class="nt">"warmup_num_steps"</span><span class="p">:</span> <span class="mi">500</span>
     <span class="p">}</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If you already have a command line that you have been using with <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.Trainer</span></code></a> args, you can continue
using those and the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> will automatically convert them into the corresponding DeepSpeed
configuration at run time. For example, you could use the following configuration file:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="nt">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
       <span class="nt">"allgather_partitions"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"overlap_comm"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_scatter"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"contiguous_gradients"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"cpu_offload"</span><span class="p">:</span> <span class="kc">true</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>and the following command line arguments:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--learning_rate 3e-5 --warmup_steps <span class="m">500</span> --adam_beta1 <span class="m">0</span>.8 --adam_beta2 <span class="m">0</span>.999 --adam_epsilon 1e-8 <span class="se">\</span>
--weight_decay 3e-7 --lr_scheduler_type constant_with_warmup --fp16 --fp16_backend amp
</pre></div>
</div>
<p>to achieve the same configuration as provided by the longer json file in the first example.</p>
<p>When you execute the program, DeepSpeed will log the configuration it received from the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>
to the console, so you can see exactly what the final configuration was passed to it.</p>
</div>
<div class="section" id="shared-configuration">
<h4>Shared Configuration<a class="headerlink" href="#shared-configuration" title="Permalink to this headline">¶</a></h4>
<p>Some configuration information is required by both the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and DeepSpeed to function
correctly, therefore, to prevent conflicting definitions, which could lead to hard to detect errors, we chose to
configure those via the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> command line arguments.</p>
<p>Therefore, the following DeepSpeed configuration params shouldn’t be used with the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_batch_size</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_micro_batch_size_per_gpu</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span></code></p></li>
</ul>
<p>as these will be automatically derived from the run time environment and the following 2 command line arguments:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--per_device_train_batch_size <span class="m">8</span> --gradient_accumulation_steps <span class="m">2</span>
</pre></div>
</div>
<p>which are always required to be supplied.</p>
<p>Of course, you will need to adjust the values in this example to your situation.</p>
</div>
<div class="section" id="zero">
<h4>ZeRO<a class="headerlink" href="#zero" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code> section of the configuration file is the most important part (<a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training">docs</a>), since that is where you define
which ZeRO stages you want to enable and how to configure them.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="nt">"zero_optimization"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"stage"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
       <span class="nt">"allgather_partitions"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"allgather_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"overlap_comm"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_scatter"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"reduce_bucket_size"</span><span class="p">:</span> <span class="mf">5e8</span><span class="p">,</span>
       <span class="nt">"contiguous_gradients"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
       <span class="nt">"cpu_offload"</span><span class="p">:</span> <span class="kc">true</span>
   <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Notes:</p>
<ul class="simple">
<li><p>enabling <code class="docutils literal notranslate"><span class="pre">cpu_offload</span></code> should reduce GPU RAM usage (it requires <code class="docutils literal notranslate"><span class="pre">"stage":</span> <span class="pre">2</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"overlap_comm":</span> <span class="pre">true</span></code> trades off increased GPU RAM usage to lower all-reduce latency. <code class="docutils literal notranslate"><span class="pre">overlap_comm</span></code> uses 4.5x
the <code class="docutils literal notranslate"><span class="pre">allgather_bucket_size</span></code> and <code class="docutils literal notranslate"><span class="pre">reduce_bucket_size</span></code> values. So if they are set to 5e8, this requires a 9GB
footprint (<code class="docutils literal notranslate"><span class="pre">5e8</span> <span class="pre">x</span> <span class="pre">2Bytes</span> <span class="pre">x</span> <span class="pre">2</span> <span class="pre">x</span> <span class="pre">4.5</span></code>). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting
OOM-errors you will need to reduce those parameters to about <code class="docutils literal notranslate"><span class="pre">2e8</span></code>, which would require 3.6GB.</p></li>
</ul>
<p>This section has to be configured exclusively via DeepSpeed configuration - the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> provides
no equivalent command line arguments.</p>
</div>
<div class="section" id="optimizer">
<h4>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline">¶</a></h4>
<p>DeepSpeed’s main optimizers are Adam, OneBitAdam, and Lamb. These have been thoroughly tested with ZeRO and are thus
recommended to be used. It, however, can import other optimizers from <code class="docutils literal notranslate"><span class="pre">torch</span></code>. The full documentation is <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#optimizer-parameters">here</a>.</p>
<p>If you don’t configure the <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> entry in the configuration file, the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> will
automatically set it to <code class="docutils literal notranslate"><span class="pre">AdamW</span></code> and will use the supplied values or the defaults for the following command line
arguments: <code class="docutils literal notranslate"><span class="pre">--learning_rate</span></code>, <code class="docutils literal notranslate"><span class="pre">--adam_beta1</span></code>, <code class="docutils literal notranslate"><span class="pre">--adam_beta2</span></code>, <code class="docutils literal notranslate"><span class="pre">--adam_epsilon</span></code> and <code class="docutils literal notranslate"><span class="pre">--weight_decay</span></code>.</p>
<p>Here is an example of the pre-configured <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> entry for AdamW:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="nt">"zero_allow_untested_optimizer"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
   <span class="nt">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
       <span class="nt">"type"</span><span class="p">:</span> <span class="s2">"AdamW"</span><span class="p">,</span>
       <span class="nt">"params"</span><span class="p">:</span> <span class="p">{</span>
         <span class="nt">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
         <span class="nt">"betas"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">],</span>
         <span class="nt">"eps"</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span>
         <span class="nt">"weight_decay"</span><span class="p">:</span> <span class="mf">3e-7</span>
       <span class="p">}</span>
     <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Since AdamW isn’t on the list of tested with DeepSpeed/ZeRO optimizers, we have to add
<code class="docutils literal notranslate"><span class="pre">zero_allow_untested_optimizer</span></code> flag.</p>
<p>If you want to use one of the officially supported optimizers, configure them explicitly in the configuration file, and
make sure to adjust the values. e.g. if use Adam you will want <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> around <code class="docutils literal notranslate"><span class="pre">0.01</span></code>.</p>
</div>
<div class="section" id="scheduler">
<h4>Scheduler<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h4>
<p>DeepSpeed supports LRRangeTest, OneCycle, WarmupLR and WarmupDecayLR LR schedulers. The full documentation is <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#scheduler-parameters">here</a>.</p>
<p>If you don’t configure the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> entry in the configuration file, the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> will use
the value of <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span></code> to configure it. Currently the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> supports only 2 LR
schedulers that are also supported by DeepSpeed:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">WarmupLR</span></code> via <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span> <span class="pre">constant_with_warmup</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WarmupDecayLR</span></code> via <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span> <span class="pre">linear</span></code>. This is also the default value for <code class="docutils literal notranslate"><span class="pre">--lr_scheduler_type</span></code>,
therefore, if you don’t configure the scheduler this is scheduler that will get configured by default.</p></li>
</ul>
<p>In either case, the values of <code class="docutils literal notranslate"><span class="pre">--learning_rate</span></code> and <code class="docutils literal notranslate"><span class="pre">--warmup_steps</span></code> will be used for the configuration.</p>
<p>In other words, if you don’t use the configuration file to set the <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> entry, provide either:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--lr_scheduler_type constant_with_warmup --learning_rate 3e-5 --warmup_steps <span class="m">500</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--lr_scheduler_type linear --learning_rate 3e-5 --warmup_steps <span class="m">500</span>
</pre></div>
</div>
<p>with the desired values. If you don’t pass these arguments, reasonable default values will be used instead.</p>
<p>In the case of WarmupDecayLR <code class="docutils literal notranslate"><span class="pre">total_num_steps</span></code> gets set either via the <code class="docutils literal notranslate"><span class="pre">--max_steps</span></code> command line argument, or if
it is not provided, derived automatically at run time based on the environment and the size of the dataset and other
command line arguments.</p>
<p>Here is an example of the pre-configured <code class="docutils literal notranslate"><span class="pre">scheduler</span></code> entry for WarmupLR (<code class="docutils literal notranslate"><span class="pre">constant_with_warmup</span></code> in the
<a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> API):</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="nt">"scheduler"</span><span class="p">:</span> <span class="p">{</span>
         <span class="nt">"type"</span><span class="p">:</span> <span class="s2">"WarmupLR"</span><span class="p">,</span>
         <span class="nt">"params"</span><span class="p">:</span> <span class="p">{</span>
             <span class="nt">"warmup_min_lr"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
             <span class="nt">"warmup_max_lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
             <span class="nt">"warmup_num_steps"</span><span class="p">:</span> <span class="mi">1000</span>
         <span class="p">}</span>
     <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="automatic-mixed-precision">
<h4>Automatic Mixed Precision<a class="headerlink" href="#automatic-mixed-precision" title="Permalink to this headline">¶</a></h4>
<p>You can work with FP16 in one of the following ways:</p>
<ol class="arabic simple">
<li><p>Pytorch native amp, as documented <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#fp16-training-options">here</a>.</p></li>
<li><p>NVIDIA’s apex, as documented <a class="reference external" href="https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options">here</a>.</p></li>
</ol>
<p>If you want to use an equivalent of the Pytorch native amp, you can either configure the <code class="docutils literal notranslate"><span class="pre">fp16</span></code> entry in the
configuration file, or use the following command line arguments: <code class="docutils literal notranslate"><span class="pre">--fp16</span> <span class="pre">--fp16_backend</span> <span class="pre">amp</span></code>.</p>
<p>Here is an example of the <code class="docutils literal notranslate"><span class="pre">fp16</span></code> configuration:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">"fp16"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"enabled"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
        <span class="nt">"loss_scale"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="nt">"loss_scale_window"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="nt">"hysteresis"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="nt">"min_loss_scale"</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If you want to use NVIDIA’s apex instead, you can can either configure the <code class="docutils literal notranslate"><span class="pre">amp</span></code> entry in the configuration file, or
use the following command line arguments: <code class="docutils literal notranslate"><span class="pre">--fp16</span> <span class="pre">--fp16_backend</span> <span class="pre">apex</span> <span class="pre">--fp16_opt_level</span> <span class="pre">01</span></code>.</p>
<p>Here is an example of the <code class="docutils literal notranslate"><span class="pre">amp</span></code> configuration:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">"amp"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"enabled"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
        <span class="nt">"opt_level"</span><span class="p">:</span> <span class="s2">"O1"</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-clipping">
<h4>Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permalink to this headline">¶</a></h4>
<p>If you don’t configure the <code class="docutils literal notranslate"><span class="pre">gradient_clipping</span></code> entry in the configuration file, the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>
will use the value of the <code class="docutils literal notranslate"><span class="pre">--max_grad_norm</span></code> command line argument to set it.</p>
<p>Here is an example of the <code class="docutils literal notranslate"><span class="pre">gradient_clipping</span></code> configuration:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">"gradient_clipping"</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="notes">
<h4>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>DeepSpeed works with the PyTorch <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> but not TF <a class="reference internal" href="#transformers.TFTrainer" title="transformers.TFTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTrainer</span></code></a>.</p></li>
<li><p>While DeepSpeed has a pip installable PyPI package, it is highly recommended that it gets installed from <a class="reference external" href="https://github.com/microsoft/deepspeed#installation">source</a> to best match your hardware and also if you need to enable
certain features, like 1-bit Adam, which aren’t available in the pypi distribution.</p></li>
<li><p>You don’t have to use the <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> to use DeepSpeed with HuggingFace <code class="docutils literal notranslate"><span class="pre">transformers</span></code> - you can
use any model with your own trainer, and you will have to adapt the latter according to <a class="reference external" href="https://www.deepspeed.ai/getting-started/#writing-deepspeed-models">the DeepSpeed integration
instructions</a>.</p></li>
</ul>
</div>
<div class="section" id="main-deepspeed-resources">
<h4>Main DeepSpeed Resources<a class="headerlink" href="#main-deepspeed-resources" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/microsoft/deepspeed">Project’s github</a></p></li>
<li><p><a class="reference external" href="https://www.deepspeed.ai/getting-started/">Usage docs</a></p></li>
<li><p><a class="reference external" href="https://deepspeed.readthedocs.io/en/latest/index.html">API docs</a></p></li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/search/?q=deepspeed">Blog posts</a></p></li>
</ul>
<p>Finally, please, remember that, HuggingFace <a class="reference internal" href="#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> only integrates DeepSpeed, therefore if you
have any problems or questions with regards to DeepSpeed usage, please, file an issue with <a class="reference external" href="https://github.com/microsoft/DeepSpeed/issues">DeepSpeed GitHub</a>.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="../model_doc/albert.html" rel="next" title="ALBERT">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="tokenizer.html" rel="prev" title="Tokenizer"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>