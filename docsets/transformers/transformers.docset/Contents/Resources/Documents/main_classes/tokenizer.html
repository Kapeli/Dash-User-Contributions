
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tokenizer ‚Äî transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="trainer.html" rel="next" title="Trainer"/>
<link href="processors.html" rel="prev" title="Processors"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ü§ó Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ü§ó Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tokenizer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedtokenizer">PreTrainedTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedtokenizerfast">PreTrainedTokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batchencoding">BatchEncoding</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> ¬ª</li>
<li>Tokenizer</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/tokenizer.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="tokenizer">
<h1>Tokenizer<a class="headerlink" href="#tokenizer" title="Permalink to this headline">¬∂</a></h1>
<p>A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most
of the tokenizers are available in two flavors: a full python implementation and a ‚ÄúFast‚Äù implementation based on the
Rust library <a class="reference external" href="https://github.com/huggingface/tokenizers">tokenizers</a>. The ‚ÄúFast‚Äù implementations allows:</p>
<ol class="arabic simple">
<li><p>a significant speed-up in particular when doing batched tokenization and</p></li>
<li><p>additional methods to map between the original string (character and words) and the token space (e.g. getting the
index of the token comprising a given character or the span of characters corresponding to a given token). Currently
no ‚ÄúFast‚Äù implementation is available for the SentencePiece-based tokenizers (for T5, ALBERT, CamemBERT, XLMRoBERTa
and XLNet models).</p></li>
</ol>
<p>The base classes <a class="reference internal" href="#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> and <a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>
implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and
‚ÄúFast‚Äù tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library
(downloaded from HuggingFace‚Äôs AWS S3 repository). They both rely on
<a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a> that contains the common methods, and
<a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin" title="transformers.tokenization_utils_base.SpecialTokensMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpecialTokensMixin</span></code></a>.</p>
<p><a class="reference internal" href="#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> and <a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a> thus implement the main
methods for using all the tokenizers:</p>
<ul class="simple">
<li><p>Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and
encoding/decoding (i.e., tokenizing and converting to integers).</p></li>
<li><p>Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece‚Ä¶).</p></li>
<li><p>Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the
tokenizer for easy access and making sure they are not split during tokenization.</p></li>
</ul>
<p><a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> holds the output of the tokenizer‚Äôs encoding methods (<code class="docutils literal notranslate"><span class="pre">__call__</span></code>,
<code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> and <code class="docutils literal notranslate"><span class="pre">batch_encode_plus</span></code>) and is derived from a Python dictionary. When the tokenizer is a pure python
tokenizer, this class behaves just like a standard python dictionary and holds the various model inputs computed by
these methods (<code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>‚Ä¶). When the tokenizer is a ‚ÄúFast‚Äù tokenizer (i.e., backed by
HuggingFace <a class="reference external" href="https://github.com/huggingface/tokenizers">tokenizers library</a>), this class provides in addition
several advanced alignment methods which can be used to map between the original string (character and words) and the
token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding
to a given token).</p>
<div class="section" id="pretrainedtokenizer">
<h2>PreTrainedTokenizer<a class="headerlink" href="#pretrainedtokenizer" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.PreTrainedTokenizer"><a name="//apple_ref/cpp/Class/transformers.PreTrainedTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PreTrainedTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Base class for all slow tokenizers.</p>
<p>Inherits from <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a>.</p>
<p>Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading
pretrained tokenizers as well as adding tokens to the vocabulary.</p>
<p>This class also contain the added tokens in a unified way on top of all tokenizers so we don‚Äôt have to handle the
specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece‚Ä¶).</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>vocab_files_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>) ‚Äì A dictionary with, as keys, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of
each vocabulary file required by the model, and as associated values, the filename for saving the associated
file (string).</p></li>
<li><p><strong>pretrained_vocab_files_map</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">str]]</span></code>) ‚Äì A dictionary of dictionaries, with the
high-level keys being the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of each vocabulary file required by the model, the
low-level being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models with, as associated values, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">url</span></code> to the associated pretrained vocabulary file.</p></li>
<li><p><strong>max_model_input_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Optinal[int]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, the maximum length of the sequence
inputs of this model, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the model has no maximum input size.</p></li>
<li><p><strong>pretrained_init_configuration</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, a dictionary of specific arguments
to pass to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the tokenizer class for this pretrained model when loading the
tokenizer with the <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>
method.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì A list of inputs expected in the forward pass of the model.</p></li>
<li><p><strong>padding_side</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The default value for the side on which the model should have padding
applied. Should be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'right'</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'left'</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>, this
will be set to the value stored for the associated model in <code class="docutils literal notranslate"><span class="pre">max_model_input_sizes</span></code> (see above). If no
value is provided, will default to VERY_LARGE_INTEGER (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int(1e30)</span></code>).</p></li>
<li><p><strong>padding_side</strong> ‚Äì (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):
The side on which the model should have padding applied. Should be selected between [‚Äòright‚Äô, ‚Äòleft‚Äô].
Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[string]</span></code>, <cite>optional</cite>) ‚Äì The list of inputs accepted by the forward pass of the model (like <code class="xref py py-obj docutils literal notranslate"><span class="pre">"token_type_ids"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"attention_mask"</span></code>). Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the beginning of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.bos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.bos_token_id</span></code>.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the end of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.eos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.eos_token_id</span></code>.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing an out-of-vocabulary token. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.unk_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.unk_token_id</span></code>.</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code class="docutils literal notranslate"><span class="pre">self.sep_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.sep_token_id</span></code>.</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.pad_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code>.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the class of the input (used by BERT for instance). Will be associated to
<code class="docutils literal notranslate"><span class="pre">self.cls_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.cls_token_id</span></code>.</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code class="docutils literal notranslate"><span class="pre">self.mask_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.mask_token_id</span></code>.</p></li>
<li><p><strong>additional_special_tokens</strong> (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A tuple or a list of additional special tokens. Add them here to ensure they won‚Äôt be split by the
tokenization process. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens_ids</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.__call__"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.__call__"></a>
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="headerlink" href="#transformers.PreTrainedTokenizer.__call__" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>text_pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.convert_ids_to_tokens"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.convert_ids_to_tokens"></a>
<code class="sig-name descname">convert_ids_to_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">skip_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">'False'</span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.convert_ids_to_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.convert_ids_to_tokens" title="Permalink to this definition">¬∂</a></dt>
<dt>
<code class="sig-name descname">convert_ids_to_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">skip_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">'False'</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span></dt>
<dd><p>Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
added tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì The token id (or token ids) to convert to tokens.</p></li>
<li><p><strong>skip_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to remove special tokens in the decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The decoded token(s).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.convert_tokens_to_ids"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.convert_tokens_to_ids"></a>
<code class="sig-name descname">convert_tokens_to_ids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí Union<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.convert_tokens_to_ids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.convert_tokens_to_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì One or several token(s) to convert to token id(s).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The token id or list of token ids.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.convert_tokens_to_string"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.convert_tokens_to_string"></a>
<code class="sig-name descname">convert_tokens_to_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.convert_tokens_to_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.convert_tokens_to_string" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a sequence of token ids in a single string. The most simple way to do it is <code class="docutils literal notranslate"><span class="pre">"</span> <span class="pre">".join(tokens)</span></code> but
we often want to remove sub-word tokenization artifacts at the same time</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì The token to join in a string.</p>
</dd>
</dl>
<p>Return: The joined tokens.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.get_added_vocab"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.get_added_vocab"></a>
<code class="sig-name descname">get_added_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí Dict<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.get_added_vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.get_added_vocab" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the added tokens in the vocabulary as a dictionary of token to index.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The added tokens.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.get_special_tokens_mask"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.get_special_tokens_mask"></a>
<code class="sig-name descname">get_special_tokens_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.get_special_tokens_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.get_special_tokens_mask" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì List of ids of the first sequence.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì List of ids of the second sequence.</p></li>
<li><p><strong>already_has_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the token list is already formatted with special tokens for the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1 for a special token, 0 for a sequence token.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A list of integers in the range [0, 1]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.num_special_tokens_to_add"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.num_special_tokens_to_add"></a>
<code class="sig-name descname">num_special_tokens_to_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pair</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.num_special_tokens_to_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.num_special_tokens_to_add" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the number of added tokens when encoding a sequence with special tokens.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not
put this inside your training loop.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether the number of added tokens should be computed in the case of a sequence pair or a single
sequence.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Number of special tokens added to sequences.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.prepare_for_tokenization"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.prepare_for_tokenization"></a>
<code class="sig-name descname">prepare_for_tokenization</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí Tuple<span class="p">[</span>str<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.prepare_for_tokenization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.prepare_for_tokenization" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Performs any necessary transformations before tokenization.</p>
<p>This method should pop the arguments from kwargs and return the remaining <code class="xref py py-obj docutils literal notranslate"><span class="pre">kwargs</span></code> as well. We test the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">kwargs</span></code> at the end of the encoding process to be sure all the arguments have been used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The text to prepare.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the text has been pretokenized.</p></li>
<li><p><strong>kwargs</strong> ‚Äì Keyword arguments to use for the tokenization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The prepared text and the unused kwargs.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.tokenize"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.tokenize"></a>
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils.html#PreTrainedTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizer.tokenize" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a string in a sequence of tokens, using the tokenizer.</p>
<p>Note that, unlike Fast tokenizers (instances of PreTrainedTokenizerFast), this method won‚Äôt replace the unknown
tokens with the <cite>unk_token</cite> yet (this is done in the <cite>encode()</cite> method)</p>
<p>Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies
(BPE/SentencePieces/WordPieces). Takes care of added tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The sequence to be encoded.</p></li>
<li><p><strong>**kwargs</strong> (<em>additional keyword arguments</em>) ‚Äì Passed along to the model-specific <code class="docutils literal notranslate"><span class="pre">prepare_for_tokenization</span></code> preprocessing method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizer.vocab_size"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizer.vocab_size"></a>
<em class="property">property </em><code class="sig-name descname">vocab_size</code><a class="headerlink" href="#transformers.PreTrainedTokenizer.vocab_size" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Size of the base vocabulary (without the added tokens).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="pretrainedtokenizerfast">
<h2>PreTrainedTokenizerFast<a class="headerlink" href="#pretrainedtokenizerfast" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.PreTrainedTokenizerFast"><a name="//apple_ref/cpp/Class/transformers.PreTrainedTokenizerFast"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PreTrainedTokenizerFast</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).</p>
<p>Inherits from <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a>.</p>
<p>Handles all the shared methods for tokenization and special tokens, as well as methods for
downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.</p>
<p>This class also contains the added tokens in a unified way on top of all tokenizers so we don‚Äôt have to handle the
specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece‚Ä¶).</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>vocab_files_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>) ‚Äì A dictionary with, as keys, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of
each vocabulary file required by the model, and as associated values, the filename for saving the associated
file (string).</p></li>
<li><p><strong>pretrained_vocab_files_map</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">str]]</span></code>) ‚Äì A dictionary of dictionaries, with the
high-level keys being the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of each vocabulary file required by the model, the
low-level being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models with, as associated values, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">url</span></code> to the associated pretrained vocabulary file.</p></li>
<li><p><strong>max_model_input_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Optinal[int]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, the maximum length of the sequence
inputs of this model, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the model has no maximum input size.</p></li>
<li><p><strong>pretrained_init_configuration</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, a dictionary of specific arguments
to pass to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the tokenizer class for this pretrained model when loading the
tokenizer with the <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>
method.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì A list of inputs expected in the forward pass of the model.</p></li>
<li><p><strong>padding_side</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The default value for the side on which the model should have padding
applied. Should be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'right'</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'left'</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>, this
will be set to the value stored for the associated model in <code class="docutils literal notranslate"><span class="pre">max_model_input_sizes</span></code> (see above). If no
value is provided, will default to VERY_LARGE_INTEGER (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int(1e30)</span></code>).</p></li>
<li><p><strong>padding_side</strong> ‚Äì (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):
The side on which the model should have padding applied. Should be selected between [‚Äòright‚Äô, ‚Äòleft‚Äô].
Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[string]</span></code>, <cite>optional</cite>) ‚Äì The list of inputs accepted by the forward pass of the model (like <code class="xref py py-obj docutils literal notranslate"><span class="pre">"token_type_ids"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"attention_mask"</span></code>). Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the beginning of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.bos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.bos_token_id</span></code>.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the end of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.eos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.eos_token_id</span></code>.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing an out-of-vocabulary token. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.unk_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.unk_token_id</span></code>.</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code class="docutils literal notranslate"><span class="pre">self.sep_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.sep_token_id</span></code>.</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.pad_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code>.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the class of the input (used by BERT for instance). Will be associated to
<code class="docutils literal notranslate"><span class="pre">self.cls_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.cls_token_id</span></code>.</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code class="docutils literal notranslate"><span class="pre">self.mask_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.mask_token_id</span></code>.</p></li>
<li><p><strong>additional_special_tokens</strong> (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A tuple or a list of additional special tokens. Add them here to ensure they won‚Äôt be split by the
tokenization process. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens_ids</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.__call__"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.__call__"></a>
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="headerlink" href="#transformers.PreTrainedTokenizerFast.__call__" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>text_pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.backend_tokenizer"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.backend_tokenizer"></a>
<em class="property">property </em><code class="sig-name descname">backend_tokenizer</code><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.backend_tokenizer" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The Rust tokenizer used as a backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.implementations.BaseTokenizer</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.convert_ids_to_tokens"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.convert_ids_to_tokens"></a>
<code class="sig-name descname">convert_ids_to_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">skip_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.convert_ids_to_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.convert_ids_to_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
added tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì The token id (or token ids) to convert to tokens.</p></li>
<li><p><strong>skip_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to remove special tokens in the decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The decoded token(s).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.convert_tokens_to_ids"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.convert_tokens_to_ids"></a>
<code class="sig-name descname">convert_tokens_to_ids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí Union<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.convert_tokens_to_ids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.convert_tokens_to_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì One or several token(s) to convert to token id(s).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The token id or list of token ids.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.convert_tokens_to_string"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.convert_tokens_to_string"></a>
<code class="sig-name descname">convert_tokens_to_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.convert_tokens_to_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.convert_tokens_to_string" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a sequence of token ids in a single string. The most simple way to do it is <code class="docutils literal notranslate"><span class="pre">"</span> <span class="pre">".join(tokens)</span></code> but
we often want to remove sub-word tokenization artifacts at the same time</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì The token to join in a string.</p>
</dd>
</dl>
<p>Return: The joined tokens.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.decoder"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.decoder"></a>
<em class="property">property </em><code class="sig-name descname">decoder</code><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.decoder" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The Rust decoder for this tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.decoders.Decoder</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.get_added_vocab"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.get_added_vocab"></a>
<code class="sig-name descname">get_added_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí Dict<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.get_added_vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.get_added_vocab" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the added tokens in the vocabulary as a dictionary of token to index.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The added tokens.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.get_vocab"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.get_vocab"></a>
<code class="sig-name descname">get_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí Dict<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.get_vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.get_vocab" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.num_special_tokens_to_add"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.num_special_tokens_to_add"></a>
<code class="sig-name descname">num_special_tokens_to_add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pair</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.num_special_tokens_to_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.num_special_tokens_to_add" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the number of added tokens when encoding a sequence with special tokens.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not
put this inside your training loop.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether the number of added tokens should be computed in the case of a sequence pair or a single
sequence.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Number of special tokens added to sequences.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.set_truncation_and_padding"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.set_truncation_and_padding"></a>
<code class="sig-name descname">set_truncation_and_padding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">padding_strategy</span><span class="p">:</span> <span class="n"><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a></span></em>, <em class="sig-param"><span class="n">truncation_strategy</span><span class="p">:</span> <span class="n"><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a></span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.set_truncation_and_padding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.set_truncation_and_padding" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers
library) and restore the tokenizer settings afterwards.</p>
<p>The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a
padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed
section.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>padding_strategy</strong> (<a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>) ‚Äì The kind of padding that will be applied to the input</p></li>
<li><p><strong>truncation_strategy</strong> (<a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>) ‚Äì The kind of truncation that will be applied to the input</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì The maximum size of a sequence.</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì The stride to use when handling overflow.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.tokenize"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.tokenize"></a>
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_fast.html#PreTrainedTokenizerFast.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.tokenize" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a string in a sequence of tokens, using the backend Rust tokenizer.</p>
<p>Note that this method behave differently between fast and slow tokenizers:</p>
<blockquote>
<div><ul class="simple">
<li><p>in fast tokenizers (instances of <a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>), this method will
replace the unknown tokens with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code>,</p></li>
<li><p>in slow tokenizers (instances of <a class="reference internal" href="#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a>), this method keep unknown
tokens unchanged.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The sequence to be encoded.</p></li>
<li><p><strong>pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) ‚Äì A second sequence to be encoded with the first.</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to add the special tokens associated with the corresponding model.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) ‚Äì Will be passed to the underlying model specific encode method. See details in
<a class="reference internal" href="#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__call__()</span></code></a></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedTokenizerFast.vocab_size"><a name="//apple_ref/cpp/Method/transformers.PreTrainedTokenizerFast.vocab_size"></a>
<em class="property">property </em><code class="sig-name descname">vocab_size</code><a class="headerlink" href="#transformers.PreTrainedTokenizerFast.vocab_size" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Size of the base vocabulary (without the added tokens).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="batchencoding">
<h2>BatchEncoding<a class="headerlink" href="#batchencoding" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.BatchEncoding"><a name="//apple_ref/cpp/Class/transformers.BatchEncoding"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">BatchEncoding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">encoding</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>tokenizers.Encoding<span class="p">, </span>Sequence<span class="p">[</span>tokenizers.Encoding<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tensor_type</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">n_sequences</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Holds the output of the <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus"><code class="xref py py-meth docutils literal notranslate"><span class="pre">encode_plus()</span></code></a> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">batch_encode()</span></code> methods (tokens,
attention_masks, etc).</p>
<p>This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes
utility methods to map from word/character space to token space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>) ‚Äì Dictionary of lists/arrays/tensors returned by the encode/batch_encode methods (‚Äòinput_ids‚Äô,
‚Äòattention_mask‚Äô, etc.).</p></li>
<li><p><strong>encoding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.Encoding</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[tokenizers.Encoding]</span></code>, <cite>optional</cite>) ‚Äì If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character
space to token space the <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.Encoding</span></code> instance or list of instance (for batches) hold this
information.</p></li>
<li><p><strong>tensor_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[None,</span> <span class="pre">str,</span> <span class="pre">TensorType]</span></code>, <cite>optional</cite>) ‚Äì You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</p></li>
<li><p><strong>prepend_batch_axis</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to add a batch axis when converting to tensors (see <code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_type</span></code> above).</p></li>
<li><p><strong>n_sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>, <cite>optional</cite>) ‚Äì You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.char_to_token"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.char_to_token"></a>
<code class="sig-name descname">char_to_token</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_char_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">char_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sequence_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.char_to_token"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.char_to_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the index of the token in the encoded output comprising a character in the original string for a sequence
of the batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_token(char_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_token(batch_index,</span> <span class="pre">char_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_char_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the word in the sequence</p></li>
<li><p><strong>char_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p></li>
<li><p><strong>sequence_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided character index belongs to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Index of the token.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.char_to_word"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.char_to_word"></a>
<code class="sig-name descname">char_to_word</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_char_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">char_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sequence_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.char_to_word"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.char_to_word" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the word in the original string corresponding to a character in the original string of a sequence of the
batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_word(char_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.char_to_word(batch_index,</span> <span class="pre">char_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_char_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the character in the original string.</p></li>
<li><p><strong>char_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the character in the
original string.</p></li>
<li><p><strong>sequence_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided character index belongs to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Index or indices of the associated encoded token(s).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.convert_to_tensors"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.convert_to_tensors"></a>
<code class="sig-name descname">convert_to_tensors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor_type</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.convert_to_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.convert_to_tensors" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Convert the inner content to tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì The type of tensors to use. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, should be one of the values of the enum
<a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, no modification is done.</p></li>
<li><p><strong>prepend_batch_axis</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to add the batch dimension during the conversion.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.encodings"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.encodings"></a>
<em class="property">property </em><code class="sig-name descname">encodings</code><a class="headerlink" href="#transformers.BatchEncoding.encodings" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The list all encodings from the tokenization process. Returns
<code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the input was tokenized through Python (i.e., not a fast) tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[tokenizers.Encoding]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.is_fast"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.is_fast"></a>
<em class="property">property </em><code class="sig-name descname">is_fast</code><a class="headerlink" href="#transformers.BatchEncoding.is_fast" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Indicate whether this <a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> was generated from the result of a
<a class="reference internal" href="#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a> or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.items"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.items"></a>
<code class="sig-name descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí a set-like object providing a view on D‚Äôs items<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.items"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.items" title="Permalink to this definition">¬∂</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.keys"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.keys"></a>
<code class="sig-name descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí a set-like object providing a view on D‚Äôs keys<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.keys" title="Permalink to this definition">¬∂</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.n_sequences"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.n_sequences"></a>
<em class="property">property </em><code class="sig-name descname">n_sequences</code><a class="headerlink" href="#transformers.BatchEncoding.n_sequences" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The number of sequences used to generate each sample from the batch encoded in this
<a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a>. Currently can be one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> (unknown), <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> (a single
sentence) or <code class="xref py py-obj docutils literal notranslate"><span class="pre">2</span></code> (a pair of sentences)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.sequence_ids"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.sequence_ids"></a>
<code class="sig-name descname">sequence_ids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>Optional<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.sequence_ids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.sequence_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Return a list mapping the tokens to the id of their original sentences:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> for special tokens added around or between sequences,</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> for tokens corresponding to words in the first sequence,</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> for tokens corresponding to words in the second sequence when a pair of sequences was jointly
encoded.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The index to access in the batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list indicating the sequence id corresponding to each token. Special tokens
added by the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their
corresponding sequence.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.to"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.to"></a>
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>torch.device<span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí <a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding">BatchEncoding</a><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.to" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Send all values to device by calling <code class="xref py py-obj docutils literal notranslate"><span class="pre">v.to(device)</span></code> (PyTorch only).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>) ‚Äì The device to put the tensors on.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same instance of <a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> after
modification.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.token_to_chars"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.token_to_chars"></a>
<code class="sig-name descname">token_to_chars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_token_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">token_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.CharSpan" title="transformers.tokenization_utils_base.CharSpan">transformers.tokenization_utils_base.CharSpan</a><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.token_to_chars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.token_to_chars" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the character span corresponding to an encoded token in a sequence of the batch.</p>
<p>Character spans are returned as a <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.CharSpan" title="transformers.tokenization_utils_base.CharSpan"><code class="xref py py-class docutils literal notranslate"><span class="pre">CharSpan</span></code></a> with:</p>
<ul class="simple">
<li><p><strong>start</strong> ‚Äì Index of the first character in the original string associated to the token.</p></li>
<li><p><strong>end</strong> ‚Äì Index of the character following the last character in the original string associated to the
token.</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_chars(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_chars(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the token in the sequence.</p></li>
<li><p><strong>token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token or tokens in
the sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Span of characters in the original string.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.CharSpan" title="transformers.tokenization_utils_base.CharSpan"><code class="xref py py-class docutils literal notranslate"><span class="pre">CharSpan</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.token_to_sequence"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.token_to_sequence"></a>
<code class="sig-name descname">token_to_sequence</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_token_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">token_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.token_to_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.token_to_sequence" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the index of the sequence represented by the given token. In the general use case, this method returns
<code class="xref py py-obj docutils literal notranslate"><span class="pre">0</span></code> for a single sequence or the first sequence of a pair, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code> for the second sequence of a pair</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_sequence(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_sequence(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,
words are defined by the user). In this case it allows to easily associate encoded tokens with provided
tokenized words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of
the token in the sequence.</p></li>
<li><p><strong>token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token in the
sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Index of the word in the input sequence.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.token_to_word"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.token_to_word"></a>
<code class="sig-name descname">token_to_word</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_token_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">token_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.token_to_word"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.token_to_word" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.</p>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_word(token_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.token_to_word(batch_index,</span> <span class="pre">token_index)</span></code> if batch size is greater than 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,
words are defined by the user). In this case it allows to easily associate encoded tokens with provided
tokenized words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the token in the sequence.</p></li>
<li><p><strong>token_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the token in the
sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Index of the word in the input sequence.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.tokens"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.tokens"></a>
<code class="sig-name descname">tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to
integer indices) at a given batch index (only works for the output of a fast tokenizer).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The index to access in the batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of tokens at that index.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.values"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.values"></a>
<code class="sig-name descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí an object providing a view on D‚Äôs values<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.values" title="Permalink to this definition">¬∂</a></dt>
<dd></dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.word_ids"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.word_ids"></a>
<code class="sig-name descname">word_ids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>Optional<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.word_ids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.word_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The index to access in the batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list indicating the word corresponding to each token. Special tokens added by
the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their corresponding
word (several tokens will be mapped to the same word index if they are parts of that word).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.word_to_chars"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.word_to_chars"></a>
<code class="sig-name descname">word_to_chars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_word_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">word_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sequence_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.CharSpan" title="transformers.tokenization_utils_base.CharSpan">transformers.tokenization_utils_base.CharSpan</a><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.word_to_chars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.word_to_chars" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the character span in the original string corresponding to given word in a sequence of the batch.</p>
<p>Character spans are returned as a CharSpan NamedTuple with:</p>
<ul class="simple">
<li><p>start: index of the first character in the original string</p></li>
<li><p>end: index of the character following the last character in the original string</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_chars(word_index)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_chars(batch_index,</span> <span class="pre">word_index)</span></code> if batch size is greater or equal to 1</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_word_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of
the word in the sequence</p></li>
<li><p><strong>word_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p></li>
<li><p><strong>sequence_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided word index belongs to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Span(s) of the associated character or characters in the string.
CharSpan are NamedTuple with:</p>
<blockquote>
<div><ul class="simple">
<li><p>start: index of the first character associated to the token in the original string</p></li>
<li><p>end: index of the character following the last character associated to the token in the original
string</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">CharSpan</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[CharSpan]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.word_to_tokens"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.word_to_tokens"></a>
<code class="sig-name descname">word_to_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_or_word_index</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">word_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sequence_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí Optional<span class="p">[</span><a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TokenSpan" title="transformers.tokenization_utils_base.TokenSpan">transformers.tokenization_utils_base.TokenSpan</a><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.word_to_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.word_to_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Get the encoded token span corresponding to a word in a sequence of the batch.</p>
<p>Token spans are returned as a <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TokenSpan" title="transformers.tokenization_utils_base.TokenSpan"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenSpan</span></code></a> with:</p>
<ul class="simple">
<li><p><strong>start</strong> ‚Äì Index of the first token.</p></li>
<li><p><strong>end</strong> ‚Äì Index of the token following the last token.</p></li>
</ul>
<p>Can be called as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_tokens(word_index,</span> <span class="pre">sequence_index:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">0)</span></code> if batch size is 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self.word_to_tokens(batch_index,</span> <span class="pre">word_index,</span> <span class="pre">sequence_index:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">0)</span></code> if batch size is greater or equal
to 1</p></li>
</ul>
<p>This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words
are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized
words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_or_word_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of
the word in the sequence.</p></li>
<li><p><strong>word_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If a batch index is provided in <cite>batch_or_token_index</cite>, this can be the index of the word in the
sequence.</p></li>
<li><p><strong>sequence_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0
or 1) the provided word index belongs to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Optional <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TokenSpan" title="transformers.tokenization_utils_base.TokenSpan"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenSpan</span></code></a> Span of tokens in the encoded sequence.
Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if no tokens correspond to the word.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BatchEncoding.words"><a name="//apple_ref/cpp/Method/transformers.BatchEncoding.words"></a>
<code class="sig-name descname">words</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_index</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>Optional<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#BatchEncoding.words"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BatchEncoding.words" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The index to access in the batch.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list indicating the word corresponding to each token. Special tokens added by
the tokenizer are mapped to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> and other tokens are mapped to the index of their corresponding
word (several tokens will be mapped to the same word index if they are parts of that word).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Optional[int]]</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="trainer.html" rel="next" title="Trainer">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="processors.html" rel="prev" title="Processors"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        ¬© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>