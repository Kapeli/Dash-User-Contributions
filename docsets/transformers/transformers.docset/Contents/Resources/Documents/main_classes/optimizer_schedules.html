
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Optimization â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="output.html" rel="next" title="Model outputs"/>
<link href="model.html" rel="prev" title="Models"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adamw-pytorch">AdamW (PyTorch)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adafactor-pytorch">AdaFactor (PyTorch)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adamweightdecay-tensorflow">AdamWeightDecay (TensorFlow)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#schedules">Schedules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#learning-rate-schedules-pytorch">Learning Rate Schedules (Pytorch)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#warmup-tensorflow">Warmup (TensorFlow)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-strategies">Gradient Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gradientaccumulator-tensorflow">GradientAccumulator (TensorFlow)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Optimization</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/optimizer_schedules.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="optimization">
<h1>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">Â¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">.optimization</span></code> module provides:</p>
<ul class="simple">
<li><p>an optimizer with weight decay fixed that can be used to fine-tuned models, and</p></li>
<li><p>several schedules in the form of schedule objects that inherit from <code class="docutils literal notranslate"><span class="pre">_LRSchedule</span></code>:</p></li>
<li><p>a gradient accumulation class to accumulate the gradients of multiple batches</p></li>
</ul>
<div class="section" id="adamw-pytorch">
<h2>AdamW (PyTorch)<a class="headerlink" href="#adamw-pytorch" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AdamW"><a name="//apple_ref/cpp/Class/transformers.AdamW"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AdamW</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Iterable<span class="p">[</span>torch.nn.parameter.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">lr</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">betas</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(0.9, 0.999)</span></em>, <em class="sig-param"><span class="n">eps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-06</span></em>, <em class="sig-param"><span class="n">weight_decay</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">correct_bias</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#AdamW"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AdamW" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implements Adam algorithm with weight decay fix as introduced in <a class="reference external" href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable[torch.nn.parameter.Parameter]</span></code>) â€“ Iterable of parameters to optimize or dictionaries defining parameter groups.</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-3) â€“ The learning rate to use.</p></li>
<li><p><strong>betas</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[float,float]</span></code>, <cite>optional</cite>, defaults to (0.9, 0.999)) â€“ Adamâ€™s betas parameters (b1, b2).</p></li>
<li><p><strong>eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-6) â€“ Adamâ€™s epsilon for numerical stability.</p></li>
<li><p><strong>weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) â€“ Decoupled weight decay to apply.</p></li>
<li><p><strong>correct_bias</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>True</cite>) â€“ Whether ot not to correct bias in Adam (for instance, in Bert TF repository they use <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.AdamW.step"><a name="//apple_ref/cpp/Method/transformers.AdamW.step"></a>
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">closure</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#AdamW.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AdamW.step" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>, <cite>optional</cite>) â€“ A closure that reevaluates the model and returns the loss.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="adafactor-pytorch">
<h2>AdaFactor (PyTorch)<a class="headerlink" href="#adafactor-pytorch" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.Adafactor"><a name="//apple_ref/cpp/Class/transformers.Adafactor"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">Adafactor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">(1e-30, 0.001)</span></em>, <em class="sig-param"><span class="n">clip_threshold</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">decay_rate</span><span class="o">=</span><span class="default_value">- 0.8</span></em>, <em class="sig-param"><span class="n">beta1</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_decay</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale_parameter</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">relative_step</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">warmup_init</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#Adafactor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.Adafactor" title="Permalink to this definition">Â¶</a></dt>
<dd><p>AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
<a class="reference external" href="https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py">https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py</a></p>
<p>Paper: <cite>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</cite> <a class="reference external" href="https://arxiv.org/abs/1804.04235">https://arxiv.org/abs/1804.04235</a> Note that
this optimizer internally adjusts the learning rate depending on the <em>scale_parameter</em>, <em>relative_step</em> and
<em>warmup_init</em> options. To use a manual (external) learning rate schedule you should set <cite>scale_parameter=False</cite> and
<cite>relative_step=False</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable[torch.nn.parameter.Parameter]</span></code>) â€“ Iterable of parameters to optimize or dictionaries defining parameter groups.</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) â€“ The external learning rate.</p></li>
<li><p><strong>eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[float,</span> <span class="pre">float]</span></code>, <cite>optional</cite>, defaults to (1e-30, 1e-3)) â€“ Regularization constants for square gradient and parameter scale respectively</p></li>
<li><p><strong>clip_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults 1.0) â€“ Threshold of root mean square of final gradient update</p></li>
<li><p><strong>decay_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to -0.8) â€“ Coefficient used to compute running averages of square</p></li>
<li><p><strong>beta1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) â€“ Coefficient used for computing running averages of gradient</p></li>
<li><p><strong>weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) â€“ Weight decay (L2 penalty)</p></li>
<li><p><strong>scale_parameter</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ If True, learning rate is scaled by root mean square</p></li>
<li><p><strong>relative_step</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ If True, time-dependent learning rate is computed instead of external learning rate</p></li>
<li><p><strong>warmup_init</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Time-dependent learning rate computation depends on whether warm-up initialization is being used</p></li>
</ul>
</dd>
</dl>
<p>This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested.</p>
<p>Recommended T5 finetuning settings:</p>
<blockquote>
<div><ul class="simple">
<li><p>Scheduled LR warm-up to fixed LR</p></li>
<li><p>disable relative updates</p></li>
<li><p>use clip threshold: <a class="reference external" href="https://arxiv.org/abs/2004.14546">https://arxiv.org/abs/2004.14546</a></p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Adafactor</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">relative_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">warmup_init</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Alternatively, relative_step with warmup_init can be used.</p></li>
<li><p>Training without LR warmup or clip threshold is not recommended. Additional optimizer operations like
gradient clipping should not be used alongside Adafactor.</p></li>
</ul>
</div></blockquote>
<p>Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># replace AdamW with Adafactor</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-30</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">),</span>
    <span class="n">clip_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">decay_rate</span><span class="o">=-</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">beta1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">relative_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">scale_parameter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">warmup_init</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</div>
<div class="section" id="adamweightdecay-tensorflow">
<h2>AdamWeightDecay (TensorFlow)<a class="headerlink" href="#adamweightdecay-tensorflow" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AdamWeightDecay"><a name="//apple_ref/cpp/Class/transformers.AdamWeightDecay"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AdamWeightDecay</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">beta_1</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">beta_2</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.999</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">amsgrad</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">weight_decay_rate</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">include_in_weight_decay</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">exclude_from_weight_decay</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'AdamWeightDecay'</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization_tf.html#AdamWeightDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AdamWeightDecay" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is <em>not</em> the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in <a class="reference external" href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a>.</p>
<p>Instead we want ot decay the weights in a manner that doesnâ€™t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[float,</span> <span class="pre">tf.keras.optimizers.schedules.LearningRateSchedule]</span></code>, <cite>optional</cite>, defaults to 1e-3) â€“ The learning rate to use or a schedule.</p></li>
<li><p><strong>beta_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.9) â€“ The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.</p></li>
<li><p><strong>beta_2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.999) â€“ The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.</p></li>
<li><p><strong>epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-7) â€“ The epsilon parameter in Adam, which is a small constant for numerical stability.</p></li>
<li><p><strong>amsgrad</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, default to <cite>False</cite>) â€“ Whether to apply AMSGrad variant of this algorithm or not, see <a class="reference external" href="https://arxiv.org/abs/1904.09237">On the Convergence of Adam and Beyond</a>.</p></li>
<li><p><strong>weight_decay_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) â€“ The weight decay to apply.</p></li>
<li><p><strong>include_in_weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) â€“ List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_from_weight_decay</span></code>).</p></li>
<li><p><strong>exclude_from_weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) â€“ List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">include_in_weight_decay</span></code> is passed, the names in it will supersede this list.</p></li>
<li><p><strong>name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to â€˜AdamWeightDecayâ€™) â€“ Optional name for the operations created when applying gradients.</p></li>
<li><p><strong>kwargs</strong> â€“ Keyward arguments. Allowed to be {<code class="docutils literal notranslate"><span class="pre">clipnorm</span></code>, <code class="docutils literal notranslate"><span class="pre">clipvalue</span></code>, <code class="docutils literal notranslate"><span class="pre">lr</span></code>, <code class="docutils literal notranslate"><span class="pre">decay</span></code>}. <code class="docutils literal notranslate"><span class="pre">clipnorm</span></code> is clip
gradients by norm; <code class="docutils literal notranslate"><span class="pre">clipvalue</span></code> is clip gradients by value, <code class="docutils literal notranslate"><span class="pre">decay</span></code> is included for backward
compatibility to allow time inverse decay of learning rate. <code class="docutils literal notranslate"><span class="pre">lr</span></code> is included for backward compatibility,
recommended to use <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> instead.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.create_optimizer"><a name="//apple_ref/cpp/Function/transformers.create_optimizer"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">create_optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">num_train_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">min_lr_ratio</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">adam_beta1</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">adam_beta2</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.999</span></em>, <em class="sig-param"><span class="n">adam_epsilon</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">weight_decay_rate</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">power</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">include_in_weight_decay</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization_tf.html#create_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.create_optimizer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_lr</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) â€“ The desired learning rate at the end of the warmup phase.</p></li>
<li><p><strong>num_train_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The total number of training steps.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of warmup steps.</p></li>
<li><p><strong>min_lr_ratio</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) â€“ The final learning rate at the end of the linear decay will be <code class="xref py py-obj docutils literal notranslate"><span class="pre">init_lr</span> <span class="pre">*</span> <span class="pre">min_lr_ratio</span></code>.</p></li>
<li><p><strong>adam_beta1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.9) â€“ The beta1 to use in Adam.</p></li>
<li><p><strong>adam_beta2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.999) â€“ The beta2 to use in Adam.</p></li>
<li><p><strong>adam_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-8) â€“ The epsilon to use in Adam.</p></li>
<li><p><strong>weight_decay_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) â€“ The weight decay to use.</p></li>
<li><p><strong>power</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ The power to use for PolynomialDecay.</p></li>
<li><p><strong>include_in_weight_decay</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <cite>optional</cite>) â€“ List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="schedules">
<h2>Schedules<a class="headerlink" href="#schedules" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="learning-rate-schedules-pytorch">
<h3>Learning Rate Schedules (Pytorch)<a class="headerlink" href="#learning-rate-schedules-pytorch" title="Permalink to this headline">Â¶</a></h3>
<dl class="py class">
<dt id="transformers.SchedulerType"><a name="//apple_ref/cpp/Class/transformers.SchedulerType"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">SchedulerType</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_utils.html#SchedulerType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.SchedulerType" title="Permalink to this definition">Â¶</a></dt>
<dd><p>An enumeration.</p>
</dd></dl>
<dl class="py function">
<dt id="transformers.get_scheduler"><a name="//apple_ref/cpp/Function/transformers.get_scheduler"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_scheduler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>transformers.trainer_utils.SchedulerType<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_training_steps</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_scheduler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_scheduler" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Unified API to get any scheduler from its name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>:obj:`SchedulerType</cite>) â€“ The name of the scheduler to use.</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>) â€“ The optimizer that will be used during training.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if itâ€™s unset and the scheduler type requires it.</p></li>
<li><p><strong>num_training_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if itâ€™s unset and the scheduler type requires it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.get_constant_schedule"><a name="//apple_ref/cpp/Function/transformers.get_constant_schedule"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_constant_schedule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_constant_schedule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_constant_schedule" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a constant learning rate, using the learning rate set in optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.get_constant_schedule_with_warmup"><a name="//apple_ref/cpp/Function/transformers.get_constant_schedule_with_warmup"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_constant_schedule_with_warmup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_constant_schedule_with_warmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_constant_schedule_with_warmup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup phase.</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
<a class="reference external image-reference" href="/imgs/warmup_constant_schedule.png"><img alt="" src="../_images/warmup_constant_schedule.png"/></a>
<dl class="py function">
<dt id="transformers.get_cosine_schedule_with_warmup"><a name="//apple_ref/cpp/Function/transformers.get_cosine_schedule_with_warmup"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_cosine_schedule_with_warmup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_training_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_cycles</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_cosine_schedule_with_warmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_cosine_schedule_with_warmup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup phase.</p></li>
<li><p><strong>num_training_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The total number of training steps.</p></li>
<li><p><strong>num_cycles</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.5) â€“ The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
<a class="reference external image-reference" href="/imgs/warmup_cosine_schedule.png"><img alt="" src="../_images/warmup_cosine_schedule.png"/></a>
<dl class="py function">
<dt id="transformers.get_cosine_with_hard_restarts_schedule_with_warmup"><a name="//apple_ref/cpp/Function/transformers.get_cosine_with_hard_restarts_schedule_with_warmup"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_cosine_with_hard_restarts_schedule_with_warmup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_training_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_cycles</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_cosine_with_hard_restarts_schedule_with_warmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_cosine_with_hard_restarts_schedule_with_warmup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup phase.</p></li>
<li><p><strong>num_training_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The total number of training steps.</p></li>
<li><p><strong>num_cycles</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ The number of hard restarts to use.</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
<a class="reference external image-reference" href="/imgs/warmup_cosine_hard_restarts_schedule.png"><img alt="" src="../_images/warmup_cosine_hard_restarts_schedule.png"/></a>
<dl class="py function">
<dt id="transformers.get_linear_schedule_with_warmup"><a name="//apple_ref/cpp/Function/transformers.get_linear_schedule_with_warmup"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_linear_schedule_with_warmup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span></em>, <em class="sig-param"><span class="n">num_training_steps</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_linear_schedule_with_warmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_linear_schedule_with_warmup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup phase.</p></li>
<li><p><strong>num_training_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The total number of training steps.</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
<a class="reference external image-reference" href="/imgs/warmup_linear_schedule.png"><img alt="" src="../_images/warmup_linear_schedule.png"/></a>
<dl class="py function">
<dt id="transformers.get_polynomial_decay_schedule_with_warmup"><a name="//apple_ref/cpp/Function/transformers.get_polynomial_decay_schedule_with_warmup"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">get_polynomial_decay_schedule_with_warmup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">num_warmup_steps</span></em>, <em class="sig-param"><span class="n">num_training_steps</span></em>, <em class="sig-param"><span class="n">lr_end</span><span class="o">=</span><span class="default_value">1e-07</span></em>, <em class="sig-param"><span class="n">power</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">last_epoch</span><span class="o">=</span><span class="default_value">- 1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization.html#get_polynomial_decay_schedule_with_warmup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.get_polynomial_decay_schedule_with_warmup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by <cite>lr_end</cite>, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) â€“ The optimizer for which to schedule the learning rate.</p></li>
<li><p><strong>num_warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup phase.</p></li>
<li><p><strong>num_training_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The total number of training steps.</p></li>
<li><p><strong>lr_end</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-7) â€“ The end LR.</p></li>
<li><p><strong>power</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ Power factor.</p></li>
<li><p><strong>last_epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -1) â€“ The index of the last epoch when resuming training.</p></li>
</ul>
</dd>
</dl>
<p>Note: <cite>power</cite> defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
<a class="reference external" href="https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37">https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</a></p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code> with the appropriate schedule.</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="warmup-tensorflow">
<h3>Warmup (TensorFlow)<a class="headerlink" href="#warmup-tensorflow" title="Permalink to this headline">Â¶</a></h3>
<dl class="py class">
<dt id="transformers.WarmUp"><a name="//apple_ref/cpp/Class/transformers.WarmUp"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">WarmUp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">initial_learning_rate</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">decay_schedule_fn</span><span class="p">:</span> <span class="n">Callable</span></em>, <em class="sig-param"><span class="n">warmup_steps</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">power</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/optimization_tf.html#WarmUp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.WarmUp" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Applies a warmup schedule on a given learning rate decay schedule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initial_learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) â€“ The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).</p></li>
<li><p><strong>decay_schedule_fn</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>) â€“ The schedule function to apply after the warmup for the rest of training.</p></li>
<li><p><strong>warmup_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of steps for the warmup part of training.</p></li>
<li><p><strong>power</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) â€“ The power to use for the polynomial warmup (defaults is a linear warmup).</p></li>
<li><p><strong>name</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Optional name prefix for the returned tensors during the schedule.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
</div>
<div class="section" id="gradient-strategies">
<h2>Gradient Strategies<a class="headerlink" href="#gradient-strategies" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="gradientaccumulator-tensorflow">
<h3>GradientAccumulator (TensorFlow)<a class="headerlink" href="#gradientaccumulator-tensorflow" title="Permalink to this headline">Â¶</a></h3>
<dl class="py class">
<dt id="transformers.GradientAccumulator"><a name="//apple_ref/cpp/Class/transformers.GradientAccumulator"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">GradientAccumulator</code><a class="reference internal" href="../_modules/transformers/optimization_tf.html#GradientAccumulator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.GradientAccumulator" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call <code class="docutils literal notranslate"><span class="pre">.gradients</span></code>, scale the gradients if required, and pass the result to <code class="docutils literal notranslate"><span class="pre">apply_gradients</span></code>.</p>
</dd></dl>
</div>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="output.html" rel="next" title="Model outputs">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="model.html" rel="prev" title="Models"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>