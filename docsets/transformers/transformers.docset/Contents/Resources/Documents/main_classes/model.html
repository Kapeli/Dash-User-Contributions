
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Models â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="optimizer_schedules.html" rel="next" title="Optimization"/>
<link href="logging.html" rel="prev" title="Logging"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedmodel">PreTrainedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#moduleutilsmixin">ModuleUtilsMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfpretrainedmodel">TFPreTrainedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfmodelutilsmixin">TFModelUtilsMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="#flaxpretrainedmodel">FlaxPreTrainedModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generation">Generation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Models</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/model.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">Â¶</a></h1>
<p>The base classes <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>, <a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>, and
<a class="reference internal" href="#transformers.FlaxPreTrainedModel" title="transformers.FlaxPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaxPreTrainedModel</span></code></a> implement the common methods for loading/saving a model either from a local
file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFaceâ€™s AWS
S3 repository).</p>
<p><a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> and <a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a> also implement a few methods which
are common among all the models to:</p>
<ul class="simple">
<li><p>resize the input token embeddings when new tokens are added to the vocabulary</p></li>
<li><p>prune the attention heads of the model.</p></li>
</ul>
<p>The other methods that are common to each model are defined in <a class="reference internal" href="#transformers.modeling_utils.ModuleUtilsMixin" title="transformers.modeling_utils.ModuleUtilsMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleUtilsMixin</span></code></a>
(for the PyTorch models) and <code class="xref py py-class docutils literal notranslate"><span class="pre">TFModuleUtilsMixin</span></code> (for the TensorFlow models) or
for text generation, <a class="reference internal" href="#transformers.generation_utils.GenerationMixin" title="transformers.generation_utils.GenerationMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">GenerationMixin</span></code></a> (for the PyTorch models) and
<a class="reference internal" href="#transformers.generation_tf_utils.TFGenerationMixin" title="transformers.generation_tf_utils.TFGenerationMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGenerationMixin</span></code></a> (for the TensorFlow models)</p>
<div class="section" id="pretrainedmodel">
<h2>PreTrainedModel<a class="headerlink" href="#pretrainedmodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.PreTrainedModel"><a name="//apple_ref/cpp/Class/transformers.PreTrainedModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PreTrainedModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.configuration_utils.PretrainedConfig</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Base class for all models.</p>
<p><a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> takes care of storing the configuration of the models and handles methods
for loading, downloading and saving models as well as a few methods common to all models to:</p>
<blockquote>
<div><ul class="simple">
<li><p>resize the input embeddings,</p></li>
<li><p>prune heads in the self-attention heads.</p></li>
</ul>
</div></blockquote>
<p>Class attributes (overridden by derived classes):</p>
<blockquote>
<div><ul>
<li><p><strong>config_class</strong> (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ A subclass of
<a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> to use as configuration class for this model architecture.</p></li>
<li><p><strong>load_tf_weights</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>) â€“ A python <cite>method</cite> for loading a TensorFlow checkpoint in a PyTorch
model, taking as arguments:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>) â€“ An instance of the model on which to load the
TensorFlow checkpoint.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedConfig</span></code>) â€“ An instance of the configuration associated to
the model.</p></li>
<li><p><strong>path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ A path to the TensorFlow checkpoint.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>base_model_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ A string indicating the attribute associated to the base model in
derived classes of the same architecture adding modules on top of the base model.</p></li>
<li><p><strong>is_parallelizable</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) â€“ A flag indicating whether this model supports model parallelization.</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt id="transformers.PreTrainedModel.base_model"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.base_model"></a>
<em class="property">property </em><code class="sig-name descname">base_model</code><a class="headerlink" href="#transformers.PreTrainedModel.base_model" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The main body of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.dummy_inputs"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.dummy_inputs"></a>
<em class="property">property </em><code class="sig-name descname">dummy_inputs</code><a class="headerlink" href="#transformers.PreTrainedModel.dummy_inputs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Dummy inputs to do a forward pass in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (Dropout modules are deactivated). To
train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code>.</p>
<p>The warning <cite>Weights from XXX not initialized from pretrained model</cite> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <cite>Weights from XXX not used in YYY</cite> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <code class="docutils literal notranslate"><span class="pre">config</span></code> and <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>).</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (sequence of positional arguments, <cite>optional</cite>) â€“ All remaning positional arguments will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[PretrainedConfig,</span> <span class="pre">str,</span> <span class="pre">os.PathLike]</span></code>, <cite>optional</cite>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>an instance of a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>,</p></li>
<li><p>a string or path valid as input to <a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>.</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuation. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[str,</span> <span class="pre">os.PathLike]</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (i.e., do not try to download the model).</p></li>
<li><p><strong>use_auth_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>) â€“ The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>mirror</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) â€“ Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.</p></li>
<li><p><strong>kwargs</strong> (remaining dictionary of keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/my_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.get_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.get_input_embeddings"></a>
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ torch.nn.modules.module.Module<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.get_input_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the modelâ€™s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ torch.nn.modules.module.Module<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.get_output_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the modelâ€™s output embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A torch module mapping hidden states to vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.init_weights"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.init_weights"></a>
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.init_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.init_weights" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Initializes and prunes weights if needed.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.prune_heads"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.prune_heads"></a>
<code class="sig-name descname">prune_heads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">heads_to_prune</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.prune_heads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.prune_heads" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>heads_to_prune</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">List[int]]</span></code>) â€“ Dictionary with keys being selected layer indices (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) and associated values being the list of
heads to prune in said layer (list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>). For instance {1: [0, 2], 2: [2, 3]} will prune heads
0 and 2 on layer 1 and heads 2 and 3 on layer 2.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.resize_token_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.resize_token_embeddings"></a>
<code class="sig-name descname">resize_token_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ torch.nn.modules.sparse.Embedding<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.resize_token_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.resize_token_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resizes input token embeddings matrix of the model if <code class="xref py py-obj docutils literal notranslate"><span class="pre">new_num_tokens</span> <span class="pre">!=</span> <span class="pre">config.vocab_size</span></code>.</p>
<p>Takes care of tying weights embeddings afterwards if the model class has a <a class="reference internal" href="#transformers.PreTrainedModel.tie_weights" title="transformers.PreTrainedModel.tie_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tie_weights()</span></code></a> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_num_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The number of new tokens in the embedding matrix. Increasing the size will add newly initialized
vectors at the end. Reducing the size will remove vectors from the end. If not provided or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>,
just returns a pointer to the input tokens <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code> module of the model without doing
anything.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pointer to the input tokens Embeddings Module of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.save_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
<cite>:func:`~transformers.PreTrainedModel.from_pretrained`</cite> class method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ Directory to which to save. Will be created if it doesnâ€™t exist.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.set_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.set_input_embeddings"></a>
<code class="sig-name descname">set_input_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.set_input_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Set modelâ€™s input embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) â€“ A module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.PreTrainedModel.tie_weights"><a name="//apple_ref/cpp/Method/transformers.PreTrainedModel.tie_weights"></a>
<code class="sig-name descname">tie_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PreTrainedModel.tie_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PreTrainedModel.tie_weights" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Tie the weights between the input embeddings and the output embeddings.</p>
<p>If the <code class="xref py py-obj docutils literal notranslate"><span class="pre">torchscript</span></code> flag is set in the configuration, canâ€™t handle parameter sharing so we are cloning
the weights instead.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="moduleutilsmixin">
<h2>ModuleUtilsMixin<a class="headerlink" href="#moduleutilsmixin" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.modeling_utils.ModuleUtilsMixin"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.ModuleUtilsMixin"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">ModuleUtilsMixin</code><a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A few utilities for <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Modules</span></code>, to be used as a mixin.</p>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks"></a>
<code class="sig-name descname">add_memory_hooks</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.add_memory_hooks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</p>
<p>Increase in memory consumption is stored in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_rss_diff</span></code> attribute for each module and can be reset to
zero with <code class="xref py py-obj docutils literal notranslate"><span class="pre">model.reset_memory_hooks_state()</span></code>.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.device"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.device"></a>
<em class="property">property </em><code class="sig-name descname">device</code><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.device" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The device on which the module is (assuming that all the module parameters are on the same
device).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.dtype"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.dtype"></a>
<em class="property">property </em><code class="sig-name descname">dtype</code><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.dtype" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The dtype of the module (assuming that all the module parameters have the same dtype).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.estimate_tokens"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.estimate_tokens"></a>
<code class="sig-name descname">estimate_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> â†’ int<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.estimate_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.estimate_tokens" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Helper function to estimate the total number of tokens from the model inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>) â€“ The model inputs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The total number of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.floating_point_ops"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.floating_point_ops"></a>
<code class="sig-name descname">floating_point_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">exclude_embeddings</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> â†’ int<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.floating_point_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.floating_point_ops" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a
batch with this transformer model. Default approximation neglects the quadratic dependency on the number of
tokens (valid if <code class="xref py py-obj docutils literal notranslate"><span class="pre">12</span> <span class="pre">*</span> <span class="pre">d_model</span> <span class="pre">&lt;&lt;</span> <span class="pre">sequence_length</span></code>) as laid out in <a class="reference external" href="https://arxiv.org/pdf/2001.08361.pdf">this paper</a> section 2.1. Should be overridden for transformers with parameter
re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The batch size for the forward pass.</p></li>
<li><p><strong>sequence_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of tokens in each line of the batch.</p></li>
<li><p><strong>exclude_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) â€“ Whether or not to count embedding and softmax operations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of floating-point operations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask"></a>
<code class="sig-name descname">get_extended_attention_mask</code><span class="sig-paren">(</span><em class="sig-param">attention_mask: torch.Tensor, input_shape: Tuple[int], device: &lt;property object at 0x7ff9f08fa040&gt;</em><span class="sig-paren">)</span> â†’ torch.Tensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.get_extended_attention_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) â€“ Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</p></li>
<li><p><strong>input_shape</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[int]</span></code>) â€“ The shape of the input to the model.</p></li>
<li><p><strong>device</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>):
The device of the input to the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> The extended attention mask, with a the same dtype as <code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask.dtype</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.get_head_mask"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.get_head_mask"></a>
<code class="sig-name descname">get_head_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_hidden_layers</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">is_attention_chunked</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> â†’ torch.Tensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.get_head_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.get_head_mask" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prepare the head mask if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_heads]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_hidden_layers</span> <span class="pre">x</span> <span class="pre">num_heads]</span></code>, <cite>optional</cite>) â€“ The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of hidden layers in the model.</p></li>
<li><p><strong>is_attention_chunked</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional, defaults to :obj:`False</cite>):
Whether or not the attentions scores are computed by chunks or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[num_hidden_layers</span> <span class="pre">x</span> <span class="pre">batch</span> <span class="pre">x</span> <span class="pre">num_heads</span> <span class="pre">x</span> <span class="pre">seq_length</span> <span class="pre">x</span> <span class="pre">seq_length]</span></code> or
list with <code class="xref py py-obj docutils literal notranslate"><span class="pre">[None]</span></code> for each layer.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask"></a>
<code class="sig-name descname">invert_attention_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> â†’ torch.Tensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.invert_attention_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Invert an attention mask (e.g., switches 0. and 1.).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>encoder_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) â€“ An attention mask.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The inverted attention mask.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.num_parameters"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.num_parameters"></a>
<code class="sig-name descname">num_parameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">only_trainable</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">exclude_embeddings</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> â†’ int<a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.num_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.num_parameters" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get number of (optionally, trainable or non-embeddings) parameters in the module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>only_trainable</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to return only the number of trainable parameters</p></li>
<li><p><strong>exclude_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to return only the number of non-embeddings parameters</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of parameters.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state"></a>
<code class="sig-name descname">reset_memory_hooks_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#ModuleUtilsMixin.reset_memory_hooks_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Reset the <code class="xref py py-obj docutils literal notranslate"><span class="pre">mem_rss_diff</span></code> attribute of each module (see
<a class="reference internal" href="#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks" title="transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_memory_hooks()</span></code></a>).</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfpretrainedmodel">
<h2>TFPreTrainedModel<a class="headerlink" href="#tfpretrainedmodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFPreTrainedModel"><a name="//apple_ref/cpp/Class/transformers.TFPreTrainedModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFPreTrainedModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Base class for all TF models.</p>
<p><a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a> takes care of storing the configuration of the models and handles methods
for loading, downloading and saving models as well as a few methods common to all models to:</p>
<blockquote>
<div><ul class="simple">
<li><p>resize the input embeddings,</p></li>
<li><p>prune heads in the self-attention heads.</p></li>
</ul>
</div></blockquote>
<p>Class attributes (overridden by derived classes):</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>config_class</strong> (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ A subclass of
<a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> to use as configuration class for this model architecture.</p></li>
<li><p><strong>base_model_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ A string indicating the attribute associated to the base model in
derived classes of the same architecture adding modules on top of the base model.</p></li>
</ul>
</div></blockquote>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.dummy_inputs"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.dummy_inputs"></a>
<em class="property">property </em><code class="sig-name descname">dummy_inputs</code><a class="headerlink" href="#transformers.TFPreTrainedModel.dummy_inputs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Dummy inputs to build the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The dummy inputs.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">tf.Tensor]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate a pretrained TF 2.0 model from a pre-trained model configuration.</p>
<p>The warning <cite>Weights from XXX not initialized from pretrained model</cite> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <cite>Weights from XXX not used in YYY</cite> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <code class="docutils literal notranslate"><span class="pre">config</span></code> and <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>).</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (sequence of positional arguments, <cite>optional</cite>) â€“ All remaning positional arguments will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></li>
<li><p><strong>config</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[PretrainedConfig,</span> <span class="pre">str]</span></code>, <cite>optional</cite>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>an instance of a class derived from <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>,</p></li>
<li><p>a string valid as input to <a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>.</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuation. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="#transformers.TFPreTrainedModel.save_pretrained" title="transformers.TFPreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>from_pt</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):
Load the model weights from a PyTorch state_dict save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>):
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try doanloading the model).</p></li>
<li><p><strong>use_auth_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>) â€“ The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>mirror</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) â€“ Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.</p></li>
<li><p><strong>kwargs</strong> (remaining dictionary of keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">TFBertModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a Pytorch model file instead of a TensorFlow checkpoint (slower, for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/my_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/my_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_bias"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_bias"></a>
<code class="sig-name descname">get_bias</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>tensorflow.python.ops.variables.Variable<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_bias" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Dict of bias attached to an LM head. The key represents the name of the bias attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The weights representing the bias, None if not an LM model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_input_embeddings"></a>
<code class="sig-name descname">get_input_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ tensorflow.python.keras.engine.base_layer.Layer<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_input_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the modelâ€™s input embeddings layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The embeddings layer mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_lm_head"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_lm_head"></a>
<code class="sig-name descname">get_lm_head</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ tensorflow.python.keras.engine.base_layer.Layer<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_lm_head"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_lm_head" title="Permalink to this definition">Â¶</a></dt>
<dd><p>The LM Head layer. This method must be overwritten by all the models that have a lm head.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The LM head layer if the model has one, None if not.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_output_embeddings"></a>
<code class="sig-name descname">get_output_embeddings</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Union<span class="p">[</span>None<span class="p">, </span>tensorflow.python.keras.engine.base_layer.Layer<span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_output_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the modelâ€™s output embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The new weights mapping vocabulary to hidden states.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_output_layer_with_bias"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_output_layer_with_bias"></a>
<code class="sig-name descname">get_output_layer_with_bias</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Union<span class="p">[</span>None<span class="p">, </span>tensorflow.python.keras.engine.base_layer.Layer<span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_output_layer_with_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_output_layer_with_bias" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the layer that handles a bias attribute in case the model has an LM head with weights tied to the
embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The layer that handles the bias, None if not an LM model.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.get_prefix_bias_name"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.get_prefix_bias_name"></a>
<code class="sig-name descname">get_prefix_bias_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span> â†’ Union<span class="p">[</span>None<span class="p">, </span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.get_prefix_bias_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.get_prefix_bias_name" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the concatenated prefix name of the bias from the model name to the parent layer</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The prefix name of the bias.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.prune_heads"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.prune_heads"></a>
<code class="sig-name descname">prune_heads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">heads_to_prune</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.prune_heads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.prune_heads" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>heads_to_prune</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">List[int]]</span></code>) â€“ Dictionary with keys being selected layer indices (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) and associated values being the list of
heads to prune in said layer (list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>). For instance {1: [0, 2], 2: [2, 3]} will prune heads
0 and 2 on layer 1 and heads 2 and 3 on layer 2.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.resize_token_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.resize_token_embeddings"></a>
<code class="sig-name descname">resize_token_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_num_tokens</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ tensorflow.python.ops.variables.Variable<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.resize_token_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.resize_token_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resizes input token embeddings matrix of the model if <code class="xref py py-obj docutils literal notranslate"><span class="pre">new_num_tokens</span> <span class="pre">!=</span> <span class="pre">config.vocab_size</span></code>.</p>
<p>Takes care of tying weights embeddings afterwards if the model class has a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tie_weights()</span></code> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_num_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The number of new tokens in the embedding matrix. Increasing the size will add newly initialized
vectors at the end. Reducing the size will remove vectors from the end. If not provided or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>,
just returns a pointer to the input tokens <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code> module of the model without doing
anything.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pointer to the input tokens Embeddings Module of the model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span></em>, <em class="sig-param"><span class="n">saved_model</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">version</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.save_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
<a class="reference internal" href="#transformers.TFPreTrainedModel.from_pretrained" title="transformers.TFPreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ Directory to which to save. Will be created if it doesnâ€™t exist.</p></li>
<li><p><strong>saved_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ If the model has to be saved in saved model format as well or not.</p></li>
<li><p><strong>version</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ The version of the saved model. A saved model needs to be versioned in order to be properly loaded by
TensorFlow Serving as detailed in the official documentation
<a class="reference external" href="https://www.tensorflow.org/tfx/serving/serving_basic">https://www.tensorflow.org/tfx/serving/serving_basic</a></p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.serving"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.serving"></a>
<code class="sig-name descname">serving</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.serving"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.serving" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Method used for serving the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">tf.Tensor]</span></code>) â€“ The input of the saved model as a dictionnary of tensors.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.serving_output"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.serving_output"></a>
<code class="sig-name descname">serving_output</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.serving_output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.serving_output" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prepare the output of the saved model. Each model must implement this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">TFBaseModelOutput</span></code>) â€“ The output returned by the model.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.set_bias"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.set_bias"></a>
<code class="sig-name descname">set_bias</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.set_bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.set_bias" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Set all the bias in the LM head.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[tf.Variable]</span></code>) â€“ All the new bias attached to an LM head.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.set_input_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.set_input_embeddings"></a>
<code class="sig-name descname">set_input_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.set_input_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.set_input_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Set modelâ€™s input embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code>) â€“ The new weights mapping hidden states to vocabulary.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFPreTrainedModel.set_output_embeddings"><a name="//apple_ref/cpp/Method/transformers.TFPreTrainedModel.set_output_embeddings"></a>
<code class="sig-name descname">set_output_embeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFPreTrainedModel.set_output_embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFPreTrainedModel.set_output_embeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Set modelâ€™s output embeddings</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Variable</span></code>) â€“ The new weights mapping hidden states to vocabulary.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfmodelutilsmixin">
<h2>TFModelUtilsMixin<a class="headerlink" href="#tfmodelutilsmixin" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFModelUtilsMixin"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFModelUtilsMixin"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFModelUtilsMixin</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFModelUtilsMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFModelUtilsMixin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A few utilities for <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>, to be used as a mixin.</p>
<dl class="py method">
<dt id="transformers.modeling_tf_utils.TFModelUtilsMixin.num_parameters"><a name="//apple_ref/cpp/Method/transformers.modeling_tf_utils.TFModelUtilsMixin.num_parameters"></a>
<code class="sig-name descname">num_parameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">only_trainable</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> â†’ int<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFModelUtilsMixin.num_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFModelUtilsMixin.num_parameters" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get the number of (optionally, trainable) parameters in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>only_trainable</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to return only the number of trainable parameters</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of parameters.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="flaxpretrainedmodel">
<h2>FlaxPreTrainedModel<a class="headerlink" href="#flaxpretrainedmodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.FlaxPreTrainedModel"><a name="//apple_ref/cpp/Class/transformers.FlaxPreTrainedModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">FlaxPreTrainedModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/utils/dummy_flax_objects.html#FlaxPreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.FlaxPreTrainedModel" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>
</div>
<div class="section" id="generation">
<h2>Generation<a class="headerlink" href="#generation" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.generation_utils.GenerationMixin"><a name="//apple_ref/cpp/Class/transformers.generation_utils.GenerationMixin"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.generation_utils.</code><code class="sig-name descname">GenerationMixin</code><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A class containing all of the functions supporting generation, to be used as a mixin in
<a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>.</p>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.adjust_logits_during_generation"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.adjust_logits_during_generation"></a>
<code class="sig-name descname">adjust_logits_during_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> â†’ torch.FloatTensor<a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.adjust_logits_during_generation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.adjust_logits_during_generation" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implement in subclasses of <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> for custom behavior to adjust the logits in
the generate method.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.beam_sample"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.beam_sample"></a>
<code class="sig-name descname">beam_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">beam_scorer</span><span class="p">:</span> <span class="n">transformers.generation_beam_search.BeamScorer</span></em>, <em class="sig-param"><span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput" title="transformers.generation_utils.BeamSampleEncoderDecoderOutput">transformers.generation_utils.BeamSampleEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput" title="transformers.generation_utils.BeamSampleDecoderOnlyOutput">transformers.generation_utils.BeamSampleDecoderOnlyOutput</a><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.beam_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.beam_sample" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head using beam search with multinomial sampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>beam_scorer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">BeamScorer</span></code>) â€“ A derived instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> that defines how beam hypotheses are
constructed, stored and sorted during generation. For more information, the documentation of
<a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> should be read.</p></li>
<li><p><strong>logits_processor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessor" title="transformers.LogitsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessor</span></code></a> used to modify the prediction scores of the language modeling
head applied at each generation step.</p></li>
<li><p><strong>logits_warper</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsWarper" title="transformers.LogitsWarper"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsWarper</span></code></a> used to warp the prediction score distribution of the language
modeling head applied before multinomial sampling at each generation step.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific kwargs will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model. If
model is an encoder-decoder model the kwargs should include <code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_outputs</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput" title="transformers.generation_utils.BeamSampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleDecoderOnlyOutput</span></code></a>,
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput" title="transformers.generation_utils.BeamSampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleEncoderDecoderOutput</span></code></a> or obj:<cite>torch.LongTensor</cite>: A
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> containing the generated tokens (default behaviour) or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput" title="transformers.generation_utils.BeamSampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleDecoderOnlyOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput" title="transformers.generation_utils.BeamSampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleEncoderDecoderOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">LogitsProcessorList</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">TopKLogitsWarper</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">TemperatureLogitsWarper</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">BeamSearchScorer</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">"translate English to German: How old are you?"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run beam search using 3 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">"encoder_outputs"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span><span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_warper</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="mf">0.7</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">beam_sample</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.beam_search"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.beam_search"></a>
<code class="sig-name descname">beam_search</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">beam_scorer</span><span class="p">:</span> <span class="n">transformers.generation_beam_search.BeamScorer</span></em>, <em class="sig-param"><span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput">transformers.generation_utils.BeamSearchEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput">transformers.generation_utils.BeamSearchDecoderOnlyOutput</a><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.beam_search" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head using beam search decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>beam_scorer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">BeamScorer</span></code>) â€“ An derived instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> that defines how beam hypotheses are
constructed, stored and sorted during generation. For more information, the documentation of
<a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> should be read.</p></li>
<li><p><strong>logits_processor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessor" title="transformers.LogitsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessor</span></code></a> used to modify the prediction scores of the language modeling
head applied at each generation step.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific kwargs will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model. If
model is an encoder-decoder model the kwargs should include <code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_outputs</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">generation_utilsBeamSearchDecoderOnlyOutput</span></code>,
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchEncoderDecoderOutput</span></code></a> or obj:<cite>torch.LongTensor</cite>: A
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> containing the generated tokens (default behaviour) or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchDecoderOnlyOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchEncoderDecoderOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
<span class="gp">... </span>   <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">LogitsProcessorList</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">BeamSearchScorer</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">"translate English to German: How old are you?"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>


<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run beam search using 3 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">"encoder_outputs"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span><span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.generate"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.generate"></a>
<code class="sig-name descname">generate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">min_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_sample</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">early_stopping</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beams</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">temperature</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_k</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_p</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bad_words_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Iterable<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">length_penalty</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_repeat_ngram_size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_return_sequences</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beam_groups</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">diversity_penalty</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput" title="transformers.generation_utils.GreedySearchEncoderDecoderOutput">transformers.generation_utils.GreedySearchEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput" title="transformers.generation_utils.GreedySearchDecoderOnlyOutput">transformers.generation_utils.GreedySearchDecoderOnlyOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput" title="transformers.generation_utils.SampleEncoderDecoderOutput">transformers.generation_utils.SampleEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput" title="transformers.generation_utils.SampleDecoderOnlyOutput">transformers.generation_utils.SampleDecoderOnlyOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput">transformers.generation_utils.BeamSearchEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput">transformers.generation_utils.BeamSearchDecoderOnlyOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput" title="transformers.generation_utils.BeamSampleEncoderDecoderOutput">transformers.generation_utils.BeamSampleEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput" title="transformers.generation_utils.BeamSampleDecoderOnlyOutput">transformers.generation_utils.BeamSampleDecoderOnlyOutput</a><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.generate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
multinomial sampling, beam-search decoding, and beam-search multinomial sampling.</p>
<p>Apart from <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask</span></code>, all the arguments below will default to the value of the
attribute of the same name inside the <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> of the model. The default values
indicated are the default values of those config.</p>
<p>Most of these parameters are explained in more detail in <a class="reference external" href="https://huggingface.co/blog/how-to-generate">this blog post</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>min_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 10) â€“ The minimum length of the sequence to be generated.</p></li>
<li><p><strong>do_sample</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to use sampling ; use greedy decoding otherwise.</p></li>
<li><p><strong>early_stopping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether to stop the beam search when at least <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> sentences are finished per batch or not.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of beams for beam search. 1 means no beam search.</p></li>
<li><p><strong>temperature</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults tp 1.0) â€“ The value used to module the next token probabilities.</p></li>
<li><p><strong>top_k</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 50) â€“ The number of highest probability vocabulary tokens to keep for top-k-filtering.</p></li>
<li><p><strong>top_p</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code class="xref py py-obj docutils literal notranslate"><span class="pre">top_p</span></code> or
higher are kept for generation.</p></li>
<li><p><strong>repetition_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ The parameter for repetition penalty. 1.0 means no penalty. See <a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">this paper</a> for more details.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>bos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>beginning-of-sequence</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>length_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.</p></li>
<li><p><strong>no_repeat_ngram_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) â€“ If set to int &gt; 0, all ngrams of that size can only occur once.</p></li>
<li><p><strong>bad_words_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[int]]</span></code>, <cite>optional</cite>) â€“ List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer(bad_word,</span>
<span class="pre">add_prefix_space=True).input_ids</span></code>.</p></li>
<li><p><strong>num_return_sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ The number of independently computed returned sequences for each element in the batch.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ Mask to avoid performing attention on padding token indices. Mask values are in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, 1 for
tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same
shape as <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> that masks the pad token. <a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p></li>
<li><p><strong>decoder_start_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ If an encoder-decoder model starts decoding with a different token than <cite>bos</cite>, the id of that token.</p></li>
<li><p><strong>use_cache</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.</p></li>
<li><p><strong>num_beam_groups</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of groups to divide <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_beams</span></code> into in order to ensure diversity among different groups of
beams. <a class="reference external" href="https://arxiv.org/pdf/1610.02424.pdf">this paper</a> for more details.</p></li>
<li><p><strong>diversity_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.0) â€“ This value is subtracted from a beamâ€™s score if it generates a token same as any beam from other group
at a particular time. Note that <code class="xref py py-obj docutils literal notranslate"><span class="pre">diversity_penalty</span></code> is only effective if <code class="docutils literal notranslate"><span class="pre">group</span> <span class="pre">beam</span> <span class="pre">search</span></code> is
enabled.</p></li>
<li><p><strong>prefix_allowed_tokens_fn</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[[int,</span> <span class="pre">torch.Tensor],</span> <span class="pre">List[int]]</span></code>, <cite>optional</cite>):
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> and the batch ID
<code class="xref py py-obj docutils literal notranslate"><span class="pre">batch_id</span></code>. It has to return a list with the allowed tokens for the next generation step
conditioned on the previously generated tokens <code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> and the batch ID <code class="xref py py-obj docutils literal notranslate"><span class="pre">batch_id</span></code>. This
argument is useful for constrained generation conditioned on the prefix, as described in
<a class="reference external" href="https://arxiv.org/abs/2010.00904">Autoregressive Entity Retrieval</a>.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific kwargs will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model. If the
model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific
kwargs should be prefixed with <cite>decoder_</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A
<a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> (if <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or when
<code class="docutils literal notranslate"><span class="pre">config.return_dict_in_generate=True</span></code>) or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>.</p>
<blockquote>
<div><p>If the model is <cite>not</cite> an encoder-decoder model (<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code>), the
possible <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> types are:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput" title="transformers.generation_utils.GreedySearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchDecoderOnlyOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput" title="transformers.generation_utils.SampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleDecoderOnlyOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchDecoderOnlyOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput" title="transformers.generation_utils.BeamSampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleDecoderOnlyOutput</span></code></a></p></li>
</ul>
</div></blockquote>
<p>If the model is an encoder-decoder model (<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>), the possible
<a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> types are:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput" title="transformers.generation_utils.GreedySearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchEncoderDecoderOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput" title="transformers.generation_utils.SampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleEncoderDecoderOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchEncoderDecoderOutput</span></code></a>,</p></li>
<li><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput" title="transformers.generation_utils.BeamSampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSampleEncoderDecoderOutput</span></code></a></p></li>
</ul>
</div></blockquote>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></p>
</dd>
</dl>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilgpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilgpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># do greedy decoding without providing a prompt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">document</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span><span class="s2">"at least two people were killed in a suspected bomb attack on a passenger bus "</span>
<span class="gp">... </span><span class="s2">"in the strife-torn southern philippines on monday , the military said."</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># encode input contex</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># generate 3 independent sequences using beam search decoding (5 beams)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># with T5 encoder-decoder model conditioned on short news article.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilgpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilgpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_context</span> <span class="o">=</span> <span class="s2">"The dog"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># encode input context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># generate 3 candidates using sampling</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"ctrl"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"ctrl"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># "Legal" is one of the control codes for ctrl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_context</span> <span class="o">=</span> <span class="s2">"Legal My neighbor is"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># encode input context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_context</span> <span class="o">=</span> <span class="s2">"My cute dog"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get tokens of words that should not be generated</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bad_words_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">bad_word</span><span class="p">,</span> <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span> <span class="k">for</span> <span class="n">bad_word</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"idiot"</span><span class="p">,</span> <span class="s2">"stupid"</span><span class="p">,</span> <span class="s2">"shut up"</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># encode input context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># generate sequences without allowing bad_words to be generated</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.greedy_search"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.greedy_search"></a>
<code class="sig-name descname">greedy_search</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput" title="transformers.generation_utils.GreedySearchEncoderDecoderOutput">transformers.generation_utils.GreedySearchEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput" title="transformers.generation_utils.GreedySearchDecoderOnlyOutput">transformers.generation_utils.GreedySearchDecoderOnlyOutput</a><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.greedy_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.greedy_search" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head using greedy decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>logits_processor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessor" title="transformers.LogitsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessor</span></code></a> used to modify the prediction scores of the language modeling
head applied at each generation step.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific keyword arguments will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the
model. If model is an encoder-decoder model the kwargs should include <code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_outputs</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput" title="transformers.generation_utils.GreedySearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchDecoderOnlyOutput</span></code></a>,
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput" title="transformers.generation_utils.GreedySearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchEncoderDecoderOutput</span></code></a> or obj:<cite>torch.LongTensor</cite>: A
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> containing the generated tokens (default behaviour) or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput" title="transformers.generation_utils.GreedySearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchDecoderOnlyOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput" title="transformers.generation_utils.GreedySearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">GreedySearchEncoderDecoderOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
<span class="gp">... </span><span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="gp">... </span><span class="n">AutoModelForCausalLM</span><span class="p">,</span>
<span class="gp">... </span><span class="n">LogitsProcessorList</span><span class="p">,</span>
<span class="gp">... </span><span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_prompt</span> <span class="o">=</span> <span class="s2">"Today is a beautiful day, and"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">greedy_search</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.group_beam_search"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.group_beam_search"></a>
<code class="sig-name descname">group_beam_search</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">beam_scorer</span><span class="p">:</span> <span class="n">transformers.generation_beam_search.BeamScorer</span></em>, <em class="sig-param"><span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.group_beam_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.group_beam_search" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head using beam search decoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>beam_scorer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">BeamScorer</span></code>) â€“ An derived instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> that defines how beam hypotheses are
constructed, stored and sorted during generation. For more information, the documentation of
<a class="reference internal" href="../internal/generation_utils.html#transformers.BeamScorer" title="transformers.BeamScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamScorer</span></code></a> should be read.</p></li>
<li><p><strong>logits_processor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessor" title="transformers.LogitsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessor</span></code></a> used to modify the prediction scores of the language modeling
head applied at each generation step.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific kwargs that will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model. If
model is an encoder-decoder model the kwargs should include <code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_outputs</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchDecoderOnlyOutput</span></code></a>,
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchEncoderDecoderOutput</span></code></a> or obj:<cite>torch.LongTensor</cite>: A
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> containing the generated tokens (default behaviour) or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchDecoderOnlyOutput</span></code></a> if
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput" title="transformers.generation_utils.BeamSearchDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchDecoderOnlyOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput" title="transformers.generation_utils.BeamSearchEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">BeamSearchEncoderDecoderOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
<span class="gp">... </span>   <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">LogitsProcessorList</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">HammingDiversityLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">BeamSearchScorer</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"t5-base"</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">"translate English to German: How old are you?"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>


<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run diverse beam search using 6 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">"encoder_outputs"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span><span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">HammingDiversityLogitsProcessor</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">group_beam_search</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation"></a>
<code class="sig-name descname">prepare_inputs_for_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> â†’ Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.prepare_inputs_for_generation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implement in subclasses of <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> for custom behavior to prepare inputs in the
generate method.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_utils.GenerationMixin.sample"><a name="//apple_ref/cpp/Method/transformers.generation_utils.GenerationMixin.sample"></a>
<code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>transformers.generation_logits_process.LogitsProcessorList<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">model_kwargs</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput" title="transformers.generation_utils.SampleEncoderDecoderOutput">transformers.generation_utils.SampleEncoderDecoderOutput</a><span class="p">, </span><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput" title="transformers.generation_utils.SampleDecoderOnlyOutput">transformers.generation_utils.SampleDecoderOnlyOutput</a><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/generation_utils.html#GenerationMixin.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_utils.GenerationMixin.sample" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head using multinomial sampling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>logits_processor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessor" title="transformers.LogitsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessor</span></code></a> used to modify the prediction scores of the language modeling
head applied at each generation step.</p></li>
<li><p><strong>logits_warper</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code>, <cite>optional</cite>) â€“ An instance of <a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsProcessorList" title="transformers.LogitsProcessorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsProcessorList</span></code></a>. List of instances of class derived from
<a class="reference internal" href="../internal/generation_utils.html#transformers.LogitsWarper" title="transformers.LogitsWarper"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogitsWarper</span></code></a> used to warp the prediction score distribution of the language
modeling head applied before multinomial sampling at each generation step.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under
returned tensors for more details.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return trhe hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors
for more details.</p></li>
<li><p><strong>output_scores</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return the prediction scores. See <code class="docutils literal notranslate"><span class="pre">scores</span></code> under returned tensors for more details.</p></li>
<li><p><strong>return_dict_in_generate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <cite>False</cite>) â€“ Whether or not to return a <a class="reference internal" href="output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
<li><p><strong>model_kwargs</strong> â€“ Additional model specific kwargs will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model. If
model is an encoder-decoder model the kwargs should include <code class="xref py py-obj docutils literal notranslate"><span class="pre">encoder_outputs</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput" title="transformers.generation_utils.SampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleDecoderOnlyOutput</span></code></a>,
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput" title="transformers.generation_utils.SampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleEncoderDecoderOutput</span></code></a> or obj:<cite>torch.LongTensor</cite>: A
<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> containing the generated tokens (default behaviour) or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput" title="transformers.generation_utils.SampleDecoderOnlyOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleDecoderOnlyOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate=True</span></code> or a
<a class="reference internal" href="../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput" title="transformers.generation_utils.SampleEncoderDecoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SampleEncoderDecoderOutput</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">model.config.is_encoder_decoder=True</span></code>.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
<span class="gp">... </span>   <span class="n">AutoTokenizer</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">LogitsProcessorList</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">TopKLogitsWarper</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">TemperatureLogitsWarper</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"gpt2"</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">input_prompt</span> <span class="o">=</span> <span class="s2">"Today is a beautiful day, and"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_warper</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="mf">0.7</span><span class="p">),</span>
<span class="gp">... </span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">"Generated:"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.generation_tf_utils.TFGenerationMixin"><a name="//apple_ref/cpp/Class/transformers.generation_tf_utils.TFGenerationMixin"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.generation_tf_utils.</code><code class="sig-name descname">TFGenerationMixin</code><a class="reference internal" href="../_modules/transformers/generation_tf_utils.html#TFGenerationMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_tf_utils.TFGenerationMixin" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A class containing all of the functions supporting generation, to be used as a mixin in
<a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>.</p>
<dl class="py method">
<dt id="transformers.generation_tf_utils.TFGenerationMixin.adjust_logits_during_generation"><a name="//apple_ref/cpp/Method/transformers.generation_tf_utils.TFGenerationMixin.adjust_logits_during_generation"></a>
<code class="sig-name descname">adjust_logits_during_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/generation_tf_utils.html#TFGenerationMixin.adjust_logits_during_generation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_tf_utils.TFGenerationMixin.adjust_logits_during_generation" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implement in subclasses of <a class="reference internal" href="#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> for custom behavior to adjust the logits in
the generate method.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_tf_utils.TFGenerationMixin.generate"><a name="//apple_ref/cpp/Method/transformers.generation_tf_utils.TFGenerationMixin.generate"></a>
<code class="sig-name descname">generate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">min_length</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_sample</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">early_stopping</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_beams</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">temperature</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_k</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">top_p</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">repetition_penalty</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bad_words_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">bos_token_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">length_penalty</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_return_sequences</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">decoder_start_token_id</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/generation_tf_utils.html#TFGenerationMixin.generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_tf_utils.TFGenerationMixin.generate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.</p>
<p>Adapted in part from <a class="reference external" href="https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529">Facebookâ€™s XLM beam search code</a>.</p>
<p>Apart from <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">attention_mask</span></code>, all the arguments below will default to the value of the
attribute of the same name inside the <a class="reference internal" href="configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> of the model. The default values
indicated are the default values of those config.</p>
<p>Most of these parameters are explained in more detail in <a class="reference external" href="https://huggingface.co/blog/how-to-generate">this blog post</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">dtype=tf.int32</span></code> and shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ The sequence used as a prompt for the generation. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> the method initializes it as an empty
<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 20) â€“ The maximum length of the sequence to be generated.</p></li>
<li><p><strong>min_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 10) â€“ The minimum length of the sequence to be generated.</p></li>
<li><p><strong>do_sample</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to use sampling ; use greedy decoding otherwise.</p></li>
<li><p><strong>early_stopping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether to stop the beam search when at least <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> sentences are finished per batch or not.</p></li>
<li><p><strong>num_beams</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ Number of beams for beam search. 1 means no beam search.</p></li>
<li><p><strong>temperature</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ The value used to module the next token probabilities.</p></li>
<li><p><strong>top_k</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 50) â€“ The number of highest probability vocabulary tokens to keep for top-k-filtering.</p></li>
<li><p><strong>top_p</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code class="docutils literal notranslate"><span class="pre">top_p</span></code> or
higher are kept for generation.</p></li>
<li><p><strong>repetition_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ The parameter for repetition penalty. 1.0 means no penalty. See <a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">this paper</a> for more details.</p></li>
<li><p><strong>pad_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>padding</cite> token.</p></li>
<li><p><strong>bos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>beginning-of-sequence</cite> token.</p></li>
<li><p><strong>eos_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The id of the <cite>end-of-sequence</cite> token.</p></li>
<li><p><strong>length_penalty</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1.0) â€“ <p>Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.</p>
</p></li>
<li><p><strong>no_repeat_ngram_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) â€“ If set to int &gt; 0, all ngrams of that size can only occur once.</p></li>
<li><p><strong>bad_words_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) â€“ List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.encode(bad_word,</span> <span class="pre">add_prefix_space=True)</span></code>.</p></li>
<li><p><strong>num_return_sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ The number of independently computed returned sequences for each element in the batch.</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">dtype=tf.int32</span></code> and shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) â€“ <p>Mask to avoid performing attention on padding token indices. Mask values are in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, 1 for
tokens that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> that masks the pad token.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>decoder_start_token_id</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ If an encoder-decoder model starts decoding with a different token than <cite>bos</cite>, the id of that token.</p></li>
<li><p><strong>use_cache</strong> â€“ (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.</p></li>
<li><p><strong>model_specific_kwargs</strong> â€“ Additional model specific kwargs will be forwarded to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code> function of the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>obj:<cite>(batch_size * num_return_sequences,
sequence_length)</cite>: The generated sequences. The second dimension (sequence_length) is either equal to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or shorter if all batches finished early due to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">eos_token_id</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">dtype=tf.int32</span></code> and shape</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>  <span class="c1"># do greedy decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'openai-gpt'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'distilgpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'The dog'</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># generate 3 candidates using sampling</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1">#  3 output sequences were generated</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Generated </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'ctrl'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'Legal My neighbor is'</span>  <span class="c1"># "Legal" is one of the control codes for ctrl</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># generate sequences</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generated: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>   <span class="c1"># Initialize tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="n">input_context</span> <span class="o">=</span> <span class="s1">'My cute dog'</span>
<span class="n">bad_words_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">bad_word</span><span class="p">,</span> <span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">bad_word</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'idiot'</span><span class="p">,</span> <span class="s1">'stupid'</span><span class="p">,</span> <span class="s1">'shut up'</span><span class="p">]]</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)</span>  <span class="c1"># encode input context</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">)</span>  <span class="c1"># generate sequences without allowing bad_words to be generated</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.generation_tf_utils.TFGenerationMixin.prepare_inputs_for_generation"><a name="//apple_ref/cpp/Method/transformers.generation_tf_utils.TFGenerationMixin.prepare_inputs_for_generation"></a>
<code class="sig-name descname">prepare_inputs_for_generation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/generation_tf_utils.html#TFGenerationMixin.prepare_inputs_for_generation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.generation_tf_utils.TFGenerationMixin.prepare_inputs_for_generation" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implement in subclasses of <a class="reference internal" href="#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a> for custom behavior to prepare inputs in
the generate method.</p>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="optimizer_schedules.html" rel="next" title="Optimization">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="logging.html" rel="prev" title="Logging"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>