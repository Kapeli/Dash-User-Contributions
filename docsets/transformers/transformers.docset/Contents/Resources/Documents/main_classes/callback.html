
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Callbacks — transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="configuration.html" rel="next" title="Configuration"/>
<link href="../benchmarks.html" rel="prev" title="Benchmarks"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using 🤗 Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">🤗 Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Callbacks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#available-callbacks">Available Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainercallback">TrainerCallback</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainerstate">TrainerState</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainercontrol">TrainerControl</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> »</li>
<li>Callbacks</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/main_classes/callback.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="callbacks">
<h1>Callbacks<a class="headerlink" href="#callbacks" title="Permalink to this headline">¶</a></h1>
<p>Callbacks are objects that can customize the behavior of the training loop in the PyTorch
<a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> (this feature is not yet implemented in TensorFlow) that can inspect the training loop
state (for progress reporting, logging on TensorBoard or other ML platforms…) and take decisions (like early
stopping).</p>
<p>Callbacks are “read only” pieces of code, apart from the <a class="reference internal" href="#transformers.TrainerControl" title="transformers.TrainerControl"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerControl</span></code></a> object they return, they
cannot change anything in the training loop. For customizations that require changes in the training loop, you should
subclass <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and override the methods you need (see <a class="reference internal" href="trainer.html"><span class="doc">Trainer</span></a> for examples).</p>
<p>By default a <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> will use the following callbacks:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#transformers.DefaultFlowCallback" title="transformers.DefaultFlowCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultFlowCallback</span></code></a> which handles the default behavior for logging, saving and evaluation.</p></li>
<li><p><a class="reference internal" href="#transformers.PrinterCallback" title="transformers.PrinterCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">PrinterCallback</span></code></a> or <a class="reference internal" href="#transformers.ProgressCallback" title="transformers.ProgressCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProgressCallback</span></code></a> to display progress and print the
logs (the first one is used if you deactivate tqdm through the <a class="reference internal" href="trainer.html#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a>, otherwise
it’s the second one).</p></li>
<li><p><a class="reference internal" href="#transformers.integrations.TensorBoardCallback" title="transformers.integrations.TensorBoardCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorBoardCallback</span></code></a> if tensorboard is accessible (either through PyTorch &gt;= 1.4
or tensorboardX).</p></li>
<li><p><a class="reference internal" href="#transformers.integrations.WandbCallback" title="transformers.integrations.WandbCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">WandbCallback</span></code></a> if <a class="reference external" href="https://www.wandb.com/">wandb</a> is installed.</p></li>
<li><p><a class="reference internal" href="#transformers.integrations.CometCallback" title="transformers.integrations.CometCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">CometCallback</span></code></a> if <a class="reference external" href="https://www.comet.ml/site/">comet_ml</a> is installed.</p></li>
<li><p><a class="reference internal" href="#transformers.integrations.MLflowCallback" title="transformers.integrations.MLflowCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLflowCallback</span></code></a> if <a class="reference external" href="https://www.mlflow.org/">mlflow</a> is installed.</p></li>
<li><p><a class="reference internal" href="#transformers.integrations.AzureMLCallback" title="transformers.integrations.AzureMLCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">AzureMLCallback</span></code></a> if <a class="reference external" href="https://pypi.org/project/azureml-sdk/">azureml-sdk</a> is
installed.</p></li>
</ul>
<p>The main class that implements callbacks is <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a>. It gets the
<a class="reference internal" href="trainer.html#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a> used to instantiate the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>, can access that
Trainer’s internal state via <a class="reference internal" href="#transformers.TrainerState" title="transformers.TrainerState"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerState</span></code></a>, and can take some actions on the training loop via
<a class="reference internal" href="#transformers.TrainerControl" title="transformers.TrainerControl"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerControl</span></code></a>.</p>
<div class="section" id="available-callbacks">
<h2>Available Callbacks<a class="headerlink" href="#available-callbacks" title="Permalink to this headline">¶</a></h2>
<p>Here is the list of the available <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> in the library:</p>
<dl class="py class">
<dt id="transformers.integrations.CometCallback"><a name="//apple_ref/cpp/Class/transformers.integrations.CometCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.integrations.</code><code class="sig-name descname">CometCallback</code><a class="reference internal" href="../_modules/transformers/integrations.html#CometCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.CometCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that sends the logs to <a class="reference external" href="https://www.comet.ml/site/">Comet ML</a>.</p>
<dl class="py method">
<dt id="transformers.integrations.CometCallback.setup"><a name="//apple_ref/cpp/Method/transformers.integrations.CometCallback.setup"></a>
<code class="sig-name descname">setup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span></em>, <em class="sig-param"><span class="n">state</span></em>, <em class="sig-param"><span class="n">model</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/integrations.html#CometCallback.setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.CometCallback.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optional Comet.ml integration.</p>
<dl class="simple">
<dt>Environment:</dt><dd><dl class="simple">
<dt>COMET_MODE (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>“OFFLINE”, “ONLINE”, or “DISABLED”</p>
</dd>
<dt>COMET_PROJECT_NAME (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>Comet.ml project name for experiments</p>
</dd>
<dt>COMET_OFFLINE_DIRECTORY (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>Folder to use for saving offline experiments when <code class="xref py py-obj docutils literal notranslate"><span class="pre">COMET_MODE</span></code> is “OFFLINE”</p>
</dd>
</dl>
</dd>
</dl>
<p>For a number of configurable items in the environment, see <a class="reference external" href="https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables">here</a>.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.DefaultFlowCallback"><a name="//apple_ref/cpp/Class/transformers.DefaultFlowCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DefaultFlowCallback</code><a class="reference internal" href="../_modules/transformers/trainer_callback.html#DefaultFlowCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DefaultFlowCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that handles the default flow of the training loop for logs, evaluation
and checkpoints.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.PrinterCallback"><a name="//apple_ref/cpp/Class/transformers.PrinterCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">PrinterCallback</code><a class="reference internal" href="../_modules/transformers/trainer_callback.html#PrinterCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.PrinterCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A bare <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that just prints the logs.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.ProgressCallback"><a name="//apple_ref/cpp/Class/transformers.ProgressCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">ProgressCallback</code><a class="reference internal" href="../_modules/transformers/trainer_callback.html#ProgressCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.ProgressCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that displays the progress of training or evaluation.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.EarlyStoppingCallback"><a name="//apple_ref/cpp/Class/transformers.EarlyStoppingCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">EarlyStoppingCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">early_stopping_patience</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">early_stopping_threshold</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#EarlyStoppingCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.EarlyStoppingCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that handles early stopping.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>early_stopping_patience</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) – Use with <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> to stop training when the specified metric worsens for
<code class="xref py py-obj docutils literal notranslate"><span class="pre">early_stopping_patience</span></code> evaluation calls.</p></li>
<li><p><strong>early_stopping_threshold</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) – Use with TrainingArguments <code class="xref py py-obj docutils literal notranslate"><span class="pre">metric_for_best_model</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">early_stopping_patience</span></code> to denote how
much the specified metric must improve to satisfy early stopping conditions. `</p></li>
</ul>
</dd>
</dl>
<p>This callback depends on <a class="reference internal" href="trainer.html#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a> argument <cite>load_best_model_at_end</cite> functionality
to set best_metric in <a class="reference internal" href="#transformers.TrainerState" title="transformers.TrainerState"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerState</span></code></a>.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.integrations.TensorBoardCallback"><a name="//apple_ref/cpp/Class/transformers.integrations.TensorBoardCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.integrations.</code><code class="sig-name descname">TensorBoardCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tb_writer</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/integrations.html#TensorBoardCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.TensorBoardCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that sends the logs to <a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tb_writer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">SummaryWriter</span></code>, <cite>optional</cite>) – The writer to use. Will instantiate one if not set.</p>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.integrations.WandbCallback"><a name="//apple_ref/cpp/Class/transformers.integrations.WandbCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.integrations.</code><code class="sig-name descname">WandbCallback</code><a class="reference internal" href="../_modules/transformers/integrations.html#WandbCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.WandbCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that sends the logs to <a class="reference external" href="https://www.wandb.com/">Weight and Biases</a>.</p>
<dl class="py method">
<dt id="transformers.integrations.WandbCallback.setup"><a name="//apple_ref/cpp/Method/transformers.integrations.WandbCallback.setup"></a>
<code class="sig-name descname">setup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span></em>, <em class="sig-param"><span class="n">state</span></em>, <em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">reinit</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/integrations.html#WandbCallback.setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.WandbCallback.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optional Weights &amp; Biases (<cite>wandb</cite>) integration.</p>
<p>One can subclass and override this method to customize the setup if needed. Find more information <a class="reference external" href="https://docs.wandb.com/huggingface">here</a>. You can also override the following environment variables:</p>
<dl class="simple">
<dt>Environment:</dt><dd><dl class="simple">
<dt>WANDB_LOG_MODEL (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to log model as artifact at the end of training.</p>
</dd>
<dt>WANDB_WATCH (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite> defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gradients"</span></code>):</dt><dd><p>Can be <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gradients"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"all"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"false"</span></code>. Set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"false"</span></code> to disable gradient
logging or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"all"</span></code> to log gradients and parameters.</p>
</dd>
<dt>WANDB_PROJECT (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"huggingface"</span></code>):</dt><dd><p>Set this to a custom string to store results in a different project.</p>
</dd>
<dt>WANDB_DISABLED (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to disable wandb entirely.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.integrations.MLflowCallback"><a name="//apple_ref/cpp/Class/transformers.integrations.MLflowCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.integrations.</code><code class="sig-name descname">MLflowCallback</code><a class="reference internal" href="../_modules/transformers/integrations.html#MLflowCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.MLflowCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that sends the logs to <a class="reference external" href="https://www.mlflow.org/">MLflow</a>.</p>
<dl class="py method">
<dt id="transformers.integrations.MLflowCallback.setup"><a name="//apple_ref/cpp/Method/transformers.integrations.MLflowCallback.setup"></a>
<code class="sig-name descname">setup</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span></em>, <em class="sig-param"><span class="n">state</span></em>, <em class="sig-param"><span class="n">model</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/integrations.html#MLflowCallback.setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.MLflowCallback.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Setup the optional MLflow integration.</p>
<dl>
<dt>Environment:</dt><dd><dl>
<dt>HF_MLFLOW_LOG_ARTIFACTS (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>Whether to use MLflow .log_artifact() facility to log artifacts.</p>
<p>This only makes sense if logging to a remote server, e.g. s3 or GCS. If set to <cite>True</cite> or <cite>1</cite>, will copy
whatever is in TrainerArgument’s output_dir to the local or remote artifact storage. Using it without a
remote storage will just copy the files to your artifact location.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.integrations.AzureMLCallback"><a name="//apple_ref/cpp/Class/transformers.integrations.AzureMLCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.integrations.</code><code class="sig-name descname">AzureMLCallback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">azureml_run</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/integrations.html#AzureMLCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.integrations.AzureMLCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> that sends the logs to <a class="reference external" href="https://pypi.org/project/azureml-sdk/">AzureML</a>.</p>
</dd></dl>
</div>
<div class="section" id="trainercallback">
<h2>TrainerCallback<a class="headerlink" href="#trainercallback" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TrainerCallback"><a name="//apple_ref/cpp/Class/transformers.TrainerCallback"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TrainerCallback</code><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback" title="Permalink to this definition">¶</a></dt>
<dd><p>A class for objects that will inspect the state of the training loop at some events and take some decisions. At
each of those events the following arguments are available:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<a class="reference internal" href="trainer.html#transformers.TrainingArguments" title="transformers.TrainingArguments"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingArguments</span></code></a>) – The training arguments used to instantiate the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.</p></li>
<li><p><strong>state</strong> (<a class="reference internal" href="#transformers.TrainerState" title="transformers.TrainerState"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerState</span></code></a>) – The current state of the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.</p></li>
<li><p><strong>control</strong> (<a class="reference internal" href="#transformers.TrainerControl" title="transformers.TrainerControl"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerControl</span></code></a>) – The object that is returned to the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> and can be used to make some decisions.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>) – The model being trained.</p></li>
<li><p><strong>tokenizer</strong> (<a class="reference internal" href="tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a>) – The tokenizer used for encoding the data.</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>) – The optimizer used for the training steps.</p></li>
<li><p><strong>lr_scheduler</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.LambdaLR</span></code>) – The scheduler used for setting the learning rate.</p></li>
<li><p><strong>train_dataloader</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataloader.DataLoader</span></code>, <cite>optional</cite>) – The current dataloader used for training.</p></li>
<li><p><strong>eval_dataloader</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.dataloader.DataLoader</span></code>, <cite>optional</cite>) – The current dataloader used for training.</p></li>
<li><p><strong>metrics</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>) – <p>The metrics computed by the last evaluation phase.</p>
<p>Those are only accessible in the event <a class="reference internal" href="#transformers.TrainerCallback.on_evaluate" title="transformers.TrainerCallback.on_evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_evaluate</span></code></a>.</p>
</p></li>
<li><p><strong>logs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">float]</span></code>) – <p>The values to log.</p>
<p>Those are only accessible in the event <a class="reference internal" href="#transformers.TrainerCallback.on_log" title="transformers.TrainerCallback.on_log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_log</span></code></a>.</p>
</p></li>
</ul>
</dd>
</dl>
<p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">control</span></code> object is the only one that can be changed by the callback, in which case the event that changes
it should return the modified version.</p>
<p>The argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">args</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">state</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">control</span></code> are positionals for all events, all the others are
grouped in <code class="xref py py-obj docutils literal notranslate"><span class="pre">kwargs</span></code>. You can unpack the ones you need in the signature of the event using them. As an example,
see the code of the simple <code class="xref py py-class docutils literal notranslate"><span class="pre">PrinterCallback</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PrinterCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"total_flos"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_local_process_zero</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_epoch_begin"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_epoch_begin"></a>
<code class="sig-name descname">on_epoch_begin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_epoch_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_epoch_begin" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the beginning of an epoch.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_epoch_end"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_epoch_end"></a>
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the end of an epoch.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_evaluate"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_evaluate"></a>
<code class="sig-name descname">on_evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called after an evaluation phase.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_init_end"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_init_end"></a>
<code class="sig-name descname">on_init_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_init_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_init_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the end of the initialization of the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_log"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_log"></a>
<code class="sig-name descname">on_log</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_log"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called after logging the last logs.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_prediction_step"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_prediction_step"></a>
<code class="sig-name descname">on_prediction_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_prediction_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_prediction_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called after a prediction step.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_save"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_save"></a>
<code class="sig-name descname">on_save</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called after a checkpoint save.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_step_begin"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_step_begin"></a>
<code class="sig-name descname">on_step_begin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_step_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_step_begin" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the beginning of a training step. If using gradient accumulation, one training step might take
several inputs.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_step_end"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_step_end"></a>
<code class="sig-name descname">on_step_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_step_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the end of a training step. If using gradient accumulation, one training step might take
several inputs.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_train_begin"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_train_begin"></a>
<code class="sig-name descname">on_train_begin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_train_begin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_train_begin" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the beginning of training.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerCallback.on_train_end"><a name="//apple_ref/cpp/Method/transformers.TrainerCallback.on_train_end"></a>
<code class="sig-name descname">on_train_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span><span class="p">:</span> <span class="n">transformers.training_args.TrainingArguments</span></em>, <em class="sig-param"><span class="n">state</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerState</span></em>, <em class="sig-param"><span class="n">control</span><span class="p">:</span> <span class="n">transformers.trainer_callback.TrainerControl</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerCallback.on_train_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerCallback.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Event called at the end of training.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="trainerstate">
<h2>TrainerState<a class="headerlink" href="#trainerstate" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TrainerState"><a name="//apple_ref/cpp/Class/transformers.TrainerState"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TrainerState</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">max_steps</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">num_train_epochs</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">total_flos</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">log_history</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">best_metric</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">best_model_checkpoint</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">is_local_process_zero</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">is_world_process_zero</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">is_hyper_param_search</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">trial_name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trial_params</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>str<span class="p">, </span>float<span class="p">, </span>int<span class="p">, </span>bool<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerState"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerState" title="Permalink to this definition">¶</a></dt>
<dd><p>A class containing the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> inner state that will be saved along the model and optimizer
when checkpointing and passed to the <a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In all this class, one step is to be understood as one update step. When using gradient accumulation, one
update step may require several forward and backward passes: if you use <code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient_accumulation_steps=n</span></code>,
then one update step requires going throuch <cite>n</cite> batches.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) – Only set during training, will represent the epoch the training is at (the decimal part being the
percentage of the current epoch completed).</p></li>
<li><p><strong>global_step</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – During training, represents the number of update steps completed.</p></li>
<li><p><strong>max_steps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – The number of update steps to do during the current training.</p></li>
<li><p><strong>total_flos</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – The total number of floating operations done by the model since the beginning of training.</p></li>
<li><p><strong>log_history</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">float]]</span></code>, <cite>optional</cite>) – The list of logs done since the beginning of training.</p></li>
<li><p><strong>best_metric</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) – When tracking the best model, the value of the best metric encountered so far.</p></li>
<li><p><strong>best_model_checkpoint</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) – When tracking the best model, the value of the name of the checkpoint for the best model encountered so
far.</p></li>
<li><p><strong>is_local_process_zero</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on
several machines) main process.</p></li>
<li><p><strong>is_world_process_zero</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not this process is the global main process (when training in a distributed fashion on several
machines, this is only going to be <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> for one process).</p></li>
<li><p><strong>is_hyper_param_search</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – Whether we are in the process of a hyper parameter search using Trainer.hyperparameter_search. This will
impact the way data will be logged in TensorBoard.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TrainerState.load_from_json"><a name="//apple_ref/cpp/Method/transformers.TrainerState.load_from_json"></a>
<em class="property">classmethod </em><code class="sig-name descname">load_from_json</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">json_path</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerState.load_from_json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerState.load_from_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an instance from the content of <code class="xref py py-obj docutils literal notranslate"><span class="pre">json_path</span></code>.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.TrainerState.save_to_json"><a name="//apple_ref/cpp/Method/transformers.TrainerState.save_to_json"></a>
<code class="sig-name descname">save_to_json</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">json_path</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerState.save_to_json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerState.save_to_json" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the content of this instance in JSON format inside <code class="xref py py-obj docutils literal notranslate"><span class="pre">json_path</span></code>.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="trainercontrol">
<h2>TrainerControl<a class="headerlink" href="#trainercontrol" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TrainerControl"><a name="//apple_ref/cpp/Class/transformers.TrainerControl"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TrainerControl</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">should_training_stop</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">should_epoch_stop</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">should_save</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">should_evaluate</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">should_log</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#TrainerControl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TrainerControl" title="Permalink to this definition">¶</a></dt>
<dd><p>A class that handles the <a class="reference internal" href="trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a> control flow. This class is used by the
<a class="reference internal" href="#transformers.TrainerCallback" title="transformers.TrainerCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></a> to activate some switches in the training loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>should_training_stop</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not the training should be interrupted.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, this variable will not be set back to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>. The training will just stop.</p>
</p></li>
<li><p><strong>should_epoch_stop</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not the current epoch should be interrupted.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, this variable will be set back to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> at the beginning of the next epoch.</p>
</p></li>
<li><p><strong>should_save</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not the model should be saved at this step.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, this variable will be set back to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> at the beginning of the next step.</p>
</p></li>
<li><p><strong>should_evaluate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not the model should be evaluated at this step.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, this variable will be set back to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> at the beginning of the next step.</p>
</p></li>
<li><p><strong>should_log</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Whether or not the logs should be reported at this step.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, this variable will be set back to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> at the beginning of the next step.</p>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="configuration.html" rel="next" title="Configuration">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="../benchmarks.html" rel="prev" title="Benchmarks"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>