

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Benchmarks &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Callbacks" href="main_classes/callback.html" />
    <link rel="prev" title="Perplexity of fixed-length models" href="perplexity.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Benchmarks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#how-to-benchmark-transformer-models">How to benchmark ðŸ¤— Transformer models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#benchmark-best-practices">Benchmark best practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sharing-your-benchmark">Sharing your benchmark</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Benchmarks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/benchmarks.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="benchmarks">
<h1>Benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this headline">Â¶</a></h1>
<p>Letâ€™s take a look at how ðŸ¤— Transformer models can be benchmarked, best practices, and already available benchmarks.</p>
<p>A notebook explaining in more detail how to benchmark ðŸ¤— Transformer models can be found <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/notebooks/05-benchmark.ipynb">here</a>.</p>
<div class="section" id="how-to-benchmark-transformer-models">
<h2>How to benchmark ðŸ¤— Transformer models<a class="headerlink" href="#how-to-benchmark-transformer-models" title="Permalink to this headline">Â¶</a></h2>
<p>The classes <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchBenchmark</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorFlowBenchmark</span></code> allow to flexibly
benchmark ðŸ¤— Transformer models. The benchmark classes allow us to measure the <cite>peak memory usage</cite> and <cite>required time</cite>
for both <cite>inference</cite> and <cite>training</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hereby, <cite>inference</cite> is defined by a single forward pass, and <cite>training</cite> is defined by a single forward pass and
backward pass.</p>
</div>
<p>The benchmark classes <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchBenchmark</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorFlowBenchmark</span></code> expect an
object of type <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchBenchmarkArguments</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">TensorFlowBenchmarkArguments</span></code>, respectively, for instantiation.
<code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchBenchmarkArguments</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorFlowBenchmarkArguments</span></code> are data
classes and contain all relevant configurations for their corresponding benchmark class. In the following example, it
is shown how a BERT model of type <cite>bert-base-cased</cite> can be benchmarked.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">PyTorchBenchmark</span><span class="p">,</span> <span class="n">PyTorchBenchmarkArguments</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">PyTorchBenchmarkArguments</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">],</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">sequence_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">PyTorchBenchmark</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TensorFlowBenchmark</span><span class="p">,</span> <span class="n">TensorFlowBenchmarkArguments</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">TensorFlowBenchmarkArguments</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">],</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">sequence_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">TensorFlowBenchmark</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, three arguments are given to the benchmark argument data classes, namely <code class="docutils literal notranslate"><span class="pre">models</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_sizes</span></code>, and
<code class="docutils literal notranslate"><span class="pre">sequence_lengths</span></code>. The argument <code class="docutils literal notranslate"><span class="pre">models</span></code> is required and expects a <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> of model identifiers from the
<a class="reference external" href="https://huggingface.co/models">model hub</a> The <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> arguments <code class="docutils literal notranslate"><span class="pre">batch_sizes</span></code> and <code class="docutils literal notranslate"><span class="pre">sequence_lengths</span></code> define
the size of the <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> on which the model is benchmarked. There are many more parameters that can be configured
via the benchmark argument data classes. For more detail on these one can either directly consult the files
<code class="docutils literal notranslate"><span class="pre">src/transformers/benchmark/benchmark_args_utils.py</span></code>, <code class="docutils literal notranslate"><span class="pre">src/transformers/benchmark/benchmark_args.py</span></code> (for PyTorch)
and <code class="docutils literal notranslate"><span class="pre">src/transformers/benchmark/benchmark_args_tf.py</span></code> (for Tensorflow). Alternatively, running the following shell
commands from root will print out a descriptive list of all configurable parameters for PyTorch and Tensorflow
respectively.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## PYTORCH CODE</span>
python examples/benchmarking/run_benchmark.py --help

<span class="c1">## TENSORFLOW CODE</span>
python examples/benchmarking/run_benchmark_tf.py --help
</pre></div>
</div>
<p>An instantiated benchmark object can then simply be run by calling <code class="docutils literal notranslate"><span class="pre">benchmark.run()</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="go">====================       INFERENCE - SPEED - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length     Time in s</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base-uncased          8               8             0.006</span>
<span class="go">bert-base-uncased          8               32            0.006</span>
<span class="go">bert-base-uncased          8              128            0.018</span>
<span class="go">bert-base-uncased          8              512            0.088</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================      INFERENCE - MEMORY - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length    Memory in MB</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base-uncased          8               8             1227</span>
<span class="go">bert-base-uncased          8               32            1281</span>
<span class="go">bert-base-uncased          8              128            1307</span>
<span class="go">bert-base-uncased          8              512            1539</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================        ENVIRONMENT INFORMATION         ====================</span>

<span class="go">- transformers_version: 2.11.0</span>
<span class="go">- framework: PyTorch</span>
<span class="go">- use_torchscript: False</span>
<span class="go">- framework_version: 1.4.0</span>
<span class="go">- python_version: 3.6.10</span>
<span class="go">- system: Linux</span>
<span class="go">- cpu: x86_64</span>
<span class="go">- architecture: 64bit</span>
<span class="go">- date: 2020-06-29</span>
<span class="go">- time: 08:58:43.371351</span>
<span class="go">- fp16: False</span>
<span class="go">- use_multiprocessing: True</span>
<span class="go">- only_pretrain_model: False</span>
<span class="go">- cpu_ram_mb: 32088</span>
<span class="go">- use_gpu: True</span>
<span class="go">- num_gpus: 1</span>
<span class="go">- gpu: TITAN RTX</span>
<span class="go">- gpu_ram_mb: 24217</span>
<span class="go">- gpu_power_watts: 280.0</span>
<span class="go">- gpu_performance_state: 2</span>
<span class="go">- use_tpu: False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="go">====================       INFERENCE - SPEED - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length     Time in s</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base-uncased          8               8             0.005</span>
<span class="go">bert-base-uncased          8               32            0.008</span>
<span class="go">bert-base-uncased          8              128            0.022</span>
<span class="go">bert-base-uncased          8              512            0.105</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================      INFERENCE - MEMORY - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length    Memory in MB</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base-uncased          8               8             1330</span>
<span class="go">bert-base-uncased          8               32            1330</span>
<span class="go">bert-base-uncased          8              128            1330</span>
<span class="go">bert-base-uncased          8              512            1770</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================        ENVIRONMENT INFORMATION         ====================</span>

<span class="go">- transformers_version: 2.11.0</span>
<span class="go">- framework: Tensorflow</span>
<span class="go">- use_xla: False</span>
<span class="go">- framework_version: 2.2.0</span>
<span class="go">- python_version: 3.6.10</span>
<span class="go">- system: Linux</span>
<span class="go">- cpu: x86_64</span>
<span class="go">- architecture: 64bit</span>
<span class="go">- date: 2020-06-29</span>
<span class="go">- time: 09:26:35.617317</span>
<span class="go">- fp16: False</span>
<span class="go">- use_multiprocessing: True</span>
<span class="go">- only_pretrain_model: False</span>
<span class="go">- cpu_ram_mb: 32088</span>
<span class="go">- use_gpu: True</span>
<span class="go">- num_gpus: 1</span>
<span class="go">- gpu: TITAN RTX</span>
<span class="go">- gpu_ram_mb: 24217</span>
<span class="go">- gpu_power_watts: 280.0</span>
<span class="go">- gpu_performance_state: 2</span>
<span class="go">- use_tpu: False</span>
</pre></div>
</div>
<p>By default, the <cite>time</cite> and the <cite>required memory</cite> for <cite>inference</cite> are benchmarked. In the example output above the first
two sections show the result corresponding to <cite>inference time</cite> and <cite>inference memory</cite>. In addition, all relevant
information about the computing environment, <cite>e.g.</cite> the GPU type, the system, the library versions, etcâ€¦ are printed
out in the third section under <cite>ENVIRONMENT INFORMATION</cite>. This information can optionally be saved in a <cite>.csv</cite> file
when adding the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">save_to_csv=True</span></code> to <code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchBenchmarkArguments</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">TensorFlowBenchmarkArguments</span></code> respectively. In this case, every section is saved in a separate
<cite>.csv</cite> file. The path to each <cite>.csv</cite> file can optionally be defined via the argument data classes.</p>
<p>Instead of benchmarking pre-trained models via their model identifier, <cite>e.g.</cite> <cite>bert-base-uncased</cite>, the user can
alternatively benchmark an arbitrary configuration of any available model class. In this case, a <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> of
configurations must be inserted with the benchmark args as follows.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">## PYTORCH CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">PyTorchBenchmark</span><span class="p">,</span> <span class="n">PyTorchBenchmarkArguments</span><span class="p">,</span> <span class="n">BertConfig</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">PyTorchBenchmarkArguments</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bert-base&quot;</span><span class="p">,</span> <span class="s2">&quot;bert-384-hid&quot;</span><span class="p">,</span> <span class="s2">&quot;bert-6-lay&quot;</span><span class="p">],</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">sequence_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_base</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_384_hid</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">384</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_6_lay</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">PyTorchBenchmark</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">configs</span><span class="o">=</span><span class="p">[</span><span class="n">config_base</span><span class="p">,</span> <span class="n">config_384_hid</span><span class="p">,</span> <span class="n">config_6_lay</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="go">====================       INFERENCE - SPEED - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length       Time in s</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base                  8              128            0.006</span>
<span class="go">bert-base                  8              512            0.006</span>
<span class="go">bert-base                  8              128            0.018</span>
<span class="go">bert-base                  8              512            0.088</span>
<span class="go">bert-384-hid              8               8             0.006</span>
<span class="go">bert-384-hid              8               32            0.006</span>
<span class="go">bert-384-hid              8              128            0.011</span>
<span class="go">bert-384-hid              8              512            0.054</span>
<span class="go">bert-6-lay                 8               8             0.003</span>
<span class="go">bert-6-lay                 8               32            0.004</span>
<span class="go">bert-6-lay                 8              128            0.009</span>
<span class="go">bert-6-lay                 8              512            0.044</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================      INFERENCE - MEMORY - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length      Memory in MB</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base                  8               8             1277</span>
<span class="go">bert-base                  8               32            1281</span>
<span class="go">bert-base                  8              128            1307</span>
<span class="go">bert-base                  8              512            1539</span>
<span class="go">bert-384-hid              8               8             1005</span>
<span class="go">bert-384-hid              8               32            1027</span>
<span class="go">bert-384-hid              8              128            1035</span>
<span class="go">bert-384-hid              8              512            1255</span>
<span class="go">bert-6-lay                 8               8             1097</span>
<span class="go">bert-6-lay                 8               32            1101</span>
<span class="go">bert-6-lay                 8              128            1127</span>
<span class="go">bert-6-lay                 8              512            1359</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================        ENVIRONMENT INFORMATION         ====================</span>

<span class="go">- transformers_version: 2.11.0</span>
<span class="go">- framework: PyTorch</span>
<span class="go">- use_torchscript: False</span>
<span class="go">- framework_version: 1.4.0</span>
<span class="go">- python_version: 3.6.10</span>
<span class="go">- system: Linux</span>
<span class="go">- cpu: x86_64</span>
<span class="go">- architecture: 64bit</span>
<span class="go">- date: 2020-06-29</span>
<span class="go">- time: 09:35:25.143267</span>
<span class="go">- fp16: False</span>
<span class="go">- use_multiprocessing: True</span>
<span class="go">- only_pretrain_model: False</span>
<span class="go">- cpu_ram_mb: 32088</span>
<span class="go">- use_gpu: True</span>
<span class="go">- num_gpus: 1</span>
<span class="go">- gpu: TITAN RTX</span>
<span class="go">- gpu_ram_mb: 24217</span>
<span class="go">- gpu_power_watts: 280.0</span>
<span class="go">- gpu_performance_state: 2</span>
<span class="go">- use_tpu: False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1">## TENSORFLOW CODE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TensorFlowBenchmark</span><span class="p">,</span> <span class="n">TensorFlowBenchmarkArguments</span><span class="p">,</span> <span class="n">BertConfig</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">TensorFlowBenchmarkArguments</span><span class="p">(</span><span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bert-base&quot;</span><span class="p">,</span> <span class="s2">&quot;bert-384-hid&quot;</span><span class="p">,</span> <span class="s2">&quot;bert-6-lay&quot;</span><span class="p">],</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">sequence_lengths</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_base</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_384_hid</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">384</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config_6_lay</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">TensorFlowBenchmark</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">configs</span><span class="o">=</span><span class="p">[</span><span class="n">config_base</span><span class="p">,</span> <span class="n">config_384_hid</span><span class="p">,</span> <span class="n">config_6_lay</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">benchmark</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="go">====================       INFERENCE - SPEED - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length       Time in s</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base                  8               8             0.005</span>
<span class="go">bert-base                  8               32            0.008</span>
<span class="go">bert-base                  8              128            0.022</span>
<span class="go">bert-base                  8              512            0.106</span>
<span class="go">bert-384-hid              8               8             0.005</span>
<span class="go">bert-384-hid              8               32            0.007</span>
<span class="go">bert-384-hid              8              128            0.018</span>
<span class="go">bert-384-hid              8              512            0.064</span>
<span class="go">bert-6-lay                 8               8             0.002</span>
<span class="go">bert-6-lay                 8               32            0.003</span>
<span class="go">bert-6-lay                 8              128            0.0011</span>
<span class="go">bert-6-lay                 8              512            0.074</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================      INFERENCE - MEMORY - RESULT       ====================</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">Model Name             Batch Size     Seq Length      Memory in MB</span>
<span class="go">--------------------------------------------------------------------------------</span>
<span class="go">bert-base                  8               8             1330</span>
<span class="go">bert-base                  8               32            1330</span>
<span class="go">bert-base                  8              128            1330</span>
<span class="go">bert-base                  8              512            1770</span>
<span class="go">bert-384-hid              8               8             1330</span>
<span class="go">bert-384-hid              8               32            1330</span>
<span class="go">bert-384-hid              8              128            1330</span>
<span class="go">bert-384-hid              8              512            1540</span>
<span class="go">bert-6-lay                 8               8             1330</span>
<span class="go">bert-6-lay                 8               32            1330</span>
<span class="go">bert-6-lay                 8              128            1330</span>
<span class="go">bert-6-lay                 8              512            1540</span>
<span class="go">--------------------------------------------------------------------------------</span>

<span class="go">====================        ENVIRONMENT INFORMATION         ====================</span>

<span class="go">- transformers_version: 2.11.0</span>
<span class="go">- framework: Tensorflow</span>
<span class="go">- use_xla: False</span>
<span class="go">- framework_version: 2.2.0</span>
<span class="go">- python_version: 3.6.10</span>
<span class="go">- system: Linux</span>
<span class="go">- cpu: x86_64</span>
<span class="go">- architecture: 64bit</span>
<span class="go">- date: 2020-06-29</span>
<span class="go">- time: 09:38:15.487125</span>
<span class="go">- fp16: False</span>
<span class="go">- use_multiprocessing: True</span>
<span class="go">- only_pretrain_model: False</span>
<span class="go">- cpu_ram_mb: 32088</span>
<span class="go">- use_gpu: True</span>
<span class="go">- num_gpus: 1</span>
<span class="go">- gpu: TITAN RTX</span>
<span class="go">- gpu_ram_mb: 24217</span>
<span class="go">- gpu_power_watts: 280.0</span>
<span class="go">- gpu_performance_state: 2</span>
<span class="go">- use_tpu: False</span>
</pre></div>
</div>
<p>Again, <cite>inference time</cite> and <cite>required memory</cite> for <cite>inference</cite> are measured, but this time for customized configurations
of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">BertModel</span></code> class. This feature can especially be helpful when deciding for which configuration the model
should be trained.</p>
</div>
<div class="section" id="benchmark-best-practices">
<h2>Benchmark best practices<a class="headerlink" href="#benchmark-best-practices" title="Permalink to this headline">Â¶</a></h2>
<p>This section lists a couple of best practices one should be aware of when benchmarking a model.</p>
<ul class="simple">
<li><p>Currently, only single device benchmarking is supported. When benchmarking on GPU, it is recommended that the user
specifies on which device the code should be run by setting the <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environment variable in the
shell, <cite>e.g.</cite> <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">CUDA_VISIBLE_DEVICES=0</span></code> before running the code.</p></li>
<li><p>The option <code class="xref py py-obj docutils literal notranslate"><span class="pre">no_multi_processing</span></code> should only be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> for testing and debugging. To ensure accurate
memory measurement it is recommended to run each memory benchmark in a separate process by making sure
<code class="xref py py-obj docutils literal notranslate"><span class="pre">no_multi_processing</span></code> is set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>One should always state the environment information when sharing the results of a model benchmark. Results can vary
heavily between different GPU devices, library versions, etc., so that benchmark results on their own are not very
useful for the community.</p></li>
</ul>
</div>
<div class="section" id="sharing-your-benchmark">
<h2>Sharing your benchmark<a class="headerlink" href="#sharing-your-benchmark" title="Permalink to this headline">Â¶</a></h2>
<p>Previously all available core models (10 at the time) have been benchmarked for <cite>inference time</cite>, across many different
settings: using PyTorch, with and without TorchScript, using TensorFlow, with and without XLA. All of those tests were
done across CPUs (except for TensorFlow XLA) and GPUs.</p>
<p>The approach is detailed in the <a class="reference external" href="https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2">following blogpost</a> and the results are
available <a class="reference external" href="https://docs.google.com/spreadsheets/d/1sryqufw2D0XlUH4sq3e9Wnxu5EAQkaohzrJbd5HdQ_w/edit?usp=sharing">here</a>.</p>
<p>With the new <cite>benchmark</cite> tools, it is easier than ever to share your benchmark results with the community
<a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/benchmarking/README.md">here</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="main_classes/callback.html" class="btn btn-neutral float-right" title="Callbacks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="perplexity.html" class="btn btn-neutral float-left" title="Perplexity of fixed-length models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>