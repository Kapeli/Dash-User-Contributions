
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Custom Layers and Utilities â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="pipelines_utils.html" rel="next" title="Utilities for pipelines"/>
<link href="../model_doc/xlnet.html" rel="prev" title="XLNet"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Custom Layers and Utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pytorch-custom-modules">Pytorch custom modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pytorch-helper-functions">PyTorch Helper Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-custom-layers">TensorFlow custom layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-loss-functions">TensorFlow loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-helper-functions">TensorFlow Helper Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Custom Layers and Utilities</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/internal/modeling_utils.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="custom-layers-and-utilities">
<h1>Custom Layers and Utilities<a class="headerlink" href="#custom-layers-and-utilities" title="Permalink to this headline">Â¶</a></h1>
<p>This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling.</p>
<p>Most of those are only useful if you are studying the code of the models in the library.</p>
<div class="section" id="pytorch-custom-modules">
<h2>Pytorch custom modules<a class="headerlink" href="#pytorch-custom-modules" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.modeling_utils.Conv1D"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.Conv1D"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">Conv1D</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">nf</span></em>, <em class="sig-param"><span class="n">nx</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#Conv1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.Conv1D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).</p>
<p>Basically works like a linear layer but the weights are transposed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of output features.</p></li>
<li><p><strong>nx</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of input features.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.PoolerStartLogits"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.PoolerStartLogits"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">PoolerStartLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.configuration_utils.PretrainedConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerStartLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerStartLogits" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute SQuAD start logits from sequence hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ The config used by the model, will be used to grab the <code class="xref py py-obj docutils literal notranslate"><span class="pre">hidden_size</span></code> of the model.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_utils.PoolerStartLogits.forward"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.PoolerStartLogits.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ torch.FloatTensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerStartLogits.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerStartLogits.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>) â€“ The final hidden states of the model.</p></li>
<li><p><strong>p_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len)</span></code>, <cite>optional</cite>) â€“ Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The start logits for SQuAD.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.PoolerEndLogits"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.PoolerEndLogits"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">PoolerEndLogits</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.configuration_utils.PretrainedConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerEndLogits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerEndLogits" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute SQuAD end logits from sequence hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ The config used by the model, will be used to grab the <code class="xref py py-obj docutils literal notranslate"><span class="pre">hidden_size</span></code> of the model and the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">layer_norm_eps</span></code> to use.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_utils.PoolerEndLogits.forward"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.PoolerEndLogits.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">start_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ torch.FloatTensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerEndLogits.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerEndLogits.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>) â€“ The final hidden states of the model.</p></li>
<li><p><strong>start_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) â€“ The hidden states of the first tokens for the labeled span.</p></li>
<li><p><strong>start_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>) â€“ The position of the first token for the labeled span.</p></li>
<li><p><strong>p_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len)</span></code>, <cite>optional</cite>) â€“ Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One of <code class="docutils literal notranslate"><span class="pre">start_states</span></code> or <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> should be not obj:<cite>None</cite>. If both are set,
<code class="docutils literal notranslate"><span class="pre">start_positions</span></code> overrides <code class="docutils literal notranslate"><span class="pre">start_states</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The end logits for SQuAD.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.PoolerAnswerClass"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.PoolerAnswerClass"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">PoolerAnswerClass</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerAnswerClass"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerAnswerClass" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute SQuAD 2.0 answer class from classification and start tokens hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ The config used by the model, will be used to grab the <code class="xref py py-obj docutils literal notranslate"><span class="pre">hidden_size</span></code> of the model.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_utils.PoolerAnswerClass.forward"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.PoolerAnswerClass.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">start_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ torch.FloatTensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#PoolerAnswerClass.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.PoolerAnswerClass.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>) â€“ The final hidden states of the model.</p></li>
<li><p><strong>start_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) â€“ The hidden states of the first tokens for the labeled span.</p></li>
<li><p><strong>start_positions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>) â€“ The position of the first token for the labeled span.</p></li>
<li><p><strong>cls_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>) â€“ Position of the CLS token for each sentence in the batch. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, takes the last token.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One of <code class="docutils literal notranslate"><span class="pre">start_states</span></code> or <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> should be not obj:<cite>None</cite>. If both are set,
<code class="docutils literal notranslate"><span class="pre">start_positions</span></code> overrides <code class="docutils literal notranslate"><span class="pre">start_states</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The SQuAD 2.0 answer class.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.SquadHeadOutput"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.SquadHeadOutput"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">SquadHeadOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_top_log_probs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_top_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">end_top_log_probs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">end_top_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cls_logits</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#SquadHeadOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.SquadHeadOutput" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Base class for outputs of question answering models using a <a class="reference internal" href="#transformers.modeling_utils.SQuADHead" title="transformers.modeling_utils.SQuADHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">SQuADHead</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned if both <code class="xref py py-obj docutils literal notranslate"><span class="pre">start_positions</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">end_positions</span></code> are provided) â€“ Classification loss as the sum of start token, end token (and is_impossible if provided) classification
losses.</p></li>
<li><p><strong>start_top_log_probs</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the top config.start_n_top start token possibilities (beam-search).</p></li>
<li><p><strong>start_top_index</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Indices for the top config.start_n_top start token possibilities (beam-search).</p></li>
<li><p><strong>end_top_log_probs</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the top <code class="docutils literal notranslate"><span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top</span></code> end token possibilities
(beam-search).</p></li>
<li><p><strong>end_top_index</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Indices for the top <code class="docutils literal notranslate"><span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top</span></code> end token possibilities (beam-search).</p></li>
<li><p><strong>cls_logits</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the <code class="docutils literal notranslate"><span class="pre">is_impossible</span></code> label of the answers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.SQuADHead"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.SQuADHead"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">SQuADHead</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#SQuADHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.SQuADHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A SQuAD head inspired by XLNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ The config used by the model, will be used to grab the <code class="xref py py-obj docutils literal notranslate"><span class="pre">hidden_size</span></code> of the model and the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">layer_norm_eps</span></code> to use.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_utils.SQuADHead.forward"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.SQuADHead.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">end_positions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">is_impossible</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span><a class="reference internal" href="#transformers.modeling_utils.SquadHeadOutput" title="transformers.modeling_utils.SquadHeadOutput">transformers.modeling_utils.SquadHeadOutput</a><span class="p">, </span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#SQuADHead.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.SQuADHead.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><blockquote>
<div><dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size)</span></code>):</dt><dd><p>Final hidden states of the model on the sequence tokens.</p>
</dd>
<dt>start_positions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>):</dt><dd><p>Positions of the first token for the labeled span.</p>
</dd>
<dt>end_positions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>):</dt><dd><p>Positions of the last token for the labeled span.</p>
</dd>
<dt>cls_index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>):</dt><dd><p>Position of the CLS token for each sentence in the batch. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, takes the last token.</p>
</dd>
<dt>is_impossible (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>):</dt><dd><p>Whether the question has a possible answer in the paragraph or not.</p>
</dd>
<dt>p_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len)</span></code>, <cite>optional</cite>):</dt><dd><p>Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token
should be masked.</p>
</dd>
<dt>return_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="#transformers.modeling_utils.SquadHeadOutput" title="transformers.modeling_utils.SquadHeadOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquadHeadOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<code class="xref py py-class docutils literal notranslate"><span class="pre">~transformers.</span></code>) and inputs.</p>
<ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned if both <code class="xref py py-obj docutils literal notranslate"><span class="pre">start_positions</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">end_positions</span></code> are provided) â€“ Classification loss as the sum of start token, end token (and is_impossible if provided) classification
losses.</p></li>
<li><p><strong>start_top_log_probs</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the top config.start_n_top start token possibilities (beam-search).</p></li>
<li><p><strong>start_top_index</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Indices for the top config.start_n_top start token possibilities (beam-search).</p></li>
<li><p><strong>end_top_log_probs</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the top <code class="docutils literal notranslate"><span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top</span></code> end token possibilities
(beam-search).</p></li>
<li><p><strong>end_top_index</strong> (<code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Indices for the top <code class="docutils literal notranslate"><span class="pre">config.start_n_top</span> <span class="pre">*</span> <span class="pre">config.end_n_top</span></code> end token possibilities (beam-search).</p></li>
<li><p><strong>cls_logits</strong> (<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, returned if <code class="docutils literal notranslate"><span class="pre">start_positions</span></code> or <code class="docutils literal notranslate"><span class="pre">end_positions</span></code> is not provided) â€“ Log probabilities for the <code class="docutils literal notranslate"><span class="pre">is_impossible</span></code> label of the answers.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#transformers.modeling_utils.SquadHeadOutput" title="transformers.modeling_utils.SquadHeadOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">SquadHeadOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_utils.SequenceSummary"><a name="//apple_ref/cpp/Class/transformers.modeling_utils.SequenceSummary"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">SequenceSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.configuration_utils.PretrainedConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#SequenceSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.SequenceSummary" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute a single vector summary of a sequence hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The config used by the model. Relevant arguments in the config class of the model are (refer to the actual
config class of your model for the default values it uses):</p>
<ul>
<li><p><strong>summary_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ The method to use to make this summary. Accepted values are:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"last"</span></code> â€“ Take the last token hidden state (like XLNet)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"first"</span></code> â€“ Take the first token hidden state (like Bert)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"mean"</span></code> â€“ Take the mean of all tokens hidden states</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"cls_index"</span></code> â€“ Supply a Tensor of classification token position (GPT/GPT-2)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"attn"</span></code> â€“ Not implemented now, use multi-head attention</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>summary_use_proj</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) â€“ Add a projection after the vector extraction.</p></li>
<li><p><strong>summary_proj_to_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) â€“ If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the projection outputs to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span></code> classes (otherwise to <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.hidden_size</span></code>).</p></li>
<li><p><strong>summary_activation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[str]</span></code>) â€“ Set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"tanh"</span></code> to add a tanh activation to the
output, another string or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> will add no activation.</p></li>
<li><p><strong>summary_first_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) â€“ Optional dropout probability before the projection and
activation.</p></li>
<li><p><strong>summary_last_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>)â€“ Optional dropout probability after the projection and
activation.</p></li>
</ul>
</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_utils.SequenceSummary.forward"><a name="//apple_ref/cpp/Method/transformers.modeling_utils.SequenceSummary.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.LongTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ torch.FloatTensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#SequenceSummary.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.SequenceSummary.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute a single vector summary of a sequence hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_size]</span></code>) â€“ The hidden states of the last layer.</p></li>
<li><p><strong>cls_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">...]</span></code> where â€¦ are optional leading dimensions of <code class="xref py py-obj docutils literal notranslate"><span class="pre">hidden_states</span></code>, <cite>optional</cite>) â€“ Used if <code class="xref py py-obj docutils literal notranslate"><span class="pre">summary_type</span> <span class="pre">==</span> <span class="pre">"cls_index"</span></code> and takes the last token of the sequence as classification
token.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The summary of the sequence hidden states.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="pytorch-helper-functions">
<h2>PyTorch Helper Functions<a class="headerlink" href="#pytorch-helper-functions" title="Permalink to this headline">Â¶</a></h2>
<dl class="py function">
<dt id="transformers.apply_chunking_to_forward"><a name="//apple_ref/cpp/Function/transformers.apply_chunking_to_forward"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">apply_chunking_to_forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">forward_fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span><span class="p">â€¦</span><span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">chunk_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">chunk_dim</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">input_tensors</span></em><span class="sig-paren">)</span> â†’ torch.Tensor<a class="reference internal" href="../_modules/transformers/modeling_utils.html#apply_chunking_to_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.apply_chunking_to_forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This function chunks the <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_tensors</span></code> into smaller input tensor parts of size <code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk_size</span></code> over the
dimension <code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk_dim</span></code>. It then applies a layer <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward_fn</span></code> to each chunk independently to save memory.</p>
<p>If the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward_fn</span></code> is independent across the <code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk_dim</span></code> this function will yield the same result as
directly applying <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward_fn</span></code> to <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_tensors</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_fn</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable[...,</span> <span class="pre">torch.Tensor]</span></code>) â€“ The forward function of the model.</p></li>
<li><p><strong>chunk_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The chunk size of a chunked tensor: <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_chunks</span> <span class="pre">=</span> <span class="pre">len(input_tensors[0])</span> <span class="pre">/</span> <span class="pre">chunk_size</span></code>.</p></li>
<li><p><strong>chunk_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The dimension over which the <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_tensors</span></code> should be chunked.</p></li>
<li><p><strong>input_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor]</span></code>) â€“ The input tensors of <code class="docutils literal notranslate"><span class="pre">forward_fn</span></code> which will be chunked</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor with the same shape as the <code class="xref py py-obj docutils literal notranslate"><span class="pre">forward_fn</span></code> would have given if applied`.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># rename the usual forward() fn to forward_chunk()</span>
<span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_states</span>

<span class="c1"># implement a chunked forward function</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">apply_chunking_to_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_utils.find_pruneable_heads_and_indices"><a name="//apple_ref/cpp/Function/transformers.modeling_utils.find_pruneable_heads_and_indices"></a>
<code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">find_pruneable_heads_and_indices</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">heads</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">n_heads</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">head_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">already_pruned_heads</span><span class="p">:</span> <span class="n">Set<span class="p">[</span>int<span class="p">]</span></span></em><span class="sig-paren">)</span> â†’ Tuple<span class="p">[</span>Set<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>torch.LongTensor<span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#find_pruneable_heads_and_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.find_pruneable_heads_and_indices" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Finds the heads and their indices taking <code class="xref py py-obj docutils literal notranslate"><span class="pre">already_pruned_heads</span></code> into account.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) â€“ List of the indices of heads to prune.</p></li>
<li><p><strong>n_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of heads in the model.</p></li>
<li><p><strong>head_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The size of each head.</p></li>
<li><p><strong>already_pruned_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Set[int]</span></code>) â€“ A set of already pruned heads.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple with the remaining heads and their corresponding indices.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Set[int],</span> <span class="pre">torch.LongTensor]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_utils.prune_layer"><a name="//apple_ref/cpp/Function/transformers.modeling_utils.prune_layer"></a>
<code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">prune_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.nn.modules.linear.Linear<span class="p">, </span><a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D">transformers.modeling_utils.Conv1D</a><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">index</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ Union<span class="p">[</span>torch.nn.modules.linear.Linear<span class="p">, </span><a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D">transformers.modeling_utils.Conv1D</a><span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_utils.html#prune_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.prune_layer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prune a Conv1D or linear layer to keep only entries in index.</p>
<p>Used to remove heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.nn.Linear,</span> <span class="pre">Conv1D]</span></code>) â€“ The layer to prune.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>) â€“ The indices to keep in the layer.</p></li>
<li><p><strong>dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ The dimension on which to keep the indices.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The pruned layer as a new layer with
<code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> or <a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1D</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_utils.prune_conv1d_layer"><a name="//apple_ref/cpp/Function/transformers.modeling_utils.prune_conv1d_layer"></a>
<code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">prune_conv1d_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D">transformers.modeling_utils.Conv1D</a></span></em>, <em class="sig-param"><span class="n">index</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span> â†’ <a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D">transformers.modeling_utils.Conv1D</a><a class="reference internal" href="../_modules/transformers/modeling_utils.html#prune_conv1d_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.prune_conv1d_layer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights
are transposed.</p>
<p>Used to remove heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1D</span></code></a>) â€“ The layer to prune.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>) â€“ The indices to keep in the layer.</p></li>
<li><p><strong>dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) â€“ The dimension on which to keep the indices.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The pruned layer as a new layer with <code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.modeling_utils.Conv1D" title="transformers.modeling_utils.Conv1D"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1D</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_utils.prune_linear_layer"><a name="//apple_ref/cpp/Function/transformers.modeling_utils.prune_linear_layer"></a>
<code class="sig-prename descclassname">transformers.modeling_utils.</code><code class="sig-name descname">prune_linear_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n">torch.nn.modules.linear.Linear</span></em>, <em class="sig-param"><span class="n">index</span><span class="p">:</span> <span class="n">torch.LongTensor</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> â†’ torch.nn.modules.linear.Linear<a class="reference internal" href="../_modules/transformers/modeling_utils.html#prune_linear_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_utils.prune_linear_layer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Prune a linear layer to keep only entries in index.</p>
<p>Used to remove heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code>) â€“ The layer to prune.</p></li>
<li><p><strong>index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>) â€“ The indices to keep in the layer.</p></li>
<li><p><strong>dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) â€“ The dimension on which to keep the indices.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The pruned layer as a new layer with <code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code></p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="tensorflow-custom-layers">
<h2>TensorFlow custom layers<a class="headerlink" href="#tensorflow-custom-layers" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFConv1D"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFConv1D"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFConv1D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFConv1D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFConv1D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).</p>
<p>Basically works like a linear layer but the weights are transposed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of output features.</p></li>
<li><p><strong>nx</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of input features.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) â€“ The standard deviation to use to initialize the weights.</p></li>
<li><p><strong>kwargs</strong> â€“ Additional keyword arguments passed along to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFSharedEmbeddings"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFSharedEmbeddings"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFSharedEmbeddings</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFSharedEmbeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFSharedEmbeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Construct shared token embeddings.</p>
<p>The weights of the embedding layer is usually shared with the weights of the linear decoder when doing language
modeling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The size of the vocabulary, e.g., the number of unique tokens.</p></li>
<li><p><strong>hidden_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The size of the embedding vectors.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>) â€“ The standard deviation to use when initializing the weights. If no value is provided, it will default to
<span class="math notranslate nohighlight">\(1/\sqrt{hidden\_size}\)</span>.</p></li>
<li><p><strong>kwargs</strong> â€“ Additional keyword arguments passed along to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_tf_utils.TFSharedEmbeddings.call"><a name="//apple_ref/cpp/Method/transformers.modeling_tf_utils.TFSharedEmbeddings.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">mode</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'embedding'</span></em><span class="sig-paren">)</span> â†’ tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFSharedEmbeddings.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFSharedEmbeddings.call" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get token embeddings of inputs or decode final hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code>) â€“ <p>In embedding mode, should be an int64 tensor with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">length]</span></code>.</p>
<p>In linear mode, should be a float tensor with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">length,</span> <span class="pre">hidden_size]</span></code>.</p>
</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"embedding"</span></code>) â€“ A valid value is either <code class="xref py py-obj docutils literal notranslate"><span class="pre">"embedding"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"linear"</span></code>, the first one indicates that the layer
should be used as an embedding layer, the second one that the layer should be used as a linear decoder.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>In embedding mode, the output is a float32 embedding tensor, with shape
<code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">length,</span> <span class="pre">embedding_size]</span></code>.</p>
<p>In linear mode, the output is a float32 with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">length,</span> <span class="pre">vocab_size]</span></code>.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> â€“ if <code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code> is not valid.</p>
</dd>
</dl>
<p>Shared weights logic is adapted from <a class="reference external" href="https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24">here</a>.</p>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFSequenceSummary"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFSequenceSummary"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFSequenceSummary</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFSequenceSummary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFSequenceSummary" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute a single vector summary of a sequence hidden states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The config used by the model. Relevant arguments in the config class of the model are (refer to the actual
config class of your model for the default values it uses):</p>
<ul>
<li><p><strong>summary_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ The method to use to make this summary. Accepted values are:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"last"</span></code> â€“ Take the last token hidden state (like XLNet)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"first"</span></code> â€“ Take the first token hidden state (like Bert)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"mean"</span></code> â€“ Take the mean of all tokens hidden states</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"cls_index"</span></code> â€“ Supply a Tensor of classification token position (GPT/GPT-2)</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">"attn"</span></code> â€“ Not implemented now, use multi-head attention</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>summary_use_proj</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) â€“ Add a projection after the vector extraction.</p></li>
<li><p><strong>summary_proj_to_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) â€“ If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, the projection outputs to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span></code> classes (otherwise to <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.hidden_size</span></code>).</p></li>
<li><p><strong>summary_activation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[str]</span></code>) â€“ Set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"tanh"</span></code> to add a tanh activation to the
output, another string or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> will add no activation.</p></li>
<li><p><strong>summary_first_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) â€“ Optional dropout probability before the projection and
activation.</p></li>
<li><p><strong>summary_last_dropout</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>)â€“ Optional dropout probability after the projection and
activation.</p></li>
</ul>
</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, defaults to 0.02) â€“ The standard deviation to use to initialize the weights.</p></li>
<li><p><strong>kwargs</strong> â€“ Additional keyword arguments passed along to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layer</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.modeling_tf_utils.TFSequenceSummary.call"><a name="//apple_ref/cpp/Method/transformers.modeling_tf_utils.TFSequenceSummary.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">cls_index</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">training</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFSequenceSummary.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFSequenceSummary.call" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is where the layerâ€™s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> â€“ Input tensor, or list/tuple of input tensors.</p></li>
<li><p><strong>**kwargs</strong> â€“ Additional keyword arguments. Currently unused.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tensorflow-loss-functions">
<h2>TensorFlow loss functions<a class="headerlink" href="#tensorflow-loss-functions" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFCausalLanguageModelingLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFCausalLanguageModelingLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFCausalLanguageModelingLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFCausalLanguageModelingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFCausalLanguageModelingLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for causal language modeling (CLM), that is, the task of guessing the next token.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.</p>
</div>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFMaskedLanguageModelingLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFMaskedLanguageModelingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for masked language modeling (MLM), that is, the task of guessing the masked tokens.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.</p>
</div>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFMultipleChoiceLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFMultipleChoiceLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFMultipleChoiceLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFMultipleChoiceLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFMultipleChoiceLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for multiple choice tasks.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFQuestionAnsweringLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFQuestionAnsweringLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFQuestionAnsweringLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFQuestionAnsweringLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFQuestionAnsweringLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for question answering.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFSequenceClassificationLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFSequenceClassificationLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFSequenceClassificationLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFSequenceClassificationLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFSequenceClassificationLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for sequence classification.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.modeling_tf_utils.TFTokenClassificationLoss"><a name="//apple_ref/cpp/Class/transformers.modeling_tf_utils.TFTokenClassificationLoss"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">TFTokenClassificationLoss</code><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#TFTokenClassificationLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.TFTokenClassificationLoss" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loss function suitable for token classification.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any label of -100 will be ignored (along with the corresponding logits) in the loss computation.</p>
</div>
</dd></dl>
</div>
<div class="section" id="tensorflow-helper-functions">
<h2>TensorFlow Helper Functions<a class="headerlink" href="#tensorflow-helper-functions" title="Permalink to this headline">Â¶</a></h2>
<dl class="py function">
<dt id="transformers.modeling_tf_utils.get_initializer"><a name="//apple_ref/cpp/Function/transformers.modeling_tf_utils.get_initializer"></a>
<code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">get_initializer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">initializer_range</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.02</span></em><span class="sig-paren">)</span> â†’ tensorflow.python.keras.initializers.initializers_v2.TruncatedNormal<a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#get_initializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.get_initializer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Creates a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.initializers.TruncatedNormal</span></code> with the given range.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>initializer_range</strong> (<cite>float</cite>, defaults to 0.02) â€“ Standard deviation of the initializer range.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The truncated normal initializer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.initializers.TruncatedNormal</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_tf_utils.keras_serializable"><a name="//apple_ref/cpp/Function/transformers.modeling_tf_utils.keras_serializable"></a>
<code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">keras_serializable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cls</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#keras_serializable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.keras_serializable" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Decorate a Keras Layer class to support Keras serialization.</p>
<p>This is done by:</p>
<ol class="arabic simple">
<li><p>Adding a <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers_config</span></code> dict to the Keras config dictionary in <code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code> (called by Keras at
serialization time.</p></li>
<li><p>Wrapping <code class="xref py py-obj docutils literal notranslate"><span class="pre">__init__</span></code> to accept that <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers_config</span></code> dict (passed by Keras at deserialization
time) and convert it to a config object for the actual layer initializer.</p></li>
<li><p>Registering the class as a custom object in Keras (if the Tensorflow version supports this), so that it does not
need to be supplied in <code class="xref py py-obj docutils literal notranslate"><span class="pre">custom_objects</span></code> in the call to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.models.load_model</span></code>.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cls</strong> (a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.layers.Layers</span> <span class="pre">subclass</span></code>) â€“ Typically a <code class="xref py py-obj docutils literal notranslate"><span class="pre">TF.MainLayer</span></code> class in this project, in general must accept a <code class="xref py py-obj docutils literal notranslate"><span class="pre">config</span></code> argument to
its initializer.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same class object, with modifications for Keras deserialization.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.modeling_tf_utils.shape_list"><a name="//apple_ref/cpp/Function/transformers.modeling_tf_utils.shape_list"></a>
<code class="sig-prename descclassname">transformers.modeling_tf_utils.</code><code class="sig-name descname">shape_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> â†’ List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/modeling_tf_utils.html#shape_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.modeling_tf_utils.shape_list" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Deal with dynamic shape in tensorflow cleanly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code>) â€“ The tensor we want the shape of.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The shape of the tensor as a list.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="pipelines_utils.html" rel="next" title="Utilities for pipelines">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="../model_doc/xlnet.html" rel="prev" title="XLNet"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>