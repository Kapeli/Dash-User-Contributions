
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Utilities for Trainer â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="generation_utils.html" rel="next" title="Utilities for Generation"/>
<link href="tokenization_utils.html" rel="prev" title="Utilities for Tokenizers"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Utilities for Trainer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#callbacks-internals">Callbacks internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-evaluation">Distributed Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Distributed Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Utilities for Trainer</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/internal/trainer_utils.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="utilities-for-trainer">
<h1>Utilities for Trainer<a class="headerlink" href="#utilities-for-trainer" title="Permalink to this headline">Â¶</a></h1>
<p>This page lists all the utility functions used by <a class="reference internal" href="../main_classes/trainer.html#transformers.Trainer" title="transformers.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a>.</p>
<p>Most of those are only useful if you are studying the code of the Trainer in the library.</p>
<div class="section" id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.EvalPrediction"><a name="//apple_ref/cpp/Class/transformers.EvalPrediction"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">EvalPrediction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">predictions</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>numpy.ndarray<span class="p">, </span>Tuple<span class="p">[</span>numpy.ndarray<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">label_ids</span><span class="p">:</span> <span class="n">numpy.ndarray</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_utils.html#EvalPrediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.EvalPrediction" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Evaluation output (always contains labels), to be used to compute metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>) â€“ Predictions of the model.</p></li>
<li><p><strong>label_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>) â€“ Targets to be matched.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.EvaluationStrategy"><a name="//apple_ref/cpp/Class/transformers.EvaluationStrategy"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">EvaluationStrategy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_utils.html#EvaluationStrategy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.EvaluationStrategy" title="Permalink to this definition">Â¶</a></dt>
<dd><p>An enumeration.</p>
</dd></dl>
<dl class="py function">
<dt id="transformers.set_seed"><a name="//apple_ref/cpp/Function/transformers.set_seed"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">set_seed</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">seed</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_utils.html#set_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.set_seed" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Helper function for reproducible behavior to set the seed in <code class="docutils literal notranslate"><span class="pre">random</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, <code class="docutils literal notranslate"><span class="pre">torch</span></code> and/or <code class="docutils literal notranslate"><span class="pre">tf</span></code> (if
installed).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The seed to set.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt id="transformers.torch_distributed_zero_first"><a name="//apple_ref/cpp/Function/transformers.torch_distributed_zero_first"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">torch_distributed_zero_first</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">local_rank</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_pt_utils.html#torch_distributed_zero_first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.torch_distributed_zero_first" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Decorator to make all processes in distributed training wait for each local_master to do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>local_rank</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The rank of the local process.</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="callbacks-internals">
<h2>Callbacks internals<a class="headerlink" href="#callbacks-internals" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.trainer_callback.CallbackHandler"><a name="//apple_ref/cpp/Class/transformers.trainer_callback.CallbackHandler"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.trainer_callback.</code><code class="sig-name descname">CallbackHandler</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">callbacks</span></em>, <em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">tokenizer</span></em>, <em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">lr_scheduler</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_callback.html#CallbackHandler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.trainer_callback.CallbackHandler" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Internal class that just calls the list of callbacks in order.</p>
</dd></dl>
</div>
<div class="section" id="distributed-evaluation">
<h2>Distributed Evaluation<a class="headerlink" href="#distributed-evaluation" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.trainer_pt_utils.DistributedTensorGatherer"><a name="//apple_ref/cpp/Class/transformers.trainer_pt_utils.DistributedTensorGatherer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.trainer_pt_utils.</code><code class="sig-name descname">DistributedTensorGatherer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">world_size</span></em>, <em class="sig-param"><span class="n">num_samples</span></em>, <em class="sig-param"><span class="n">make_multiple_of</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">padding_index</span><span class="o">=</span><span class="default_value">- 100</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_pt_utils.html#DistributedTensorGatherer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.trainer_pt_utils.DistributedTensorGatherer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A class responsible for properly gathering tensors (or nested list/tuple of tensors) on the CPU by chunks.</p>
<p>If our dataset has 16 samples with a batch size of 2 on 3 processes and we gather then transfer on CPU at every
step, our sampler will generate the following indices:</p>
<blockquote>
<div><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">9,</span> <span class="pre">10,</span> <span class="pre">11,</span> <span class="pre">12,</span> <span class="pre">13,</span> <span class="pre">14,</span> <span class="pre">15,</span> <span class="pre">0,</span> <span class="pre">1]</span></code></p>
</div></blockquote>
<p>to get something of size a multiple of 3 (so that each process gets the same dataset length). Then process 0, 1 and
2 will be responsible of making predictions for the following samples:</p>
<blockquote>
<div><ul class="simple">
<li><p>P0: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5]</span></code></p></li>
<li><p>P1: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[6,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">9,</span> <span class="pre">10,</span> <span class="pre">11]</span></code></p></li>
<li><p>P2: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[12,</span> <span class="pre">13,</span> <span class="pre">14,</span> <span class="pre">15,</span> <span class="pre">0,</span> <span class="pre">1]</span></code></p></li>
</ul>
</div></blockquote>
<p>The first batch treated on each process will be</p>
<blockquote>
<div><ul class="simple">
<li><p>P0: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code></p></li>
<li><p>P1: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[6,</span> <span class="pre">7]</span></code></p></li>
<li><p>P2: <code class="xref py py-obj docutils literal notranslate"><span class="pre">[12,</span> <span class="pre">13]</span></code></p></li>
</ul>
</div></blockquote>
<p>So if we gather at the end of the first batch, we will get a tensor (nested list/tuple of tensor) corresponding to
the following indices:</p>
<blockquote>
<div><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">12,</span> <span class="pre">13]</span></code></p>
</div></blockquote>
<p>If we directly concatenate our results without taking any precautions, the user will then get the predictions for
the indices in this order at the end of the prediction loop:</p>
<blockquote>
<div><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">12,</span> <span class="pre">13,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">8,</span> <span class="pre">9,</span> <span class="pre">14,</span> <span class="pre">15,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">10,</span> <span class="pre">11,</span> <span class="pre">0,</span> <span class="pre">1]</span></code></p>
</div></blockquote>
<p>For some reason, thatâ€™s not going to roll their boat. This class is there to solve that problem.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>world_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of processes used in the distributed training.</p></li>
<li><p><strong>num_samples</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) â€“ The number of samples in our dataset.</p></li>
<li><p><strong>make_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) â€“ If passed, the class assumes the datasets passed to each process are made to be a multiple of this argument
(by adding samples).</p></li>
<li><p><strong>padding_index</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to -100) â€“ The padding index to use if the arrays donâ€™t all have the same sequence length.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.trainer_pt_utils.DistributedTensorGatherer.add_arrays"><a name="//apple_ref/cpp/Method/transformers.trainer_pt_utils.DistributedTensorGatherer.add_arrays"></a>
<code class="sig-name descname">add_arrays</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">arrays</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_pt_utils.html#DistributedTensorGatherer.add_arrays"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.trainer_pt_utils.DistributedTensorGatherer.add_arrays" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Add <code class="xref py py-obj docutils literal notranslate"><span class="pre">arrays</span></code> to the internal storage, Will initialize the storage to the full size at the first arrays
passed so that if weâ€™re bound to get an OOM, it happens at the beginning.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.trainer_pt_utils.DistributedTensorGatherer.finalize"><a name="//apple_ref/cpp/Method/transformers.trainer_pt_utils.DistributedTensorGatherer.finalize"></a>
<code class="sig-name descname">finalize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/trainer_pt_utils.html#DistributedTensorGatherer.finalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.trainer_pt_utils.DistributedTensorGatherer.finalize" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Return the properly gathered arrays and truncate to the number of samples (since the sampler added some extras
to get each process a dataset of the same length).</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="id1">
<h2>Distributed Evaluation<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.HfArgumentParser"><a name="//apple_ref/cpp/Class/transformers.HfArgumentParser"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">HfArgumentParser</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataclass_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>DataClassType<span class="p">, </span>Iterable<span class="p">[</span>DataClassType<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/hf_argparser.html#HfArgumentParser"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.HfArgumentParser" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This subclass of <cite>argparse.ArgumentParser</cite> uses type hints on dataclasses to generate arguments.</p>
<p>The class is designed to play well with the native argparse. In particular, you can add more (non-dataclass backed)
arguments to the parser after initialization and youâ€™ll get the output back after parsing as an additional
namespace.</p>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="generation_utils.html" rel="next" title="Utilities for Generation">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="tokenization_utils.html" rel="prev" title="Utilities for Tokenizers"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>