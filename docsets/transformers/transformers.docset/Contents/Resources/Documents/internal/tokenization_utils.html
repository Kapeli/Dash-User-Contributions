
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Utilities for Tokenizers ‚Äî transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="trainer_utils.html" rel="next" title="Utilities for Trainer"/>
<link href="pipelines_utils.html" rel="prev" title="Utilities for pipelines"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ü§ó Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ü§ó Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Utilities for Tokenizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pretrainedtokenizerbase">PreTrainedTokenizerBase</a></li>
<li class="toctree-l2"><a class="reference internal" href="#specialtokensmixin">SpecialTokensMixin</a></li>
<li class="toctree-l2"><a class="reference internal" href="#enums-and-namedtuples">Enums and namedtuples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> ¬ª</li>
<li>Utilities for Tokenizers</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/internal/tokenization_utils.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="utilities-for-tokenizers">
<h1>Utilities for Tokenizers<a class="headerlink" href="#utilities-for-tokenizers" title="Permalink to this headline">¬∂</a></h1>
<p>This page lists all the utility functions used by the tokenizers, mainly the class
<a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a> that implements the common methods between
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a> and the mixin
<a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin" title="transformers.tokenization_utils_base.SpecialTokensMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpecialTokensMixin</span></code></a>.</p>
<p>Most of those are only useful if you are studying the code of the tokenizers in the library.</p>
<div class="section" id="pretrainedtokenizerbase">
<h2>PreTrainedTokenizerBase<a class="headerlink" href="#pretrainedtokenizerbase" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.PreTrainedTokenizerBase"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">PreTrainedTokenizerBase</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Base class for <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>.</p>
<p>Handles shared (mostly boiler plate) methods for those two classes.</p>
<p>Class attributes (overridden by derived classes)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>vocab_files_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>) ‚Äì A dictionary with, as keys, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of
each vocabulary file required by the model, and as associated values, the filename for saving the associated
file (string).</p></li>
<li><p><strong>pretrained_vocab_files_map</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">str]]</span></code>) ‚Äì A dictionary of dictionaries, with the
high-level keys being the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> keyword name of each vocabulary file required by the model, the
low-level being the <code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models with, as associated values, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">url</span></code> to the associated pretrained vocabulary file.</p></li>
<li><p><strong>max_model_input_sizes</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Optinal[int]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, the maximum length of the sequence
inputs of this model, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the model has no maximum input size.</p></li>
<li><p><strong>pretrained_init_configuration</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">Any]]</span></code>) ‚Äì A dictionary with, as keys, the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">short-cut-names</span></code> of the pretrained models, and as associated values, a dictionary of specific arguments
to pass to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the tokenizer class for this pretrained model when loading the
tokenizer with the <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>
method.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì A list of inputs expected in the forward pass of the model.</p></li>
<li><p><strong>padding_side</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The default value for the side on which the model should have padding
applied. Should be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'right'</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'left'</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>, this
will be set to the value stored for the associated model in <code class="docutils literal notranslate"><span class="pre">max_model_input_sizes</span></code> (see above). If no
value is provided, will default to VERY_LARGE_INTEGER (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int(1e30)</span></code>).</p></li>
<li><p><strong>padding_side</strong> ‚Äì (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):
The side on which the model should have padding applied. Should be selected between [‚Äòright‚Äô, ‚Äòleft‚Äô].
Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>model_input_names</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[string]</span></code>, <cite>optional</cite>) ‚Äì The list of inputs accepted by the forward pass of the model (like <code class="xref py py-obj docutils literal notranslate"><span class="pre">"token_type_ids"</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"attention_mask"</span></code>). Default value is picked from the class attribute of the same name.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the beginning of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.bos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.bos_token_id</span></code>.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the end of a sentence. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.eos_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.eos_token_id</span></code>.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing an out-of-vocabulary token. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.unk_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.unk_token_id</span></code>.</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code class="docutils literal notranslate"><span class="pre">self.sep_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.sep_token_id</span></code>.</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.pad_token</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code>.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the class of the input (used by BERT for instance). Will be associated to
<code class="docutils literal notranslate"><span class="pre">self.cls_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.cls_token_id</span></code>.</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code class="docutils literal notranslate"><span class="pre">self.mask_token</span></code> and <code class="docutils literal notranslate"><span class="pre">self.mask_token_id</span></code>.</p></li>
<li><p><strong>additional_special_tokens</strong> (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A tuple or a list of additional special tokens. Add them here to ensure they won‚Äôt be split by the
tokenization process. Will be associated to <code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens</span></code> and
<code class="docutils literal notranslate"><span class="pre">self.additional_special_tokens_ids</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"></a>
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>text_pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>) ‚Äì The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code class="xref py py-obj docutils literal notranslate"><span class="pre">is_split_into_words=True</span></code> (to lift the ambiguity with a batch of sequences).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.as_target_tokenizer"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.as_target_tokenizer"></a>
<code class="sig-name descname">as_target_tokenizer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.as_target_tokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.as_target_tokenizer" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to
sequence-to-sequence models that need a slightly different processing for the labels.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode"></a>
<code class="sig-name descname">batch_decode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sequences</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">, </span>np.ndarray<span class="p">, </span>torch.Tensor<span class="p">, </span>tf.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">skip_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.batch_decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Convert a list of lists of token ids into a list of strings by calling decode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[List[int],</span> <span class="pre">List[List[int]],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>) ‚Äì List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p></li>
<li><p><strong>skip_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to remove special tokens in the decoding.</p></li>
<li><p><strong>clean_up_tokenization_spaces</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to clean up the tokenization spaces.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) ‚Äì Will be passed to the underlying model specific decode method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of decoded sentences.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus"></a>
<code class="sig-name descname">batch_encode_plus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>Tuple<span class="p">[</span>str<span class="p">, </span>str<span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>Tuple<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>Tuple<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.batch_encode_plus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_text_or_text_pairs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[str,</span> <span class="pre">str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[str]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[str],</span> <span class="pre">List[str]]]</span></code>, and for not-fast tokenizers, also <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[List[int],</span> <span class="pre">List[int]]]</span></code>) ‚Äì Batch of sequences or pair of sequences to be encoded. This can be a list of
string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see
details in <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code>).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens"></a>
<code class="sig-name descname">build_inputs_with_special_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.build_inputs_with_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.</p>
<p>This implementation does not add special tokens and this method should be overridden in a subclass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì The first tokenized sequence.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì The second tokenized sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The model input with special tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization"></a>
<em class="property">static </em><code class="sig-name descname">clean_up_tokenization</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out_string</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.clean_up_tokenization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>out_string</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The text to clean up.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The cleaned-up string.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.convert_tokens_to_string"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.convert_tokens_to_string"></a>
<code class="sig-name descname">convert_tokens_to_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.convert_tokens_to_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.convert_tokens_to_string" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a sequence of token ids in a single string. The most simple way to do it is <code class="docutils literal notranslate"><span class="pre">"</span> <span class="pre">".join(tokens)</span></code> but
we often want to remove sub-word tokenization artifacts at the same time</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì The token to join in a string.</p>
</dd>
</dl>
<p>Return: The joined tokens.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences"></a>
<code class="sig-name descname">create_token_type_ids_from_sequences</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.create_token_type_ids_from_sequences"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Create the token type IDs corresponding to the sequences passed. <a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
<p>Should be overridden in a subclass if the model has a special way of building those.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì The first tokenized sequence.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì The second tokenized sequence.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The token type ids.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode"></a>
<code class="sig-name descname">decode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>np.ndarray<span class="p">, </span>torch.Tensor<span class="p">, </span>tf.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">skip_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí str<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.</p>
<p>Similar to doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">List[int],</span> <span class="pre">np.ndarray,</span> <span class="pre">torch.Tensor,</span> <span class="pre">tf.Tensor]</span></code>) ‚Äì List of tokenized input ids. Can be obtained using the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p></li>
<li><p><strong>skip_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to remove special tokens in the decoding.</p></li>
<li><p><strong>clean_up_tokenization_spaces</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to clean up the tokenization spaces.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) ‚Äì Will be passed to the underlying model specific decode method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The decoded sentence.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode"></a>
<code class="sig-name descname">encode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.</p>
<p>Same as doing <code class="docutils literal notranslate"><span class="pre">self.convert_tokens_to_ids(self.tokenize(text))</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p></li>
<li><p><strong>text_pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>**kwargs</strong> ‚Äì Passed along to the <cite>.tokenize()</cite> method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tokenized ids of the
text.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus"></a>
<code class="sig-name descname">encode_plus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">is_split_into_words</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.encode_plus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Tokenize and prepare for the model a sequence or a pair of sequences.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method is deprecated, <code class="docutils literal notranslate"><span class="pre">__call__</span></code> should be used instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> (the latter only for not-fast tokenizers)) ‚Äì The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the
<code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code>
method).</p></li>
<li><p><strong>text_pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using
the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method) or a list of integers (tokenized string ids using the
<code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> method).</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em>, <em class="sig-param"><span class="o">*</span><span class="n">init_inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Instantiate a <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase</span></code></a> (or a derived class) from
a predefined tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) ‚Äì <p>Can be either:</p>
<ul>
<li><p>A string, the <cite>model id</cite> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under a
user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved
using the <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>
method, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>(<strong>Deprecated</strong>, not applicable to all derived classes) A path or url to a single saved vocabulary
file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) ‚Äì Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to force the (re-)download the vocabulary files and override the cached versions if they
exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to delete incompletely received files. Attempt to resume the download if such a file
exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) ‚Äì A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>use_auth_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <cite>bool</cite>, <cite>optional</cite>) ‚Äì The token to use as HTTP bearer authorization for remote files. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, will use the token
generated when running <code class="xref py py-obj docutils literal notranslate"><span class="pre">transformers-cli</span> <span class="pre">login</span></code> (stored in <code class="xref py py-obj docutils literal notranslate"><span class="pre">huggingface</span></code>).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) ‚Äì The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>subfolder</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) ‚Äì In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.</p></li>
<li><p><strong>inputs</strong> (additional positional arguments, <cite>optional</cite>) ‚Äì Will be passed along to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) ‚Äì Will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. Can be used to set special tokens like
<code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> for more details.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_auth_token=True</span></code> is required when you want to use a private model.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer</span>
<span class="c1"># Download vocabulary from huggingface.co and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="c1"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'dbmdz/bert-base-german-cased'</span><span class="p">)</span>

<span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/'</span><span class="p">)</span>

<span class="c1"># If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/saved_model/my_vocab.txt'</span><span class="p">)</span>

<span class="c1"># You can link tokens to special vocabulary when instantiating</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="s1">'&lt;unk&gt;'</span><span class="p">)</span>
<span class="c1"># You should be sure '&lt;unk&gt;' is in the vocabulary when doing that.</span>
<span class="c1"># Otherwise use tokenizer.add_special_tokens({'unk_token': '&lt;unk&gt;'}) instead)</span>
<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">==</span> <span class="s1">'&lt;unk&gt;'</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask"></a>
<code class="sig-name descname">get_special_tokens_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.get_special_tokens_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> or <code class="docutils literal notranslate"><span class="pre">encode_plus</span></code> methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì List of ids of the first sequence.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì List of ids of the second sequence.</p></li>
<li><p><strong>already_has_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the token list is already formatted with special tokens for the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>1 for a special token, 0 for a sequence token.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A list of integers in the range [0, 1]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_vocab"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_vocab"></a>
<code class="sig-name descname">get_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí Dict<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.get_vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_vocab" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair"></a>
<em class="property">property </em><code class="sig-name descname">max_len_sentences_pair</code><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The maximum combined length of a pair of sentences that can be fed to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence"></a>
<em class="property">property </em><code class="sig-name descname">max_len_single_sentence</code><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence" title="Permalink to this definition">¬∂</a></dt>
<dd><p>The maximum length of a sentence that can be fed to the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad"></a>
<code class="sig-name descname">pad</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">encoded_inputs</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>transformers.tokenization_utils_base.BatchEncoding<span class="p">, </span>List<span class="p">[</span>transformers.tokenization_utils_base.BatchEncoding<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.</p>
<p>Padding side (left/right) padding token ids are defined at the tokenizer level (with <code class="docutils literal notranslate"><span class="pre">self.padding_side</span></code>,
<code class="docutils literal notranslate"><span class="pre">self.pad_token_id</span></code> and <code class="docutils literal notranslate"><span class="pre">self.pad_token_type_id</span></code>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">encoded_inputs</span></code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code class="docutils literal notranslate"><span class="pre">return_tensors</span></code>. In the
case of PyTorch tensors, you will lose the specific device of your tensors however.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoded_inputs</strong> (<a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a>, list of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[int]]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[List[int]]</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">List[int]]]</span></code>) ‚Äì <p>Tokenized inputs. Can represent one input (<a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span>
<span class="pre">List[int]]</span></code>) or a batch of tokenized inputs (list of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a>, <cite>Dict[str,
List[List[int]]]</cite> or <cite>List[Dict[str, List[int]]]</cite>) so you can use this method during preprocessing as
well as in a PyTorch Dataloader collate function.</p>
<p>Instead of <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.</p>
</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì <dl class="simple">
<dt>Select a strategy to pad the returned sequences (according to the model‚Äôs padding side and padding</dt><dd><p>index) among:</p>
</dd>
</dl>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì Maximum length of the returned list and optionally padding length (see above).</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
&gt;= 7.5 (Volta).</p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model"></a>
<code class="sig-name descname">prepare_for_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy">transformers.tokenization_utils_base.PaddingStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType">transformers.tokenization_utils_base.TensorType</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_length</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.prepare_for_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p></li>
<li><p><strong>pair_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to encode the sequences with the special tokens relative to their model.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a number along with <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code>, the overflowing tokens returned when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.</p></li>
<li><p><strong>is_split_into_words</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.</p></li>
<li><p><strong>pad_to_multiple_of</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) ‚Äì <p>Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer‚Äôs default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>return_overflowing_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return overflowing token sequences.</p></li>
<li><p><strong>return_special_tokens_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return special tokens mask information.</p></li>
<li><p><strong>return_offsets_mapping</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Whether or not to return <code class="xref py py-obj docutils literal notranslate"><span class="pre">(char_start,</span> <span class="pre">char_end)</span></code> for each token.</p>
<p>This is only available on fast tokenizers inheriting from
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>, if using Python‚Äôs tokenizer, this method will raise
<code class="xref py py-obj docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</p></li>
<li><p><strong>return_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to return the lengths of the encoded inputs.</p></li>
<li><p><strong>verbose</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether or not to print more information and warnings.</p></li>
<li><p><strong>**kwargs</strong> ‚Äì passed to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.tokenize()</span></code> method</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul>
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to a model.</p>
<p><a class="reference external" href="../glossary.html#input-ids">What are input IDs?</a></p>
</li>
<li><p><strong>token_type_ids</strong> ‚Äì List of token type ids to be fed to a model (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_token_type_ids=True</span></code>
or if <cite>‚Äútoken_type_ids‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model (when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_attention_mask=True</span></code> or if <cite>‚Äúattention_mask‚Äù</cite> is in <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.model_input_names</span></code>).</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</li>
<li><p><strong>overflowing_tokens</strong> ‚Äì List of overflowing tokens sequences (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>num_truncated_tokens</strong> ‚Äì Number of tokens truncated (when a <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> is specified and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">return_overflowing_tokens=True</span></code>).</p></li>
<li><p><strong>special_tokens_mask</strong> ‚Äì List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">add_special_tokens=True</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_special_tokens_mask=True</span></code>).</p></li>
<li><p><strong>length</strong> ‚Äì The length of the inputs (when <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_length=True</span></code>)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_seq2seq_batch"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_seq2seq_batch"></a>
<code class="sig-name descname">prepare_seq2seq_batch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">src_texts</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_target_length</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">padding</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'longest'</span></em>, <em class="sig-param"><span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">truncation</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí transformers.tokenization_utils_base.BatchEncoding<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.prepare_seq2seq_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_seq2seq_batch" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Prepare model inputs for translation. For best performance, translate one sentence at a time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_texts</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) ‚Äì List of documents to summarize or source language texts.</p></li>
<li><p><strong>tgt_texts</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, <cite>optional</cite>) ‚Äì List of summaries or target language texts.</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì Controls the maximum length for encoder inputs (documents to summarize or source language texts) If
left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.</p></li>
<li><p><strong>max_target_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) ‚Äì Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set
to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the max_length value.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) ‚Äì <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair
if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with
sequence lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>**kwargs</strong> ‚Äì Additional keyword arguments passed along to <code class="xref py py-obj docutils literal notranslate"><span class="pre">self.__call__</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a> with the following fields:</p>
<ul class="simple">
<li><p><strong>input_ids</strong> ‚Äì List of token ids to be fed to the encoder.</p></li>
<li><p><strong>attention_mask</strong> ‚Äì List of indices specifying which tokens should be attended to by the model.</p></li>
<li><p><strong>labels</strong> ‚Äì List of token ids for tgt_texts.</p></li>
</ul>
<p>The full set of keys <code class="docutils literal notranslate"><span class="pre">[input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">labels]</span></code>, will only be returned if tgt_texts is passed.
Otherwise, input_ids, attention_mask will be the only keys.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="../main_classes/tokenizer.html#transformers.BatchEncoding" title="transformers.BatchEncoding"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchEncoding</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained"></a>
<code class="sig-name descname">save_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>os.PathLike<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">legacy_format</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí Tuple<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Save the full tokenizer state.</p>
<p>This method make sure the full tokenizer can then be re-loaded using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A ‚Äúfast‚Äù tokenizer (instance of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code></a>) saved with this method will
not be possible to load back in a ‚Äúslow‚Äù tokenizer, i.e. in a <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer</span></code></a>
instance. It can only be loaded in a ‚Äúfast‚Äù tokenizer, i.e. in a
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizerFast</span></code></a> instance.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This won‚Äôt save modifications you may have applied to the tokenizer after the instantiation (for instance,
modifying <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.do_lower_case</span></code> after creation).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) ‚Äì The path to a directory where the tokenizer will be saved.</p></li>
<li><p><strong>legacy_format</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) ‚Äì Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a
separate added_tokens files or in the unified JSON file format for the <cite>tokenizers</cite> library. It‚Äôs only
possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with
‚Äúslow‚Äù tokenizers (not powered by the <cite>tokenizers</cite> library).</p></li>
<li><p><strong>filename_prefix</strong> ‚Äì (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):
A prefix to add to the names of the files saved by the tokenizer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>A tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_vocabulary"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_vocabulary"></a>
<code class="sig-name descname">save_vocabulary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> ‚Üí Tuple<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.save_vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_vocabulary" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method won‚Äôt save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The directory in which to save the vocabulary.</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) ‚Äì An optional prefix to add to the named of the saved files.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Paths to the files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.tokenize"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.tokenize"></a>
<code class="sig-name descname">tokenize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">text</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">pair</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">add_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> ‚Üí List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.tokenize" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Converts a string in a sequence of tokens, using the backend Rust tokenizer.</p>
<p>Note that this method behave differently between fast and slow tokenizers:</p>
<blockquote>
<div><ul class="simple">
<li><p>in fast tokenizers (instances of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a>), this method will
replace the unknown tokens with the <code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code>,</p></li>
<li><p>in slow tokenizers (instances of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a>), this method keep unknown
tokens unchanged.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) ‚Äì The sequence to be encoded.</p></li>
<li><p><strong>pair</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) ‚Äì A second sequence to be encoded with the first.</p></li>
<li><p><strong>add_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì Whether or not to add the special tokens associated with the corresponding model.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) ‚Äì Will be passed to the underlying model specific encode method. See details in
<a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__call__()</span></code></a></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences"></a>
<code class="sig-name descname">truncate_sequences</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_tokens_to_remove</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy">transformers.tokenization_utils_base.TruncationStrategy</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'longest_first'</span></em>, <em class="sig-param"><span class="n">stride</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> ‚Üí Tuple<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PreTrainedTokenizerBase.truncate_sequences"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Truncates a sequence pair in-place following the strategy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) ‚Äì Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p></li>
<li><p><strong>pair_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) ‚Äì Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code>
and <code class="docutils literal notranslate"><span class="pre">convert_tokens_to_ids</span></code> methods.</p></li>
<li><p><strong>num_tokens_to_remove</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì Number of tokens to remove using the truncation strategy.</p></li>
<li><p><strong>truncation_strategy</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>The strategy to follow for truncation. Can be:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will
truncate token by token, removing a token from the longest sequence in the pair if a pair of
sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or
to the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>stride</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì If set to a positive number, the overflowing tokens returned will contain some tokens from the main
sequence returned. The value of this argument defines the number of additional tokens.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The truncated <code class="docutils literal notranslate"><span class="pre">ids</span></code>, the truncated <code class="docutils literal notranslate"><span class="pre">pair_ids</span></code> and the
list of overflowing tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[List[int],</span> <span class="pre">List[int],</span> <span class="pre">List[int]]</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="specialtokensmixin">
<h2>SpecialTokensMixin<a class="headerlink" href="#specialtokensmixin" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.SpecialTokensMixin"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">SpecialTokensMixin</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#SpecialTokensMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin" title="Permalink to this definition">¬∂</a></dt>
<dd><p>A mixin derived by <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast" title="transformers.PreTrainedTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizerFast</span></code></a> to
handle specific behaviors related to special tokens. In particular, this class hold the attributes which can be
used to directly access these special tokens in a model-independent manner and allow to set and update the special
tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the beginning of a sentence.</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the end of a sentence.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing an out-of-vocabulary token.</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token separating two different sentences in the same input (used by BERT for instance).</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing the class of the input (used by BERT for instance).</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT).</p></li>
<li><p><strong>additional_special_tokens</strong> (tuple or list of <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>, <cite>optional</cite>) ‚Äì A tuple or a list of additional special tokens.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens"></a>
<code class="sig-name descname">add_special_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">special_tokens_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>str<span class="p">, </span>tokenizers.AddedToken<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#SpecialTokensMixin.add_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If
special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the
current vocabulary).</p>
<p>Using : obj:<cite>add_special_tokens</cite> will ensure your special tokens can be used in several ways:</p>
<ul class="simple">
<li><p>Special tokens are carefully handled by the tokenizer (they are never split).</p></li>
<li><p>You can easily refer to special tokens using tokenizer class attributes like <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.cls_token</span></code>. This
makes it easy to develop model-agnostic training and fine-tuning scripts.</p></li>
</ul>
<p>When possible, special tokens are already registered for provided pretrained models (for instance
<a class="reference internal" href="../model_doc/bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> <a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin.cls_token" title="transformers.tokenization_utils_base.SpecialTokensMixin.cls_token"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cls_token</span></code></a> is already registered to be :obj`‚Äô[CLS]‚Äô` and XLM‚Äôs one
is also registered to be <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;/s&gt;'</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>special_tokens_dict</strong> (dictionary <cite>str</cite> to <cite>str</cite> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>) ‚Äì <p>Keys should be in the list of predefined special attributes: [<code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>, <code class="docutils literal notranslate"><span class="pre">mask_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>].</p>
<p>Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer
assign the index of the <code class="docutils literal notranslate"><span class="pre">unk_token</span></code> to them).</p>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Number of tokens added to the vocabulary.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let's see how to add a new classification token to GPT-2</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="n">special_tokens_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'cls_token'</span><span class="p">:</span> <span class="s1">'&lt;CLS&gt;'</span><span class="p">}</span>

<span class="n">num_added_toks</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'We have added'</span><span class="p">,</span> <span class="n">num_added_toks</span><span class="p">,</span> <span class="s1">'tokens'</span><span class="p">)</span>
<span class="c1"># Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">==</span> <span class="s1">'&lt;CLS&gt;'</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens"></a>
<code class="sig-name descname">add_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">new_tokens</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>tokenizers.AddedToken<span class="p">, </span>List<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>tokenizers.AddedToken<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#SpecialTokensMixin.add_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to
it with indices starting from length of the current vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>new_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> or a list of <cite>str</cite> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code>) ‚Äì Tokens are only added if they are not already in the vocabulary. <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> wraps a
string token to let you personalize its behavior: whether this token should only match against a single
word, whether this token should strip all potential whitespaces on the left side, whether this token
should strip all potential whitespaces on the right side, etc.</p></li>
<li><p><strong>special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) ‚Äì <p>Can be used to specify if the token is a special token. This mostly change the normalization behavior
(special tokens like CLS or [MASK] are usually not lower-cased for instance).</p>
<p>See details for <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> in HuggingFace tokenizers library.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Number of tokens added to the vocabulary.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let's see how to increase the vocabulary of Bert model and tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="n">num_added_toks</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="s1">'new_tok1'</span><span class="p">,</span> <span class="s1">'my_new-tok2'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'We have added'</span><span class="p">,</span> <span class="n">num_added_toks</span><span class="p">,</span> <span class="s1">'tokens'</span><span class="p">)</span>
 <span class="c1"># Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens"></a>
<em class="property">property </em><code class="sig-name descname">additional_special_tokens</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>All the additional special tokens you may want to use. Log an error if used while not having
been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids"></a>
<em class="property">property </em><code class="sig-name descname">additional_special_tokens_ids</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Ids of all the additional special tokens in the vocabulary. Log an error if used while not
having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.all_special_ids"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.all_special_ids"></a>
<em class="property">property </em><code class="sig-name descname">all_special_ids</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.all_special_ids" title="Permalink to this definition">¬∂</a></dt>
<dd><p>List the ids of the special tokens(<code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;unk&gt;'</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;cls&gt;'</span></code>, etc.) mapped to class
attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens"></a>
<em class="property">property </em><code class="sig-name descname">all_special_tokens</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>All the special tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;unk&gt;'</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;cls&gt;'</span></code>, etc.) mapped to class attributes.</p>
<p>Convert tokens of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> type to string.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens_extended"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens_extended"></a>
<em class="property">property </em><code class="sig-name descname">all_special_tokens_extended</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens_extended" title="Permalink to this definition">¬∂</a></dt>
<dd><p>All the special tokens (<code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;unk&gt;'</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;cls&gt;'</span></code>, etc.)
mapped to class attributes.</p>
<p>Don‚Äôt convert tokens of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> type to string so they can be used to control more finely
how special tokens are tokenized.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Union[str,</span> <span class="pre">tokenizers.AddedToken]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.bos_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.bos_token"></a>
<em class="property">property </em><code class="sig-name descname">bos_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.bos_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Beginning of sentence token. Log an error if used while not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id"></a>
<em class="property">property </em><code class="sig-name descname">bos_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the beginning of sentence token in the vocabulary. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token
has not been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.cls_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.cls_token"></a>
<em class="property">property </em><code class="sig-name descname">cls_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.cls_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Classification token, to extract a summary of an input sequence leveraging self-attention along the
full depth of the model. Log an error if used while not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id"></a>
<em class="property">property </em><code class="sig-name descname">cls_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the classification token in the vocabulary, to extract a summary of an input
sequence leveraging self-attention along the full depth of the model.</p>
<p>Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has not been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.eos_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.eos_token"></a>
<em class="property">property </em><code class="sig-name descname">eos_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.eos_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>End of sentence token. Log an error if used while not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id"></a>
<em class="property">property </em><code class="sig-name descname">eos_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the end of sentence token in the vocabulary. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has
not been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.mask_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.mask_token"></a>
<em class="property">property </em><code class="sig-name descname">mask_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.mask_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Mask token, to use when training a model with masked-language modeling. Log an error if used while
not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id"></a>
<em class="property">property </em><code class="sig-name descname">mask_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the mask token in the vocabulary, used when training a model with masked-language
modeling. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has not been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.pad_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.pad_token"></a>
<em class="property">property </em><code class="sig-name descname">pad_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.pad_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Padding token. Log an error if used while not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id"></a>
<em class="property">property </em><code class="sig-name descname">pad_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the padding token in the vocabulary. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has not been
set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_type_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_type_id"></a>
<em class="property">property </em><code class="sig-name descname">pad_token_type_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_type_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the padding token type in the vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens"></a>
<code class="sig-name descname">sanitize_special_tokens</code><span class="sig-paren">(</span><span class="sig-paren">)</span> ‚Üí int<a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#SpecialTokensMixin.sanitize_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Make sure that all the special tokens attributes of the tokenizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.mask_token</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.cls_token</span></code>, etc.) are in the vocabulary.</p>
<p>Add the missing ones to the vocabulary if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The number of tokens added in the vocabulary during the operation.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.sep_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.sep_token"></a>
<em class="property">property </em><code class="sig-name descname">sep_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.sep_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Separation token, to separate context and query in an input sequence. Log an error if used while
not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id"></a>
<em class="property">property </em><code class="sig-name descname">sep_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the separation token in the vocabulary, to separate context and query in an input
sequence. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has not been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map"></a>
<em class="property">property </em><code class="sig-name descname">special_tokens_map</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map" title="Permalink to this definition">¬∂</a></dt>
<dd><p>A dictionary mapping special token class attributes (<a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin.cls_token" title="transformers.tokenization_utils_base.SpecialTokensMixin.cls_token"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cls_token</span></code></a>,
<a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin.unk_token" title="transformers.tokenization_utils_base.SpecialTokensMixin.unk_token"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code></a>, etc.) to their values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;unk&gt;'</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;cls&gt;'</span></code>, etc.).</p>
<p>Convert potential tokens of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> type to string.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[str,</span> <span class="pre">List[str]]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map_extended"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map_extended"></a>
<em class="property">property </em><code class="sig-name descname">special_tokens_map_extended</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map_extended" title="Permalink to this definition">¬∂</a></dt>
<dd><p>A dictionary
mapping special token class attributes (<a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin.cls_token" title="transformers.tokenization_utils_base.SpecialTokensMixin.cls_token"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cls_token</span></code></a>, <a class="reference internal" href="#transformers.tokenization_utils_base.SpecialTokensMixin.unk_token" title="transformers.tokenization_utils_base.SpecialTokensMixin.unk_token"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unk_token</span></code></a>, etc.) to their values
(<code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;unk&gt;'</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">'&lt;cls&gt;'</span></code>, etc.).</p>
<p>Don‚Äôt convert tokens of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizers.AddedToken</span></code> type to string so they can be used to control more finely
how special tokens are tokenized.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[str,</span> <span class="pre">tokenizers.AddedToken,</span> <span class="pre">List[Union[str,</span> <span class="pre">tokenizers.AddedToken]]]]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.unk_token"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.unk_token"></a>
<em class="property">property </em><code class="sig-name descname">unk_token</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.unk_token" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Unknown token. Log an error if used while not having been set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id"><a name="//apple_ref/cpp/Method/transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id"></a>
<em class="property">property </em><code class="sig-name descname">unk_token_id</code><a class="headerlink" href="#transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Id of the unknown token in the vocabulary. Returns <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code> if the token has not been
set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="enums-and-namedtuples">
<h2>Enums and namedtuples<a class="headerlink" href="#enums-and-namedtuples" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.ExplicitEnum"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.ExplicitEnum"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">ExplicitEnum</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#ExplicitEnum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.ExplicitEnum" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Enum with more explicit error message for missing values.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.PaddingStrategy"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.PaddingStrategy"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">PaddingStrategy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#PaddingStrategy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.PaddingStrategy" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Possible values for the <code class="docutils literal notranslate"><span class="pre">padding</span></code> argument in <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.__call__()</span></code></a>. Useful for tab-completion
in an IDE.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.TensorType"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.TensorType"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">TensorType</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#TensorType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.TensorType" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Possible values for the <code class="docutils literal notranslate"><span class="pre">return_tensors</span></code> argument in <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.__call__()</span></code></a>. Useful for
tab-completion in an IDE.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.TruncationStrategy"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.TruncationStrategy"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">TruncationStrategy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#TruncationStrategy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.TruncationStrategy" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Possible values for the <code class="docutils literal notranslate"><span class="pre">truncation</span></code> argument in <a class="reference internal" href="#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__" title="transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">PreTrainedTokenizerBase.__call__()</span></code></a>. Useful for
tab-completion in an IDE.</p>
</dd></dl>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.CharSpan"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.CharSpan"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">CharSpan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#CharSpan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.CharSpan" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Character span in the original string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the first character in the original string.</p></li>
<li><p><strong>end</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the character following the last character in the original string.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.tokenization_utils_base.TokenSpan"><a name="//apple_ref/cpp/Class/transformers.tokenization_utils_base.TokenSpan"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.tokenization_utils_base.</code><code class="sig-name descname">TokenSpan</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_utils_base.html#TokenSpan"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.tokenization_utils_base.TokenSpan" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Token span in an encoded string (list of tokens).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the first token in the span.</p></li>
<li><p><strong>end</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Index of the token following the last token in the span.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="trainer_utils.html" rel="next" title="Utilities for Trainer">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="pipelines_utils.html" rel="prev" title="Utilities for pipelines"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        ¬© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>