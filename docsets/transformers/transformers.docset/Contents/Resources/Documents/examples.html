

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Examples &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fine-tuning with custom datasets" href="custom_datasets.html" />
    <link rel="prev" title="Pretrained models" href="pretrained_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#important-note">Important note</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-big-table-of-tasks">The Big Table of Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-training-and-mixed-precision">Distributed training and mixed precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-on-tpus">Running on TPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logging-experiment-tracking">Logging &amp; Experiment tracking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#weights-biases">Weights &amp; Biases</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comet-ml">Comet.ml</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/examples.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="examples">
<h1>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">Â¶</a></h1>
<p>This folder contains actively maintained examples of use of ðŸ¤— Transformers organized along NLP tasks. If you are looking for an example that used to
be in this folder, it may have moved to our <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/research_projects">research projects</a> subfolder (which contains frozen snapshots of research projects).</p>
<div class="section" id="important-note">
<h2>Important note<a class="headerlink" href="#important-note" title="Permalink to this headline">Â¶</a></h2>
<p><strong>Important</strong></p>
<p>To make sure you can successfully run the latest versions of the example scripts, you have to <strong>install the library from source</strong> and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/huggingface/transformers
<span class="nb">cd</span> transformers
pip install .
</pre></div>
</div>
<p>Then cd in the example folder of your choice and run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install -r requirements.txt
</pre></div>
</div>
<p>Alternatively, you can run the version of the examples as they were for your current version of Transformers via (for instance with v3.5.1):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git checkout tags/v3.5.1
</pre></div>
</div>
</div>
<div class="section" id="the-big-table-of-tasks">
<h2>The Big Table of Tasks<a class="headerlink" href="#the-big-table-of-tasks" title="Permalink to this headline">Â¶</a></h2>
<p>Here is the list of all our examples:</p>
<ul class="simple">
<li><p>with information on whether they are <strong>built on top of <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>/<code class="docutils literal notranslate"><span class="pre">TFTrainer</span></code></strong> (if not, they still work, they might
just lack some features),</p></li>
<li><p>whether or not they leverage the <a class="reference external" href="https://github.com/huggingface/datasets">ðŸ¤— Datasets</a> library.</p></li>
<li><p>links to <strong>Colab notebooks</strong> to walk through the scripts and run them easily,</p></li>
</ul>
<!--
Coming soon!
- links to **Cloud deployments** to be able to deploy large-scale trainings in the Cloud with little to no setup.
--><table border="1" class="docutils">
<thead>
<tr>
<th>Task</th>
<th>Example datasets</th>
<th align="center">Trainer support</th>
<th align="center">TFTrainer support</th>
<th align="center">ðŸ¤— Datasets</th>
<th align="center">Colab</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/language-modeling"><strong><code>language-modeling</code></strong></a></td>
<td>Raw text</td>
<td align="center">âœ…</td>
<td align="center">-</td>
<td align="center">âœ…</td>
<td align="center"><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/multiple-choice"><strong><code>multiple-choice</code></strong></a></td>
<td>SWAG, RACE, ARC</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center"><a href="https://colab.research.google.com/github/ViktorAlm/notebooks/blob/master/MPC_GPU_Demo_for_TF_and_PT.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/question-answering"><strong><code>question-answering</code></strong></a></td>
<td>SQuAD</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center"><a href="https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/seq2seq"><strong><code>summarization</code></strong></a></td>
<td>CNN/Daily Mail</td>
<td align="center">âœ…</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/text-classification"><strong><code>text-classification</code></strong></a></td>
<td>GLUE, XNLI</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center"><a href="https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/text-generation"><strong><code>text-generation</code></strong></a></td>
<td>-</td>
<td align="center">n/a</td>
<td align="center">n/a</td>
<td align="center">-</td>
<td align="center"><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/token-classification"><strong><code>token-classification</code></strong></a></td>
<td>CoNLL NER</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center">âœ…</td>
<td align="center"><a href="https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td><a href="https://github.com/huggingface/transformers/tree/master/examples/seq2seq"><strong><code>translation</code></strong></a></td>
<td>WMT</td>
<td align="center">âœ…</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
</tbody>
</table><!--
## One-click Deploy to Cloud (wip)

**Coming soon!**
--></div>
<div class="section" id="distributed-training-and-mixed-precision">
<h2>Distributed training and mixed precision<a class="headerlink" href="#distributed-training-and-mixed-precision" title="Permalink to this headline">Â¶</a></h2>
<p>All the PyTorch scripts mentioned above work out of the box with distributed training and mixed precision, thanks to
the <a class="reference external" href="https://huggingface.co/transformers/main_classes/trainer.html">Trainer API</a>. To launch one of them on <em>n</em> GPUS,
use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node number_of_gpu_you_have path_to_script.py <span class="se">\</span>
	--all_arguments_of_the_script 
</pre></div>
</div>
<p>As an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text
classification MNLI task using the <code class="docutils literal notranslate"><span class="pre">run_glue</span></code> script, with 8 GPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch <span class="se">\</span>
    --nproc_per_node <span class="m">8</span> text-classification/run_glue.py <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --task_name mnli <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_device_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /tmp/mnli_output/
</pre></div>
</div>
<p>If you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision
training with PyTorch 1.6.0 or latest, or by installing the <a class="reference external" href="https://github.com/NVIDIA/apex">Apex</a> library for previous
versions. Just add the flag <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> to your command launching one of the scripts mentioned above!</p>
<p>Using mixed precision training usually results in 2x-speedup for training with the same final results (as shown in
<a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/text-classification#mixed-precision-training">this table</a>
for text classification).</p>
</div>
<div class="section" id="running-on-tpus">
<h2>Running on TPUs<a class="headerlink" href="#running-on-tpus" title="Permalink to this headline">Â¶</a></h2>
<p>When using Tensorflow, TPUs are supported out of the box as a <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>.</p>
<p>When using PyTorch, we support TPUs thanks to <code class="docutils literal notranslate"><span class="pre">pytorch/xla</span></code>. For more context and information on how to setup your TPU environment refer to Googleâ€™s documentation and to the
very detailed <a class="reference external" href="https://github.com/pytorch/xla/blob/master/README.md">pytorch/xla README</a>.</p>
<p>In this repo, we provide a very simple launcher script named
<a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/xla_spawn.py">xla_spawn.py</a> that lets you run our
example scripts on multiple TPU cores without any boilerplate. Just pass a <code class="docutils literal notranslate"><span class="pre">--num_cores</span></code> flag to this script, then your
regular training script with its arguments (this is similar to the <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> helper for
<code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python xla_spawn.py --num_cores num_tpu_you_have <span class="se">\</span>
    path_to_script.py <span class="se">\</span>
	--all_arguments_of_the_script 
</pre></div>
</div>
<p>As an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text
classification MNLI task using the <code class="docutils literal notranslate"><span class="pre">run_glue</span></code> script, with 8 TPUs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python xla_spawn.py --num_cores <span class="m">8</span> <span class="se">\</span>
    text-classification/run_glue.py <span class="se">\</span>
    --model_name_or_path bert-large-uncased-whole-word-masking <span class="se">\</span>
    --task_name mnli <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --max_seq_length <span class="m">128</span> <span class="se">\</span>
    --per_device_train_batch_size <span class="m">8</span> <span class="se">\</span>
    --learning_rate 2e-5 <span class="se">\</span>
    --num_train_epochs <span class="m">3</span>.0 <span class="se">\</span>
    --output_dir /tmp/mnli_output/
</pre></div>
</div>
</div>
<div class="section" id="logging-experiment-tracking">
<h2>Logging &amp; Experiment tracking<a class="headerlink" href="#logging-experiment-tracking" title="Permalink to this headline">Â¶</a></h2>
<p>You can easily log and monitor your runs code. The following are currently supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a></p></li>
<li><p><a class="reference external" href="https://docs.wandb.com/library/integrations/huggingface">Weights &amp; Biases</a></p></li>
<li><p><a class="reference external" href="https://www.comet.ml/docs/python-sdk/huggingface/">Comet ML</a></p></li>
</ul>
<div class="section" id="weights-biases">
<h3>Weights &amp; Biases<a class="headerlink" href="#weights-biases" title="Permalink to this headline">Â¶</a></h3>
<p>To use Weights &amp; Biases, install the wandb package with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install wandb
</pre></div>
</div>
<p>Then log in the command line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wandb login
</pre></div>
</div>
<p>If you are in Jupyter or Colab, you should login with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">wandb</span>
<span class="n">wandb</span><span class="o">.</span><span class="n">login</span><span class="p">()</span>
</pre></div>
</div>
<p>Whenever you use <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> or <code class="docutils literal notranslate"><span class="pre">TFTrainer</span></code> classes, your losses, evaluation metrics, model topology and gradients (for <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> only) will automatically be logged.</p>
<p>When using ðŸ¤— Transformers with PyTorch Lightning, runs can be tracked through <code class="docutils literal notranslate"><span class="pre">WandbLogger</span></code>. Refer to related <a class="reference external" href="https://docs.wandb.com/library/integrations/lightning">documentation &amp; examples</a>.</p>
</div>
<div class="section" id="comet-ml">
<h3>Comet.ml<a class="headerlink" href="#comet-ml" title="Permalink to this headline">Â¶</a></h3>
<p>To use <code class="docutils literal notranslate"><span class="pre">comet_ml</span></code>, install the Python package with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install comet_ml
</pre></div>
</div>
<p>or if in a Conda environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda install -c comet_ml -c anaconda -c conda-forge comet_ml
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="custom_datasets.html" class="btn btn-neutral float-right" title="Fine-tuning with custom datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="pretrained_models.html" class="btn btn-neutral float-left" title="Pretrained models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>