

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.generation_utils &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.generation_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.generation_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2020 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">ModelOutput</span>
<span class="kn">from</span> <span class="nn">.generation_beam_search</span> <span class="kn">import</span> <span class="n">BeamScorer</span><span class="p">,</span> <span class="n">BeamSearchScorer</span>
<span class="kn">from</span> <span class="nn">.generation_logits_process</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HammingDiversityLogitsProcessor</span><span class="p">,</span>
    <span class="n">LogitsProcessorList</span><span class="p">,</span>
    <span class="n">MinLengthLogitsProcessor</span><span class="p">,</span>
    <span class="n">NoBadWordsLogitsProcessor</span><span class="p">,</span>
    <span class="n">NoRepeatNGramLogitsProcessor</span><span class="p">,</span>
    <span class="n">PrefixConstrainedLogitsProcessor</span><span class="p">,</span>
    <span class="n">RepetitionPenaltyLogitsProcessor</span><span class="p">,</span>
    <span class="n">TemperatureLogitsWarper</span><span class="p">,</span>
    <span class="n">TopKLogitsWarper</span><span class="p">,</span>
    <span class="n">TopPLogitsWarper</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="GreedySearchDecoderOnlyOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.GreedySearchDecoderOnlyOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GreedySearchDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of decoder-only generation models using greedy search.</span>


<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of</span>
<span class="sd">            shape :obj:`(batch_size, config.vocab_size)`).</span>
<span class="sd">        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="GreedySearchEncoderDecoderOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.GreedySearchEncoderDecoderOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GreedySearchEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of encoder-decoder generation models using greedy search. Hidden states and attention</span>
<span class="sd">    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the</span>
<span class="sd">    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)</span>


<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of</span>
<span class="sd">            shape :obj:`(batch_size, config.vocab_size)`).</span>
<span class="sd">        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,</span>
<span class="sd">            num_heads, sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_heads, generated_length, sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="SampleDecoderOnlyOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.SampleDecoderOnlyOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SampleDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of decoder-only generation models using sampling.</span>


<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of</span>
<span class="sd">            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).</span>
<span class="sd">        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(num_return_sequences*batch_size, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="SampleEncoderDecoderOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.SampleEncoderDecoderOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SampleEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of encoder-decoder generation models using sampling. Hidden states and attention weights of</span>
<span class="sd">    the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states</span>
<span class="sd">    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)</span>


<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)</span>
<span class="sd">            at each generation step. :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of</span>
<span class="sd">            shape :obj:`(batch_size*num_return_sequences, config.vocab_size)`).</span>
<span class="sd">        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape</span>
<span class="sd">            :obj:`(batch_size*num_return_sequences, num_heads, sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size*num_return_sequences, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="BeamSearchDecoderOnlyOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.BeamSearchDecoderOnlyOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BeamSearchDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of decoder-only generation models using beam search.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Final beam scores of the generated ``sequences``.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log</span>
<span class="sd">            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam</span>
<span class="sd">            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape</span>
<span class="sd">            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).</span>
<span class="sd">        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,</span>
<span class="sd">            hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="BeamSearchEncoderDecoderOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.BeamSearchEncoderDecoderOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BeamSearchEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of encoder-decoder generation models using beam search. Hidden states and attention weights</span>
<span class="sd">    of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the encoder_hidden_states</span>
<span class="sd">    attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_return_sequences)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Final beam scores of the generated ``sequences``.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log</span>
<span class="sd">            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam</span>
<span class="sd">            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape</span>
<span class="sd">            :obj:`(batch_size*num_beams, config.vocab_size)`).</span>
<span class="sd">        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,</span>
<span class="sd">            num_heads, sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, num_heads,</span>
<span class="sd">            generated_length, sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams*num_return_sequences, generated_length,</span>
<span class="sd">            hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="BeamSampleDecoderOnlyOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.BeamSampleDecoderOnlyOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BeamSampleDecoderOnlyOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of decoder-only generation models using beam sample.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_return_sequences, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Final beam scores of the generated ``sequences``.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log</span>
<span class="sd">            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam</span>
<span class="sd">            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape</span>
<span class="sd">            :obj:`(batch_size*num_beams*num_return_sequences, config.vocab_size)`).</span>
<span class="sd">        attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="BeamSampleEncoderDecoderOutput"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.BeamSampleEncoderDecoderOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BeamSampleEncoderDecoderOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of encoder-decoder generation models using beam sampling. Hidden states and attention</span>
<span class="sd">    weights of the decoder (respectively the encoder) can be accessed via the encoder_attentions and the</span>
<span class="sd">    encoder_hidden_states attributes (respectively the decoder_attentions and the decoder_hidden_states attributes)</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (:obj:`torch.LongTensor` of shape :obj:`(batch_size*num_beams, sequence_length)`):</span>
<span class="sd">            The generated sequences. The second dimension (sequence_length) is either equal to :obj:`max_length` or</span>
<span class="sd">            shorter if all batches finished early due to the :obj:`eos_token_id`.</span>
<span class="sd">        sequences_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size * num_return_sequence)`, `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Final beam scores of the generated ``sequences``.</span>
<span class="sd">        scores (:obj:`tuple(torch.FloatTensor)` `optional`, returned when ``output_scores=True`` is passed or when ``config.output_scores=True``):</span>
<span class="sd">            Processed beam scores for each vocabulary token at each generation step. Beam scores consisting of log</span>
<span class="sd">            softmax scores for each vocabulary token and sum of log softmax of previously generated tokens in this beam</span>
<span class="sd">            . :obj:`(max_length,)`-shaped tuple of :obj:`torch.FloatTensor` with each tensor of shape</span>
<span class="sd">            :obj:`(batch_size*num_beams, config.vocab_size)`).</span>
<span class="sd">        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer of the decoder) of shape :obj:`(batch_size,</span>
<span class="sd">            num_heads, sequence_length, sequence_length)`.</span>
<span class="sd">        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)</span>
<span class="sd">            of shape :obj:`(batch_size*num_beams, sequence_length, hidden_size)`.</span>
<span class="sd">        decoder_attentions (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_attentions=True`` is passed or ``config.output_attentions=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, num_heads, generated_length,</span>
<span class="sd">            sequence_length)`.</span>
<span class="sd">        decoder_hidden_states (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of</span>
<span class="sd">            :obj:`torch.FloatTensor` of shape :obj:`(batch_size*num_beams, generated_length, hidden_size)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">sequences_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">decoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span></div>


<span class="n">GreedySearchOutput</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">GreedySearchEncoderDecoderOutput</span><span class="p">,</span> <span class="n">GreedySearchDecoderOnlyOutput</span><span class="p">]</span>
<span class="n">SampleOutput</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">SampleEncoderDecoderOutput</span><span class="p">,</span> <span class="n">SampleDecoderOnlyOutput</span><span class="p">]</span>
<span class="n">BeamSearchOutput</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">BeamSearchEncoderDecoderOutput</span><span class="p">,</span> <span class="n">BeamSearchDecoderOnlyOutput</span><span class="p">]</span>
<span class="n">BeamSampleOutput</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">BeamSampleEncoderDecoderOutput</span><span class="p">,</span> <span class="n">BeamSampleDecoderOnlyOutput</span><span class="p">]</span>


<div class="viewcode-block" id="GenerationMixin"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin">[docs]</a><span class="k">class</span> <span class="nc">GenerationMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class containing all of the functions supporting generation, to be used as a mixin in</span>
<span class="sd">    :class:`~transformers.PreTrainedModel`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="GenerationMixin.prepare_inputs_for_generation"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the</span>
<span class="sd">        generate method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">}</span></div>

<div class="viewcode-block" id="GenerationMixin.adjust_logits_during_generation"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.adjust_logits_during_generation">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_logits_during_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in</span>
<span class="sd">        the generate method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">logits</span></div>

    <span class="k">def</span> <span class="nf">_prepare_input_ids_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`bos_token_id` has to be defined when no `input_ids` are provided.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">bos_token_id</span>

    <span class="k">def</span> <span class="nf">_prepare_attention_mask_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
        <span class="n">is_pad_token_in_inputs_ids</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">)</span>
        <span class="n">is_pad_token_not_equal_to_eos_token_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="o">!=</span> <span class="n">eos_token_id</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_pad_token_in_inputs_ids</span> <span class="ow">and</span> <span class="n">is_pad_token_not_equal_to_eos_token_id</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_encoder_decoder_kwargs_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">model_kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># retrieve encoder hidden states</span>
        <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()</span>
        <span class="n">encoder_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">argument</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">argument</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">argument</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;decoder_&quot;</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]:</span> <span class="n">ModelOutput</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">encoder_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="k">def</span> <span class="nf">_prepare_decoder_input_ids_for_generation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
        <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_decoder_start_token_id</span><span class="p">(</span><span class="n">decoder_start_token_id</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">)</span>
        <span class="n">decoder_input_ids</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">*</span> <span class="n">decoder_start_token_id</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">decoder_input_ids</span>

    <span class="k">def</span> <span class="nf">_get_pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setting `pad_token_id` to `eos_token_id`:</span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2"> for open-end generation.&quot;</span><span class="p">)</span>
            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>
        <span class="k">return</span> <span class="n">pad_token_id</span>

    <span class="k">def</span> <span class="nf">_get_decoder_start_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decoder_start_token_id</span> <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="p">)</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span>

        <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">decoder_start_token_id</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;decoder&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="s2">&quot;decoder_start_token_id&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="k">elif</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">bos_token_id</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;decoder&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="s2">&quot;bos_token_id&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.&quot;</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_expand_inputs_for_generation</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">expand_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">:</span> <span class="n">ModelOutput</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="n">expanded_return_idx</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">expand_size</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">expanded_return_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">expanded_return_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">expanded_return_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">encoder_outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">encoder_outputs</span><span class="p">[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span>
                <span class="mi">0</span><span class="p">,</span> <span class="n">expanded_return_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder_outputs</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_init_sequence_length_for_generation</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sequence_lengths</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span><span class="p">,</span> <span class="n">cur_len</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_update_seq_length_for_generation</span><span class="p">(</span>
        <span class="n">sequence_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">unfinished_sequences</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">is_eos_in_next_token</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="c1"># check if sentence is not finished yet</span>
        <span class="n">is_sent_unfinished</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">is_eos_in_next_token</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

        <span class="c1"># update sentence length</span>
        <span class="n">sequence_lengths</span> <span class="o">=</span> <span class="n">sequence_lengths</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">is_sent_unfinished</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
        <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="o">~</span><span class="n">is_eos_in_next_token</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_update_model_kwargs_for_generation</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">ModelOutput</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">is_encoder_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># update past</span>
        <span class="k">if</span> <span class="s2">&quot;past_key_values&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_key_values</span>
        <span class="k">elif</span> <span class="s2">&quot;mems&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">mems</span>
        <span class="k">elif</span> <span class="s2">&quot;past_buckets_states&quot;</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">past_buckets_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># update token_type_ids with last value</span>
        <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># update attention mask</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">model_kwargs</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function is used to re-order the :obj:`past_key_values` or :obj:`mems` cache if</span>
<span class="sd">        :meth:`~transformers.PretrainedModel.beam_search` or :meth:`~transformers.PretrainedModel.beam_sample` is</span>
<span class="sd">        called. This is required to match :obj:`past_key_values` or :obj:`mems` with the correct beam_idx at every</span>
<span class="sd">        generation step.</span>

<span class="sd">        For custom re-ordering of :obj:`past_key_values` or :obj:`mems`, the function should be implemented in</span>
<span class="sd">        subclasses of :class:`~transformers.PreTrainedModel`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">layer_past</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">beam_idx</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer_past</span><span class="o">.</span><span class="n">device</span><span class="p">))</span> <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_logits_warper</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LogitsProcessorList</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant</span>
<span class="sd">        :obj:`~transformers.LogitsWarper` instances used for multinomial sampling.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init warp parameters</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">top_k</span> <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span> <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span> <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span>
        <span class="c1"># instantiate warpers list</span>
        <span class="n">warpers</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>

        <span class="c1"># the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files</span>
        <span class="c1"># all samplers can be found in `generation_utils_samplers.py`</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="n">temperature</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">warpers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TopPLogitsWarper</span><span class="p">(</span><span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">warpers</span>

    <span class="k">def</span> <span class="nf">_get_logits_processor</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bad_words_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">min_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_beam_groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">diversity_penalty</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LogitsProcessorList</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This class returns a :obj:`~transformers.LogitsProcessorList` list object that contains all relevant</span>
<span class="sd">        :obj:`~transformers.LogitsProcessor` instances used to modify the scores of the language model head.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init warp parameters</span>
        <span class="n">repetition_penalty</span> <span class="o">=</span> <span class="n">repetition_penalty</span> <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">no_repeat_ngram_size</span> <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span>
        <span class="p">)</span>
        <span class="n">bad_words_ids</span> <span class="o">=</span> <span class="n">bad_words_ids</span> <span class="k">if</span> <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bad_words_ids</span>
        <span class="n">min_length</span> <span class="o">=</span> <span class="n">min_length</span> <span class="k">if</span> <span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_length</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">diversity_penalty</span> <span class="o">=</span> <span class="n">diversity_penalty</span> <span class="k">if</span> <span class="n">diversity_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">diversity_penalty</span>
        <span class="c1"># instantiate processors list</span>
        <span class="n">processors</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>

        <span class="c1"># the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files</span>
        <span class="c1"># all samplers can be found in `generation_utils_samplers.py`</span>
        <span class="k">if</span> <span class="n">diversity_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">diversity_penalty</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">HammingDiversityLogitsProcessor</span><span class="p">(</span>
                    <span class="n">diversity_penalty</span><span class="o">=</span><span class="n">diversity_penalty</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">num_beam_groups</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RepetitionPenaltyLogitsProcessor</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NoRepeatNGramLogitsProcessor</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">NoBadWordsLogitsProcessor</span><span class="p">(</span><span class="n">bad_words_ids</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">min_length</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">prefix_allowed_tokens_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">processors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PrefixConstrainedLogitsProcessor</span><span class="p">(</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">processors</span>

<div class="viewcode-block" id="GenerationMixin.generate"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.generate">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bad_words_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_beam_groups</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">diversity_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_allowed_tokens_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GreedySearchOutput</span><span class="p">,</span> <span class="n">SampleOutput</span><span class="p">,</span> <span class="n">BeamSearchOutput</span><span class="p">,</span> <span class="n">BeamSampleOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,</span>
<span class="sd">        multinomial sampling, beam-search decoding, and beam-search multinomial sampling.</span>

<span class="sd">        Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the</span>
<span class="sd">        attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values</span>
<span class="sd">        indicated are the default values of those config.</span>

<span class="sd">        Most of these parameters are explained in more detail in `this blog post</span>
<span class="sd">        &lt;https://huggingface.co/blog/how-to-generate&gt;`__.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            min_length (:obj:`int`, `optional`, defaults to 10):</span>
<span class="sd">                The minimum length of the sequence to be generated.</span>
<span class="sd">            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to use sampling ; use greedy decoding otherwise.</span>
<span class="sd">            early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.</span>
<span class="sd">            num_beams (:obj:`int`, `optional`, defaults to 1):</span>
<span class="sd">                Number of beams for beam search. 1 means no beam search.</span>
<span class="sd">            temperature (:obj:`float`, `optional`, defaults tp 1.0):</span>
<span class="sd">                The value used to module the next token probabilities.</span>
<span class="sd">            top_k (:obj:`int`, `optional`, defaults to 50):</span>
<span class="sd">                The number of highest probability vocabulary tokens to keep for top-k-filtering.</span>
<span class="sd">            top_p (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                If set to float &lt; 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or</span>
<span class="sd">                higher are kept for generation.</span>
<span class="sd">            repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                The parameter for repetition penalty. 1.0 means no penalty. See `this paper</span>
<span class="sd">                &lt;https://arxiv.org/pdf/1909.05858.pdf&gt;`__ for more details.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            bos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `beginning-of-sequence` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            length_penalty (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the</span>
<span class="sd">                model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer</span>
<span class="sd">                sequences.</span>
<span class="sd">            no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If set to int &gt; 0, all ngrams of that size can only occur once.</span>
<span class="sd">            bad_words_ids(:obj:`List[List[int]]`, `optional`):</span>
<span class="sd">                List of token ids that are not allowed to be generated. In order to get the tokens of the words that</span>
<span class="sd">                should not appear in the generated text, use :obj:`tokenizer(bad_word,</span>
<span class="sd">                add_prefix_space=True).input_ids`.</span>
<span class="sd">            num_return_sequences(:obj:`int`, `optional`, defaults to 1):</span>
<span class="sd">                The number of independently computed returned sequences for each element in the batch.</span>
<span class="sd">            attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for</span>
<span class="sd">                tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same</span>
<span class="sd">                shape as :obj:`input_ids` that masks the pad token. `What are attention masks?</span>
<span class="sd">                &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="sd">            decoder_start_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.</span>
<span class="sd">            use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not the model should use the past last key/values attentions (if applicable to the model) to</span>
<span class="sd">                speed up decoding.</span>
<span class="sd">            num_beam_groups (:obj:`int`, `optional`, defaults to 1):</span>
<span class="sd">                Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of</span>
<span class="sd">                beams. `this paper &lt;https://arxiv.org/pdf/1610.02424.pdf&gt;`__ for more details.</span>
<span class="sd">            diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):</span>
<span class="sd">                This value is subtracted from a beam&#39;s score if it generates a token same as any beam from other group</span>
<span class="sd">                at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is</span>
<span class="sd">                enabled.</span>
<span class="sd">            prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):</span>
<span class="sd">                If provided, this function constraints the beam search to allowed tokens only at each step. If not</span>
<span class="sd">                provided no constraint is applied. This function takes 2 arguments :obj:`inputs_ids` and the batch ID</span>
<span class="sd">                :obj:`batch_id`. It has to return a list with the allowed tokens for the next generation step</span>
<span class="sd">                conditioned on the previously generated tokens :obj:`inputs_ids` and the batch ID :obj:`batch_id`. This</span>
<span class="sd">                argument is useful for constrained generation conditioned on the prefix, as described in</span>
<span class="sd">                `Autoregressive Entity Retrieval &lt;https://arxiv.org/abs/2010.00904&gt;`__.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>

<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the</span>
<span class="sd">                model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific</span>
<span class="sd">                kwargs should be prefixed with `decoder_`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A</span>
<span class="sd">            :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when</span>
<span class="sd">            ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.</span>

<span class="sd">                If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the</span>
<span class="sd">                possible :class:`~transformers.file_utils.ModelOutput` types are:</span>

<span class="sd">                    - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`</span>

<span class="sd">                If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible</span>
<span class="sd">                :class:`~transformers.file_utils.ModelOutput` types are:</span>

<span class="sd">                    - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,</span>
<span class="sd">                    - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`</span>

<span class="sd">        Examples::</span>
<span class="sd">            &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilgpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilgpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # do greedy decoding without providing a prompt</span>
<span class="sd">            &gt;&gt;&gt; outputs = model.generate(max_length=40)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.decode(outputs[0], skip_special_tokens=True))</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">            &gt;&gt;&gt; document = (</span>
<span class="sd">            ... &quot;at least two people were killed in a suspected bomb attack on a passenger bus &quot;</span>
<span class="sd">            ... &quot;in the strife-torn southern philippines on monday , the military said.&quot;</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; # encode input contex</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(document, return_tensors=&quot;pt&quot;).input_ids</span>
<span class="sd">            &gt;&gt;&gt; # generate 3 independent sequences using beam search decoding (5 beams)</span>
<span class="sd">            &gt;&gt;&gt; # with T5 encoder-decoder model conditioned on short news article.</span>
<span class="sd">            &gt;&gt;&gt; outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;distilgpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;distilgpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; input_context = &quot;The dog&quot;</span>
<span class="sd">            &gt;&gt;&gt; # encode input context</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(input_context, return_tensors=&quot;pt&quot;).input_ids</span>
<span class="sd">            &gt;&gt;&gt; # generate 3 candidates using sampling</span>
<span class="sd">            &gt;&gt;&gt; outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;ctrl&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;ctrl&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="sd">            &gt;&gt;&gt; input_context = &quot;Legal My neighbor is&quot;</span>
<span class="sd">            &gt;&gt;&gt; # encode input context</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(input_context, return_tensors=&quot;pt&quot;).input_ids</span>
<span class="sd">            &gt;&gt;&gt; outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.decode(outputs[0], skip_special_tokens=True))</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; input_context = &quot;My cute dog&quot;</span>
<span class="sd">            &gt;&gt;&gt; # get tokens of words that should not be generated</span>
<span class="sd">            &gt;&gt;&gt; bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [&quot;idiot&quot;, &quot;stupid&quot;, &quot;shut up&quot;]]</span>
<span class="sd">            &gt;&gt;&gt; # encode input context</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(input_context, return_tensors=&quot;pt&quot;).input_ids</span>
<span class="sd">            &gt;&gt;&gt; # generate sequences without allowing bad_words to be generated</span>
<span class="sd">            &gt;&gt;&gt; outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.decode(outputs[0], skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># set init values</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">num_beam_groups</span> <span class="o">=</span> <span class="n">num_beam_groups</span> <span class="k">if</span> <span class="n">num_beam_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_beam_groups</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">do_sample</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">num_return_sequences</span> <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_return_sequences</span>
        <span class="p">)</span>

        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>

        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;output_attentions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_attentions</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;output_hidden_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_hidden_states</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># init `input_ids` with bos_token_id</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_input_ids_for_generation</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># init `attention_mask` depending on `pad_token_id`</span>
            <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_attention_mask_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span>
            <span class="p">)</span>

        <span class="c1"># special case if pad_token_id is not defined</span>
        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Setting `pad_token_id` to `eos_token_id`:</span><span class="si">{</span><span class="n">eos_token_id</span><span class="si">}</span><span class="s2"> for open-end generation.&quot;</span><span class="p">)</span>
            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># add encoder_outputs to model_kwargs</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_encoder_decoder_kwargs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># set input_ids as decoder_input_ids</span>
            <span class="k">if</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="ow">in</span> <span class="n">model_kwargs</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;decoder_input_ids&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_decoder_input_ids_for_generation</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span> <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;encoder_outputs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_kwargs</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">],</span> <span class="n">ModelOutput</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Make sure that `model_kwargs` include `encoder_outputs` of type `ModelOutput`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">input_ids_string</span> <span class="o">=</span> <span class="s2">&quot;decoder_input_ids&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="s2">&quot;input_ids&quot;</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Input length of </span><span class="si">{</span><span class="n">input_ids_string</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, but ``max_length`` is set to </span><span class="si">{</span><span class="n">max_length</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="s2">&quot;This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># determine generation mode</span>
        <span class="n">is_greedy_gen_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_beams</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_beam_groups</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span>
        <span class="n">is_sample_gen_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_beams</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_beam_groups</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">is_beam_gen_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_beam_groups</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span>
        <span class="n">is_beam_sample_gen_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_beam_groups</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">is_group_beam_gen_mode</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_beam_groups</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_beam_groups</span> <span class="o">&gt;</span> <span class="n">num_beams</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_beam_groups` has to be smaller or equal to `num_beams`&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_group_beam_gen_mode</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># set model_kwargs</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;use_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">use_cache</span>

        <span class="c1"># get distribution pre_processing samplers</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_processor</span><span class="p">(</span>
            <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
            <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
            <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">,</span>
            <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="n">prefix_allowed_tokens_fn</span><span class="o">=</span><span class="n">prefix_allowed_tokens_fn</span><span class="p">,</span>
            <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
            <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">num_beam_groups</span><span class="p">,</span>
            <span class="n">diversity_penalty</span><span class="o">=</span><span class="n">diversity_penalty</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_greedy_gen_mode</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;num_return_sequences has to be 1, but is </span><span class="si">{</span><span class="n">num_return_sequences</span><span class="si">}</span><span class="s2"> when doing greedy search.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># greedy search</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">output_scores</span><span class="o">=</span><span class="n">output_scores</span><span class="p">,</span>
                <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">is_sample_gen_mode</span><span class="p">:</span>
            <span class="c1"># get probability distribution warper</span>
            <span class="n">logits_warper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span>
            <span class="p">)</span>

            <span class="c1"># expand input_ids with `num_return_sequences` additional sequences per batch</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># sample</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">output_scores</span><span class="o">=</span><span class="n">output_scores</span><span class="p">,</span>
                <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">is_beam_gen_mode</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span> <span class="k">if</span> <span class="n">early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">early_stopping</span>

            <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="n">num_beams</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_return_sequences` has to be smaller or equal to `num_beams`.&quot;</span><span class="p">)</span>

            <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># interleave with `num_beams`</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">expand_size</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">output_scores</span><span class="o">=</span><span class="n">output_scores</span><span class="p">,</span>
                <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">is_beam_sample_gen_mode</span><span class="p">:</span>
            <span class="n">logits_warper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_logits_warper</span><span class="p">(</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span>
            <span class="p">)</span>

            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_return_sequences</span>

            <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># interleave with `num_beams * num_return_sequences`</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">expand_size</span><span class="o">=</span><span class="n">num_beams</span> <span class="o">*</span> <span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_sample</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">output_scores</span><span class="o">=</span><span class="n">output_scores</span><span class="p">,</span>
                <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">elif</span> <span class="n">is_group_beam_gen_mode</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span> <span class="k">if</span> <span class="n">early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">early_stopping</span>

            <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="n">num_beams</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_return_sequences` has to be smaller or equal to `num_beams`.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_beams</span> <span class="o">%</span> <span class="n">num_beam_groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_beams` should be divisible by `num_beam_groups` for group beam search.&quot;</span><span class="p">)</span>

            <span class="n">diverse_beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">do_early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">num_beam_hyps_to_keep</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">num_beam_groups</span><span class="o">=</span><span class="n">num_beam_groups</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># interleave with `num_beams`</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">expand_size</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">diverse_beam_scorer</span><span class="p">,</span>
                <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">output_scores</span><span class="o">=</span><span class="n">output_scores</span><span class="p">,</span>
                <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="n">return_dict_in_generate</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="GenerationMixin.greedy_search"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.greedy_search">[docs]</a>    <span class="k">def</span> <span class="nf">greedy_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">GreedySearchOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head using greedy decoding.</span>



<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            logits_processor (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling</span>
<span class="sd">                head applied at each generation step.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>

<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific keyword arguments will be forwarded to the :obj:`forward` function of the</span>
<span class="sd">                model. If model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,</span>
<span class="sd">            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A</span>
<span class="sd">            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a</span>
<span class="sd">            :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=True``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import (</span>
<span class="sd">            ... AutoTokenizer,</span>
<span class="sd">            ... AutoModelForCausalLM,</span>
<span class="sd">            ... LogitsProcessorList,</span>
<span class="sd">            ... MinLengthLogitsProcessor,</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span>

<span class="sd">            &gt;&gt;&gt; # set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="sd">            &gt;&gt;&gt; model.config.pad_token_id = model.config.eos_token_id</span>

<span class="sd">            &gt;&gt;&gt; input_prompt = &quot;Today is a beautiful day, and&quot;</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(input_prompt, return_tensors=&quot;pt&quot;).input_ids</span>

<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_processor = LogitsProcessorList([</span>
<span class="sd">            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),</span>
<span class="sd">            ... ])</span>

<span class="sd">            &gt;&gt;&gt; outputs = model.greedy_search(input_ids, logits_processor=logits_processor)</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># init sequence length tensors</span>
        <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_sequence_length_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span>
        <span class="p">)</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="c1"># prepare model inputs</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># forward pass to get next token</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_logits</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># pre-process distribution</span>
            <span class="n">next_tokens_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">)</span>

            <span class="c1"># argmax</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_tokens_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># add code that transfomers next_tokens to tokens_to_add</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;If eos_token_id is defined, make sure that pad_token_id is defined.&quot;</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">*</span> <span class="n">unfinished_sequences</span> <span class="o">+</span> <span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sequences</span><span class="p">)</span>

            <span class="c1"># add token and increase length by one</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># update sequence length</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_seq_length_for_generation</span><span class="p">(</span>
                    <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="p">)</span>

            <span class="c1"># update model kwargs</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>

            <span class="c1"># stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length</span>
            <span class="k">if</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># increase cur_len</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GreedySearchEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GreedySearchDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span></div>

<div class="viewcode-block" id="GenerationMixin.sample"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.sample">[docs]</a>    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">SampleOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head using multinomial sampling.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            logits_processor (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling</span>
<span class="sd">                head applied at each generation step.</span>
<span class="sd">            logits_warper (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language</span>
<span class="sd">                modeling head applied before multinomial sampling at each generation step.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If</span>
<span class="sd">                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,</span>
<span class="sd">            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A</span>
<span class="sd">            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            :class:`~transformers.generation_utils.SampleDecoderOnlyOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a</span>
<span class="sd">            :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=True``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import (</span>
<span class="sd">            ...    AutoTokenizer,</span>
<span class="sd">            ...    AutoModelForCausalLM,</span>
<span class="sd">            ...    LogitsProcessorList,</span>
<span class="sd">            ...    MinLengthLogitsProcessor,</span>
<span class="sd">            ...    TopKLogitsWarper,</span>
<span class="sd">            ...    TemperatureLogitsWarper,</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span>

<span class="sd">            &gt;&gt;&gt; # set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="sd">            &gt;&gt;&gt; model.config.pad_token_id = model.config.eos_token_id</span>

<span class="sd">            &gt;&gt;&gt; input_prompt = &quot;Today is a beautiful day, and&quot;</span>
<span class="sd">            &gt;&gt;&gt; input_ids = tokenizer(input_prompt, return_tensors=&quot;pt&quot;).input_ids</span>

<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_processor = LogitsProcessorList([</span>
<span class="sd">            ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),</span>
<span class="sd">            ... ])</span>
<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_warper = LogitsProcessorList([</span>
<span class="sd">            ...     TopKLogitsWarper(50),</span>
<span class="sd">            ...     TemperatureLogitsWarper(0.7),</span>
<span class="sd">            ... ])</span>

<span class="sd">            &gt;&gt;&gt; outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">logits_warper</span> <span class="o">=</span> <span class="n">logits_warper</span> <span class="k">if</span> <span class="n">logits_warper</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="c1"># init sequence length tensors</span>
        <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_sequence_length_for_generation</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span>
        <span class="p">)</span>

        <span class="c1"># auto-regressive generation</span>
        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="c1"># prepare model inputs</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># forward pass to get next token</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># pre-process distribution</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># sample</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># add code that transfomers next_tokens to tokens_to_add</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;If eos_token_id is defined, make sure that pad_token_id is defined.&quot;</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">*</span> <span class="n">unfinished_sequences</span> <span class="o">+</span> <span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sequences</span><span class="p">)</span>

            <span class="c1"># add token and increase length by one</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="c1"># update sequence length</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_seq_length_for_generation</span><span class="p">(</span>
                    <span class="n">sequence_lengths</span><span class="p">,</span> <span class="n">unfinished_sequences</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="p">)</span>

            <span class="c1"># stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length</span>
            <span class="k">if</span> <span class="n">unfinished_sequences</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># update model kwargs</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">SampleEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">SampleDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">input_ids</span></div>

<div class="viewcode-block" id="GenerationMixin.beam_search"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.beam_search">[docs]</a>    <span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">BeamScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">BeamSearchOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head using beam search decoding.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            beam_scorer (:obj:`BeamScorer`):</span>
<span class="sd">                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are</span>
<span class="sd">                constructed, stored and sorted during generation. For more information, the documentation of</span>
<span class="sd">                :class:`~transformers.BeamScorer` should be read.</span>
<span class="sd">            logits_processor (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling</span>
<span class="sd">                head applied at each generation step.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If</span>
<span class="sd">                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A</span>
<span class="sd">            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=True``.</span>


<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import (</span>
<span class="sd">            ...    AutoTokenizer,</span>
<span class="sd">            ...    AutoModelForSeq2SeqLM,</span>
<span class="sd">            ...    LogitsProcessorList,</span>
<span class="sd">            ...    MinLengthLogitsProcessor,</span>
<span class="sd">            ...    BeamSearchScorer,</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>

<span class="sd">            &gt;&gt;&gt; encoder_input_str = &quot;translate English to German: How old are you?&quot;</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_ids = tokenizer(encoder_input_str, return_tensors=&quot;pt&quot;).input_ids</span>


<span class="sd">            &gt;&gt;&gt; # lets run beam search using 3 beams</span>
<span class="sd">            &gt;&gt;&gt; num_beams = 3</span>
<span class="sd">            &gt;&gt;&gt; # define decoder start token ids</span>
<span class="sd">            &gt;&gt;&gt; input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)</span>
<span class="sd">            &gt;&gt;&gt; input_ids = input_ids * model.config.decoder_start_token_id</span>

<span class="sd">            &gt;&gt;&gt; # add encoder_outputs to model keyword arguments</span>
<span class="sd">            &gt;&gt;&gt; model_kwargs = {</span>
<span class="sd">            ...     &quot;encoder_outputs&quot;: model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)</span>
<span class="sd">            ... }</span>

<span class="sd">            &gt;&gt;&gt; # instantiate beam scorer</span>
<span class="sd">            &gt;&gt;&gt; beam_scorer = BeamSearchScorer(</span>
<span class="sd">            ...     batch_size=1,</span>
<span class="sd">            ...     max_length=model.config.max_length,</span>
<span class="sd">            ...     num_beams=num_beams,</span>
<span class="sd">            ...     device=model.device,</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_processor = LogitsProcessorList([</span>
<span class="sd">            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),</span>
<span class="sd">            ... ])</span>

<span class="sd">            &gt;&gt;&gt; outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="n">batch_beam_size</span>
        <span class="p">),</span> <span class="s2">&quot;Batch dimension of `input_ids` should be {num_beams * batch_size}, but is </span><span class="si">{batch_beam_size}</span><span class="s2">.&quot;</span>

        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># adjust tokens for Bart, *e.g.*</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
            <span class="p">)</span>

            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># reshape for beam search</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

            <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">next_token_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>

            <span class="n">next_indices</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">//</span> <span class="n">vocab_size</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

            <span class="c1"># stateless</span>
            <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">next_token_scores</span><span class="p">,</span>
                <span class="n">next_tokens</span><span class="p">,</span>
                <span class="n">next_indices</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
            <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">next_indices</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span></div>

<div class="viewcode-block" id="GenerationMixin.beam_sample"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.beam_sample">[docs]</a>    <span class="k">def</span> <span class="nf">beam_sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">BeamScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logits_warper</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">BeamSampleOutput</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head using beam search with multinomial sampling.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            beam_scorer (:obj:`BeamScorer`):</span>
<span class="sd">                A derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are</span>
<span class="sd">                constructed, stored and sorted during generation. For more information, the documentation of</span>
<span class="sd">                :class:`~transformers.BeamScorer` should be read.</span>
<span class="sd">            logits_processor (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling</span>
<span class="sd">                head applied at each generation step.</span>
<span class="sd">            logits_warper (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language</span>
<span class="sd">                modeling head applied before multinomial sampling at each generation step.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If</span>
<span class="sd">                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`,</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A</span>
<span class="sd">            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=True``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import (</span>
<span class="sd">            ...     AutoTokenizer,</span>
<span class="sd">            ...     AutoModelForSeq2SeqLM,</span>
<span class="sd">            ...     LogitsProcessorList,</span>
<span class="sd">            ...     MinLengthLogitsProcessor,</span>
<span class="sd">            ...     TopKLogitsWarper,</span>
<span class="sd">            ...     TemperatureLogitsWarper,</span>
<span class="sd">            ...     BeamSearchScorer,</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>

<span class="sd">            &gt;&gt;&gt; encoder_input_str = &quot;translate English to German: How old are you?&quot;</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_ids = tokenizer(encoder_input_str, return_tensors=&quot;pt&quot;).input_ids</span>

<span class="sd">            &gt;&gt;&gt; # lets run beam search using 3 beams</span>
<span class="sd">            &gt;&gt;&gt; num_beams = 3</span>
<span class="sd">            &gt;&gt;&gt; # define decoder start token ids</span>
<span class="sd">            &gt;&gt;&gt; input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)</span>
<span class="sd">            &gt;&gt;&gt; input_ids = input_ids * model.config.decoder_start_token_id</span>

<span class="sd">            &gt;&gt;&gt; # add encoder_outputs to model keyword arguments</span>
<span class="sd">            &gt;&gt;&gt; model_kwargs = {</span>
<span class="sd">            ...     &quot;encoder_outputs&quot;: model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)</span>
<span class="sd">            ... }</span>

<span class="sd">            &gt;&gt;&gt; # instantiate beam scorer</span>
<span class="sd">            &gt;&gt;&gt; beam_scorer = BeamSearchScorer(</span>
<span class="sd">            ...     batch_size=1,</span>
<span class="sd">            ...     max_length=model.config.max_length,</span>
<span class="sd">            ...     num_beams=num_beams,</span>
<span class="sd">            ...     device=model.device,</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_processor = LogitsProcessorList([</span>
<span class="sd">            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)</span>
<span class="sd">            ... ])</span>
<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_warper = LogitsProcessorList([</span>
<span class="sd">            ...     TopKLogitsWarper(50),</span>
<span class="sd">            ...     TemperatureLogitsWarper(0.7),</span>
<span class="sd">            ... ])</span>

<span class="sd">            &gt;&gt;&gt; outputs = model.beam_sample(</span>
<span class="sd">            ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># adjust token scores (a no-op by default)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
                <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
            <span class="p">)</span>

            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_warper</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">next_token_scores</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="c1"># reshape for beam search</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>
            <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">)</span>

            <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">next_token_scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">_indices</span><span class="p">)</span>

            <span class="n">next_indices</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">//</span> <span class="n">vocab_size</span>
            <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

            <span class="c1"># stateless</span>
            <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">next_token_scores</span><span class="p">,</span>
                <span class="n">next_tokens</span><span class="p">,</span>
                <span class="n">next_indices</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
            <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">],</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">next_indices</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span></div>

<div class="viewcode-block" id="GenerationMixin.group_beam_search"><a class="viewcode-back" href="../../main_classes/model.html#transformers.generation_utils.GenerationMixin.group_beam_search">[docs]</a>    <span class="k">def</span> <span class="nf">group_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">beam_scorer</span><span class="p">:</span> <span class="n">BeamScorer</span><span class="p">,</span>
        <span class="n">logits_processor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogitsProcessorList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict_in_generate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head using beam search decoding.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`torch.LongTensor` of shape :obj:`(1,)`.</span>
<span class="sd">            beam_scorer (:obj:`BeamScorer`):</span>
<span class="sd">                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are</span>
<span class="sd">                constructed, stored and sorted during generation. For more information, the documentation of</span>
<span class="sd">                :class:`~transformers.BeamScorer` should be read.</span>
<span class="sd">            logits_processor (:obj:`LogitsProcessorList`, `optional`):</span>
<span class="sd">                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from</span>
<span class="sd">                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling</span>
<span class="sd">                head applied at each generation step.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            output_attentions (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under</span>
<span class="sd">                returned tensors for more details.</span>
<span class="sd">            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors</span>
<span class="sd">                for more details.</span>
<span class="sd">            output_scores (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.</span>
<span class="sd">            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="sd">            model_kwargs:</span>
<span class="sd">                Additional model specific kwargs that will be forwarded to the :obj:`forward` function of the model. If</span>
<span class="sd">                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A</span>
<span class="sd">            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a</span>
<span class="sd">            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if</span>
<span class="sd">            ``model.config.is_encoder_decoder=True``.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import (</span>
<span class="sd">            ...    AutoTokenizer,</span>
<span class="sd">            ...    AutoModelForSeq2SeqLM,</span>
<span class="sd">            ...    LogitsProcessorList,</span>
<span class="sd">            ...    MinLengthLogitsProcessor,</span>
<span class="sd">            ...    HammingDiversityLogitsProcessor,</span>
<span class="sd">            ...    BeamSearchScorer,</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>

<span class="sd">            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;t5-base&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = AutoModelForSeq2SeqLM.from_pretrained(&quot;t5-base&quot;)</span>

<span class="sd">            &gt;&gt;&gt; encoder_input_str = &quot;translate English to German: How old are you?&quot;</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_ids = tokenizer(encoder_input_str, return_tensors=&quot;pt&quot;).input_ids</span>


<span class="sd">            &gt;&gt;&gt; # lets run diverse beam search using 6 beams</span>
<span class="sd">            &gt;&gt;&gt; num_beams = 6</span>
<span class="sd">            &gt;&gt;&gt; # define decoder start token ids</span>
<span class="sd">            &gt;&gt;&gt; input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)</span>
<span class="sd">            &gt;&gt;&gt; input_ids = input_ids * model.config.decoder_start_token_id</span>

<span class="sd">            &gt;&gt;&gt; # add encoder_outputs to model keyword arguments</span>
<span class="sd">            &gt;&gt;&gt; model_kwargs = {</span>
<span class="sd">            ...     &quot;encoder_outputs&quot;: model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)</span>
<span class="sd">            ... }</span>

<span class="sd">            &gt;&gt;&gt; # instantiate beam scorer</span>
<span class="sd">            &gt;&gt;&gt; beam_scorer = BeamSearchScorer(</span>
<span class="sd">            ...     batch_size=1,</span>
<span class="sd">            ...     max_length=model.config.max_length,</span>
<span class="sd">            ...     num_beams=num_beams,</span>
<span class="sd">            ...     device=model.device,</span>
<span class="sd">            ...     num_beam_groups=3</span>
<span class="sd">            ... )</span>

<span class="sd">            &gt;&gt;&gt; # instantiate logits processors</span>
<span class="sd">            &gt;&gt;&gt; logits_processor = LogitsProcessorList([</span>
<span class="sd">            ...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),</span>
<span class="sd">            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),</span>
<span class="sd">            ... ])</span>

<span class="sd">            &gt;&gt;&gt; outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;Generated:&quot;, tokenizer.batch_decode(outputs, skip_special_tokens=True))</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># init values</span>
        <span class="n">logits_processor</span> <span class="o">=</span> <span class="n">logits_processor</span> <span class="k">if</span> <span class="n">logits_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">LogitsProcessorList</span><span class="p">()</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">output_scores</span> <span class="o">=</span> <span class="n">output_scores</span> <span class="k">if</span> <span class="n">output_scores</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_scores</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">return_dict_in_generate</span> <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict_in_generate</span>
        <span class="p">)</span>

        <span class="c1"># init attention / hidden states / scores tuples</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_scores</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_attentions</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">decoder_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="n">output_hidden_states</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># if model is an encoder-decoder, retrieve encoder attention weights and hidden states</span>
        <span class="k">if</span> <span class="n">return_dict_in_generate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="n">encoder_attentions</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attentions&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;encoder_outputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_scorer</span><span class="o">.</span><span class="n">_beam_hyps</span><span class="p">)</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">num_beam_groups</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">num_beam_groups</span>
        <span class="n">num_sub_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="o">//</span> <span class="n">num_beam_groups</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>

        <span class="n">batch_beam_size</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="n">batch_beam_size</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Batch dimension of `input_ids` should be </span><span class="si">{</span><span class="n">num_beams</span> <span class="o">*</span> <span class="n">batch_size</span><span class="si">}</span><span class="s2">, but is </span><span class="si">{</span><span class="n">batch_beam_size</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in</span>
        <span class="c1"># the same group don&#39;t produce same tokens everytime.</span>
        <span class="n">beam_scores</span><span class="p">[:,</span> <span class="p">::</span><span class="n">num_sub_beams</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="c1"># predicted tokens in cur_len step</span>
            <span class="n">current_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># indices which will form the beams in the next time step</span>
            <span class="n">reordering_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># do one decoder step on all beams of all sentences in batch</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">beam_group_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beam_groups</span><span class="p">):</span>
                <span class="n">group_start_idx</span> <span class="o">=</span> <span class="n">beam_group_idx</span> <span class="o">*</span> <span class="n">num_sub_beams</span>
                <span class="n">group_end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">group_start_idx</span> <span class="o">+</span> <span class="n">num_sub_beams</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">)</span>
                <span class="n">group_size</span> <span class="o">=</span> <span class="n">group_end_idx</span> <span class="o">-</span> <span class="n">group_start_idx</span>

                <span class="c1"># indices of beams of current group among all sentences in batch</span>
                <span class="n">batch_group_indices</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">processed_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="n">batch_group_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_start_idx</span><span class="p">,</span> <span class="n">group_end_idx</span><span class="p">)]</span>
                    <span class="p">)</span>
                <span class="n">group_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span>

                <span class="c1"># select outputs of beams of current group only</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

                <span class="c1"># adjust tokens for Bart, *e.g.*</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
                <span class="p">)</span>

                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * group_size, vocab_size)</span>
                <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">(</span>
                    <span class="n">group_input_ids</span><span class="p">,</span> <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">current_tokens</span><span class="o">=</span><span class="n">current_tokens</span><span class="p">,</span> <span class="n">beam_group_idx</span><span class="o">=</span><span class="n">beam_group_idx</span>
                <span class="p">)</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span>
                    <span class="n">next_token_scores</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">processed_score</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token_scores</span>

                <span class="c1"># reshape for beam search</span>
                <span class="n">next_token_scores</span> <span class="o">=</span> <span class="n">next_token_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

                <span class="n">next_token_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                    <span class="n">next_token_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>

                <span class="n">next_indices</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">//</span> <span class="n">vocab_size</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

                <span class="c1"># stateless</span>
                <span class="n">beam_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">process</span><span class="p">(</span>
                    <span class="n">group_input_ids</span><span class="p">,</span>
                    <span class="n">next_token_scores</span><span class="p">,</span>
                    <span class="n">next_tokens</span><span class="p">,</span>
                    <span class="n">next_indices</span><span class="p">,</span>
                    <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                    <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">beam_scores</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_scores&quot;</span><span class="p">]</span>
                <span class="n">beam_next_tokens</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_tokens&quot;</span><span class="p">]</span>
                <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">beam_outputs</span><span class="p">[</span><span class="s2">&quot;next_beam_indices&quot;</span><span class="p">]</span>

                <span class="n">input_ids</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">]</span>
                <span class="n">group_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">group_input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">beam_next_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">current_tokens</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

                <span class="c1"># (beam_idx // group_size) -&gt; batch_idx</span>
                <span class="c1"># (beam_idx % group_size) -&gt; offset of idx inside the group</span>
                <span class="n">reordering_indices</span><span class="p">[</span><span class="n">batch_group_indices</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">num_beams</span> <span class="o">*</span> <span class="p">(</span><span class="n">beam_idx</span> <span class="o">//</span> <span class="n">group_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">group_start_idx</span> <span class="o">+</span> <span class="p">(</span><span class="n">beam_idx</span> <span class="o">%</span> <span class="n">group_size</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># Store scores, attentions and hidden_states when required</span>
            <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">output_scores</span><span class="p">:</span>
                    <span class="n">scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">processed_score</span><span class="p">,)</span>
                <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                    <span class="n">decoder_attentions</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_attentions</span><span class="p">,)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,)</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                    <span class="n">decoder_hidden_states</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">decoder_hidden_states</span><span class="p">,)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
                        <span class="k">else</span> <span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,)</span>
                    <span class="p">)</span>

            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">,</span> <span class="n">is_encoder_decoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past&quot;</span><span class="p">],</span> <span class="n">reordering_indices</span><span class="p">)</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">current_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">is_done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">sequence_outputs</span> <span class="o">=</span> <span class="n">beam_scorer</span><span class="o">.</span><span class="n">finalize</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">next_indices</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_in_generate</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">output_scores</span><span class="p">:</span>
                <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchEncoderDecoderOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">encoder_attentions</span><span class="o">=</span><span class="n">encoder_attentions</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">decoder_attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">decoder_hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">BeamSearchDecoderOnlyOutput</span><span class="p">(</span>
                    <span class="n">sequences</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">],</span>
                    <span class="n">sequences_scores</span><span class="o">=</span><span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequence_scores&quot;</span><span class="p">],</span>
                    <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span>
                    <span class="n">attentions</span><span class="o">=</span><span class="n">decoder_attentions</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">decoder_hidden_states</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sequence_outputs</span><span class="p">[</span><span class="s2">&quot;sequences&quot;</span><span class="p">]</span></div></div>


<div class="viewcode-block" id="top_k_top_p_filtering"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.generation_utils.top_k_top_p_filtering">[docs]</a><span class="k">def</span> <span class="nf">top_k_top_p_filtering</span><span class="p">(</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">filter_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span>
    <span class="n">min_tokens_to_keep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span>

<span class="sd">    Args:</span>
<span class="sd">        logits: logits distribution shape (batch size, vocabulary size)</span>
<span class="sd">        if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span>
<span class="sd">        if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span>
<span class="sd">            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span>
<span class="sd">        Make sure we keep at least min_tokens_to_keep per batch example in the output</span>
<span class="sd">    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">top_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">filter_value</span><span class="o">=</span><span class="n">filter_value</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">)(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="n">logits</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">TopPLogitsWarper</span><span class="p">(</span><span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="n">min_tokens_to_keep</span><span class="p">)(</span><span class="kc">None</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logits</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>