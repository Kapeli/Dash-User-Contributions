

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.generation_tf_utils &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.generation_tf_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.generation_tf_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="TFGenerationMixin"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFGenerationMixin">[docs]</a><span class="k">class</span> <span class="nc">TFGenerationMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class containing all of the functions supporting generation, to be used as a mixin in</span>
<span class="sd">    :class:`~transformers.TFPreTrainedModel`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="TFGenerationMixin.prepare_inputs_for_generation"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFGenerationMixin.prepare_inputs_for_generation">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implement in subclasses of :class:`~transformers.TFPreTrainedModel` for custom behavior to prepare inputs in</span>
<span class="sd">        the generate method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">}</span></div>

    <span class="k">def</span> <span class="nf">_use_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">use_cache</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;During generation, decide whether to pass the `past` variable to the next forward pass.&quot;&quot;&quot;</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;use_cache&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;mem_len&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mem_len</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

<div class="viewcode-block" id="TFGenerationMixin.generate"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFGenerationMixin.generate">[docs]</a>    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bad_words_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,</span>
<span class="sd">        beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.</span>

<span class="sd">        Adapted in part from `Facebook&#39;s XLM beam search code</span>
<span class="sd">        &lt;https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529&gt;`__.</span>

<span class="sd">        Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the</span>
<span class="sd">        attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values</span>
<span class="sd">        indicated are the default values of those config.</span>

<span class="sd">        Most of these parameters are explained in more detail in `this blog post</span>
<span class="sd">        &lt;https://huggingface.co/blog/how-to-generate&gt;`__.</span>

<span class="sd">        Parameters:</span>

<span class="sd">            input_ids (:obj:`tf.Tensor` of :obj:`dtype=tf.int32` and shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty</span>
<span class="sd">                :obj:`tf.Tensor` of shape :obj:`(1,)`.</span>
<span class="sd">            max_length (:obj:`int`, `optional`, defaults to 20):</span>
<span class="sd">                The maximum length of the sequence to be generated.</span>
<span class="sd">            min_length (:obj:`int`, `optional`, defaults to 10):</span>
<span class="sd">                The minimum length of the sequence to be generated.</span>
<span class="sd">            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to use sampling ; use greedy decoding otherwise.</span>
<span class="sd">            early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.</span>
<span class="sd">            num_beams (:obj:`int`, `optional`, defaults to 1):</span>
<span class="sd">                Number of beams for beam search. 1 means no beam search.</span>
<span class="sd">            temperature (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                The value used to module the next token probabilities.</span>
<span class="sd">            top_k (:obj:`int`, `optional`, defaults to 50):</span>
<span class="sd">                The number of highest probability vocabulary tokens to keep for top-k-filtering.</span>
<span class="sd">            top_p (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                If set to float &lt; 1, only the most probable tokens with probabilities that add up to ``top_p`` or</span>
<span class="sd">                higher are kept for generation.</span>
<span class="sd">            repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                The parameter for repetition penalty. 1.0 means no penalty. See `this paper</span>
<span class="sd">                &lt;https://arxiv.org/pdf/1909.05858.pdf&gt;`__ for more details.</span>
<span class="sd">            pad_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `padding` token.</span>
<span class="sd">            bos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `beginning-of-sequence` token.</span>
<span class="sd">            eos_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                The id of the `end-of-sequence` token.</span>
<span class="sd">            length_penalty (:obj:`float`, `optional`, defaults to 1.0):</span>
<span class="sd">                Exponential penalty to the length. 1.0 means no penalty.</span>

<span class="sd">                Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in</span>
<span class="sd">                order to encourage the model to produce longer sequences.</span>
<span class="sd">            no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If set to int &gt; 0, all ngrams of that size can only occur once.</span>
<span class="sd">            bad_words_ids(:obj:`List[int]`, `optional`):</span>
<span class="sd">                List of token ids that are not allowed to be generated. In order to get the tokens of the words that</span>
<span class="sd">                should not appear in the generated text, use :obj:`tokenizer.encode(bad_word, add_prefix_space=True)`.</span>
<span class="sd">            num_return_sequences(:obj:`int`, `optional`, defaults to 1):</span>
<span class="sd">                The number of independently computed returned sequences for each element in the batch.</span>
<span class="sd">            attention_mask (:obj:`tf.Tensor` of :obj:`dtype=tf.int32` and shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for</span>
<span class="sd">                tokens that are not masked, and 0 for masked tokens.</span>

<span class="sd">                If not provided, will default to a tensor the same shape as :obj:`input_ids` that masks the pad token.</span>

<span class="sd">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="sd">            decoder_start_token_id (:obj:`int`, `optional`):</span>
<span class="sd">                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.</span>
<span class="sd">            use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not the model should use the past last key/values attentions (if applicable to the model) to</span>
<span class="sd">                speed up decoding.</span>
<span class="sd">            model_specific_kwargs:</span>
<span class="sd">                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model.</span>

<span class="sd">        Return:</span>

<span class="sd">            :obj:`tf.Tensor` of :obj:`dtype=tf.int32` and shape :obj:`(batch_size * num_return_sequences,</span>
<span class="sd">            sequence_length)`: The generated sequences. The second dimension (sequence_length) is either equal to</span>
<span class="sd">            :obj:`max_length` or shorter if all batches finished early due to the :obj:`eos_token_id`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            outputs = model.generate(max_length=40)  # do greedy decoding</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;openai-gpt&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;openai-gpt&#39;)    # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#39;The dog&#39;</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True)  # generate 3 candidates using sampling</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;ctrl&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;ctrl&#39;)    # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            input_context = &#39;Legal My neighbor is&#39;  # &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;gpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = TFAutoModelWithLMHead.from_pretrained(&#39;gpt2&#39;)    # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            input_context = &#39;My cute dog&#39;</span>
<span class="sd">            bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in [&#39;idiot&#39;, &#39;stupid&#39;, &#39;shut up&#39;]]</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;tf&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># We cannot generate if the model does not have a LM head</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;You tried to generate sequences with a model that does not have a LM Head.&quot;</span>
                <span class="s2">&quot;Please use another model class (e.g. `TFOpenAIGPTLMHeadModel`, `TFXLNetLMHeadModel`, `TFGPT2LMHeadModel`, `TFCTRLLMHeadModel`, `TFT5ForConditionalGeneration`, `TFTransfoXLLMHeadModel`)&quot;</span>
            <span class="p">)</span>

        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">min_length</span> <span class="o">=</span> <span class="n">min_length</span> <span class="k">if</span> <span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_length</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">do_sample</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span> <span class="k">if</span> <span class="n">early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">early_stopping</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span> <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">top_k</span> <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span> <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span>
        <span class="n">repetition_penalty</span> <span class="o">=</span> <span class="n">repetition_penalty</span> <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">no_repeat_ngram_size</span> <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span>
        <span class="p">)</span>
        <span class="n">bad_words_ids</span> <span class="o">=</span> <span class="n">bad_words_ids</span> <span class="k">if</span> <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bad_words_ids</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">num_return_sequences</span> <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decoder_start_token_id</span> <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># overridden by the input batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`max_length` should be a strictly positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">min_length</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`min_length` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">do_sample</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`do_sample` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`early_stopping` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`num_beams` should be a strictly positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`temperature` should be strictly positive.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`top_k` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;`top_p` should be between 0 and 1.&quot;</span>
        <span class="k">assert</span> <span class="n">repetition_penalty</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;`repetition_penalty` should be &gt;= 1.&quot;</span>
        <span class="k">assert</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;If input_ids is not defined, `bos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`pad_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`eos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">length_penalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`length_penalty` should be strictly positive.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_return_sequences</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;`num_return_sequences` should be a strictly positive integer.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bad_words_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bad_words_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated&quot;</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;you should either supply a context to complete as `input_ids` input &quot;</span>
                <span class="s2">&quot;or a `bos_token_id` (integer &gt;= 0) as a first token to start the generation.&quot;</span>
            <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bos_token_id</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input prompt should be of shape (batch_size, sequence length).&quot;</span>

        <span class="c1"># not allow to duplicate outputs when greedy decoding</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_beams</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># no_beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_return_sequences</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences &gt; 1. Please set num_return_sequences = 1&quot;</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_beams</span> <span class="o">&gt;=</span> <span class="n">num_return_sequences</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams &gt;= num_return_sequences&quot;</span>

        <span class="c1"># create attention mask if necessary</span>
        <span class="c1"># TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()):</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Setting `pad_token_id` to </span><span class="si">{}</span><span class="s2"> (first `eos_token_id`) to generate sequence&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="c1"># current position and vocab size</span>
        <span class="n">cur_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># unused</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>

        <span class="c1"># set effective batch size and effective batch multiplier according to do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="n">num_return_sequences</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation&quot;</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;get_encoder&quot;</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should have a &#39;get_encoder&#39; function defined&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should be a method&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">)</span>

            <span class="c1"># get encoder and store encoder outputs</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">input_ids_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>

            <span class="c1"># create empty decoder_input_ids</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="n">decoder_start_token_id</span>
            <span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">batch_size</span> <span class="o">==</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;expected encoder_outputs[0] to have 1st dimension bs=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>

            <span class="c1"># expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams &gt; 1 and num_return_sequences &gt; 1)</span>
            <span class="n">expanded_batch_idxs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">repeats</span><span class="o">=</span><span class="n">num_beams</span> <span class="o">*</span> <span class="n">effective_batch_mult</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
            <span class="p">)</span>
            <span class="c1"># expand encoder_outputs</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">expanded_batch_idxs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The context has </span><span class="si">{</span><span class="n">cur_len</span><span class="si">}</span><span class="s2"> number of tokens, but `max_length` is only </span><span class="si">{</span><span class="n">max_length</span><span class="si">}</span><span class="s2">. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`&quot;</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_no_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bad_words_ids</span><span class="o">=</span><span class="n">bad_words_ids</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span></div>

    <span class="k">def</span> <span class="nf">_generate_no_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bad_words_ids</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate sequences for each example without beam search (num_beams == 1). All returned sequence are generated</span>
<span class="sd">        independantly.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># length of generated sentences / unfinished sentences</span>
        <span class="n">unfinished_sents</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">max_length</span>

        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>  <span class="c1"># defined for encoder-decoder models, None for decoder-only models</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
            <span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_cache</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">use_cache</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits_penalties</span> <span class="o">=</span> <span class="n">_create_next_token_logits_penalties</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">repetition_penalty</span>
                <span class="p">)</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">next_token_logits_penalties</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_ngram_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
                <span class="c1"># create banned_tokens boolean mask</span>
                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens according to bad words</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_bad_words_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="p">)</span>

                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="c1"># create eos_token_id boolean mask</span>
                <span class="n">is_token_logit_eos_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="n">eos_token_id</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span>
                <span class="p">)</span>
                <span class="n">eos_token_indices_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">is_token_logit_eos_token</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span>

                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">eos_token_indices_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="c1"># Temperature (higher temperature =&gt; more likely to sample low probability tokens)</span>
                <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>
                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf_top_k_top_p_filtering</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">)</span>
                <span class="c1"># Sample</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Greedy decoding</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

            <span class="c1"># update generations and finished sentences</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pad finished sentences if eos_token_id exist</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span> <span class="o">*</span> <span class="n">unfinished_sents</span> <span class="o">+</span> <span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sents</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span>

            <span class="c1"># add token and increase length by one</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tokens_to_add</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">eos_in_sents</span> <span class="o">=</span> <span class="n">tokens_to_add</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="c1"># if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length</span>
                <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
                    <span class="n">unfinished_sents</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">eos_in_sents</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">sent_lengths</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">sent_lengths</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span><span class="p">)</span>
                    <span class="o">+</span> <span class="n">cur_len</span> <span class="o">*</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span>
                <span class="p">)</span>

                <span class="c1"># unfinished_sents is set to zero if eos in sentence</span>
                <span class="n">unfinished_sents</span> <span class="o">-=</span> <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span>

            <span class="c1"># stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length</span>
            <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">unfinished_sents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># extend attention_mask for new generated input if only decoder</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">shape_list</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

        <span class="c1"># if there are different sentences lengths in the batch, some batches have to be padded</span>
        <span class="n">min_sent_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span>
        <span class="n">max_sent_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_sent_length</span> <span class="o">!=</span> <span class="n">max_sent_length</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined if batches have different lengths&quot;</span>
            <span class="c1"># finished sents are filled with pad_token</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sent_length</span><span class="o">.</span><span class="n">numpy</span><span class="p">()],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_token_id</span>

            <span class="c1"># create length masks for tf.where operation</span>
            <span class="n">broad_casted_sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sent_length</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">broad_casted_range</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">max_sent_length</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="n">max_sent_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">])</span>
            <span class="p">)</span>

            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">broad_casted_range</span> <span class="o">&lt;</span> <span class="n">broad_casted_sent_lengths</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">input_ids</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="k">def</span> <span class="nf">_generate_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bad_words_ids</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generate sequences for each example with beam search.&quot;&quot;&quot;</span>

        <span class="c1"># generated hypotheses</span>
        <span class="n">generated_hyps</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">BeamHypotheses</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">beam_scores_begin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">beam_scores_end</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">beam_scores_begin</span><span class="p">,</span> <span class="n">beam_scores_end</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,))</span>

        <span class="c1"># cache compute states</span>
        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>
        <span class="c1"># to stay similar to torch : past = (encoder_outputs, None) if encoder_outputs is not None else None</span>

        <span class="c1"># done sentences</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
            <span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, cur_len, vocab_size)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_cache</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">use_cache</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits_penalties</span> <span class="o">=</span> <span class="n">_create_next_token_logits_penalties</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">repetition_penalty</span>
                <span class="p">)</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">next_token_logits_penalties</span><span class="p">)</span>

            <span class="c1"># Temperature (higher temperature =&gt; more likely to sample low probability tokens)</span>
            <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_logits_during_generation</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
                <span class="p">)</span>
            <span class="c1">#             calculate log softmax score</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="c1"># create eos_token_id boolean mask</span>
                <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>

                <span class="n">is_token_logit_eos_token</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">is</span> <span class="n">eos_token_id</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span>
                <span class="p">)</span>
                <span class="n">eos_token_indices_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">is_token_logit_eos_token</span><span class="p">,</span> <span class="p">[</span><span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">])</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">eos_token_indices_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_ngram_tokens</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span>
                <span class="p">)</span>
                <span class="c1"># create banned_tokens boolean mask</span>
                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">scores</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">bad_words_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens according to bad words</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_bad_words_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="p">)</span>

                <span class="n">banned_tokens_indices_mask</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">banned_tokens_slice</span> <span class="ow">in</span> <span class="n">banned_tokens</span><span class="p">:</span>
                    <span class="n">banned_tokens_indices_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">True</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">banned_tokens_slice</span> <span class="k">else</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>
                    <span class="p">)</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span>
                    <span class="n">scores</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">banned_tokens_indices_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="k">assert</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                    <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">tf_top_k_top_p_filtering</span><span class="p">(</span>
                    <span class="n">_scores</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">2</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>
                <span class="c1"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">))</span>

                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">sample_without_replacement</span><span class="p">(</span>
                    <span class="n">_scores</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, 2 * num_beams)</span>
                <span class="c1"># Compute next scores</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 2 * num_beams)</span>

                <span class="c1"># sort the sampled vector to make sure that the first num_beams samples are the best</span>
                <span class="n">next_scores_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;DESCENDING&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">next_scores_indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="n">next_scores_indices</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
                    <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

                <span class="c1"># re-organize to group the beam together (we are keeping top hypothesis across beams)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, num_beams * vocab_size)</span>

                <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">next_scores</span><span class="p">)</span> <span class="o">==</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">]</span>

            <span class="c1"># next batch beam content</span>
            <span class="n">next_batch_beam</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># for each sentence</span>
            <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>

                <span class="c1"># if we are done with this sentence</span>
                <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                    <span class="p">),</span> <span class="s2">&quot;Batch can only be done if at least </span><span class="si">{}</span><span class="s2"> beams have been generated&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_beams</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">),</span> <span class="s2">&quot;generated beams &gt;= num_beams -&gt; eos_token_id and pad_token have to be defined&quot;</span>
                    <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>  <span class="c1"># pad the batch</span>
                    <span class="k">continue</span>

                <span class="c1"># next sentence beam content</span>
                <span class="n">next_sent_beam</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="c1"># next tokens for this sentence</span>
                <span class="k">for</span> <span class="n">beam_token_rank</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_token_id</span><span class="p">,</span> <span class="n">beam_token_score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>
                <span class="p">):</span>
                    <span class="c1"># get beam and token IDs</span>
                    <span class="n">beam_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">//</span> <span class="n">vocab_size</span>
                    <span class="n">token_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">%</span> <span class="n">vocab_size</span>

                    <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>
                    <span class="c1"># add to generated hypotheses if end of sentence or last iteration</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">token_id</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">):</span>
                        <span class="c1"># if beam_token does not belong to top num_beams tokens, it should not be added</span>
                        <span class="n">is_beam_token_worse_than_top_num_beams</span> <span class="o">=</span> <span class="n">beam_token_rank</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                        <span class="k">if</span> <span class="n">is_beam_token_worse_than_top_num_beams</span><span class="p">:</span>
                            <span class="k">continue</span>
                        <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]),</span> <span class="n">beam_token_score</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># add next predicted token if it is not eos_token</span>
                        <span class="n">next_sent_beam</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">beam_token_score</span><span class="p">,</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">effective_beam_id</span><span class="p">))</span>

                    <span class="c1"># the beam for next step is full</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">:</span>
                        <span class="k">break</span>

                <span class="c1"># Check if we are done so that we can save a pad step if all(done)</span>
                <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="ow">or</span> <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cur_len</span>
                <span class="p">)</span>

                <span class="c1"># update next beam content</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">,</span> <span class="s2">&quot;Beam should always be full&quot;</span>
                <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># stop when we are done with each sentence</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">done</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="c1"># sanity check / prepare next batch</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">beam_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

            <span class="c1"># re-order batch and update current length</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="p">:])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">beam_idx</span><span class="p">])</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">beam_tokens</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="c1"># re-order internal states</span>
            <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="c1"># extend attention_mask for new generated input if only decoder</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">shape_list</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

        <span class="c1"># finalize all open beam hypotheses and end to generated hypotheses</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Add all open beam hypothesis to generated_hyps</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="c1"># test that beam scores match previously calculated scores if not eos and batch_idx not done</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                <span class="p">(</span><span class="n">token_id</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">eos_token_id</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">]</span> <span class="o">==</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">))[</span><span class="n">batch_idx</span><span class="p">]</span>
                <span class="p">),</span> <span class="s2">&quot;If batch_idx is not done, final next scores: </span><span class="si">{}</span><span class="s2"> have to equal to accumulated beam_scores: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">][</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">beam_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">))[</span><span class="n">batch_idx</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># need to add best num_beams hypotheses to generated hyps</span>
            <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>
                <span class="n">final_score</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">final_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span>
                <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">final_tokens</span><span class="p">,</span> <span class="n">final_score</span><span class="p">)</span>

        <span class="c1"># depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch</span>
        <span class="n">output_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="n">output_num_return_sequences_per_batch</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">num_return_sequences</span>

        <span class="c1"># select the best hypotheses</span>
        <span class="n">sent_lengths_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># retrieve best hypotheses</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">):</span>
            <span class="n">sorted_hyps</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">hypotheses</span><span class="o">.</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_num_return_sequences_per_batch</span><span class="p">):</span>
                <span class="n">best_hyp</span> <span class="o">=</span> <span class="n">sorted_hyps</span><span class="o">.</span><span class="n">pop</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">sent_lengths_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">))</span>
                <span class="n">best</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">output_batch_size</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">best</span><span class="p">),</span> <span class="s2">&quot;Output batch size </span><span class="si">{}</span><span class="s2"> must match output beam hypotheses </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">output_batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sent_lengths_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># shorter batches are filled with pad_token</span>
        <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined&quot;</span>
            <span class="n">sent_max_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">sent_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
            <span class="n">decoded_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># fill with hypothesis and eos_token_id if necessary</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">hypo</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># if sent_length is max_len do not pad</span>
                <span class="k">if</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">sent_max_len</span><span class="p">:</span>
                    <span class="n">decoded_slice</span> <span class="o">=</span> <span class="n">hypo</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># else pad to sent_max_len</span>
                    <span class="n">num_pad_tokens</span> <span class="o">=</span> <span class="n">sent_max_len</span> <span class="o">-</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">padding</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_pad_tokens</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                    <span class="n">decoded_slice</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">hypo</span><span class="p">,</span> <span class="n">padding</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                    <span class="c1"># finish sentence with EOS token</span>
                    <span class="k">if</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
                        <span class="n">decoded_slice</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">sent_max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">==</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                            <span class="n">eos_token_id</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">sent_max_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                            <span class="n">decoded_slice</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="c1"># add to list</span>
                <span class="n">decoded_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoded_slice</span><span class="p">)</span>

            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">decoded_list</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># none of the hypotheses have an eos_token</span>
            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hypo</span><span class="p">)</span> <span class="o">==</span> <span class="n">max_length</span> <span class="k">for</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="n">best</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">layer_past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">)</span>

<div class="viewcode-block" id="TFGenerationMixin.adjust_logits_during_generation"><a class="viewcode-back" href="../../main_classes/model.html#transformers.TFGenerationMixin.adjust_logits_during_generation">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_logits_during_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in</span>
<span class="sd">        the generate method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">logits</span></div></div>


<span class="k">def</span> <span class="nf">_create_next_token_logits_penalties</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">):</span>
    <span class="c1"># create logit penalties for already seen input_ids</span>
    <span class="n">token_penalties</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_list</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="n">prev_input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">prev_input_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">):</span>
        <span class="n">logit_penalized</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="n">prev_input_id</span><span class="p">]</span>
        <span class="n">logit_penalties</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">logit_penalized</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># if previous logit score is &lt; 0 then multiply repetition penalty else divide</span>
        <span class="n">logit_penalties</span><span class="p">[</span><span class="n">logit_penalized</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">repetition_penalty</span>
        <span class="n">logit_penalties</span><span class="p">[</span><span class="n">logit_penalized</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">repetition_penalty</span>
        <span class="n">np</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">token_penalties</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">prev_input_id</span><span class="p">,</span> <span class="n">logit_penalties</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">token_penalties</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">calc_banned_ngram_tokens</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">,</span> <span class="n">num_hypos</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">):</span>
    <span class="c1"># Copied from fairseq for no_repeat_ngram in beam_search</span>
    <span class="k">if</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">no_repeat_ngram_size</span><span class="p">:</span>
        <span class="c1"># return no banned tokens if we haven&#39;t generated no_repeat_ngram_size tokens yet</span>
        <span class="k">return</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="n">generated_ngrams</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">):</span>
        <span class="n">gen_tokens</span> <span class="o">=</span> <span class="n">prev_input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">generated_ngram</span> <span class="o">=</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">gen_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">)]):</span>
            <span class="n">prev_ngram_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ngram</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">generated_ngram</span><span class="p">[</span><span class="n">prev_ngram_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">generated_ngram</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prev_ngram_tuple</span><span class="p">,</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="n">ngram</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">):</span>
        <span class="c1"># Before decoding the next token, prevent decoding of ngrams that have already appeared</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">no_repeat_ngram_size</span>
        <span class="n">ngram_idx</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">cur_len</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ngram_idx</span><span class="p">,</span> <span class="p">[])</span>

    <span class="n">banned_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">banned_tokens</span>


<span class="k">def</span> <span class="nf">calc_banned_bad_words_ids</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">,</span> <span class="n">bad_words_ids</span><span class="p">):</span>
    <span class="n">banned_tokens</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_tokens_match</span><span class="p">(</span><span class="n">prev_tokens</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># if bad word tokens is just one token always ban it</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">prev_tokens</span><span class="p">):</span>
            <span class="c1"># if bad word tokens are longer than prev tokens they can&#39;t be equal</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">prev_tokens</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="c1"># if tokens match</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">for</span> <span class="n">prev_input_ids_slice</span> <span class="ow">in</span> <span class="n">prev_input_ids</span><span class="p">:</span>
        <span class="n">banned_tokens_slice</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">banned_token_seq</span> <span class="ow">in</span> <span class="n">bad_words_ids</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">banned_token_seq</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Banned words token sequences </span><span class="si">{}</span><span class="s2"> cannot have an empty list&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">bad_words_ids</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">_tokens_match</span><span class="p">(</span><span class="n">prev_input_ids_slice</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">banned_token_seq</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="c1"># if tokens do not match continue</span>
                <span class="k">continue</span>

            <span class="n">banned_tokens_slice</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">banned_token_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">banned_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">banned_tokens_slice</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">banned_tokens</span>


<div class="viewcode-block" id="tf_top_k_top_p_filtering"><a class="viewcode-back" href="../../internal/generation_utils.html#transformers.tf_top_k_top_p_filtering">[docs]</a><span class="k">def</span> <span class="nf">tf_top_k_top_p_filtering</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">filter_value</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span>

<span class="sd">    Args:</span>
<span class="sd">        logits: logits distribution shape (batch size, vocabulary size)</span>
<span class="sd">        if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span>
<span class="sd">        if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span>
<span class="sd">            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span>
<span class="sd">        Make sure we keep at least min_tokens_to_keep per batch example in the output</span>
<span class="sd">    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logits_shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">top_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="p">),</span> <span class="n">logits_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Safety check</span>
        <span class="c1"># Remove all tokens with a probability less than the last token of the top-k</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">&lt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;DESCENDING&quot;</span><span class="p">)</span>
        <span class="n">sorted_logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># expects logits to be of dim (batch_size, vocab_size)</span>

        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>

        <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span>
            <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="p">:</span><span class="n">min_tokens_to_keep</span><span class="p">]),</span>
                    <span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="n">min_tokens_to_keep</span><span class="p">:],</span>
                <span class="p">],</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Shift the indices to the right to keep also the first token above the threshold</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">sorted_indices_to_remove</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]],</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># scatter sorted tensors to original indexing</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">scatter_values_on_batch_indices</span><span class="p">(</span><span class="n">sorted_indices_to_remove</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">indices_to_remove</span><span class="p">,</span> <span class="n">filter_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span></div>


<span class="k">def</span> <span class="nf">scatter_values_on_batch_indices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">batch_indices</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">shape_list</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">)</span>
    <span class="c1"># broadcast batch dim to shape</span>
    <span class="n">broad_casted_batch_dims</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># transform batch_indices to pair_indices</span>
    <span class="n">pair_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">broad_casted_batch_dims</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_indices</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])],</span> <span class="mi">0</span><span class="p">))</span>
    <span class="c1"># scatter values to pair indices</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">scatter_nd</span><span class="p">(</span><span class="n">pair_indices</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_tensor_by_indices_to_value</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="c1"># create value_tensor since tensor value assignment is not possible in TF</span>
    <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">+</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sample_without_replacement</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    categorical sampling without replacement is currently not implemented the gumbel-max trick will do for now see</span>
<span class="sd">    https://github.com/tensorflow/tensorflow/issues/9260 for more info</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape_list</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="n">z</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">indices</span>


<span class="k">def</span> <span class="nf">shape_list</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deal with dynamic shape in tensorflow cleanly.&quot;&quot;&quot;</span>
    <span class="n">static</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">dynamic</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dynamic</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">s</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">static</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">BeamHypotheses</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize n-best list of hypotheses.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># ignoring bos_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="mf">1e9</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Number of hypotheses in the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a new hypothesis to the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">sum_logprobs</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">hyp</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="ow">or</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">hyp</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([(</span><span class="n">s</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)])</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">[</span><span class="n">sorted_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="n">sorted_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_done</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">best_sum_logprobs</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst</span>
<span class="sd">        one in the heap, then we are done with this sentence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cur_score</span> <span class="o">=</span> <span class="n">best_sum_logprobs</span> <span class="o">/</span> <span class="n">cur_len</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">&gt;=</span> <span class="n">cur_score</span>
            <span class="k">return</span> <span class="n">ret</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>