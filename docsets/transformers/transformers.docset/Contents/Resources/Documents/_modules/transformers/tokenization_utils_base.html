

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.tokenization_utils_base &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.tokenization_utils_base</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.tokenization_utils_base</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2020 The HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Base classes common to both the slow and the fast tokenization classes: PreTrainedTokenizerBase (host all the user</span>
<span class="sd">fronting encoding methods) Special token mixing (host the special tokens logic) and BatchEncoding (wrap the dictionary</span>
<span class="sd">of output with special method for the Fast tokenizers)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span><span class="p">,</span> <span class="n">UserDict</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">requests</span>

<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">add_end_docstrings</span><span class="p">,</span>
    <span class="n">cached_path</span><span class="p">,</span>
    <span class="n">hf_bucket_url</span><span class="p">,</span>
    <span class="n">is_flax_available</span><span class="p">,</span>
    <span class="n">is_remote_url</span><span class="p">,</span>
    <span class="n">is_tf_available</span><span class="p">,</span>
    <span class="n">is_tokenizers_available</span><span class="p">,</span>
    <span class="n">is_torch_available</span><span class="p">,</span>
    <span class="n">torch_required</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging</span>


<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">is_torch_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">torch</span>
    <span class="k">if</span> <span class="n">is_tf_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="k">if</span> <span class="n">is_flax_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># noqa: F401</span>


<span class="k">def</span> <span class="nf">_is_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_torch</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_torch_device</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">torch</span>

    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_tensorflow</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_jax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># noqa: F811</span>

    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>


<span class="k">if</span> <span class="n">is_tokenizers_available</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">AddedToken</span>
    <span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Encoding</span> <span class="k">as</span> <span class="n">EncodingFast</span>
<span class="k">else</span><span class="p">:</span>

    <span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eq</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">class</span> <span class="nc">AddedToken</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        AddedToken represents a token to be added to a Tokenizer An AddedToken can have special options defining the</span>
<span class="sd">        way it should behave.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">content</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
        <span class="n">single_word</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">lstrip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">rstrip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>

    <span class="nd">@dataclass</span>
    <span class="k">class</span> <span class="nc">EncodingFast</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; This is dummy class because without the `tokenizers` library we don&#39;t have these objects anyway &quot;&quot;&quot;</span>

        <span class="k">pass</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">VERY_LARGE_INTEGER</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e30</span><span class="p">)</span>  <span class="c1"># This is used to set the max input length for a model with infinite size input</span>
<span class="n">LARGE_INTEGER</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e20</span><span class="p">)</span>  <span class="c1"># This is used when we need something big but slightly smaller than VERY_LARGE_INTEGER</span>

<span class="c1"># Define type aliases and NamedTuples</span>
<span class="n">TextInput</span> <span class="o">=</span> <span class="nb">str</span>
<span class="n">PreTokenizedInput</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="n">EncodedInput</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="n">TextInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
<span class="n">PreTokenizedInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
<span class="n">EncodedInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>


<span class="c1"># Slow tokenizers used to be saved in three separated files</span>
<span class="n">SPECIAL_TOKENS_MAP_FILE</span> <span class="o">=</span> <span class="s2">&quot;special_tokens_map.json&quot;</span>
<span class="n">ADDED_TOKENS_FILE</span> <span class="o">=</span> <span class="s2">&quot;added_tokens.json&quot;</span>
<span class="n">TOKENIZER_CONFIG_FILE</span> <span class="o">=</span> <span class="s2">&quot;tokenizer_config.json&quot;</span>

<span class="c1"># Fast tokenizers (provided by HuggingFace tokenizer&#39;s library) can be saved in a single file</span>
<span class="n">FULL_TOKENIZER_FILE</span> <span class="o">=</span> <span class="s2">&quot;tokenizer.json&quot;</span>


<div class="viewcode-block" id="ExplicitEnum"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.ExplicitEnum">[docs]</a><span class="k">class</span> <span class="nc">ExplicitEnum</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enum with more explicit error message for missing values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_missing_</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%r</span><span class="s2"> is not a valid </span><span class="si">%s</span><span class="s2">, please select one of </span><span class="si">%s</span><span class="s2">&quot;</span>
            <span class="o">%</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_value2member_map_</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="TruncationStrategy"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy">[docs]</a><span class="k">class</span> <span class="nc">TruncationStrategy</span><span class="p">(</span><span class="n">ExplicitEnum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Possible values for the ``truncation`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for</span>
<span class="sd">    tab-completion in an IDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ONLY_FIRST</span> <span class="o">=</span> <span class="s2">&quot;only_first&quot;</span>
    <span class="n">ONLY_SECOND</span> <span class="o">=</span> <span class="s2">&quot;only_second&quot;</span>
    <span class="n">LONGEST_FIRST</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span>
    <span class="n">DO_NOT_TRUNCATE</span> <span class="o">=</span> <span class="s2">&quot;do_not_truncate&quot;</span></div>


<div class="viewcode-block" id="PaddingStrategy"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy">[docs]</a><span class="k">class</span> <span class="nc">PaddingStrategy</span><span class="p">(</span><span class="n">ExplicitEnum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Possible values for the ``padding`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for tab-completion</span>
<span class="sd">    in an IDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">LONGEST</span> <span class="o">=</span> <span class="s2">&quot;longest&quot;</span>
    <span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span>
    <span class="n">DO_NOT_PAD</span> <span class="o">=</span> <span class="s2">&quot;do_not_pad&quot;</span></div>


<div class="viewcode-block" id="TensorType"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType">[docs]</a><span class="k">class</span> <span class="nc">TensorType</span><span class="p">(</span><span class="n">ExplicitEnum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Possible values for the ``return_tensors`` argument in :meth:`PreTrainedTokenizerBase.__call__`. Useful for</span>
<span class="sd">    tab-completion in an IDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">PYTORCH</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span>
    <span class="n">TENSORFLOW</span> <span class="o">=</span> <span class="s2">&quot;tf&quot;</span>
    <span class="n">NUMPY</span> <span class="o">=</span> <span class="s2">&quot;np&quot;</span>
    <span class="n">JAX</span> <span class="o">=</span> <span class="s2">&quot;jax&quot;</span></div>


<div class="viewcode-block" id="CharSpan"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.CharSpan">[docs]</a><span class="k">class</span> <span class="nc">CharSpan</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Character span in the original string.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (:obj:`int`): Index of the first character in the original string.</span>
<span class="sd">        end (:obj:`int`): Index of the character following the last character in the original string.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">start</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end</span><span class="p">:</span> <span class="nb">int</span></div>


<div class="viewcode-block" id="TokenSpan"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.TokenSpan">[docs]</a><span class="k">class</span> <span class="nc">TokenSpan</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Token span in an encoded string (list of tokens).</span>

<span class="sd">    Args:</span>
<span class="sd">        start (:obj:`int`): Index of the first token in the span.</span>
<span class="sd">        end (:obj:`int`): Index of the token following the last token in the span.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">start</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end</span><span class="p">:</span> <span class="nb">int</span></div>


<span class="k">def</span> <span class="nf">to_py_obj</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">BatchEncoding</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">to_py_obj</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">to_py_obj</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">is_tf_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">_is_tensorflow</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">is_torch_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">_is_torch</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">obj</span>


<div class="viewcode-block" id="BatchEncoding"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding">[docs]</a><span class="k">class</span> <span class="nc">BatchEncoding</span><span class="p">(</span><span class="n">UserDict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Holds the output of the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus` and</span>
<span class="sd">    :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode` methods (tokens,</span>
<span class="sd">    attention_masks, etc).</span>

<span class="sd">    This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes</span>
<span class="sd">    utility methods to map from word/character space to token space.</span>

<span class="sd">    Args:</span>
<span class="sd">        data (:obj:`dict`):</span>
<span class="sd">            Dictionary of lists/arrays/tensors returned by the encode/batch_encode methods (&#39;input_ids&#39;,</span>
<span class="sd">            &#39;attention_mask&#39;, etc.).</span>
<span class="sd">        encoding (:obj:`tokenizers.Encoding` or :obj:`Sequence[tokenizers.Encoding]`, `optional`):</span>
<span class="sd">            If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character</span>
<span class="sd">            space to token space the :obj:`tokenizers.Encoding` instance or list of instance (for batches) hold this</span>
<span class="sd">            information.</span>
<span class="sd">        tensor_type (:obj:`Union[None, str, TensorType]`, `optional`):</span>
<span class="sd">            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at</span>
<span class="sd">            initialization.</span>
<span class="sd">        prepend_batch_axis (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">            Whether or not to add a batch axis when converting to tensors (see :obj:`tensor_type` above).</span>
<span class="sd">        n_sequences (:obj:`Optional[int]`, `optional`):</span>
<span class="sd">            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at</span>
<span class="sd">            initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tensor_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">EncodingFast</span><span class="p">):</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoding</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="o">=</span> <span class="n">encoding</span>

        <span class="k">if</span> <span class="n">n_sequences</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">):</span>
            <span class="n">n_sequences</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">n_sequences</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_sequences</span> <span class="o">=</span> <span class="n">n_sequences</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">=</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_sequences</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: The number of sequences used to generate each sample from the batch encoded in this</span>
<span class="sd">        :class:`~transformers.BatchEncoding`. Currently can be one of :obj:`None` (unknown), :obj:`1` (a single</span>
<span class="sd">        sentence) or :obj:`2` (a pair of sentences)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_sequences</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_fast</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`bool`: Indicate whether this :class:`~transformers.BatchEncoding` was generated from the result of a</span>
<span class="sd">        :class:`~transformers.PreTrainedTokenizerFast` or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">EncodingFast</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If the key is a string, returns the value of the dict associated to :obj:`key` (&#39;input_ids&#39;, &#39;attention_mask&#39;,</span>
<span class="sd">        etc.).</span>

<span class="sd">        If the key is an integer, get the :obj:`tokenizers.Encoding` for batch item with index :obj:`key`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="s2">&quot;Indexing with integers (to access backend Encoding for a given batch index) &quot;</span>
                <span class="s2">&quot;is not available when using Python based tokenizers&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;encodings&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;data&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="s2">&quot;encodings&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;encodings&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="BatchEncoding.keys"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.keys">[docs]</a>    <span class="k">def</span> <span class="nf">keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span></div>

<div class="viewcode-block" id="BatchEncoding.values"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.values">[docs]</a>    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()</span></div>

<div class="viewcode-block" id="BatchEncoding.items"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.items">[docs]</a>    <span class="k">def</span> <span class="nf">items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span></div>

    <span class="c1"># After this point:</span>
    <span class="c1"># Extended properties and methods only available for fast (Rust-based) tokenizers</span>
    <span class="c1"># provided by HuggingFace tokenizers library.</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encodings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[List[tokenizers.Encoding]]`: The list all encodings from the tokenization process. Returns</span>
<span class="sd">        :obj:`None` if the input was tokenized through Python (i.e., not a fast) tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span>

<div class="viewcode-block" id="BatchEncoding.tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.tokens">[docs]</a>    <span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to</span>
<span class="sd">        integer indices) at a given batch index (only works for the output of a fast tokenizer).</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[str]`: The list of tokens at that index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tokens() is not available when using Python-based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span></div>

<div class="viewcode-block" id="BatchEncoding.sequence_ids"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.sequence_ids">[docs]</a>    <span class="k">def</span> <span class="nf">sequence_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to the id of their original sentences:</span>

<span class="sd">            - :obj:`None` for special tokens added around or between sequences,</span>
<span class="sd">            - :obj:`0` for tokens corresponding to words in the first sequence,</span>
<span class="sd">            - :obj:`1` for tokens corresponding to words in the second sequence when a pair of sequences was jointly</span>
<span class="sd">              encoded.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[Optional[int]]`: A list indicating the sequence id corresponding to each token. Special tokens</span>
<span class="sd">            added by the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their</span>
<span class="sd">            corresponding sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;sequence_ids() is not available when using Python-based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">sequence_ids</span></div>

<div class="viewcode-block" id="BatchEncoding.words"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.words">[docs]</a>    <span class="k">def</span> <span class="nf">words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by</span>
<span class="sd">            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding</span>
<span class="sd">            word (several tokens will be mapped to the same word index if they are parts of that word).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;words() is not available when using Python-based tokenizers&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`BatchEncoding.words()` property is deprecated and should be replaced with the identical, &quot;</span>
            <span class="s2">&quot;but more self-explanatory `BatchEncoding.word_ids()` property.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchEncoding.word_ids"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.word_ids">[docs]</a>    <span class="k">def</span> <span class="nf">word_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (:obj:`int`, `optional`, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by</span>
<span class="sd">            the tokenizer are mapped to :obj:`None` and other tokens are mapped to the index of their corresponding</span>
<span class="sd">            word (several tokens will be mapped to the same word index if they are parts of that word).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;word_ids() is not available when using Python-based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_ids</span></div>

<div class="viewcode-block" id="BatchEncoding.token_to_sequence"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.token_to_sequence">[docs]</a>    <span class="k">def</span> <span class="nf">token_to_sequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the sequence represented by the given token. In the general use case, this method returns</span>
<span class="sd">        :obj:`0` for a single sequence or the first sequence of a pair, and :obj:`1` for the second sequence of a pair</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.token_to_sequence(token_index)`` if batch size is 1</span>
<span class="sd">        - ``self.token_to_sequence(batch_index, token_index)`` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,</span>
<span class="sd">        words are defined by the user). In this case it allows to easily associate encoded tokens with provided</span>
<span class="sd">        tokenized words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the</span>
<span class="sd">                sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Index of the word in the input sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_sequence() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">token_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_sequence</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchEncoding.token_to_word"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.token_to_word">[docs]</a>    <span class="k">def</span> <span class="nf">token_to_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.token_to_word(token_index)`` if batch size is 1</span>
<span class="sd">        - ``self.token_to_word(batch_index, token_index)`` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,</span>
<span class="sd">        words are defined by the user). In this case it allows to easily associate encoded tokens with provided</span>
<span class="sd">        tokenized words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the token in the</span>
<span class="sd">                sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Index of the word in the input sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_word() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">token_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchEncoding.word_to_tokens"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.word_to_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">word_to_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_word_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TokenSpan</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the encoded token span corresponding to a word in a sequence of the batch.</span>

<span class="sd">        Token spans are returned as a :class:`~transformers.tokenization_utils_base.TokenSpan` with:</span>

<span class="sd">        - **start** -- Index of the first token.</span>
<span class="sd">        - **end** -- Index of the token following the last token.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.word_to_tokens(word_index, sequence_index: int = 0)`` if batch size is 1</span>
<span class="sd">        - ``self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)`` if batch size is greater or equal</span>
<span class="sd">          to 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_word_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence.</span>
<span class="sd">            word_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided word index belongs to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional :class:`~transformers.tokenization_utils_base.TokenSpan` Span of tokens in the encoded sequence.</span>
<span class="sd">            Returns :obj:`None` if no tokens correspond to the word.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;word_to_tokens() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">word_index</span>
        <span class="n">span</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_to_tokens</span><span class="p">(</span><span class="n">word_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TokenSpan</span><span class="p">(</span><span class="o">*</span><span class="n">span</span><span class="p">)</span> <span class="k">if</span> <span class="n">span</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="BatchEncoding.token_to_chars"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.token_to_chars">[docs]</a>    <span class="k">def</span> <span class="nf">token_to_chars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CharSpan</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the character span corresponding to an encoded token in a sequence of the batch.</span>

<span class="sd">        Character spans are returned as a :class:`~transformers.tokenization_utils_base.CharSpan` with:</span>

<span class="sd">        - **start** -- Index of the first character in the original string associated to the token.</span>
<span class="sd">        - **end** -- Index of the character following the last character in the original string associated to the</span>
<span class="sd">          token.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.token_to_chars(token_index)`` if batch size is 1</span>
<span class="sd">        - ``self.token_to_chars(batch_index, token_index)`` if batch size is greater or equal to 1</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the token or tokens in</span>
<span class="sd">                the sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`~transformers.tokenization_utils_base.CharSpan`: Span of characters in the original string.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_chars() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">return</span> <span class="n">CharSpan</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_chars</span><span class="p">(</span><span class="n">token_index</span><span class="p">)))</span></div>

<div class="viewcode-block" id="BatchEncoding.char_to_token"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.char_to_token">[docs]</a>    <span class="k">def</span> <span class="nf">char_to_token</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_char_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">char_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the token in the encoded output comprising a character in the original string for a sequence</span>
<span class="sd">        of the batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.char_to_token(char_index)`` if batch size is 1</span>
<span class="sd">        - ``self.char_to_token(batch_index, char_index)`` if batch size is greater or equal to 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_char_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence</span>
<span class="sd">            char_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided character index belongs to.</span>


<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Index of the token.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;char_to_token() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">char_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">char_to_token</span><span class="p">(</span><span class="n">char_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchEncoding.word_to_chars"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.word_to_chars">[docs]</a>    <span class="k">def</span> <span class="nf">word_to_chars</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_word_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CharSpan</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the character span in the original string corresponding to given word in a sequence of the batch.</span>

<span class="sd">        Character spans are returned as a CharSpan NamedTuple with:</span>

<span class="sd">        - start: index of the first character in the original string</span>
<span class="sd">        - end: index of the character following the last character in the original string</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.word_to_chars(word_index)`` if batch size is 1</span>
<span class="sd">        - ``self.word_to_chars(batch_index, word_index)`` if batch size is greater or equal to 1</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_word_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence</span>
<span class="sd">            word_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided word index belongs to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`CharSpan` or :obj:`List[CharSpan]`: Span(s) of the associated character or characters in the string.</span>
<span class="sd">            CharSpan are NamedTuple with:</span>

<span class="sd">                - start: index of the first character associated to the token in the original string</span>
<span class="sd">                - end: index of the character following the last character associated to the token in the original</span>
<span class="sd">                  string</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;word_to_chars() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">return</span> <span class="n">CharSpan</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span><span class="n">word_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)))</span></div>

<div class="viewcode-block" id="BatchEncoding.char_to_word"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.char_to_word">[docs]</a>    <span class="k">def</span> <span class="nf">char_to_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_char_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">char_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the word in the original string corresponding to a character in the original string of a sequence of the</span>
<span class="sd">        batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - ``self.char_to_word(char_index)`` if batch size is 1</span>
<span class="sd">        - ``self.char_to_word(batch_index, char_index)`` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_char_index (:obj:`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the character in the original string.</span>
<span class="sd">            char_index (:obj:`int`, `optional`):</span>
<span class="sd">                If a batch index is provided in `batch_or_token_index`, this can be the index of the character in the</span>
<span class="sd">                original string.</span>
<span class="sd">            sequence_index (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided character index belongs to.</span>


<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int` or :obj:`List[int]`: Index or indices of the associated encoded token(s).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;char_to_word() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">char_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">char_to_word</span><span class="p">(</span><span class="n">char_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span></div>

<div class="viewcode-block" id="BatchEncoding.convert_to_tensors"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.convert_to_tensors">[docs]</a>    <span class="k">def</span> <span class="nf">convert_to_tensors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensor_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert the inner content to tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            tensor_type (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):</span>
<span class="sd">                The type of tensors to use. If :obj:`str`, should be one of the values of the enum</span>
<span class="sd">                :class:`~transformers.tokenization_utils_base.TensorType`. If :obj:`None`, no modification is done.</span>
<span class="sd">            prepend_batch_axis (:obj:`int`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to add the batch dimension during the conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tensor_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="c1"># Convert to TensorType</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">):</span>
            <span class="n">tensor_type</span> <span class="o">=</span> <span class="n">TensorType</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">)</span>

        <span class="c1"># Get a function reference for the correct framework</span>
        <span class="k">if</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">TENSORFLOW</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tf_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.&quot;</span>
                <span class="p">)</span>
            <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">is_tensor</span>
        <span class="k">elif</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">PYTORCH</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_torch_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Unable to convert output to PyTorch tensors format, PyTorch is not installed.&quot;</span><span class="p">)</span>
            <span class="kn">import</span> <span class="nn">torch</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span>
        <span class="k">elif</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">JAX</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_flax_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Unable to convert output to JAX tensors format, JAX is not installed.&quot;</span><span class="p">)</span>
            <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># noqa: F811</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">_is_jax</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">_is_numpy</span>
        <span class="c1"># (mfuntowicz: This code is unreachable)</span>
        <span class="c1"># else:</span>
        <span class="c1">#     raise ImportError(</span>
        <span class="c1">#         &quot;Unable to convert output to tensors format {}&quot;.format(tensor_type)</span>
        <span class="c1">#     )</span>

        <span class="c1"># Do the tensor conversion in batch</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">prepend_batch_axis</span><span class="p">:</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                    <span class="n">tensor</span> <span class="o">=</span> <span class="n">as_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

                    <span class="c1"># Removing this for now in favor of controlling the shape with `prepend_batch_axis`</span>
                    <span class="c1"># # at-least2d</span>
                    <span class="c1"># if tensor.ndim &gt; 2:</span>
                    <span class="c1">#     tensor = tensor.squeeze(0)</span>
                    <span class="c1"># elif tensor.ndim &lt; 2:</span>
                    <span class="c1">#     tensor = tensor[None, :]</span>

                    <span class="bp">self</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
            <span class="k">except</span><span class="p">:</span>  <span class="c1"># noqa E722</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Unable to create tensor returning overflowing tokens of different lengths. &quot;</span>
                        <span class="s2">&quot;Please see if a fast version of this tokenizer is available to have this feature available.&quot;</span>
                    <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to create tensor, you should probably activate truncation and/or padding &quot;</span>
                    <span class="s2">&quot;with &#39;padding=True&#39; &#39;truncation=True&#39; to have batched tensors with the same length.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="BatchEncoding.to"><a class="viewcode-back" href="../../main_classes/tokenizer.html#transformers.tokenization_utils_base.BatchEncoding.to">[docs]</a>    <span class="nd">@torch_required</span>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch.device&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;BatchEncoding&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Send all values to device by calling :obj:`v.to(device)` (PyTorch only).</span>

<span class="sd">        Args:</span>
<span class="sd">            device (:obj:`str` or :obj:`torch.device`): The device to put the tensors on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`~transformers.BatchEncoding`: The same instance of :class:`~transformers.BatchEncoding` after</span>
<span class="sd">            modification.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># This check catches things like APEX blindly calling &quot;to&quot; on all inputs to a module</span>
        <span class="c1"># Otherwise it passes the casts down and casts the LongTensor containing the token idxs</span>
        <span class="c1"># into a HalfTensor</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_torch_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Attempting to cast a BatchEncoding to another type, </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">. This is not supported.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="SpecialTokensMixin"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin">[docs]</a><span class="k">class</span> <span class="nc">SpecialTokensMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A mixin derived by :class:`~transformers.PreTrainedTokenizer` and :class:`~transformers.PreTrainedTokenizerFast` to</span>
<span class="sd">    handle specific behaviors related to special tokens. In particular, this class hold the attributes which can be</span>
<span class="sd">    used to directly access these special tokens in a model-independent manner and allow to set and update the special</span>
<span class="sd">    tokens.</span>

<span class="sd">    Args:</span>
<span class="sd">        bos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token representing the beginning of a sentence.</span>
<span class="sd">        eos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token representing the end of a sentence.</span>
<span class="sd">        unk_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token representing an out-of-vocabulary token.</span>
<span class="sd">        sep_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token separating two different sentences in the same input (used by BERT for instance).</span>
<span class="sd">        pad_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by</span>
<span class="sd">            attention mechanisms or loss computation.</span>
<span class="sd">        cls_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token representing the class of the input (used by BERT for instance).</span>
<span class="sd">        mask_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A special token representing a masked token (used by masked-language modeling pretraining objectives, like</span>
<span class="sd">            BERT).</span>
<span class="sd">        additional_special_tokens (tuple or list of :obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="sd">            A tuple or a list of additional special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SPECIAL_TOKENS_ATTRIBUTES</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;bos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;unk_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sep_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;pad_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cls_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mask_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="c1"># We directly set the hidden value to allow initialization with special tokens</span>
        <span class="c1"># which are not yet in the vocabulary. Necessary for serialization/de-serialization</span>
        <span class="c1"># TODO clean this up at some point (probably by switching to fast tokenizers)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)),</span> <span class="sa">f</span><span class="s2">&quot;Value </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> is not a list or tuple&quot;</span>
                    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span><span class="p">),</span> <span class="s2">&quot;One of the tokens is not a string&quot;</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)):</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;special token </span><span class="si">{}</span><span class="s2"> has to be either str or AddedToken but got: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
                    <span class="p">)</span>

<div class="viewcode-block" id="SpecialTokensMixin.sanitize_special_tokens"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">sanitize_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,</span>
<span class="sd">        :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.</span>

<span class="sd">        Add the missing ones to the vocabulary if needed.</span>

<span class="sd">        Return:</span>
<span class="sd">            :obj:`int`: The number of tokens added in the vocabulary during the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="SpecialTokensMixin.add_special_tokens"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If</span>
<span class="sd">        special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the</span>
<span class="sd">        current vocabulary).</span>

<span class="sd">        Using : obj:`add_special_tokens` will ensure your special tokens can be used in several ways:</span>

<span class="sd">        - Special tokens are carefully handled by the tokenizer (they are never split).</span>
<span class="sd">        - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This</span>
<span class="sd">          makes it easy to develop model-agnostic training and fine-tuning scripts.</span>

<span class="sd">        When possible, special tokens are already registered for provided pretrained models (for instance</span>
<span class="sd">        :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`&#39;[CLS]&#39;` and XLM&#39;s one</span>
<span class="sd">        is also registered to be :obj:`&#39;&lt;/s&gt;&#39;`).</span>

<span class="sd">        Args:</span>
<span class="sd">            special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):</span>
<span class="sd">                Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,</span>
<span class="sd">                ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,</span>
<span class="sd">                ``additional_special_tokens``].</span>

<span class="sd">                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer</span>
<span class="sd">                assign the index of the ``unk_token`` to them).</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # Let&#39;s see how to add a new classification token to GPT-2</span>
<span class="sd">            tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)</span>
<span class="sd">            model = GPT2Model.from_pretrained(&#39;gpt2&#39;)</span>

<span class="sd">            special_tokens_dict = {&#39;cls_token&#39;: &#39;&lt;CLS&gt;&#39;}</span>

<span class="sd">            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span>
<span class="sd">            print(&#39;We have added&#39;, num_added_toks, &#39;tokens&#39;)</span>
<span class="sd">            # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="sd">            model.resize_token_embeddings(len(tokenizer))</span>

<span class="sd">            assert tokenizer.cls_token == &#39;&lt;CLS&gt;&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">special_tokens_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="n">added_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is not a special token&quot;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Assigning </span><span class="si">%s</span><span class="s2"> to the </span><span class="si">%s</span><span class="s2"> key of the tokenizer&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Tokens </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> for key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> should all be str or AddedToken instances&quot;</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Token </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> for key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> should be a str or an AddedToken instance&quot;</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="n">value</span><span class="p">],</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">added_tokens</span></div>

<div class="viewcode-block" id="SpecialTokensMixin.add_tokens"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">add_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]],</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to</span>
<span class="sd">        it with indices starting from length of the current vocabulary.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):</span>
<span class="sd">                Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a</span>
<span class="sd">                string token to let you personalize its behavior: whether this token should only match against a single</span>
<span class="sd">                word, whether this token should strip all potential whitespaces on the left side, whether this token</span>
<span class="sd">                should strip all potential whitespaces on the right side, etc.</span>
<span class="sd">            special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Can be used to specify if the token is a special token. This mostly change the normalization behavior</span>
<span class="sd">                (special tokens like CLS or [MASK] are usually not lower-cased for instance).</span>

<span class="sd">                See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # Let&#39;s see how to increase the vocabulary of Bert model and tokenizer</span>
<span class="sd">            tokenizer = BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;)</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span>

<span class="sd">            num_added_toks = tokenizer.add_tokens([&#39;new_tok1&#39;, &#39;my_new-tok2&#39;])</span>
<span class="sd">            print(&#39;We have added&#39;, num_added_toks, &#39;tokens&#39;)</span>
<span class="sd">             # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="sd">            model.resize_token_embeddings(len(tokenizer))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">new_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_tokens</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">]],</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using bos_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: End of sentence token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using eos_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Unknown token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using unk_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while</span>
<span class="sd">        not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using sep_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Padding token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using pad_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the</span>
<span class="sd">        full depth of the model. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using cls_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while</span>
<span class="sd">        not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using mask_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having</span>
<span class="sd">        been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using additional_special_tokens, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span><span class="p">]</span>

    <span class="nd">@bos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@eos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@unk_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@sep_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@pad_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@cls_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@mask_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@additional_special_tokens</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token</span>
<span class="sd">        has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has</span>
<span class="sd">        not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been</span>
<span class="sd">        set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input</span>
<span class="sd">        sequence. Returns :obj:`None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been</span>
<span class="sd">        set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`int`: Id of the padding token type in the vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input</span>
<span class="sd">        sequence leveraging self-attention along the full depth of the model.</span>

<span class="sd">        Returns :obj:`None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language</span>
<span class="sd">        modeling. Returns :obj:`None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not</span>
<span class="sd">        having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_special_tokens</span><span class="p">)</span>

    <span class="nd">@bos_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@eos_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@unk_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@sep_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@pad_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@cls_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@mask_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@additional_special_tokens_ids</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">special_tokens_map</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,</span>
<span class="sd">        :obj:`unk_token`, etc.) to their values (:obj:`&#39;&lt;unk&gt;&#39;`, :obj:`&#39;&lt;cls&gt;&#39;`, etc.).</span>

<span class="sd">        Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">attr_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">attr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_value</span><span class="p">:</span>
                <span class="n">set_attr</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">set_attr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">special_tokens_map_extended</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary</span>
<span class="sd">        mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values</span>
<span class="sd">        (:obj:`&#39;&lt;unk&gt;&#39;`, :obj:`&#39;&lt;cls&gt;&#39;`, etc.).</span>

<span class="sd">        Don&#39;t convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely</span>
<span class="sd">        how special tokens are tokenized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">attr_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">attr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_value</span><span class="p">:</span>
                <span class="n">set_attr</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">attr_value</span>
        <span class="k">return</span> <span class="n">set_attr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`List[str]`: All the special tokens (:obj:`&#39;&lt;unk&gt;&#39;`, :obj:`&#39;&lt;cls&gt;&#39;`, etc.) mapped to class attributes.</span>

<span class="sd">        Convert tokens of :obj:`tokenizers.AddedToken` type to string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">all_toks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_tokens_extended</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`&#39;&lt;unk&gt;&#39;`, :obj:`&#39;&lt;cls&gt;&#39;`, etc.)</span>
<span class="sd">        mapped to class attributes.</span>

<span class="sd">        Don&#39;t convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely</span>
<span class="sd">        how special tokens are tokenized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span>
        <span class="k">for</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">set_attr</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">all_toks</span> <span class="o">=</span> <span class="n">all_toks</span> <span class="o">+</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="k">else</span> <span class="p">[</span><span class="n">attr_value</span><span class="p">])</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">OrderedDict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">all_toks</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">all_toks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`List[int]`: List the ids of the special tokens(:obj:`&#39;&lt;unk&gt;&#39;`, :obj:`&#39;&lt;cls&gt;&#39;`, etc.) mapped to class</span>
<span class="sd">        attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="n">all_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">all_toks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_ids</span></div>


<span class="n">ENCODE_KWARGS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="s2">                Whether or not to encode the sequences with the special tokens relative to their model.</span>
<span class="s2">            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Activates and controls padding. Accepts the following values:</span>

<span class="s2">                * :obj:`True` or :obj:`&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a</span>
<span class="s2">                  single sequence if provided).</span>
<span class="s2">                * :obj:`&#39;max_length&#39;`: Pad to a maximum length specified with the argument :obj:`max_length` or to the</span>
<span class="s2">                  maximum acceptable input length for the model if that argument is not provided.</span>
<span class="s2">                * :obj:`False` or :obj:`&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of</span>
<span class="s2">                  different lengths).</span>
<span class="s2">            truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Activates and controls truncation. Accepts the following values:</span>

<span class="s2">                * :obj:`True` or :obj:`&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument</span>
<span class="s2">                  :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not</span>
<span class="s2">                  provided. This will truncate token by token, removing a token from the longest sequence in the pair</span>
<span class="s2">                  if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="s2">                * :obj:`&#39;only_first&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or to</span>
<span class="s2">                  the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="s2">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="s2">                * :obj:`&#39;only_second&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or</span>
<span class="s2">                  to the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="s2">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="s2">                * :obj:`False` or :obj:`&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with</span>
<span class="s2">                  sequence lengths greater than the model maximum admissible input size).</span>
<span class="s2">            max_length (:obj:`int`, `optional`):</span>
<span class="s2">                Controls the maximum length to use by one of the truncation/padding parameters.</span>

<span class="s2">                If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum</span>
<span class="s2">                length is required by one of the truncation/padding parameters. If the model has no specific maximum</span>
<span class="s2">                input length (like XLNet) truncation/padding to a maximum length will be deactivated.</span>
<span class="s2">            stride (:obj:`int`, `optional`, defaults to 0):</span>
<span class="s2">                If set to a number along with :obj:`max_length`, the overflowing tokens returned when</span>
<span class="s2">                :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence</span>
<span class="s2">                returned to provide some overlap between truncated and overflowing sequences. The value of this</span>
<span class="s2">                argument defines the number of overlapping tokens.</span>
<span class="s2">            is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer</span>
<span class="s2">                will skip the pre-tokenization step. This is useful for NER or token classification.</span>
<span class="s2">            pad_to_multiple_of (:obj:`int`, `optional`):</span>
<span class="s2">                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable</span>
<span class="s2">                the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</span>
<span class="s2">            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):</span>
<span class="s2">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="s2">                * :obj:`&#39;tf&#39;`: Return TensorFlow :obj:`tf.constant` objects.</span>
<span class="s2">                * :obj:`&#39;pt&#39;`: Return PyTorch :obj:`torch.Tensor` objects.</span>
<span class="s2">                * :obj:`&#39;np&#39;`: Return Numpy :obj:`np.ndarray` objects.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            return_token_type_ids (:obj:`bool`, `optional`):</span>
<span class="s2">                Whether to return token type IDs. If left to the default, will return the token type IDs according to</span>
<span class="s2">                the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="s2">                `What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`__</span>
<span class="s2">            return_attention_mask (:obj:`bool`, `optional`):</span>
<span class="s2">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="s2">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="s2">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="s2">            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Whether or not to return overflowing token sequences.</span>
<span class="s2">            return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Whether or not to return special tokens mask information.</span>
<span class="s2">            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Whether or not to return :obj:`(char_start, char_end)` for each token.</span>

<span class="s2">                This is only available on fast tokenizers inheriting from</span>
<span class="s2">                :class:`~transformers.PreTrainedTokenizerFast`, if using Python&#39;s tokenizer, this method will raise</span>
<span class="s2">                :obj:`NotImplementedError`.</span>
<span class="s2">            return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="s2">                Whether or not to return the lengths of the encoded inputs.</span>
<span class="s2">            verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="s2">                Whether or not to print more information and warnings.</span>
<span class="s2">            **kwargs: passed to the :obj:`self.tokenize()` method</span>

<span class="s2">        Return:</span>
<span class="s2">            :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:</span>

<span class="s2">            - **input_ids** -- List of token ids to be fed to a model.</span>

<span class="s2">              `What are input IDs? &lt;../glossary.html#input-ids&gt;`__</span>

<span class="s2">            - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`</span>
<span class="s2">              or if `&quot;token_type_ids&quot;` is in :obj:`self.model_input_names`).</span>

<span class="s2">              `What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`__</span>

<span class="s2">            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when</span>
<span class="s2">              :obj:`return_attention_mask=True` or if `&quot;attention_mask&quot;` is in :obj:`self.model_input_names`).</span>

<span class="s2">              `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>

<span class="s2">            - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and</span>
<span class="s2">              :obj:`return_overflowing_tokens=True`).</span>
<span class="s2">            - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and</span>
<span class="s2">              :obj:`return_overflowing_tokens=True`).</span>
<span class="s2">            - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying</span>
<span class="s2">              regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).</span>
<span class="s2">            - **length** -- The length of the inputs (when :obj:`return_length=True`)</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">INIT_TOKENIZER_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Class attributes (overridden by derived classes)</span>

<span class="s2">        - **vocab_files_names** (:obj:`Dict[str, str]`) -- A dictionary with, as keys, the ``__init__`` keyword name of</span>
<span class="s2">          each vocabulary file required by the model, and as associated values, the filename for saving the associated</span>
<span class="s2">          file (string).</span>
<span class="s2">        - **pretrained_vocab_files_map** (:obj:`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the</span>
<span class="s2">          high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the</span>
<span class="s2">          low-level being the :obj:`short-cut-names` of the pretrained models with, as associated values, the</span>
<span class="s2">          :obj:`url` to the associated pretrained vocabulary file.</span>
<span class="s2">        - **max_model_input_sizes** (:obj:`Dict[str, Optinal[int]]`) -- A dictionary with, as keys, the</span>
<span class="s2">          :obj:`short-cut-names` of the pretrained models, and as associated values, the maximum length of the sequence</span>
<span class="s2">          inputs of this model, or :obj:`None` if the model has no maximum input size.</span>
<span class="s2">        - **pretrained_init_configuration** (:obj:`Dict[str, Dict[str, Any]]`) -- A dictionary with, as keys, the</span>
<span class="s2">          :obj:`short-cut-names` of the pretrained models, and as associated values, a dictionary of specific arguments</span>
<span class="s2">          to pass to the ``__init__`` method of the tokenizer class for this pretrained model when loading the</span>
<span class="s2">          tokenizer with the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`</span>
<span class="s2">          method.</span>
<span class="s2">        - **model_input_names** (:obj:`List[str]`) -- A list of inputs expected in the forward pass of the model.</span>
<span class="s2">        - **padding_side** (:obj:`str`) -- The default value for the side on which the model should have padding</span>
<span class="s2">          applied. Should be :obj:`&#39;right&#39;` or :obj:`&#39;left&#39;`.</span>

<span class="s2">    Args:</span>
<span class="s2">        model_max_length (:obj:`int`, `optional`):</span>
<span class="s2">            The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is</span>
<span class="s2">            loaded with :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`, this</span>
<span class="s2">            will be set to the value stored for the associated model in ``max_model_input_sizes`` (see above). If no</span>
<span class="s2">            value is provided, will default to VERY_LARGE_INTEGER (:obj:`int(1e30)`).</span>
<span class="s2">        padding_side: (:obj:`str`, `optional`):</span>
<span class="s2">            The side on which the model should have padding applied. Should be selected between [&#39;right&#39;, &#39;left&#39;].</span>
<span class="s2">            Default value is picked from the class attribute of the same name.</span>
<span class="s2">        model_input_names (:obj:`List[string]`, `optional`):</span>
<span class="s2">            The list of inputs accepted by the forward pass of the model (like :obj:`&quot;token_type_ids&quot;` or</span>
<span class="s2">            :obj:`&quot;attention_mask&quot;`). Default value is picked from the class attribute of the same name.</span>
<span class="s2">        bos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token representing the beginning of a sentence. Will be associated to ``self.bos_token`` and</span>
<span class="s2">            ``self.bos_token_id``.</span>
<span class="s2">        eos_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token representing the end of a sentence. Will be associated to ``self.eos_token`` and</span>
<span class="s2">            ``self.eos_token_id``.</span>
<span class="s2">        unk_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token representing an out-of-vocabulary token. Will be associated to ``self.unk_token`` and</span>
<span class="s2">            ``self.unk_token_id``.</span>
<span class="s2">        sep_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token separating two different sentences in the same input (used by BERT for instance). Will be</span>
<span class="s2">            associated to ``self.sep_token`` and ``self.sep_token_id``.</span>
<span class="s2">        pad_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by</span>
<span class="s2">            attention mechanisms or loss computation. Will be associated to ``self.pad_token`` and</span>
<span class="s2">            ``self.pad_token_id``.</span>
<span class="s2">        cls_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token representing the class of the input (used by BERT for instance). Will be associated to</span>
<span class="s2">            ``self.cls_token`` and ``self.cls_token_id``.</span>
<span class="s2">        mask_token (:obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A special token representing a masked token (used by masked-language modeling pretraining objectives, like</span>
<span class="s2">            BERT). Will be associated to ``self.mask_token`` and ``self.mask_token_id``.</span>
<span class="s2">        additional_special_tokens (tuple or list of :obj:`str` or :obj:`tokenizers.AddedToken`, `optional`):</span>
<span class="s2">            A tuple or a list of additional special tokens. Add them here to ensure they won&#39;t be split by the</span>
<span class="s2">            tokenization process. Will be associated to ``self.additional_special_tokens`` and</span>
<span class="s2">            ``self.additional_special_tokens_ids``.</span>
<span class="s2">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="PreTrainedTokenizerBase"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase">[docs]</a><span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">INIT_TOKENIZER_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PreTrainedTokenizerBase</span><span class="p">(</span><span class="n">SpecialTokensMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for :class:`~transformers.PreTrainedTokenizer` and :class:`~transformers.PreTrainedTokenizerFast`.</span>

<span class="sd">    Handles shared (mostly boiler plate) methods for those two classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vocab_files_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_vocab_files_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_init_configuration</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">max_model_input_sizes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model_input_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
    <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
    <span class="n">slow_tokenizer_class</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span> <span class="o">=</span> <span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name_or_path&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># For backward compatibility we fallback to set model_max_length from max_len if provided</span>
        <span class="n">model_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;max_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">=</span> <span class="n">model_max_length</span> <span class="k">if</span> <span class="n">model_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">VERY_LARGE_INTEGER</span>

        <span class="c1"># Padding side is right by default and overridden in subclasses. If specified in the kwargs, it is changed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;padding_side&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;right&quot;</span><span class="p">,</span>
            <span class="s2">&quot;left&quot;</span><span class="p">,</span>
        <span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Padding side should be selected between &#39;right&#39; and &#39;left&#39;, current value: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model_input_names&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">{}</span>
        <span class="p">)</span>  <span class="c1"># Use to store when we have already noticed a deprecation warning (avoid overlogging).</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_len_single_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`int`: The maximum length of a sentence that can be fed to the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_len_sentences_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@max_len_single_sentence</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">max_len_single_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># For backward compatibility, allow to try to setup &#39;max_len_single_sentence&#39;.</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_len_single_sentence&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Setting &#39;max_len_single_sentence&#39; is now deprecated. &quot;</span> <span class="s2">&quot;This value is automatically set up.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;max_len_single_sentence&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Setting &#39;max_len_single_sentence&#39; is now deprecated. &quot;</span> <span class="s2">&quot;This value is automatically set up.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@max_len_sentences_pair</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">max_len_sentences_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># For backward compatibility, allow to try to setup &#39;max_len_sentences_pair&#39;.</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_len_sentences_pair&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Setting &#39;max_len_sentences_pair&#39; is now deprecated. &quot;</span> <span class="s2">&quot;This value is automatically set up.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;max_len_sentences_pair&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Setting &#39;max_len_sentences_pair&#39; is now deprecated. &quot;</span> <span class="s2">&quot;This value is automatically set up.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;PreTrainedTokenizerFast&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fast</span> <span class="k">else</span> <span class="s1">&#39;PreTrainedTokenizer&#39;</span><span class="si">}</span><span class="s2">(name_or_path=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;vocab_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">, model_max_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2">, is_fast=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_fast</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;padding_side=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="si">}</span><span class="s2">&#39;, special_tokens=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.get_vocab"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the vocabulary as a dictionary of token to index.</span>

<span class="sd">        :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when</span>
<span class="sd">        :obj:`token` is in the vocab.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`Dict[str, int]`: The vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.from_pretrained"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span> <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from</span>
<span class="sd">        a predefined tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.</span>
<span class="sd">                  Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a</span>
<span class="sd">                  user or organization name, like ``dbmdz/bert-base-german-cased``.</span>
<span class="sd">                - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved</span>
<span class="sd">                  using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`</span>
<span class="sd">                  method, e.g., ``./my_model_directory/``.</span>
<span class="sd">                - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary</span>
<span class="sd">                  file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,</span>
<span class="sd">                  ``./my_model_directory/vocab.txt``.</span>
<span class="sd">            cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):</span>
<span class="sd">                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the</span>
<span class="sd">                standard cache should not be used.</span>
<span class="sd">            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they</span>
<span class="sd">                exist.</span>
<span class="sd">            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to delete incompletely received files. Attempt to resume the download if such a file</span>
<span class="sd">                exists.</span>
<span class="sd">            proxies (:obj:`Dict[str, str], `optional`):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            use_auth_token (:obj:`str` or `bool`, `optional`):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token</span>
<span class="sd">                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).</span>
<span class="sd">            revision(:obj:`str`, `optional`, defaults to :obj:`&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a</span>
<span class="sd">                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any</span>
<span class="sd">                identifier allowed by git.</span>
<span class="sd">            subfolder (:obj:`str`, `optional`):</span>
<span class="sd">                In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for</span>
<span class="sd">                facebook/rag-token-base), specify it here.</span>
<span class="sd">            inputs (additional positional arguments, `optional`):</span>
<span class="sd">                Will be passed along to the Tokenizer ``__init__`` method.</span>
<span class="sd">            kwargs (additional keyword arguments, `optional`):</span>
<span class="sd">                Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like</span>
<span class="sd">                ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,</span>
<span class="sd">                ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.</span>

<span class="sd">        .. note::</span>

<span class="sd">            Passing :obj:`use_auth_token=True` is required when you want to use a private model.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # We can&#39;t instantiate directly the base class `PreTrainedTokenizerBase` so let&#39;s show our examples on a derived class: BertTokenizer</span>
<span class="sd">            # Download vocabulary from huggingface.co and cache.</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)</span>

<span class="sd">            # Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;dbmdz/bert-base-german-cased&#39;)</span>

<span class="sd">            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`)</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;./test/saved_model/&#39;)</span>

<span class="sd">            # If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;./test/saved_model/my_vocab.txt&#39;)</span>

<span class="sd">            # You can link tokens to special vocabulary when instantiating</span>
<span class="sd">            tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;, unk_token=&#39;&lt;unk&gt;&#39;)</span>
<span class="sd">            # You should be sure &#39;&lt;unk&gt;&#39; is in the vocabulary when doing that.</span>
<span class="sd">            # Otherwise use tokenizer.add_special_tokens({&#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;}) instead)</span>
<span class="sd">            assert tokenizer.unk_token == &#39;&lt;unk&gt;&#39;</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">use_auth_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_auth_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">s3_models</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="n">vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">init_configuration</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="n">s3_models</span><span class="p">:</span>
            <span class="c1"># Get the vocabulary from AWS S3 bucket</span>
            <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">map_list</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_vocab_files_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">map_list</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span>
                <span class="ow">and</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span>
            <span class="p">):</span>
                <span class="n">init_configuration</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_init_configuration</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get the vocabulary from local files</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; not found in model shortcut name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;Assuming &#39;</span><span class="si">{}</span><span class="s2">&#39; is a path, a model identifier, or url to a directory containing tokenizer files.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">s3_models</span><span class="p">),</span> <span class="n">pretrained_model_name_or_path</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Calling </span><span class="si">{}</span><span class="s2">.from_pretrained() with the path to a single file or url is not supported.&quot;</span>
                        <span class="s2">&quot;Use a model identifier or the path to a directory instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Calling </span><span class="si">{}</span><span class="s2">.from_pretrained() with the path to a single file or url is deprecated&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">file_id</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># At this point pretrained_model_name_or_path is either a directory or a model identifier name</span>
                <span class="n">additional_files_names</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;added_tokens_file&quot;</span><span class="p">:</span> <span class="n">ADDED_TOKENS_FILE</span><span class="p">,</span>
                    <span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">:</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span><span class="p">,</span>
                    <span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">:</span> <span class="n">TOKENIZER_CONFIG_FILE</span><span class="p">,</span>
                    <span class="s2">&quot;tokenizer_file&quot;</span><span class="p">:</span> <span class="n">FULL_TOKENIZER_FILE</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="c1"># Look for the tokenizer files</span>
                <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="p">{</span><span class="o">**</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">,</span> <span class="o">**</span><span class="n">additional_files_names</span><span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">subfolder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">full_file_name</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Didn&#39;t find file </span><span class="si">{}</span><span class="s2">. We won&#39;t load it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">full_file_name</span><span class="p">))</span>
                            <span class="n">full_file_name</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">hf_bucket_url</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="n">filename</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span>
                            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                            <span class="n">mirror</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="p">)</span>

                    <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_file_name</span>

        <span class="c1"># Get files from url, cache, or disk depending on the case</span>
        <span class="n">resolved_vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">cached_path</span><span class="p">(</span>
                        <span class="n">file_path</span><span class="p">,</span>
                        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                        <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                        <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">HTTPError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                    <span class="k">if</span> <span class="s2">&quot;404 Client Error&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">err</span><span class="p">):</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                        <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">err</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">full_file_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">full_file_name</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t load tokenizer for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39;. Make sure that:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;- &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is a correct model identifier listed on &#39;https://huggingface.co/models&#39;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;- or &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is the correct path to a directory containing relevant tokenizer files</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">file_path</span> <span class="o">==</span> <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading file </span><span class="si">{}</span><span class="s2"> from cache at </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]))</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
            <span class="n">resolved_vocab_files</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">init_configuration</span><span class="p">,</span> <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">resolved_vocab_files</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">init_configuration</span><span class="p">,</span> <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="c1"># We instantiate fast tokenizers based on a slow tokenizer for now</span>
        <span class="c1"># In the future we can also use a direct way based on saving/instantiating</span>
        <span class="c1"># tokenizer&#39;s Tokenizer directly from it&#39;s serialization JSON</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="s2">&quot;tokenizer_file&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span> <span class="ow">or</span> <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="s2">&quot;tokenizer_file&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="bp">cls</span><span class="o">.</span><span class="n">slow_tokenizer_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">slow_tokenizer_class</span><span class="p">)</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">resolved_vocab_files</span><span class="p">),</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">init_configuration</span><span class="p">),</span>
                <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
                <span class="o">**</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Prepare tokenizer initialization kwargs</span>
        <span class="c1"># Did we saved some inputs and kwargs to reload ?</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tokenizer_config_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tokenizer_config_handle</span><span class="p">:</span>
                <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tokenizer_config_handle</span><span class="p">)</span>
            <span class="n">saved_init_inputs</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">,</span> <span class="p">())</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">init_inputs</span><span class="p">:</span>
                <span class="n">init_inputs</span> <span class="o">=</span> <span class="n">saved_init_inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">init_configuration</span>

        <span class="c1"># Update with newly provided kwargs</span>
        <span class="n">init_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Convert AddedTokens serialized as dict to class instances</span>
        <span class="k">def</span> <span class="nf">convert_added_tokens</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;__type&quot;</span> <span class="ow">in</span> <span class="n">obj</span> <span class="ow">and</span> <span class="n">obj</span><span class="p">[</span><span class="s2">&quot;__type&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;AddedToken&quot;</span><span class="p">:</span>
                <span class="n">obj</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;__type&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">obj</span>

        <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">init_kwargs</span><span class="p">)</span>

        <span class="c1"># Set max length if needed</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">:</span>
            <span class="c1"># if we&#39;re using a pretrained model, ensure the tokenizer</span>
            <span class="c1"># wont index sequences longer than the number of positional embeddings</span>
            <span class="n">model_max_length</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">model_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_max_length</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e30</span><span class="p">)),</span> <span class="n">model_max_length</span><span class="p">)</span>

        <span class="c1"># Merge resolved_vocab_files arguments in init_kwargs.</span>
        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;added_tokens_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">args_name</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">args_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">init_kwargs</span><span class="p">:</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="n">args_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_path</span>

        <span class="k">if</span> <span class="n">slow_tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;__slow_tokenizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">slow_tokenizer</span>

        <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;name_or_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

        <span class="c1"># Instantiate tokenizer.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to load vocabulary from file. &quot;</span>
                <span class="s2">&quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Save inputs and kwargs for saving and re-loading with ``save_pretrained``</span>
        <span class="c1"># Removed: Now done at the base class level</span>
        <span class="c1"># tokenizer.init_inputs = init_inputs</span>
        <span class="c1"># tokenizer.init_kwargs = init_kwargs</span>

        <span class="c1"># If there is a complementary special token map, load it</span>
        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">special_tokens_map_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">special_tokens_map_handle</span><span class="p">:</span>
                <span class="n">special_tokens_map</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">special_tokens_map_handle</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">value</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">token</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="c1"># Add supplementary tokens.</span>
        <span class="n">special_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="k">if</span> <span class="n">added_tokens_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">added_tokens_handle</span><span class="p">:</span>
                <span class="n">added_tok_encoder</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">added_tokens_handle</span><span class="p">)</span>

            <span class="c1"># Sort added tokens by index</span>
            <span class="n">added_tok_encoder_sorted</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">added_tok_encoder</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

            <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">added_tok_encoder_sorted</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Non-consecutive added token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; found. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Should have index </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2"> but has index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> in saved vocabulary.&quot;</span>
                <span class="p">)</span>
                <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">))</span>

        <span class="c1"># Check all our special tokens are registered as &quot;no split&quot; token (we don&#39;t cut them) and are in the vocab</span>
        <span class="n">added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">sanitize_special_tokens</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">added_tokens</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">tokenizer</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.save_pretrained"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained">[docs]</a>    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">legacy_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the full tokenizer state.</span>


<span class="sd">        This method make sure the full tokenizer can then be re-loaded using the</span>
<span class="sd">        :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method.</span>

<span class="sd">        .. Note::</span>
<span class="sd">            A &quot;fast&quot; tokenizer (instance of :class:`transformers.PreTrainedTokenizerFast`) saved with this method will</span>
<span class="sd">            not be possible to load back in a &quot;slow&quot; tokenizer, i.e. in a :class:`transformers.PreTrainedTokenizer`</span>
<span class="sd">            instance. It can only be loaded in a &quot;fast&quot; tokenizer, i.e. in a</span>
<span class="sd">            :class:`transformers.PreTrainedTokenizerFast` instance.</span>

<span class="sd">        .. Warning::</span>
<span class="sd">           This won&#39;t save modifications you may have applied to the tokenizer after the instantiation (for instance,</span>
<span class="sd">           modifying :obj:`tokenizer.do_lower_case` after creation).</span>

<span class="sd">        Args:</span>
<span class="sd">            save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.</span>
<span class="sd">            legacy_format (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether to save the tokenizer in legacy format (default), i.e. with tokenizer specific vocabulary and a</span>
<span class="sd">                separate added_tokens files or in the unified JSON file format for the `tokenizers` library. It&#39;s only</span>
<span class="sd">                possible to save a Fast tokenizer in the unified JSON format and this format is incompatible with</span>
<span class="sd">                &quot;slow&quot; tokenizers (not powered by the `tokenizers` library).</span>
<span class="sd">            filename_prefix: (:obj:`str`, `optional`):</span>
<span class="sd">                A prefix to add to the names of the files saved by the tokenizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of :obj:`str`: The files saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Provided path (</span><span class="si">{}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">save_directory</span><span class="p">))</span>
            <span class="k">return</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span>
        <span class="p">)</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">TOKENIZER_CONFIG_FILE</span>
        <span class="p">)</span>

        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">tokenizer_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">file_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Sanitize AddedTokens</span>
        <span class="k">def</span> <span class="nf">convert_added_tokens</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">add_type_field</span><span class="p">:</span>
                    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;__type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;AddedToken&quot;</span>
                <span class="k">return</span> <span class="n">out</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="n">add_type_field</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="n">add_type_field</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">obj</span>

        <span class="c1"># add_type_field=True to allow dicts in the kwargs / differentiate from AddedToken serialization</span>
        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="c1"># Sanitize AddedTokens in special_tokens_map</span>
        <span class="n">write_dict</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">write_dict</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="n">file_names</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="n">special_tokens_map_file</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_pretrained</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">file_names</span><span class="o">=</span><span class="n">file_names</span><span class="p">,</span>
            <span class="n">legacy_format</span><span class="o">=</span><span class="n">legacy_format</span><span class="p">,</span>
            <span class="n">filename_prefix</span><span class="o">=</span><span class="n">filename_prefix</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">file_names</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">legacy_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens.</span>

<span class="sd">        Fast tokenizers can also be saved in a unique JSON file containing {config + vocab + added-tokens} using the</span>
<span class="sd">        specific :meth:`~transformers.tokenization_utils_fast.PreTrainedTokenizerFast._save_pretrained`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">legacy_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Only fast tokenizers (instances of PretrainedTokenizerFast) can be saved in non legacy format.&quot;</span>
            <span class="p">)</span>

        <span class="n">save_directory</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">ADDED_TOKENS_FILE</span>
        <span class="p">)</span>
        <span class="n">added_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_added_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">added_vocab</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">out_str</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">added_vocab</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out_str</span><span class="p">)</span>

        <span class="n">vocab_files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="n">filename_prefix</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">file_names</span> <span class="o">+</span> <span class="n">vocab_files</span> <span class="o">+</span> <span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,)</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.save_vocabulary"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_vocabulary">[docs]</a>    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save only the vocabulary of the tokenizer (vocabulary + added tokens).</span>

<span class="sd">        This method won&#39;t save the configuration and special token mappings of the tokenizer. Use</span>
<span class="sd">        :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            save_directory (:obj:`str`):</span>
<span class="sd">                The directory in which to save the vocabulary.</span>
<span class="sd">            filename_prefix (:obj:`str`, `optional`):</span>
<span class="sd">                An optional prefix to add to the named of the saved files.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`Tuple(str)`: Paths to the files saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.tokenize"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.tokenize">[docs]</a>    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string in a sequence of tokens, using the backend Rust tokenizer.</span>

<span class="sd">        Note that this method behave differently between fast and slow tokenizers:</span>

<span class="sd">            - in fast tokenizers (instances of :class:`~transformers.PreTrainedTokenizerFast`), this method will</span>
<span class="sd">              replace the unknown tokens with the :obj:`unk_token`,</span>
<span class="sd">            - in slow tokenizers (instances of :class:`~transformers.PreTrainedTokenizer`), this method keep unknown</span>
<span class="sd">              tokens unchanged.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str`):</span>
<span class="sd">                The sequence to be encoded.</span>
<span class="sd">            pair (:obj:`str`, `optional`):</span>
<span class="sd">                A second sequence to be encoded with the first.</span>
<span class="sd">            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to add the special tokens associated with the corresponding model.</span>
<span class="sd">            kwargs (additional keyword arguments, `optional`):</span>
<span class="sd">                Will be passed to the underlying model specific encode method. See details in</span>
<span class="sd">                :meth:`~transformers.PreTrainedTokenizer.__call__`</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[str]`: The list of tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.encode"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode">[docs]</a>    <span class="nd">@add_end_docstrings</span><span class="p">(</span>
        <span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            **kwargs: Passed along to the `.tokenize()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span><span class="p">,</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the</span>
<span class="sd">            text.</span>
<span class="sd">        &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.</span>

<span class="sd">        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the</span>
<span class="sd">                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``</span>
<span class="sd">                method).</span>
<span class="sd">            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the ``tokenize`` method) or a list of integers (tokenized string ids using the</span>
<span class="sd">                ``convert_tokens_to_ids`` method).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span></div>

    <span class="k">def</span> <span class="nf">num_special_tokens_to_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_padding_truncation_strategies</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the correct padding/truncation strategy with backward compatibility for old arguments (truncation_strategy</span>
<span class="sd">        and pad_to_max_length) and behaviors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_truncation_strategy</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;truncation_strategy&quot;</span><span class="p">,</span> <span class="s2">&quot;do_not_truncate&quot;</span><span class="p">)</span>
        <span class="n">old_pad_to_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pad_to_max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Backward compatibility for previous behavior, maybe we should deprecate it:</span>
        <span class="c1"># If you only set max_length, it activates truncation for max_length</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Truncation-not-explicitly-activated&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Truncation was not explicitly activated but `max_length` is provided a specific value, &quot;</span>
                        <span class="s2">&quot;please use `truncation=True` to explicitly truncate examples to max length. &quot;</span>
                        <span class="s2">&quot;Defaulting to &#39;longest_first&#39; truncation strategy. &quot;</span>
                        <span class="s2">&quot;If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy &quot;</span>
                        <span class="s2">&quot;more precisely by providing a specific strategy to `truncation`.&quot;</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Truncation-not-explicitly-activated&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">truncation</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span>

        <span class="c1"># Get padding strategy</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">old_pad_to_max_length</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;The `pad_to_max_length` argument is deprecated and will be removed in a future version, &quot;</span>
                    <span class="s2">&quot;use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or &quot;</span>
                    <span class="s2">&quot;use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific &quot;</span>
                    <span class="s2">&quot;length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the &quot;</span>
                    <span class="s2">&quot;maximal input size of the model (e.g. 512 for Bert).&quot;</span><span class="p">,</span>
                    <span class="ne">FutureWarning</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span>
        <span class="k">elif</span> <span class="n">padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span>  <span class="c1"># Default to pad to the longest sequence in the batch</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">):</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">):</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>

        <span class="c1"># Get truncation strategy</span>
        <span class="k">if</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">old_truncation_strategy</span> <span class="o">!=</span> <span class="s2">&quot;do_not_truncate&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;The `truncation_strategy` argument is deprecated and will be removed in a future version, &quot;</span>
                    <span class="s2">&quot;use `truncation=True` to truncate examples to a max length. You can give a specific &quot;</span>
                    <span class="s2">&quot;length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the &quot;</span>
                    <span class="s2">&quot;maximal input size of the model (e.g. 512 for Bert). &quot;</span>
                    <span class="s2">&quot; If you have pairs of inputs, you can give a specific truncation strategy selected among &quot;</span>
                    <span class="s2">&quot;`truncation=&#39;only_first&#39;` (will only truncate the first sentence in the pairs) &quot;</span>
                    <span class="s2">&quot;`truncation=&#39;only_second&#39;` (will only truncate the second sentence in the pairs) &quot;</span>
                    <span class="s2">&quot;or `truncation=&#39;longest_first&#39;` (will iteratively remove tokens from the longest sentence in the pairs).&quot;</span><span class="p">,</span>
                    <span class="ne">FutureWarning</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">old_truncation_strategy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span>
                <span class="p">)</span>  <span class="c1"># Default to truncate the longest sequences in pairs of inputs</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">truncation</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">truncation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>

        <span class="c1"># Set max length if needed</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">&gt;</span> <span class="n">LARGE_INTEGER</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Asking-to-pad-to-max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                <span class="s2">&quot;Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. &quot;</span>
                                <span class="s2">&quot;Default to no padding.&quot;</span>
                            <span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Asking-to-pad-to-max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>

            <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">&gt;</span> <span class="n">LARGE_INTEGER</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Asking-to-truncate-to-max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                <span class="s2">&quot;Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. &quot;</span>
                                <span class="s2">&quot;Default to no truncation.&quot;</span>
                            <span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Asking-to-truncate-to-max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>

        <span class="c1"># Test if we have a padding token</span>
        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Asking to pad but the tokenizer does not have a padding token. &quot;</span>
                <span class="s2">&quot;Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` &quot;</span>
                <span class="s2">&quot;or add a new pad token via `tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>
            <span class="ow">and</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>
            <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">%</span> <span class="n">pad_to_multiple_of</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Truncation and padding are both activated but &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;truncation length (</span><span class="si">{</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) is not a multiple of pad_to_multiple_of (</span><span class="si">{</span><span class="n">pad_to_multiple_of</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.__call__"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__">[docs]</a>    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of</span>
<span class="sd">        sequences.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Input type checking for clearer error</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                    <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)))</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;</span>
            <span class="s2">&quot;or `List[List[str]]` (batch of pretokenized examples).&quot;</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">text_pair</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                <span class="ow">and</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">text_pair</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                    <span class="ow">or</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                        <span class="ow">or</span> <span class="p">(</span>
                            <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                            <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">))</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;text_pair input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;</span>
            <span class="s2">&quot;or `List[List[str]]` (batch of pretokenized examples).&quot;</span>
        <span class="p">)</span>

        <span class="n">is_batched</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="p">(</span><span class="ow">not</span> <span class="n">is_split_into_words</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)))</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="n">is_split_into_words</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">text</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="n">batch_text_or_text_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">))</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">text</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
                <span class="n">batch_text_or_text_pairs</span><span class="o">=</span><span class="n">batch_text_or_text_pairs</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
                <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
                <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
                <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
                <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
                <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.encode_plus"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus">[docs]</a>    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize and prepare for the model a sequence or a pair of sequences.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            This method is deprecated, ``__call__`` should be used instead.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the</span>
<span class="sd">                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``</span>
<span class="sd">                method).</span>
<span class="sd">            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the ``tokenize`` method) or a list of integers (tokenized string ids using the</span>
<span class="sd">                ``convert_tokens_to_ids`` method).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">TruncationStrategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.batch_encode_plus"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus">[docs]</a>    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInputPair</span><span class="p">],</span>
        <span class="p">],</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            This method is deprecated, ``__call__`` should be used instead.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):</span>
<span class="sd">                Batch of sequences or pair of sequences to be encoded. This can be a list of</span>
<span class="sd">                string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see</span>
<span class="sd">                details in ``encode_plus``).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_encode_plus</span><span class="p">(</span>
            <span class="n">batch_text_or_text_pairs</span><span class="o">=</span><span class="n">batch_text_or_text_pairs</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInputPair</span><span class="p">],</span>
        <span class="p">],</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">TruncationStrategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.pad"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad">[docs]</a>    <span class="k">def</span> <span class="nf">pad</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoded_inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">BatchEncoding</span><span class="p">,</span>
            <span class="n">List</span><span class="p">[</span><span class="n">BatchEncoding</span><span class="p">],</span>
            <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">]],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]],</span>
        <span class="p">],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length</span>
<span class="sd">        in the batch.</span>

<span class="sd">        Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,</span>
<span class="sd">        ``self.pad_token_id`` and ``self.pad_token_type_id``)</span>

<span class="sd">        .. note::</span>

<span class="sd">            If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the</span>
<span class="sd">            result will use the same type unless you provide a different tensor type with ``return_tensors``. In the</span>
<span class="sd">            case of PyTorch tensors, you will lose the specific device of your tensors however.</span>

<span class="sd">        Args:</span>
<span class="sd">            encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):</span>
<span class="sd">                Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,</span>
<span class="sd">                List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,</span>
<span class="sd">                List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as</span>
<span class="sd">                well as in a PyTorch Dataloader collate function.</span>

<span class="sd">                Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),</span>
<span class="sd">                see the note above for the return type.</span>
<span class="sd">            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                 Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">                 index) among:</span>

<span class="sd">                * :obj:`True` or :obj:`&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a</span>
<span class="sd">                  single sequence if provided).</span>
<span class="sd">                * :obj:`&#39;max_length&#39;`: Pad to a maximum length specified with the argument :obj:`max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                * :obj:`False` or :obj:`&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of</span>
<span class="sd">                  different lengths).</span>
<span class="sd">            max_length (:obj:`int`, `optional`):</span>
<span class="sd">                Maximum length of the returned list and optionally padding length (see above).</span>
<span class="sd">            pad_to_multiple_of (:obj:`int`, `optional`):</span>
<span class="sd">                If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                &gt;= 7.5 (Volta).</span>
<span class="sd">            return_attention_mask (:obj:`bool`, `optional`):</span>
<span class="sd">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the :obj:`return_outputs` attribute.</span>

<span class="sd">                `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="sd">            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):</span>
<span class="sd">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">                * :obj:`&#39;tf&#39;`: Return TensorFlow :obj:`tf.constant` objects.</span>
<span class="sd">                * :obj:`&#39;pt&#39;`: Return PyTorch :obj:`torch.Tensor` objects.</span>
<span class="sd">                * :obj:`&#39;np&#39;`: Return Numpy :obj:`np.ndarray` objects.</span>
<span class="sd">            verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not to print more information and warnings.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If we have a list of dicts, let&#39;s convert it in a dict of lists</span>
        <span class="c1"># We do this to allow using this method as a collate_fn function in PyTorch Dataloader</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">BatchEncoding</span><span class="p">)):</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">example</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>

        <span class="k">assert</span> <span class="s2">&quot;input_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You should supply an encoding or a list of encodings to this method. &quot;</span>
            <span class="s2">&quot;An encoding is the output of one the encoding methods of the tokenizer, i.e. &quot;</span>
            <span class="s2">&quot;__call__/encode_plus/batch_encode_plus. &quot;</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">return</span> <span class="n">encoded_inputs</span>

        <span class="c1"># If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects</span>
        <span class="c1"># and rebuild them afterwards if no return_tensors is specified</span>
        <span class="c1"># Note that we lose the specific device the tensor may be on for PyTorch</span>

        <span class="n">first_element</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="c1"># first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.</span>
            <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="n">index</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]):</span>
                <span class="n">first_element</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="n">index</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">is_tf_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">_is_tensorflow</span><span class="p">(</span><span class="n">first_element</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;tf&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">elif</span> <span class="n">is_torch_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">_is_torch</span><span class="p">(</span><span class="n">first_element</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;np&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;type of </span><span class="si">{</span><span class="n">first_element</span><span class="si">}</span><span class="s2"> unknown: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">first_element</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Should be one of a python, numpy, pytorch or tensorflow object.&quot;</span>
                <span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">to_py_obj</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Convert padding_strategy in PaddingStrategy</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad</span><span class="p">(</span>
                <span class="n">encoded_inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Some items in the output dictionary have a different batch size than others.&quot;</span>

        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="k">for</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="p">:</span>
                    <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">batch_outputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.create_token_type_ids_from_sequences"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences">[docs]</a>    <span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the token type IDs corresponding to the sequences passed. `What are token type IDs?</span>
<span class="sd">        &lt;../glossary.html#token-type-ids&gt;`__</span>

<span class="sd">        Should be overridden in a subclass if the model has a special way of building those.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.</span>
<span class="sd">            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[int]`: The token type ids.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.build_inputs_with_special_tokens"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">        adding special tokens.</span>

<span class="sd">        This implementation does not add special tokens and this method should be overridden in a subclass.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (:obj:`List[int]`): The first tokenized sequence.</span>
<span class="sd">            token_ids_1 (:obj:`List[int]`, `optional`): The second tokenized sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[int]`: The model input with special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token_ids_0</span>
        <span class="k">return</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.prepare_for_model"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model">[docs]</a>    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">prepare_for_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It</span>
<span class="sd">        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and</span>
<span class="sd">        manages a moving window (with user defined stride) for overflowing tokens</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (:obj:`List[int]`):</span>
<span class="sd">                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``</span>
<span class="sd">                and ``convert_tokens_to_ids`` methods.</span>
<span class="sd">            pair_ids (:obj:`List[int]`, `optional`):</span>
<span class="sd">                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``</span>
<span class="sd">                and ``convert_tokens_to_ids`` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pair</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">len_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">len_pair_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Asking to return token_type_ids while setting add_special_tokens to False &quot;</span>
                <span class="s2">&quot;results in an undefined behavior. Please set add_special_tokens to True or &quot;</span>
                <span class="s2">&quot;set return_token_type_ids to None.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Load from model defaults</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Compute the total size of the returned encodings</span>
        <span class="n">total_len</span> <span class="o">=</span> <span class="n">len_ids</span> <span class="o">+</span> <span class="n">len_pair_ids</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="n">pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_special_tokens</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Truncation: Handle max sequence length</span>
        <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="n">total_len</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_sequences</span><span class="p">(</span>
                <span class="n">ids</span><span class="p">,</span>
                <span class="n">pair_ids</span><span class="o">=</span><span class="n">pair_ids</span><span class="p">,</span>
                <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span>
                <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflowing_tokens</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;num_truncated_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span>

        <span class="c1"># Add special tokens</span>
        <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="n">pair_ids</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="n">ids</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="p">[])</span>

        <span class="c1"># Build output dictionary</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequence</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span>
        <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

        <span class="c1"># Check lengths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

        <span class="c1"># Padding</span>
        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">or</span> <span class="n">return_attention_mask</span><span class="p">:</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">encoded_inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding_strategy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_length</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">BatchEncoding</span><span class="p">(</span>
            <span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_outputs</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.truncate_sequences"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences">[docs]</a>    <span class="k">def</span> <span class="nf">truncate_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_tokens_to_remove</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Truncates a sequence pair in-place following the strategy.</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (:obj:`List[int]`):</span>
<span class="sd">                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``</span>
<span class="sd">                and ``convert_tokens_to_ids`` methods.</span>
<span class="sd">            pair_ids (:obj:`List[int]`, `optional`):</span>
<span class="sd">                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``</span>
<span class="sd">                and ``convert_tokens_to_ids`` methods.</span>
<span class="sd">            num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                Number of tokens to remove using the truncation strategy.</span>
<span class="sd">            truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                The strategy to follow for truncation. Can be:</span>

<span class="sd">                * :obj:`&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or</span>
<span class="sd">                  to the maximum acceptable input length for the model if that argument is not provided. This will</span>
<span class="sd">                  truncate token by token, removing a token from the longest sequence in the pair if a pair of</span>
<span class="sd">                  sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`&#39;only_first&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or to</span>
<span class="sd">                  the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`&#39;only_second&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or</span>
<span class="sd">                  to the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with sequence lengths</span>
<span class="sd">                  greater than the model maximum admissible input size).</span>
<span class="sd">            stride (:obj:`int`, `optional`, defaults to 0):</span>
<span class="sd">                If set to a positive number, the overflowing tokens returned will contain some tokens from the main</span>
<span class="sd">                sequence returned. The value of this argument defines the number of additional tokens.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the</span>
<span class="sd">            list of overflowing tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_tokens_to_remove</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">truncation_strategy</span><span class="p">)</span>

        <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tokens_to_remove</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">overflowing_tokens</span><span class="p">:</span>
                        <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">window_len</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">overflowing_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:])</span>
                    <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">overflowing_tokens</span><span class="p">:</span>
                        <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">window_len</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">overflowing_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:])</span>
                    <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">ONLY_FIRST</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span><span class="p">:</span>
                <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
                <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
                <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;We need to remove </span><span class="si">{</span><span class="n">num_tokens_to_remove</span><span class="si">}</span><span class="s2"> to truncate the input&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but the first sequence has a length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Please select another truncation strategy than </span><span class="si">{</span><span class="n">truncation_strategy</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;for instance &#39;longest_first&#39; or &#39;only_second&#39;.&quot;</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">ONLY_SECOND</span> <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span><span class="p">:</span>
                <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
                <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
                <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;We need to remove </span><span class="si">{</span><span class="n">num_tokens_to_remove</span><span class="si">}</span><span class="s2"> to truncate the input&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but the second sequence has a length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Please select another truncation strategy than </span><span class="si">{</span><span class="n">truncation_strategy</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;for instance &#39;longest_first&#39; or &#39;only_first&#39;.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_pad</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoded_inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span> <span class="n">BatchEncoding</span><span class="p">],</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)</span>

<span class="sd">        Args:</span>
<span class="sd">            encoded_inputs: Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).</span>
<span class="sd">            max_length: maximum length of the returned list and optionally padding length (see below).</span>
<span class="sd">                Will truncate by taking into account the special tokens.</span>
<span class="sd">            padding_strategy: PaddingStrategy to use for padding.</span>

<span class="sd">                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch</span>
<span class="sd">                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)</span>
<span class="sd">                - PaddingStrategy.DO_NOT_PAD: Do not pad</span>
<span class="sd">                The tokenizer padding sides are defined in self.padding_side:</span>

<span class="sd">                    - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                    - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.</span>
<span class="sd">                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability</span>
<span class="sd">                &gt;= 7.5 (Volta).</span>
<span class="sd">            return_attention_mask: (optional) Set to False to avoid returning attention mask (default: set to model specifics)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load from model defaults</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">%</span> <span class="n">pad_to_multiple_of</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="p">((</span><span class="n">max_length</span> <span class="o">//</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_to_multiple_of</span>

        <span class="n">needs_to_be_padded</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">!=</span> <span class="n">max_length</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">needs_to_be_padded</span><span class="p">:</span>
            <span class="n">difference</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;special_tokens_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
                <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span>
                        <span class="s2">&quot;token_type_ids&quot;</span>
                    <span class="p">]</span>
                <span class="k">if</span> <span class="s2">&quot;special_tokens_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid padding strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">return_attention_mask</span> <span class="ow">and</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.convert_tokens_to_string"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.convert_tokens_to_string">[docs]</a>    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a sequence of token ids in a single string. The most simple way to do it is ``&quot; &quot;.join(tokens)`` but</span>
<span class="sd">        we often want to remove sub-word tokenization artifacts at the same time</span>

<span class="sd">        Args:</span>
<span class="sd">            tokens (:obj:`List[str]`): The token to join in a string.</span>
<span class="sd">        Return: The joined tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.batch_decode"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode">[docs]</a>    <span class="k">def</span> <span class="nf">batch_decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sequences</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="s2">&quot;np.ndarray&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a list of lists of token ids into a list of strings by calling decode.</span>

<span class="sd">        Args:</span>
<span class="sd">            sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):</span>
<span class="sd">                List of tokenized input ids. Can be obtained using the ``__call__`` method.</span>
<span class="sd">            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to remove special tokens in the decoding.</span>
<span class="sd">            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not to clean up the tokenization spaces.</span>
<span class="sd">            kwargs (additional keyword arguments, `optional`):</span>
<span class="sd">                Will be passed to the underlying model specific decode method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[str]`: The list of decoded sentences.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                <span class="n">seq</span><span class="p">,</span>
                <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">,</span>
                <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">clean_up_tokenization_spaces</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span>
        <span class="p">]</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.decode"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode">[docs]</a>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">token_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;np.ndarray&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special</span>
<span class="sd">        tokens and clean up tokenization spaces.</span>

<span class="sd">        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):</span>
<span class="sd">                List of tokenized input ids. Can be obtained using the ``__call__`` method.</span>
<span class="sd">            skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to remove special tokens in the decoding.</span>
<span class="sd">            clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not to clean up the tokenization spaces.</span>
<span class="sd">            kwargs (additional keyword arguments, `optional`):</span>
<span class="sd">                Will be passed to the underlying model specific decode method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`str`: The decoded sentence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert inputs to python lists</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">to_py_obj</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode</span><span class="p">(</span>
            <span class="n">token_ids</span><span class="o">=</span><span class="n">token_ids</span><span class="p">,</span>
            <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">,</span>
            <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">clean_up_tokenization_spaces</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">token_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.get_special_tokens_mask"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask">[docs]</a>    <span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (:obj:`List[int]`):</span>
<span class="sd">                List of ids of the first sequence.</span>
<span class="sd">            token_ids_1 (:obj:`List[int]`, `optional`):</span>
<span class="sd">                List of ids of the second sequence.</span>
<span class="sd">            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">already_has_special_tokens</span> <span class="ow">and</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You cannot use ``already_has_special_tokens=False`` with this tokenizer. &quot;</span>
            <span class="s2">&quot;Please use a slow (full python) tokenizer to activate this argument.&quot;</span>
            <span class="s2">&quot;Or set `return_special_tokens_mask=True` when calling the encoding method &quot;</span>
            <span class="s2">&quot;to get the special tokens mask in any tokenizer. &quot;</span>
        <span class="p">)</span>

        <span class="n">all_special_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span>  <span class="c1"># cache the property</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">all_special_ids</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_ids_0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">special_tokens_mask</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.clean_up_tokenization"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">clean_up_tokenization</span><span class="p">(</span><span class="n">out_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.</span>

<span class="sd">        Args:</span>
<span class="sd">            out_string (:obj:`str`): The text to clean up.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`str`: The cleaned-up string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">out_string</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">out_string</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; .&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ?&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; !&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ,&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39; &quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; n&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot;n&#39;t&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;m&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;m&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;s&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;ve&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;ve&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;re&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;re&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out_string</span></div>

    <span class="k">def</span> <span class="nf">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Depending on the input and internal state we might trigger a warning about a sequence that is too long for it&#39;s</span>
<span class="sd">        corresponding model</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (:obj:`List[str]`): The ids produced by the tokenization</span>
<span class="sd">            max_length (:obj:`int`, `optional`): The max_length desired (does not trigger a warning if it is set)</span>
<span class="sd">            verbose (:obj:`bool`): Whether or not to print more information and warnings.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sequence-length-is-longer-than-the-specified-maximum&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Token indices sequence length is longer than the specified maximum sequence length &quot;</span>
                    <span class="s2">&quot;for this model (</span><span class="si">{}</span><span class="s2"> &gt; </span><span class="si">{}</span><span class="s2">). Running this sequence through the model will result in &quot;</span>
                    <span class="s2">&quot;indexing errors&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;sequence-length-is-longer-than-the-specified-maximum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="PreTrainedTokenizerBase.as_target_tokenizer"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.as_target_tokenizer">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">as_target_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to</span>
<span class="sd">        sequence-to-sequence models that need a slightly different processing for the labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">yield</span></div>

<div class="viewcode-block" id="PreTrainedTokenizerBase.prepare_seq2seq_batch"><a class="viewcode-back" href="../../internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_seq2seq_batch">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_target_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest&quot;</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare model inputs for translation. For best performance, translate one sentence at a time.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            src_texts (:obj:`List[str]`):</span>
<span class="sd">                List of documents to summarize or source language texts.</span>
<span class="sd">            tgt_texts (:obj:`list`, `optional`):</span>
<span class="sd">                List of summaries or target language texts.</span>
<span class="sd">            max_length (:obj:`int`, `optional`):</span>
<span class="sd">                Controls the maximum length for encoder inputs (documents to summarize or source language texts) If</span>
<span class="sd">                left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length</span>
<span class="sd">                is required by one of the truncation/padding parameters. If the model has no specific maximum input</span>
<span class="sd">                length (like XLNet) truncation/padding to a maximum length will be deactivated.</span>
<span class="sd">            max_target_length (:obj:`int`, `optional`):</span>
<span class="sd">                Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set</span>
<span class="sd">                to :obj:`None`, this will use the max_length value.</span>
<span class="sd">            padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Activates and controls padding. Accepts the following values:</span>

<span class="sd">                * :obj:`True` or :obj:`&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a</span>
<span class="sd">                  single sequence if provided).</span>
<span class="sd">                * :obj:`&#39;max_length&#39;`: Pad to a maximum length specified with the argument :obj:`max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                * :obj:`False` or :obj:`&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of</span>
<span class="sd">                  different lengths).</span>
<span class="sd">            return_tensors (:obj:`str` or :class:`~transformers.tokenization_utils_base.TensorType`, `optional`):</span>
<span class="sd">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">                * :obj:`&#39;tf&#39;`: Return TensorFlow :obj:`tf.constant` objects.</span>
<span class="sd">                * :obj:`&#39;pt&#39;`: Return PyTorch :obj:`torch.Tensor` objects.</span>
<span class="sd">                * :obj:`&#39;np&#39;`: Return Numpy :obj:`np.ndarray` objects.</span>
<span class="sd">            truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Activates and controls truncation. Accepts the following values:</span>

<span class="sd">                * :obj:`True` or :obj:`&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument</span>
<span class="sd">                  :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not</span>
<span class="sd">                  provided. This will truncate token by token, removing a token from the longest sequence in the pair</span>
<span class="sd">                  if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`&#39;only_first&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or to</span>
<span class="sd">                  the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`&#39;only_second&#39;`: Truncate to a maximum length specified with the argument :obj:`max_length` or</span>
<span class="sd">                  to the maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                * :obj:`False` or :obj:`&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with</span>
<span class="sd">                  sequence lengths greater than the model maximum admissible input size).</span>
<span class="sd">            **kwargs:</span>
<span class="sd">                Additional keyword arguments passed along to :obj:`self.__call__`.</span>

<span class="sd">        Return:</span>
<span class="sd">            :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:</span>

<span class="sd">            - **input_ids** -- List of token ids to be fed to the encoder.</span>
<span class="sd">            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.</span>
<span class="sd">            - **labels** -- List of token ids for tgt_texts.</span>

<span class="sd">            The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.</span>
<span class="sd">            Otherwise, input_ids, attention_mask will be the only keys.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># mBART-specific kwargs that should be ignored by other models.</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;src_lang&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
            <span class="n">src_texts</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">tgt_texts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_inputs</span>
        <span class="c1"># Process tgt_texts</span>
        <span class="k">if</span> <span class="n">max_target_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_target_length</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_target_tokenizer</span><span class="p">():</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="n">tgt_texts</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_target_length</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">model_inputs</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>