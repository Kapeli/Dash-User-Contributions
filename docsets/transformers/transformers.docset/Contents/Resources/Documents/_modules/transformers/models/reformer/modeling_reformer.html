

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.models.reformer.modeling_reformer &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="../../../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.models.reformer.modeling_reformer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.models.reformer.modeling_reformer</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2020 The Trax Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;PyTorch REFORMER model. &quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">mul</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.autograd.function</span> <span class="kn">import</span> <span class="n">Function</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">MSELoss</span>

<span class="kn">from</span> <span class="nn">...activations</span> <span class="kn">import</span> <span class="n">ACT2FN</span>
<span class="kn">from</span> <span class="nn">...file_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DUMMY_INPUTS</span><span class="p">,</span>
    <span class="n">DUMMY_MASK</span><span class="p">,</span>
    <span class="n">ModelOutput</span><span class="p">,</span>
    <span class="n">add_code_sample_docstrings</span><span class="p">,</span>
    <span class="n">add_start_docstrings</span><span class="p">,</span>
    <span class="n">add_start_docstrings_to_model_forward</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">...modeling_outputs</span> <span class="kn">import</span> <span class="n">CausalLMOutput</span><span class="p">,</span> <span class="n">MaskedLMOutput</span><span class="p">,</span> <span class="n">QuestionAnsweringModelOutput</span><span class="p">,</span> <span class="n">SequenceClassifierOutput</span>
<span class="kn">from</span> <span class="nn">...modeling_utils</span> <span class="kn">import</span> <span class="n">PreTrainedModel</span><span class="p">,</span> <span class="n">apply_chunking_to_forward</span>
<span class="kn">from</span> <span class="nn">...utils</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">.configuration_reformer</span> <span class="kn">import</span> <span class="n">ReformerConfig</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">_CONFIG_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;ReformerConfig&quot;</span>
<span class="n">_TOKENIZER_FOR_DOC</span> <span class="o">=</span> <span class="s2">&quot;ReformerTokenizer&quot;</span>

<span class="n">REFORMER_PRETRAINED_MODEL_ARCHIVE_LIST</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
    <span class="s2">&quot;google/reformer-enwik8&quot;</span><span class="p">,</span>
    <span class="c1"># See all Reformer models at https://huggingface.co/models?filter=reformer</span>
<span class="p">]</span>


<span class="c1"># Define named tuples for nn.Modules here</span>
<span class="n">LSHSelfAttentionOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;LSHSelfAttentionOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;buckets&quot;</span><span class="p">])</span>
<span class="n">LocalSelfAttentionOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;LocalSelfAttentionOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_probs&quot;</span><span class="p">])</span>
<span class="n">AttentionOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;AttentionOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;buckets&quot;</span><span class="p">])</span>
<span class="n">ReformerOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;ReformerOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;attn_output&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;buckets&quot;</span><span class="p">])</span>
<span class="n">ReformerBackwardOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;ReformerBackwardOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;attn_output&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;grad_attn_output&quot;</span><span class="p">,</span> <span class="s2">&quot;grad_hidden_states&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">ReformerEncoderOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;ReformerEncoderOutput&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;all_hidden_states&quot;</span><span class="p">,</span> <span class="s2">&quot;all_attentions&quot;</span><span class="p">,</span> <span class="s2">&quot;past_buckets_states&quot;</span><span class="p">],</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">_stable_argsort</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="c1"># this function scales the vector so that torch.argsort is stable.</span>
    <span class="c1"># torch.argsort is not stable on its own</span>
    <span class="n">scale_offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">vector</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scale_offset</span> <span class="o">=</span> <span class="n">scale_offset</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">scaled_vector</span> <span class="o">=</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span> <span class="o">+</span> <span class="p">(</span><span class="n">scale_offset</span> <span class="o">%</span> <span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">scaled_vector</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_least_common_mult_chunk_len</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">attn_types</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span>
    <span class="n">attn_types_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">attn_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">attn_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;lsh&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_attn_chunk_length</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">attn_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attn_chunk_length</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">attn_types_set</span> <span class="o">==</span> <span class="nb">set</span><span class="p">([</span><span class="s2">&quot;lsh&quot;</span><span class="p">,</span> <span class="s2">&quot;local&quot;</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">lcm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lsh_attn_chunk_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attn_chunk_length</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Only attn layer types &#39;lsh&#39; and &#39;local&#39; exist, but `config.attn_layers`: </span><span class="si">{}</span><span class="s2">. Select attn layer types from [&#39;lsh&#39;, &#39;local&#39;] only.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span>
            <span class="p">)</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_min_chunk_len</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">attn_types</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span>
    <span class="n">attn_types_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">attn_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">attn_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;lsh&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_attn_chunk_length</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">attn_types</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attn_chunk_length</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">attn_types_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">attn_types_set</span> <span class="o">==</span> <span class="nb">set</span><span class="p">([</span><span class="s2">&quot;lsh&quot;</span><span class="p">,</span> <span class="s2">&quot;local&quot;</span><span class="p">]):</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lsh_attn_chunk_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attn_chunk_length</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Only attn layer types &#39;lsh&#39; and &#39;local&#39; exist, but `config.attn_layers`: </span><span class="si">{}</span><span class="s2">. Select attn layer types from [&#39;lsh&#39;, &#39;local&#39;] only.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span>
            <span class="p">)</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">AxialPositionEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs axial position embeddings. Useful for very long input sequences to save memory and time.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">axial_pos_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_embds_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">axial_pos_embds_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">least_common_mult_chunk_length</span> <span class="o">=</span> <span class="n">_get_least_common_mult_chunk_len</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">()</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_embds_dim</span><span class="p">)</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">),</span> <span class="s2">&quot;Make sure that config.axial_pos_embds factors: </span><span class="si">{}</span><span class="s2"> sum to config.hidden_size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_embds_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">)</span>

        <span class="c1"># create weights</span>
        <span class="k">for</span> <span class="n">axis</span><span class="p">,</span> <span class="n">axial_pos_embd_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_embds_dim</span><span class="p">):</span>
            <span class="c1"># create expanded shapes</span>
            <span class="n">ax_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">)</span>
            <span class="n">ax_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
            <span class="n">ax_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ax_shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">axial_pos_embd_dim</span><span class="p">,)</span>

            <span class="c1"># create tensor and init</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ax_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
        <span class="c1"># broadcast weights to correct shape</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">broadcasted_weights</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span> <span class="o">+</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:])</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">sequence_length</span>
            <span class="p">),</span> <span class="s2">&quot;If training, make sure that config.axial_pos_shape factors: </span><span class="si">{}</span><span class="s2"> multiply to sequence length. Got prod(</span><span class="si">{}</span><span class="s2">) != sequence_length: </span><span class="si">{}</span><span class="s2">. You might want to consider padding your sequence length to </span><span class="si">{}</span><span class="s2"> or changing config.axial_pos_shape.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">broadcasted_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># permute weights so that 2D correctly drops dims 1 and 2</span>
                <span class="n">transposed_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="c1"># drop entire matrix of last two dims (prev dims 1 and 2)</span>
                <span class="n">dropped_transposed_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout2d</span><span class="p">(</span>
                    <span class="n">transposed_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span>
                <span class="p">)</span>
                <span class="n">dropped_weights</span> <span class="o">=</span> <span class="n">dropped_transposed_weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="n">position_encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dropped_weights</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">position_encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">broadcasted_weights</span><span class="p">],</span>
                    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">sequence_length</span>
            <span class="p">),</span> <span class="s2">&quot;Make sure that config.axial_pos_shape factors: </span><span class="si">{}</span><span class="s2"> multiply at least to max(sequence_length, least_common_mult_chunk_length): max(</span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">least_common_mult_chunk_length</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># compute how many columns are needed</span>
            <span class="n">max_position_id</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">required_pos_encodings_columns</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">max_position_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">axial_pos_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

            <span class="c1"># cut to columns that are needed</span>
            <span class="n">position_encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:</span><span class="n">required_pos_encodings_columns</span><span class="p">]</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">broadcasted_weights</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">position_encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">position_encodings</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">position_encodings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># select correct position encodings</span>
            <span class="n">position_encodings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">position_encodings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">position_encodings</span>


<span class="k">class</span> <span class="nc">PositionEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs conventional position embeddings of shape `[max_pos_embeddings, hidden_size]`.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">position_embeddings</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">position_embeddings</span>


<span class="k">class</span> <span class="nc">ReformerEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">AxialPositionEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">axial_pos_embds</span> <span class="k">else</span> <span class="n">PositionEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_idx_pos_encodings</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>

        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                <span class="n">start_idx_pos_encodings</span><span class="p">,</span> <span class="n">start_idx_pos_encodings</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">position_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="p">),</span> <span class="s2">&quot;Sequence Length: </span><span class="si">{}</span><span class="s2"> has to be larger equal than config.max_position_embeddings: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">position_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="p">)</span>

        <span class="c1"># dropout</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># add positional embeddings</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="n">position_embeddings</span>
        <span class="k">return</span> <span class="n">embeddings</span>


<span class="k">class</span> <span class="nc">EfficientAttentionMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A few utilities for nn.Modules in Reformer, to be used as a mixin.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_look_adjacent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">num_chunks_before</span><span class="p">,</span> <span class="n">num_chunks_after</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Used to implement attention between consecutive chunks.</span>

<span class="sd">        Args:</span>
<span class="sd">            vectors: array of shape [batch_size, num_attention_heads, n_chunks, chunk_len, ...]</span>
<span class="sd">            num_chunks_before: chunks before current chunk to include in attention</span>
<span class="sd">            num_chunks_after: chunks after current chunk to include in attention</span>

<span class="sd">        Returns:</span>
<span class="sd">            tensor of shape [num_chunks, N * chunk_length, ...], where N = (1 + num_chunks_before + num_chunks_after).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_chunks_before</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">num_chunks_after</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">vectors</span>

        <span class="n">slices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="n">num_chunks_after</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">slices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">vectors</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:,</span> <span class="o">...</span><span class="p">],</span> <span class="n">vectors</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">slices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_split_hidden_size_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_attn_heads</span><span class="p">,</span> <span class="n">attn_head_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        splits hidden_size dim into attn_head_size and num_attn_heads</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_attn_heads</span><span class="p">,</span> <span class="n">attn_head_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">new_x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_merge_hidden_size_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_attn_heads</span><span class="p">,</span> <span class="n">attn_head_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        merges attn_head_size dim and num_attn_heads dim into hidden_size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_attn_heads</span> <span class="o">*</span> <span class="n">attn_head_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_split_seq_length_dim_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">dim_factor_1</span><span class="p">,</span> <span class="n">dim_factor_2</span><span class="p">,</span> <span class="n">num_attn_heads</span><span class="p">,</span> <span class="n">attn_head_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        splits sequence length dim of vectors into `dim_factor_1` and `dim_factor_2` dims</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">split_dim_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_attn_heads</span><span class="p">,</span> <span class="n">dim_factor_1</span><span class="p">,</span> <span class="n">dim_factor_2</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">split_dim_shape</span> <span class="o">+</span> <span class="p">(</span><span class="n">attn_head_size</span><span class="p">,))</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">split_dim_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input vector rank should be one of [3, 4], but is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">LSHSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">EfficientAttentionMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_attn_chunk_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hashes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_hashes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_num_chunks_before</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_num_chunks_after</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hash_seed</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hash_seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_attention_probs_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="c1"># projection matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># save mask value here. Need fp32 and fp16 mask values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;self_mask_value_float16&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;self_mask_value_float32&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask_value_float16&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e4</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask_value_float32&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e9</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">buckets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># num hashes can optionally be overwritten by user</span>
        <span class="n">num_hashes</span> <span class="o">=</span> <span class="n">num_hashes</span> <span class="k">if</span> <span class="n">num_hashes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hashes</span>

        <span class="n">do_cached_attention</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="ow">and</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># check if cache shall be used and that hidden states are already cached</span>
        <span class="k">if</span> <span class="n">do_cached_attention</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">sequence_length</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;At the moment, auto-regressive language generation is only possible one word at a time. Make sure that input sequence length </span><span class="si">{</span><span class="n">sequence_length</span><span class="si">}</span><span class="s2"> equals 1, when `past_buckets_states` is passed.&quot;</span>
            <span class="n">past_buckets</span> <span class="o">=</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">past_states</span> <span class="o">=</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># get query vector</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span>
                <span class="n">query_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">past_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">key_value_hidden_states</span><span class="p">,</span> <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_relevant_hid_states_and_buckets</span><span class="p">(</span>
                    <span class="n">query_vectors</span><span class="o">=</span><span class="n">query_vectors</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">past_states</span><span class="o">=</span><span class="n">past_states</span><span class="p">,</span>
                    <span class="n">past_buckets</span><span class="o">=</span><span class="n">past_buckets</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_query_per_attn_head</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>
                <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_per_attn_head</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>

                <span class="c1"># split key &amp; value vectors by num hashes to apply</span>
                <span class="c1"># self attention on each separately</span>
                <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                    <span class="n">query_key_vectors</span><span class="p">,</span>
                    <span class="n">num_hashes</span><span class="p">,</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                    <span class="n">value_vectors</span><span class="p">,</span>
                    <span class="n">num_hashes</span><span class="p">,</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># repeat query vectors across hash dimension</span>
                <span class="n">query_vectors</span> <span class="o">=</span> <span class="n">query_vectors</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">key_value_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_key</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>
                <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># project hidden_states to query_key and value</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># if query key is not already split</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_cached_attention</span> <span class="ow">or</span> <span class="n">past_buckets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span>
                <span class="n">query_key_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
            <span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span>
                <span class="n">value_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
            <span class="p">)</span>

        <span class="c1"># cache buckets for next incremental decoding</span>
        <span class="k">if</span> <span class="n">do_cached_attention</span> <span class="ow">and</span> <span class="n">past_buckets</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_value_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">:</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hash_vectors</span><span class="p">(</span><span class="n">query_key_vectors</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">hidden_states</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">query_key_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">),</span> <span class="s2">&quot;last dim of query_key_vectors is </span><span class="si">{}</span><span class="s2"> but should be </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">query_key_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">value_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">),</span> <span class="s2">&quot;last dim of value_vectors is </span><span class="si">{}</span><span class="s2"> but should be </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">value_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">)</span>

        <span class="n">do_standard_self_attention</span> <span class="o">=</span> <span class="p">(</span><span class="n">sequence_length</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">use_cache</span> <span class="ow">and</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="c1"># LSH attention only makes sense if chunked attention should be performed</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="c1"># set `num_buckets` on the fly, recommended way to do it</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_num_buckets</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)</span>

            <span class="c1"># use cached buckets for backprop only</span>
            <span class="k">if</span> <span class="n">buckets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># hash query key vectors into buckets</span>
                <span class="n">buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hash_vectors</span><span class="p">(</span><span class="n">query_key_vectors</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># make sure buckets has correct shape for LSH attention</span>
                <span class="n">buckets</span> <span class="o">=</span> <span class="n">buckets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_hashes</span> <span class="o">*</span> <span class="n">sequence_length</span><span class="p">)</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">num_hashes</span> <span class="o">*</span> <span class="n">sequence_length</span>
            <span class="p">),</span> <span class="s2">&quot;last dim of buckets is </span><span class="si">{}</span><span class="s2">, but should be </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_hashes</span> <span class="o">*</span> <span class="n">sequence_length</span><span class="p">)</span>

            <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">undo_sorted_bucket_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sorted_bucket_idx_and_undo_sorted_bucket_idx</span><span class="p">(</span>
                <span class="n">sequence_length</span><span class="p">,</span> <span class="n">buckets</span><span class="p">,</span> <span class="n">num_hashes</span>
            <span class="p">)</span>

            <span class="c1"># make sure bucket idx is not longer then sequence length</span>
            <span class="n">sorted_bucket_idx_per_hash</span> <span class="o">=</span> <span class="n">sorted_bucket_idx</span> <span class="o">%</span> <span class="n">sequence_length</span>

            <span class="c1"># cluster query key value vectors according to hashed buckets</span>
            <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_by_expansion</span><span class="p">(</span><span class="n">query_key_vectors</span><span class="p">,</span> <span class="n">sorted_bucket_idx_per_hash</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_by_expansion</span><span class="p">(</span><span class="n">value_vectors</span><span class="p">,</span> <span class="n">sorted_bucket_idx_per_hash</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">)</span>
            <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">query_key_vectors</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">value_vectors</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="s2">&quot;If `config.chunk_length` is `None`, make sure `config.num_chunks_after` and `config.num_chunks_before` are set to 0.&quot;</span>
        <span class="k">elif</span> <span class="n">do_cached_attention</span> <span class="ow">and</span> <span class="n">past_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use max sequence length</span>
            <span class="n">sorted_bucket_idx_per_hash</span> <span class="o">=</span> <span class="n">sorted_bucket_idx</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># get sequence length indices</span>
            <span class="n">sorted_bucket_idx_per_hash</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query_key_vectors</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="mi">1</span>
            <span class="p">)</span>

        <span class="c1"># scale key vectors</span>
        <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_and_dim_norm</span><span class="p">(</span><span class="n">query_key_vectors</span><span class="p">)</span>

        <span class="c1"># set query_vectors to query key vectors if LSH self attention</span>
        <span class="n">query_vectors</span> <span class="o">=</span> <span class="n">query_vectors</span> <span class="k">if</span> <span class="n">query_vectors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">query_key_vectors</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">query_key_vectors</span>

        <span class="c1"># get attention probs</span>
        <span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attend</span><span class="p">(</span>
            <span class="n">query_vectors</span><span class="o">=</span><span class="n">query_vectors</span><span class="p">,</span>
            <span class="n">key_vectors</span><span class="o">=</span><span class="n">key_vectors</span><span class="p">,</span>
            <span class="n">value_vectors</span><span class="o">=</span><span class="n">value_vectors</span><span class="p">,</span>
            <span class="n">sorted_bucket_idx_per_hash</span><span class="o">=</span><span class="n">sorted_bucket_idx_per_hash</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">do_standard_self_attention</span><span class="o">=</span><span class="n">do_standard_self_attention</span><span class="p">,</span>
            <span class="n">do_cached_attention</span><span class="o">=</span><span class="n">do_cached_attention</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">key_vectors</span><span class="p">,</span> <span class="n">value_vectors</span>

        <span class="c1"># re-order out_vectors and logits</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="c1"># sort clusters back to correct ordering</span>
            <span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">ReverseSort</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">undo_sorted_bucket_idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span> <span class="ow">or</span> <span class="p">(</span><span class="n">do_cached_attention</span> <span class="ow">and</span> <span class="n">past_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="c1"># sum up all hash rounds</span>
            <span class="k">if</span> <span class="n">num_hashes</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">out_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                    <span class="n">out_vectors</span><span class="p">,</span>
                    <span class="n">num_hashes</span><span class="p">,</span>
                    <span class="n">sequence_length</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                    <span class="n">logits</span><span class="p">,</span>
                    <span class="n">num_hashes</span><span class="p">,</span>
                    <span class="n">sequence_length</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">probs_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">out_vectors</span> <span class="o">*</span> <span class="n">probs_vectors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="c1"># free memory</span>
                <span class="k">del</span> <span class="n">probs_vectors</span>

            <span class="c1"># free memory</span>
            <span class="k">del</span> <span class="n">logits</span>

        <span class="k">assert</span> <span class="n">out_vectors</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
        <span class="p">),</span> <span class="s2">&quot;out_vectors have be of shape `[batch_size, config.num_attention_heads, sequence_length, config.attention_head_size]`.&quot;</span>

        <span class="n">out_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_hidden_size_dims</span><span class="p">(</span><span class="n">out_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="p">()</span>

        <span class="k">if</span> <span class="n">buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="n">buckets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">LSHSelfAttentionOutput</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">out_vectors</span><span class="p">,</span> <span class="n">attention_probs</span><span class="o">=</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">buckets</span><span class="o">=</span><span class="n">buckets</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_query_per_attn_head</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">per_head_query_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_key</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># only relevant for inference and no bias =&gt; we can use einsum here</span>
        <span class="n">query_key_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;balh,ahr-&gt;balr&quot;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">per_head_query_key</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">query_key_vectors</span>

    <span class="k">def</span> <span class="nf">_value_per_attn_head</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">per_head_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># only relevant for inference and no bias =&gt; we can use einsum here</span>
        <span class="n">value_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;balh,ahr-&gt;balr&quot;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">per_head_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value_vectors</span>

    <span class="k">def</span> <span class="nf">_hash_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">increase_num_buckets</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># See https://arxiv.org/pdf/1509.02897.pdf</span>
        <span class="c1"># We sample a different random rotation for each round of hashing to</span>
        <span class="c1"># decrease the probability of hash misses.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="s2">&quot;There should be an even number of bucktes, but `self.num_bucktes`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)</span>
            <span class="n">rotation_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span>
            <span class="n">num_buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Factorize the hash if self.num_buckets is a list or tuple</span>
            <span class="n">rotation_size</span><span class="p">,</span> <span class="n">num_buckets</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">bucket_factor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">bucket_factor</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;The number of buckets should be even, but `num_bucket`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">bucket_factor</span>
                <span class="p">)</span>
                <span class="n">rotation_size</span> <span class="o">=</span> <span class="n">rotation_size</span> <span class="o">+</span> <span class="n">bucket_factor</span>
                <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">*</span> <span class="n">bucket_factor</span>

        <span class="c1"># remove gradient</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># for determinism</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hash_seed</span><span class="p">)</span>

        <span class="n">rotations_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">rotation_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># create a random self.attention_head_size x num_hashes x num_buckets/2</span>
        <span class="n">random_rotations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">rotations_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Output dim: Batch_Size x Num_Attn_Heads x Num_Hashes x Seq_Len x Num_Buckets/2</span>
        <span class="n">rotated_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bmtd,mdhr-&gt;bmhtr&quot;</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">random_rotations</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">rotated_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">rotated_vectors</span><span class="p">,</span> <span class="o">-</span><span class="n">rotated_vectors</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated_vectors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get the buckets for them and combine.</span>
            <span class="n">buckets</span><span class="p">,</span> <span class="n">cur_sum</span><span class="p">,</span> <span class="n">cur_product</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">bucket_factor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">:</span>
                <span class="n">rotated_vectors_factor</span> <span class="o">=</span> <span class="n">rotated_vectors</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">cur_sum</span> <span class="p">:</span> <span class="n">cur_sum</span> <span class="o">+</span> <span class="p">(</span><span class="n">bucket_factor</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)]</span>
                <span class="n">cur_sum</span> <span class="o">=</span> <span class="n">cur_sum</span> <span class="o">+</span> <span class="n">bucket_factor</span> <span class="o">//</span> <span class="mi">2</span>
                <span class="n">rotated_vectors_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">rotated_vectors_factor</span><span class="p">,</span> <span class="o">-</span><span class="n">rotated_vectors_factor</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">buckets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated_vectors_factor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buckets</span> <span class="o">=</span> <span class="n">buckets</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_product</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated_vectors_factor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

                <span class="n">cur_product</span> <span class="o">=</span> <span class="n">cur_product</span> <span class="o">*</span> <span class="n">bucket_factor</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="c1"># add an extra bucket for padding tokens only</span>
            <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># assign padding tokens extra bucket</span>
            <span class="n">buckets_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">buckets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">buckets_mask</span><span class="p">,</span> <span class="n">buckets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_buckets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">buckets</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">increase_num_buckets</span><span class="p">:</span>
            <span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># buckets is now (Batch_size x Num_Attn_Heads x Num_Hashes x Seq_Len).</span>
        <span class="c1"># Next we add offsets so that bucket numbers from different hashing rounds don&#39;t overlap.</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_hashes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="p">(</span><span class="n">offsets</span> <span class="o">*</span> <span class="n">num_buckets</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># expand to batch size and num attention heads</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span> <span class="o">+</span> <span class="n">offsets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">offset_buckets</span> <span class="o">=</span> <span class="p">(</span><span class="n">buckets</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">offset_buckets</span>

    <span class="k">def</span> <span class="nf">_get_sorted_bucket_idx_and_undo_sorted_bucket_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">buckets</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">):</span>
        <span class="c1"># no gradients are needed</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># hash-based sort</span>
            <span class="n">sorted_bucket_idx</span> <span class="o">=</span> <span class="n">_stable_argsort</span><span class="p">(</span><span class="n">buckets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># create simple indices to scatter to, to have undo sort</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sorted_bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">buckets</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">sorted_bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># get undo sort</span>
            <span class="n">undo_sorted_bucket_idx</span> <span class="o">=</span> <span class="n">sorted_bucket_idx</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="o">*</span><span class="n">sorted_bucket_idx</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
            <span class="n">undo_sorted_bucket_idx</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">undo_sorted_bucket_idx</span>

    <span class="k">def</span> <span class="nf">_set_num_buckets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="c1"># `num_buckets` should be set to 2 * sequence_length // chunk_length as recommended in paper</span>
        <span class="n">num_buckets_pow_2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">sequence_length</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">))</span><span class="o">.</span><span class="n">bit_length</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># make sure buckets are power of 2</span>
        <span class="n">num_buckets</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">num_buckets_pow_2</span>

        <span class="c1"># factorize `num_buckets` if `num_buckets` becomes too large</span>
        <span class="n">num_buckets_limit</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span>
            <span class="nb">int</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">)),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">num_buckets</span> <span class="o">&gt;</span> <span class="n">num_buckets_limit</span><span class="p">:</span>
            <span class="n">num_buckets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_buckets_pow_2</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_buckets_pow_2</span> <span class="o">-</span> <span class="n">num_buckets_pow_2</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;config.num_buckets is not set. Setting config.num_buckets to </span><span class="si">{}</span><span class="s2">...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_buckets</span><span class="p">))</span>

        <span class="c1"># set num buckets in config to be properly saved</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="o">=</span> <span class="n">num_buckets</span>

    <span class="k">def</span> <span class="nf">_attend</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query_vectors</span><span class="p">,</span>
        <span class="n">key_vectors</span><span class="p">,</span>
        <span class="n">value_vectors</span><span class="p">,</span>
        <span class="n">sorted_bucket_idx_per_hash</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">,</span>
        <span class="n">do_standard_self_attention</span><span class="p">,</span>
        <span class="n">do_cached_attention</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># look at previous and following chunks if chunked attention</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">key_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">value_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>

        <span class="c1"># get logits and dots</span>
        <span class="c1"># (BS, NumAttn, NumHash x NumChunk, Chunk_L x Hidden),(BS, NumAttn, NumHash x NumChunk, Chunk_L * (Num_bef + Num_aft + 1) x Hidden) -&gt; (BS, NumAttn, NumHash x NumChunk, Chunk_L, Chunk_L * (1 + Num_bef + Num_aft))</span>
        <span class="n">query_key_dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_vectors</span><span class="p">,</span> <span class="n">key_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">query_vectors</span><span class="p">,</span> <span class="n">key_vectors</span>

        <span class="c1"># if chunked attention split bucket idxs to query and key</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="n">query_bucket_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">sorted_bucket_idx_per_hash</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span>
            <span class="p">)</span>
            <span class="n">key_value_bucket_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">query_bucket_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">do_cached_attention</span> <span class="ow">and</span> <span class="n">query_key_dots</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">key_value_bucket_idx</span> <span class="o">=</span> <span class="n">sorted_bucket_idx_per_hash</span>
            <span class="n">query_bucket_idx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">key_value_bucket_idx</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">key_value_bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="o">*</span> <span class="n">key_value_bucket_idx</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">do_cached_attention</span> <span class="ow">and</span> <span class="n">query_key_dots</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">query_bucket_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">query_key_dots</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">query_key_dots</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">key_value_bucket_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                <span class="n">query_key_dots</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query_key_dots</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">query_bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">query_bucket_idx</span> <span class="o">=</span> <span class="n">key_value_bucket_idx</span> <span class="o">=</span> <span class="n">sorted_bucket_idx_per_hash</span>

        <span class="c1"># get correct mask values depending on precision</span>
        <span class="k">if</span> <span class="n">query_key_dots</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">self_mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_mask_value_float16</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
            <span class="n">mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_value_float16</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self_mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_mask_value_float32</span>
            <span class="n">mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_value_float32</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_cached_attention</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_attn_mask</span><span class="p">(</span>
                <span class="n">query_bucket_idx</span><span class="p">,</span>
                <span class="n">key_value_bucket_idx</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">query_key_dots</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">do_standard_self_attention</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">query_key_dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">query_key_dots</span><span class="p">,</span> <span class="n">mask_value</span><span class="p">)</span>

            <span class="c1"># free memory</span>
            <span class="k">del</span> <span class="n">mask</span>

        <span class="c1"># Self mask is ALWAYS applied.</span>
        <span class="c1"># From the reformer paper (https://arxiv.org/pdf/2001.04451.pdf):</span>
        <span class="c1"># &quot; While attention to the future is not allowed, typical implementations of the</span>
        <span class="c1"># Transformer do allow a position to attend to itself.</span>
        <span class="c1"># Such behavior is undesirable in a shared-QK formulation because the dot-product</span>
        <span class="c1"># of a query vector with itself will almost always be greater than the dot product of a</span>
        <span class="c1"># query vector with a vector at another position. We therefore modify the masking</span>
        <span class="c1"># to forbid a token from attending to itself, except in situations</span>
        <span class="c1"># where a token has no other valid attention targets (e.g. the first token in a sequence) &quot;</span>

        <span class="n">self_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">query_bucket_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">key_value_bucket_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">query_bucket_idx</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="c1"># apply self_mask</span>
        <span class="n">query_key_dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">self_mask</span><span class="p">,</span> <span class="n">query_key_dots</span><span class="p">,</span> <span class="n">self_mask_value</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">self_mask</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">query_key_dots</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># dots shape is `[batch_size, num_attn_heads, num_hashes * seq_len // chunk_length, chunk_length, chunk_length * (1 + num_chunks_before + num_chunks_after)]`</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">query_key_dots</span> <span class="o">-</span> <span class="n">logits</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">query_key_dots</span>

        <span class="c1"># dropout</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">*</span> <span class="n">head_mask</span>

        <span class="c1"># attend values</span>
        <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value_vectors</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">value_vectors</span>

        <span class="c1"># merge chunk length</span>
        <span class="k">if</span> <span class="n">out_vectors</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">out_vectors</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">attention_probs</span>

    <span class="k">def</span> <span class="nf">_compute_attn_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">query_indices</span><span class="p">,</span> <span class="n">key_indices</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">query_key_dot_shape</span><span class="p">,</span> <span class="n">do_standard_self_attention</span>
    <span class="p">):</span>
        <span class="c1"># attention mask for LSH</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># if chunked attention, the attention mask has to correspond to LSH order</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
                <span class="c1"># expand attn_mask to fit with key_value_bucket_idx shape</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">query_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
                <span class="c1"># extract attention mask from LSH sorted key_indices</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">key_indices</span><span class="p">)</span>

            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">query_key_dot_shape</span><span class="p">)</span>

        <span class="c1"># Causal mask</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">query_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">key_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_indices</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># add attention mask if not None</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">causal_mask</span> <span class="o">*</span> <span class="n">attention_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">causal_mask</span>

        <span class="k">return</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="nf">_get_relevant_hid_states_and_buckets</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">query_vectors</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">past_states</span><span class="p">,</span> <span class="n">past_buckets</span>
    <span class="p">):</span>
        <span class="c1"># concat hidden states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># batch_size hidden</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># check if cached buckets include pad bucket</span>
        <span class="n">max_bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buckets</span><span class="p">)</span>

        <span class="c1"># if pad bucket was cached =&gt; need to increase num buckets for caching</span>
        <span class="n">increase_num_buckets</span> <span class="o">=</span> <span class="n">past_buckets</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">num_hashes</span> <span class="o">*</span> <span class="n">max_bucket</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># retrieve query buckets</span>
        <span class="n">query_buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hash_vectors</span><span class="p">(</span>
            <span class="n">query_vectors</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">increase_num_buckets</span><span class="o">=</span><span class="n">increase_num_buckets</span>
        <span class="p">)</span>

        <span class="c1"># concat buckets</span>
        <span class="n">concat_buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_buckets</span><span class="p">,</span> <span class="n">query_buckets</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># hash-based sort</span>
        <span class="n">bucket_idx</span> <span class="o">=</span> <span class="n">_stable_argsort</span><span class="p">(</span><span class="n">concat_buckets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># bucket_idx has shape: BatchSize x NumAttnHeads x NumHashes x SequenceLength</span>
        <span class="k">assert</span> <span class="n">bucket_idx</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="p">,</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;bucket_idx should have shape </span><span class="si">{</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span><span class="si">}</span><span class="s2">, but has shape </span><span class="si">{</span><span class="n">bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="c1"># find indices of new bucket indices</span>
        <span class="n">relevant_bucket_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">bucket_idx</span> <span class="o">==</span> <span class="p">(</span><span class="n">bucket_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>

        <span class="c1"># expand relevant bucket indices to its chunks</span>
        <span class="n">relevant_bucket_idx_chunk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand_to_indices_in_relevant_chunk</span><span class="p">(</span><span class="n">relevant_bucket_idx</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
        <span class="n">relevant_bucket_idx_chunk</span> <span class="o">=</span> <span class="n">bucket_idx</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))]</span>

        <span class="c1"># adapt bucket_idx for batch and hidden states for index select</span>
        <span class="n">bucket_idx_batch_offset</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">batch_size</span>
            <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
            <span class="o">//</span> <span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># add batch offset</span>
        <span class="n">relevant_bucket_idx_chunk_all_batch</span> <span class="o">=</span> <span class="n">relevant_bucket_idx_chunk</span> <span class="o">+</span> <span class="n">bucket_idx_batch_offset</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">))</span>

        <span class="c1"># select all relevant hidden states</span>
        <span class="n">relevant_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">relevant_bucket_idx_chunk_all_batch</span><span class="p">)</span>

        <span class="c1"># reshape hidden states and bucket_idx to correct output</span>
        <span class="n">relevant_hidden_states</span> <span class="o">=</span> <span class="n">relevant_hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="p">)</span>
        <span class="n">relevant_bucket_idx_chunk</span> <span class="o">=</span> <span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">relevant_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="o">*</span> <span class="n">num_hashes</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;There should be </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="o">*</span> <span class="n">num_hashes</span><span class="si">}</span><span class="s2"> `hidden_states`, there are </span><span class="si">{</span><span class="n">relevant_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> `hidden_states`.&quot;</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;There should be </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="si">}</span><span class="s2"> `hidden_states`, there are </span><span class="si">{</span><span class="n">relevant_bucket_idx_chunk</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> `bucket_idx`.&quot;</span>

        <span class="k">return</span> <span class="n">relevant_hidden_states</span><span class="p">,</span> <span class="n">relevant_bucket_idx_chunk</span><span class="p">,</span> <span class="n">query_buckets</span>

    <span class="k">def</span> <span class="nf">_expand_to_indices_in_relevant_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="c1"># get relevant indices of where chunk starts and its size</span>
        <span class="n">start_indices_chunk</span> <span class="o">=</span> <span class="p">((</span><span class="n">indices</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span>
        <span class="n">total_chunk_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>

        <span class="c1"># expand start indices and add correct chunk offset via arange</span>
        <span class="n">expanded_start_indices</span> <span class="o">=</span> <span class="n">start_indices_chunk</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">total_chunk_size</span><span class="p">)</span>
        <span class="n">chunk_sequence_indices</span> <span class="o">=</span> <span class="n">expanded_start_indices</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">total_chunk_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span>
        <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">total_chunk_size</span><span class="p">)</span>

        <span class="c1"># make sure that circular logic holds via % seq len</span>
        <span class="n">chunk_sequence_indices</span> <span class="o">=</span> <span class="n">chunk_sequence_indices</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">%</span> <span class="n">sequence_length</span>

        <span class="c1"># expand indices and set indices correctly</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">total_chunk_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">indices</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk_sequence_indices</span>

        <span class="k">return</span> <span class="n">indices</span>

    <span class="k">def</span> <span class="nf">_len_and_dim_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        length and attention head size dim normalization</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_len_norm</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectors</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">vectors</span>

    <span class="k">def</span> <span class="nf">_len_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        length normalization</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">norm_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">norm_x</span>

    <span class="k">def</span> <span class="nf">_gather_by_expansion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">idxs</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        expand dims of idxs and vectors for all hashes and gather</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">expanded_idxs</span> <span class="o">=</span> <span class="n">idxs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_hashes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">expanded_idxs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReverseSort</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    After chunked attention is applied which sorted clusters, original ordering has to be restored. Since customized</span>
<span class="sd">    backward function is used for Reformer, the gradients of the output vectors have to be explicitly sorted here.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">sorted_bucket_idx</span><span class="p">,</span> <span class="n">undo_sorted_bucket_idx</span><span class="p">):</span>
        <span class="c1"># save sorted_bucket_idx for backprop</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">sorted_bucket_idx</span> <span class="o">=</span> <span class="n">sorted_bucket_idx</span>

            <span class="c1"># undo sort to have correct order for next layer</span>
            <span class="n">expanded_undo_sort_indices</span> <span class="o">=</span> <span class="n">undo_sorted_bucket_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">out_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">out_vectors</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">expanded_undo_sort_indices</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">undo_sorted_bucket_idx</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_vectors</span><span class="p">,</span> <span class="n">logits</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out_vectors</span><span class="p">,</span> <span class="n">grad_logits</span><span class="p">):</span>
        <span class="c1"># get parameters saved in ctx</span>
        <span class="n">sorted_bucket_idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">sorted_bucket_idx</span>

        <span class="n">expanded_sort_indices</span> <span class="o">=</span> <span class="n">sorted_bucket_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">grad_out_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># reverse sort of forward</span>
        <span class="n">grad_out_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">grad_out_vectors</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">expanded_sort_indices</span><span class="p">)</span>
        <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">grad_logits</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sorted_bucket_idx</span><span class="p">)</span>

        <span class="c1"># return grad and `None` fillers for last 2 forward args</span>
        <span class="k">return</span> <span class="n">grad_out_vectors</span><span class="p">,</span> <span class="n">grad_logits</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">LocalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">EfficientAttentionMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attn_chunk_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">local_num_chunks_before</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">local_num_chunks_after</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="c1"># projection matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">local_attention_probs_dropout_prob</span>

        <span class="c1"># save mask value here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask_value_float16&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e4</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;mask_value_float32&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1e9</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># check if cache shall be used and that hidden states are already cached</span>
        <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">and</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;LocalSelfAttention should not make use of `buckets`. There seems to be an error when caching hidden_states_and_buckets.&quot;</span>
            <span class="n">key_value_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_relevant_hidden_states</span><span class="p">(</span>
                <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span>
            <span class="p">)</span>
            <span class="n">key_value_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_value_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># only query vector for last token</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># compute key and value for relevant chunk</span>
            <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">key_value_hidden_states</span><span class="p">)</span>

            <span class="c1"># free memory</span>
            <span class="k">del</span> <span class="n">key_value_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># project hidden_states to query, key and value</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># split last dim into `config.num_attention_heads` and `config.attention_head_size`</span>
        <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span><span class="n">query_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
        <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span><span class="n">key_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
        <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_hidden_size_dim</span><span class="p">(</span><span class="n">value_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">query_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">),</span> <span class="s2">&quot;last dim of query_key_vectors is </span><span class="si">{}</span><span class="s2"> but should be </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">query_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">key_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">),</span> <span class="s2">&quot;last dim of query_key_vectors is </span><span class="si">{}</span><span class="s2"> but should be </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">key_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">value_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">),</span> <span class="s2">&quot;last dim of query_key_vectors is </span><span class="si">{}</span><span class="s2"> but should be </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">value_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="s2">&quot;If `config.chunk_length` is `None`, make sure `config.num_chunks_after` and `config.num_chunks_before` are set to 0.&quot;</span>

        <span class="c1"># normalize key vectors</span>
        <span class="n">key_vectors</span> <span class="o">=</span> <span class="n">key_vectors</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">key_vectors</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">key_vectors</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># get sequence length indices</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query_vectors</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># if one should do normal n^2 self-attention</span>
        <span class="n">do_standard_self_attention</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span>

        <span class="c1"># if input should be chunked</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="c1"># chunk vectors</span>
            <span class="c1"># B x Num_Attn_Head x Seq_Len // chunk_len x chunk_len  x  attn_head_size</span>
            <span class="n">query_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">query_vectors</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">key_vectors</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span>
                <span class="n">value_vectors</span><span class="p">,</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># chunk indices</span>
            <span class="n">query_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
            <span class="n">key_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>

            <span class="c1"># append chunks before and after</span>
            <span class="n">key_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">key_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
            <span class="n">value_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">value_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
            <span class="n">key_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">key_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">query_indices</span> <span class="o">=</span> <span class="n">key_indices</span> <span class="o">=</span> <span class="n">indices</span>

        <span class="c1"># query-key matmul: QK^T</span>
        <span class="n">query_key_dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_vectors</span><span class="p">,</span> <span class="n">key_vectors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">query_vectors</span><span class="p">,</span> <span class="n">key_vectors</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_attn_mask</span><span class="p">(</span>
            <span class="n">query_indices</span><span class="p">,</span> <span class="n">key_indices</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">query_key_dots</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">do_standard_self_attention</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># get mask tensor depending on half precision or not</span>
            <span class="k">if</span> <span class="n">query_key_dots</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                <span class="n">mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_value_float16</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mask_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask_value_float32</span>

            <span class="n">query_key_dots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">query_key_dots</span><span class="p">,</span> <span class="n">mask_value</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">mask</span>

        <span class="c1"># softmax</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">query_key_dots</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">query_key_dots</span> <span class="o">-</span> <span class="n">logits</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">logits</span>

        <span class="c1"># dropout</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">*</span> <span class="n">head_mask</span>

        <span class="c1"># attend values</span>
        <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value_vectors</span><span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">value_vectors</span>

        <span class="c1"># merge chunk length</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
            <span class="n">out_vectors</span> <span class="o">=</span> <span class="n">out_vectors</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">out_vectors</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">out_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_hidden_size_dims</span><span class="p">(</span><span class="n">out_vectors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="p">()</span>

        <span class="k">return</span> <span class="n">LocalSelfAttentionOutput</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">out_vectors</span><span class="p">,</span> <span class="n">attention_probs</span><span class="o">=</span><span class="n">attention_probs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_attn_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">query_indices</span><span class="p">,</span> <span class="n">key_indices</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">query_key_dots_shape</span><span class="p">,</span> <span class="n">do_standard_self_attention</span>
    <span class="p">):</span>

        <span class="c1"># chunk attention mask and look before and after</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">do_standard_self_attention</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_seq_length_dim_to</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_look_adjacent</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_before</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks_after</span><span class="p">)</span>
            <span class="c1"># create attn_mask</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">query_key_dots_shape</span><span class="p">)</span>

        <span class="c1"># Causal mask</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">query_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">key_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_indices</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># add attention mask if not None</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">causal_mask</span> <span class="o">*</span> <span class="n">attention_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">causal_mask</span>

        <span class="k">return</span> <span class="n">attention_mask</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_retrieve_relevant_hidden_states</span><span class="p">(</span><span class="n">previous_hidden_states</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">,</span> <span class="n">num_chunks_before</span><span class="p">):</span>
        <span class="n">start_position</span> <span class="o">=</span> <span class="p">((</span><span class="n">previous_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">chunk_length</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_chunks_before</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_length</span>
        <span class="k">return</span> <span class="n">previous_hidden_states</span><span class="p">[:,</span> <span class="n">start_position</span><span class="p">:]</span>


<span class="k">class</span> <span class="nc">ReformerSelfOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">all_head_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">all_head_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span> <span class="nc">ReformerAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span> <span class="o">=</span> <span class="n">layer_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;lsh&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">LocalSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">([</span><span class="s2">&quot;lsh&quot;</span><span class="p">,</span> <span class="s2">&quot;local&quot;</span><span class="p">]):</span>
            <span class="c1"># get correct attn layers</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;lsh&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">LSHSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">LocalSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Only attn layer types &#39;lsh&#39; and &#39;local&#39; exist, but got `config.attn_layers`: </span><span class="si">{}</span><span class="s2">. Select attn layer types from [&#39;lsh&#39;, &#39;local&#39;] only.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attn_layers</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">ReformerSelfOutput</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">orig_sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">buckets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># make sure cached hidden states is set to None for backward pass</span>
        <span class="k">if</span> <span class="n">past_buckets_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_buckets_states_layer</span> <span class="o">=</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">past_buckets_states_layer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># use cached buckets for backprob if buckets not None for LSHSelfAttention</span>
        <span class="n">self_attention_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states_layer</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">buckets</span><span class="o">=</span><span class="n">buckets</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># add buckets if necessary</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">self_attention_outputs</span><span class="p">,</span> <span class="s2">&quot;buckets&quot;</span><span class="p">):</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="n">self_attention_outputs</span><span class="o">.</span><span class="n">buckets</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># cache hidden states for future use</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># padded input should not be cached</span>
                <span class="n">past_buckets</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">buckets</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">orig_sequence_length</span><span class="p">]</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">orig_sequence_length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span> <span class="n">buckets</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">past_buckets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">buckets</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># padded input should not be cached</span>
                <span class="n">past_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span><span class="n">orig_sequence_length</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">past_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">past_buckets_states</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">past_buckets</span><span class="p">,</span> <span class="n">past_states</span><span class="p">)</span>
        <span class="c1"># compute attention feed forward output</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">self_attention_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">AttentionOutput</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">attention_output</span><span class="p">,</span>
            <span class="n">attention_probs</span><span class="o">=</span><span class="n">self_attention_outputs</span><span class="o">.</span><span class="n">attention_probs</span><span class="p">,</span>
            <span class="n">buckets</span><span class="o">=</span><span class="n">buckets</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">ReformerFeedForwardDense</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">feed_forward_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span> <span class="nc">ReformerFeedForwardOutput</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">feed_forward_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span> <span class="nc">ChunkReformerFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">ReformerFeedForwardDense</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">ReformerFeedForwardOutput</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">apply_chunking_to_forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">forward_chunk</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span><span class="p">,</span>
            <span class="n">attention_output</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">ReformerAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">)</span>
        <span class="c1"># dropout requires to have the same</span>
        <span class="c1"># seed for forward and backward pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_seed</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_seed</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">ChunkReformerFeedForward</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_attention_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function sets a new seed for the attention layer to make dropout deterministic for both forward calls: 1</span>
<span class="sd">        normal forward call and 1 forward call in backward to recalculate activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># randomize seeds</span>
        <span class="c1"># use cuda generator if available</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="p">,</span> <span class="s2">&quot;default_generators&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_generators</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># GPU</span>
            <span class="n">device_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_generators</span><span class="p">[</span><span class="n">device_idx</span><span class="p">]</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># CPU</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_seed</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span> <span class="o">%</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_feed_forward_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function sets a new seed for the feed forward layer to make dropout deterministic for both forward calls:</span>
<span class="sd">        1 normal forward call and 1 forward call in backward to recalculate activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># randomize seeds</span>
        <span class="c1"># use cuda generator if available</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="p">,</span> <span class="s2">&quot;default_generators&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_generators</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># GPU</span>
            <span class="n">device_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_generators</span><span class="p">[</span><span class="n">device_idx</span><span class="p">]</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># CPU</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_seed</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span> <span class="o">%</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prev_attn_output</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">orig_sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># every forward pass we sample a different seed</span>
            <span class="c1"># for dropout and save for forward fn in backward pass</span>
            <span class="c1"># to have correct dropout</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_attention_seed</span><span class="p">()</span>

            <span class="n">attn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
                <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
                <span class="n">orig_sequence_length</span><span class="o">=</span><span class="n">orig_sequence_length</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_outputs</span><span class="o">.</span><span class="n">hidden_states</span>

            <span class="c1"># Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0)</span>
            <span class="c1"># Y_1 = X_1 + f(X_2)</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">prev_attn_output</span> <span class="o">+</span> <span class="n">attn_output</span>

            <span class="c1"># free memory</span>
            <span class="k">del</span> <span class="n">prev_attn_output</span>

            <span class="c1"># every forward pass we sample a different seed</span>
            <span class="c1"># for dropout and save seed for forward fn in backward</span>
            <span class="c1"># to have correct dropout</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_feed_forward_seed</span><span class="p">()</span>
            <span class="c1"># Y_2 = X_2 + g(Y_1)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ReformerOutput</span><span class="p">(</span>
            <span class="n">attn_output</span><span class="o">=</span><span class="n">attn_output</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_probs</span><span class="o">=</span><span class="n">attn_outputs</span><span class="o">.</span><span class="n">attention_probs</span><span class="p">,</span>
            <span class="n">buckets</span><span class="o">=</span><span class="n">attn_outputs</span><span class="o">.</span><span class="n">buckets</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">backward_pass</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">next_attn_output</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">grad_attn_output</span><span class="p">,</span>
        <span class="n">grad_hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">buckets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Implements the backward pass for reversible ResNets.</span>
        <span class="c1"># A good blog post on how this works can be found here:</span>
        <span class="c1"># Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0)</span>
        <span class="c1"># This code is heavily inspired by https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reversible.py</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">next_attn_output</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># set seed to have correct dropout</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_seed</span><span class="p">)</span>
            <span class="c1"># g(Y_1)</span>
            <span class="n">res_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">next_attn_output</span><span class="p">)</span>
            <span class="n">res_hidden_states</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_hidden_states</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># X_2 = Y_2 - g(Y_1)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">-</span> <span class="n">res_hidden_states</span>
            <span class="k">del</span> <span class="n">res_hidden_states</span>

            <span class="n">grad_attn_output</span> <span class="o">=</span> <span class="n">grad_attn_output</span> <span class="o">+</span> <span class="n">next_attn_output</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">next_attn_output</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># set seed to have correct dropout</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_seed</span><span class="p">)</span>
            <span class="c1"># f(X_2)</span>
            <span class="c1"># use cached buckets for backprob if buckets not None for LSHSelfAttention</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">buckets</span><span class="o">=</span><span class="n">buckets</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">hidden_states</span>
            <span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_attn_output</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># X_1 = Y_1 - f(X_2)</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">next_attn_output</span> <span class="o">-</span> <span class="n">output</span>
            <span class="k">del</span> <span class="n">output</span><span class="p">,</span> <span class="n">next_attn_output</span>

            <span class="n">grad_hidden_states</span> <span class="o">=</span> <span class="n">grad_hidden_states</span> <span class="o">+</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ReformerBackwardOutput</span><span class="p">(</span>
            <span class="n">attn_output</span><span class="o">=</span><span class="n">attn_output</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">grad_attn_output</span><span class="o">=</span><span class="n">grad_attn_output</span><span class="p">,</span>
            <span class="n">grad_hidden_states</span><span class="o">=</span><span class="n">grad_hidden_states</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_ReversibleFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    To prevent PyTorch from performing the usual backpropagation, a customized backward function is implemented here.</span>
<span class="sd">    This way it is made sure that no memory expensive activations are saved during the forward pass. This function is</span>
<span class="sd">    heavily inspired by https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reversible.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="n">ctx</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">layers</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="p">,</span>
        <span class="n">all_hidden_states</span><span class="p">,</span>
        <span class="n">all_attentions</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">,</span>
        <span class="n">orig_sequence_length</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">all_buckets</span> <span class="o">=</span> <span class="p">()</span>

        <span class="c1"># split duplicated tensor</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">layer_head_mask</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">all_hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
                <span class="n">prev_attn_output</span><span class="o">=</span><span class="n">attn_output</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">head_mask</span><span class="o">=</span><span class="n">layer_head_mask</span><span class="p">,</span>
                <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
                <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
                <span class="n">orig_sequence_length</span><span class="o">=</span><span class="n">orig_sequence_length</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="o">.</span><span class="n">attn_output</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="o">.</span><span class="n">hidden_states</span>
            <span class="n">all_buckets</span> <span class="o">=</span> <span class="n">all_buckets</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="o">.</span><span class="n">buckets</span><span class="p">,)</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_outputs</span><span class="o">.</span><span class="n">attention_probs</span><span class="p">)</span>

        <span class="c1"># Add last layer</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">all_hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># attach params to ctx for backward</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">attn_output</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">all_buckets</span> <span class="o">=</span> <span class="n">all_buckets</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

        <span class="c1"># Concatenate 2 RevNet outputs</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_hidden_states</span><span class="p">):</span>
        <span class="n">grad_attn_output</span><span class="p">,</span> <span class="n">grad_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">grad_hidden_states</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># retrieve params from ctx for backward</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>

        <span class="c1"># create tuple</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ReformerBackwardOutput</span><span class="p">(</span>
            <span class="n">attn_output</span><span class="o">=</span><span class="n">attn_output</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">grad_attn_output</span><span class="o">=</span><span class="n">grad_attn_output</span><span class="p">,</span>
            <span class="n">grad_hidden_states</span><span class="o">=</span><span class="n">grad_hidden_states</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># free memory</span>
        <span class="k">del</span> <span class="n">grad_attn_output</span><span class="p">,</span> <span class="n">grad_hidden_states</span><span class="p">,</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">hidden_states</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">all_buckets</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">all_buckets</span>
        <span class="n">head_mask</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">head_mask</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">attention_mask</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="c1"># pop last buckets from stack</span>
            <span class="n">buckets</span> <span class="o">=</span> <span class="n">all_buckets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">all_buckets</span> <span class="o">=</span> <span class="n">all_buckets</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># backprop</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">backward_pass</span><span class="p">(</span>
                <span class="n">next_attn_output</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">attn_output</span><span class="p">,</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">grad_attn_output</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">grad_attn_output</span><span class="p">,</span>
                <span class="n">grad_hidden_states</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">grad_hidden_states</span><span class="p">,</span>
                <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">buckets</span><span class="o">=</span><span class="n">buckets</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">assert</span> <span class="n">all_buckets</span> <span class="o">==</span> <span class="p">(),</span> <span class="s2">&quot;buckets have to be empty after backpropagation&quot;</span>
        <span class="n">grad_hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">output</span><span class="o">.</span><span class="n">grad_attn_output</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">grad_hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># num of return vars has to match num of forward() args</span>
        <span class="c1"># return gradient for hidden_states arg and None for other args</span>
        <span class="k">return</span> <span class="n">grad_hidden_states</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">ReformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">ReformerLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>
        <span class="c1"># Reformer is using Rev Nets, thus last layer outputs are concatenated and</span>
        <span class="c1"># Layer Norm is done over 2 * hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">orig_sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># hidden_states and attention lists to be filled if wished</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_attentions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># init cached hidden states if necessary</span>
        <span class="k">if</span> <span class="n">past_buckets_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_buckets_states</span> <span class="o">=</span> <span class="p">[((</span><span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="kc">None</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">))]</span>

        <span class="c1"># concat same tensor for reversible ResNet</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">_ReversibleFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">all_attentions</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="p">,</span>
            <span class="n">orig_sequence_length</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Apply layer norm to concatenated hidden states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># Apply dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ReformerEncoderOutput</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">all_hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">all_attentions</span><span class="o">=</span><span class="n">all_attentions</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">ReformerOnlyLMHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Reformer is using Rev Nets, thus last layer outputs are concatenated and</span>
        <span class="c1"># Layer Norm is done over 2 * hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_lm_head</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size_lm_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">))</span>

        <span class="c1"># Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">apply_chunking_to_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_lm_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span> <span class="nc">ReformerPreTrainedModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained</span>
<span class="sd">    models.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">config_class</span> <span class="o">=</span> <span class="n">ReformerConfig</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;reformer&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DUMMY_INPUTS</span><span class="p">)</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DUMMY_MASK</span><span class="p">)</span>
        <span class="n">dummy_inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">input_mask</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">dummy_inputs</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Initialize the weights &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">AxialPositionEmbeddings</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">weights</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">axial_norm_std</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="c1"># Slightly different from the TF version which uses truncated_normal for initialization</span>
            <span class="c1"># cf https://github.com/pytorch/pytorch/pull/5617</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initializer_range</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ReformerModelOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Output type of :class:`~transformers.ReformerModel`.</span>

<span class="sd">    Args:</span>
<span class="sd">        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_predict, hidden_size)`):</span>
<span class="sd">            Sequence of hidden-states at the last layer of the model.</span>

<span class="sd">            ``num_predict`` corresponds to ``target_mapping.shape[1]``. If ``target_mapping`` is ``None``, then</span>
<span class="sd">            ``num_predict`` corresponds to ``sequence_length``.</span>
<span class="sd">        past_buckets_states (:obj:`List[Tuple(torch.LongTensor, torch.FloatTensor)]`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):</span>
<span class="sd">            List of :obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length :obj:`config.n_layers`, with the first</span>
<span class="sd">            element being the previous `buckets` of shape :obj:`(batch_size, num_heads, num_hashes, sequence_length)`)</span>
<span class="sd">            and the second being the previous `hidden_states` of shape :obj:`(batch_size, sequence_length,</span>
<span class="sd">            hidden_size)`).</span>

<span class="sd">            Contains precomputed buckets and hidden-states that can be used (see ``past_buckets_states`` input) to</span>
<span class="sd">            speed up sequential decoding.</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings and one for the output of each</span>
<span class="sd">            layer) of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,</span>
<span class="sd">            sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span>
    <span class="n">past_buckets_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ReformerModelWithLMHeadOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Output type of :class:`~transformers.ReformerModelWithLMHead`.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned when ``labels`` is provided)</span>
<span class="sd">            Language modeling loss (for next-token prediction).</span>
<span class="sd">        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_predict, config.vocab_size)`):</span>
<span class="sd">            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</span>

<span class="sd">            ``num_predict`` corresponds to ``target_mapping.shape[1]``. If ``target_mapping`` is ``None``, then</span>
<span class="sd">            ``num_predict`` corresponds to ``sequence_length``.</span>
<span class="sd">        past_buckets_states (:obj:`List[Tuple(torch.LongTensor, torch.FloatTensor)]`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):</span>
<span class="sd">            List of :obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length :obj:`config.n_layers`, with the first</span>
<span class="sd">            element being the previous `buckets` of shape :obj:`(batch_size, num_heads, num_hashes, sequence_length)`)</span>
<span class="sd">            and the second being the previous `hidden_states` of shape :obj:`(batch_size, sequence_length,</span>
<span class="sd">            hidden_size)`).</span>

<span class="sd">            Contains precomputed buckets and hidden-states that can be used (see ``past_buckets_states`` input) to</span>
<span class="sd">            speed up sequential decoding.</span>
<span class="sd">        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):</span>
<span class="sd">            TTuple of :obj:`torch.FloatTensor` (one for the output of the embeddings and one for the output of each</span>
<span class="sd">            layer) of shape :obj:`(batch_size, sequence_length, hidden_size)`.</span>

<span class="sd">            Hidden-states of the model at the output of each layer plus the initial embedding outputs.</span>
<span class="sd">        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):</span>
<span class="sd">            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,</span>
<span class="sd">            sequence_length, sequence_length)`.</span>

<span class="sd">            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention</span>
<span class="sd">            heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">past_buckets_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="n">REFORMER_START_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Reformer was proposed in `Reformer: The Efficient Transformer &lt;https://arxiv.org/abs/2001.04451&gt;`__ by Nikita</span>
<span class="s2">    Kitaev, Åukasz Kaiser, Anselm Levskaya.</span>

<span class="s2">    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic</span>
<span class="s2">    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,</span>
<span class="s2">    pruning heads etc.)</span>

<span class="s2">    This model is also a PyTorch `torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;`__</span>
<span class="s2">    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to</span>
<span class="s2">    general usage and behavior.</span>

<span class="s2">    Parameters:</span>
<span class="s2">        config (:class:`~transformers.ReformerConfig`): Model configuration class with all the parameters of the model.</span>
<span class="s2">            Initializing with a config file does not load the weights associated with the model, only the</span>
<span class="s2">            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model</span>
<span class="s2">            weights.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">REFORMER_INPUTS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Args:</span>
<span class="s2">        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):</span>
<span class="s2">            Indices of input sequence tokens in the vocabulary. During training the input_ids sequence_length has to be</span>
<span class="s2">            a multiple of the relevant model&#39;s chunk lengths (lsh&#39;s, local&#39;s or both). During evaluation, the indices</span>
<span class="s2">            are automatically padded to be a multiple of the chunk length.</span>

<span class="s2">            Indices can be obtained using :class:`~transformers.ReformerTokenizer`. See</span>
<span class="s2">            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for</span>
<span class="s2">            details.</span>

<span class="s2">            `What are input IDs? &lt;../glossary.html#input-ids&gt;`__</span>
<span class="s2">        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="s2">            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:</span>

<span class="s2">            - 1 for tokens that are **not masked**,</span>
<span class="s2">            - 0 for tokens that are **masked**.</span>

<span class="s2">            `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>
<span class="s2">        position_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="s2">            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,</span>
<span class="s2">            config.max_position_embeddings - 1]``.</span>

<span class="s2">            `What are position IDs? &lt;../glossary.html#position-ids&gt;`__</span>
<span class="s2">        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):</span>
<span class="s2">            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:</span>

<span class="s2">            - 1 indicates the head is **not masked**,</span>
<span class="s2">            - 0 indicates the head is **masked**.</span>

<span class="s2">        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):</span>
<span class="s2">            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.</span>
<span class="s2">            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated</span>
<span class="s2">            vectors than the model&#39;s internal embedding lookup matrix.</span>
<span class="s2">        num_hashes (:obj:`int`, `optional`):</span>
<span class="s2">            The number of hashing rounds that should be performed during bucketing. Setting this argument overwrites</span>
<span class="s2">            the default defined in :obj:`config.num_hashes`.</span>

<span class="s2">            For more information, see :obj:`num_hashes` in :class:`~transformers.ReformerConfig`.</span>
<span class="s2">        past_buckets_states (:obj:`List[Tuple(torch.LongTensor, torch.FloatTensor)]`, `optional`):</span>
<span class="s2">            List of :obj:`Tuple(torch.LongTensor, torch.FloatTensor` of length :obj:`config.n_layers`, with the first</span>
<span class="s2">            element being the previous `buckets` of shape :obj:`(batch_size, num_heads, num_hashes, sequence_length)`)</span>
<span class="s2">            and the second being the previous `hidden_states` of shape :obj:`(batch_size, sequence_length,</span>
<span class="s2">            hidden_size)`).</span>

<span class="s2">            Contains precomputed hidden-states and buckets (only relevant for LSH Self-Attention). Can be used to speed</span>
<span class="s2">            up sequential decoding.</span>
<span class="s2">        use_cache (:obj:`bool`, `optional`):</span>
<span class="s2">            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up</span>
<span class="s2">            decoding (see :obj:`past_key_values`).</span>
<span class="s2">        output_attentions (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned</span>
<span class="s2">            tensors for more detail.</span>
<span class="s2">        output_hidden_states (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for</span>
<span class="s2">            more detail.</span>
<span class="s2">        return_dict (:obj:`bool`, `optional`):</span>
<span class="s2">            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>
<span class="s2">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="ReformerModel"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerModel">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="s2">&quot;The bare Reformer Model transformer outputting raw hidden-states&quot;</span> <span class="s2">&quot;without any specific head on top.&quot;</span><span class="p">,</span>
    <span class="n">REFORMER_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">ReformerModel</span><span class="p">(</span><span class="n">ReformerPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;`config.attn_layers` is empty. Select at least one attn layer form [&#39;lsh&#39;, &#39;local&#39;]&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">ReformerEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">ReformerEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base</span>
<span class="sd">        class PreTrainedModel</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

<div class="viewcode-block" id="ReformerModel.forward"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerModel.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_model_forward</span><span class="p">(</span><span class="n">REFORMER_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">ReformerModelOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># noqa: F841</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># noqa: F841</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="p">),</span> <span class="s2">&quot;`input_ids` have be of shape `[batch_size, sequence_length]`, but got shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_buckets_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;`past_buckets_states` can only be used for inference, not for training`.&quot;</span>

        <span class="c1"># prepare head mask</span>
        <span class="n">head_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_head_mask</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">is_attention_chunked</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># original sequence length for padding</span>
        <span class="n">orig_sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># if needs padding</span>
        <span class="n">least_common_mult_chunk_length</span> <span class="o">=</span> <span class="n">_get_least_common_mult_chunk_len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
        <span class="n">min_chunk_length</span> <span class="o">=</span> <span class="n">_get_min_chunk_len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="n">must_pad_to_match_chunk_length</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">least_common_mult_chunk_length</span> <span class="o">!=</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">min_chunk_length</span>
            <span class="ow">and</span> <span class="n">past_buckets_states</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">must_pad_to_match_chunk_length</span><span class="p">:</span>
            <span class="n">padding_length</span> <span class="o">=</span> <span class="n">least_common_mult_chunk_length</span> <span class="o">-</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">least_common_mult_chunk_length</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;If training, sequence Length </span><span class="si">{}</span><span class="s2"> has to be a multiple of least common multiple chunk_length </span><span class="si">{}</span><span class="s2">. Please consider padding the input to a length of </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">least_common_mult_chunk_length</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">padding_length</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># pad input</span>
            <span class="n">input_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_to_mult_of_chunk_length</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
                <span class="n">padding_length</span><span class="o">=</span><span class="n">padding_length</span><span class="p">,</span>
                <span class="n">padded_seq_length</span><span class="o">=</span><span class="n">least_common_mult_chunk_length</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># start index for position encoding depends on incremental decoding</span>
        <span class="k">if</span> <span class="n">past_buckets_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_idx_pos_encodings</span> <span class="o">=</span> <span class="n">past_buckets_states</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_idx_pos_encodings</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">embedding_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">start_idx_pos_encodings</span><span class="o">=</span><span class="n">start_idx_pos_encodings</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">orig_sequence_length</span><span class="o">=</span><span class="n">orig_sequence_length</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span>

        <span class="c1"># if padding was applied</span>
        <span class="k">if</span> <span class="n">must_pad_to_match_chunk_length</span><span class="p">:</span>
            <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="p">[:,</span> <span class="p">:</span><span class="n">orig_sequence_length</span><span class="p">]</span>

        <span class="n">past_buckets_states</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">past_buckets_states</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">all_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">all_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">past_buckets_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attentions</span><span class="p">]</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ReformerModelOutput</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_pad_to_mult_of_chunk_length</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">padded_seq_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Input ids are automatically padded from </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2"> to be a multiple of `config.chunk_length`: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">padding_length</span><span class="p">,</span> <span class="n">padded_seq_length</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="n">padded_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
            <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding_length</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Extend `attention_mask`</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pad_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">pad_attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding_length</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
                <span class="p">],</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Extend `input_ids` with padding to match least common multiple chunk_length</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padded_input_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

            <span class="c1"># Pad position ids if given</span>
            <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">padded_position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">padded_seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="n">padded_position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding_length</span><span class="p">)</span>
                <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">padded_position_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Extend `inputs_embeds` with padding to match least common multiple chunk_length</span>
        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padded_inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">padded_input_ids</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">padded_inputs_embeds</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">input_shape</span></div>


<div class="viewcode-block" id="ReformerModelWithLMHead"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerModelWithLMHead">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Reformer Model with a `language modeling` head on top. &quot;&quot;&quot;</span><span class="p">,</span> <span class="n">REFORMER_START_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ReformerModelWithLMHead</span><span class="p">(</span><span class="n">ReformerPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">,</span> <span class="s2">&quot;If you want to use `ReformerModelWithLMHead` make sure that `is_decoder=True`.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;local&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">local_num_chunks_after</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;If causal mask is enabled, make sure that `config.local_num_chunks_after` is set to 0 and not </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">local_num_chunks_after</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;lsh&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_layers</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">lsh_num_chunks_after</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;If causal mask is enabled, make sure that `config.lsh_num_chunks_after` is set to 1 and not </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">lsh_num_chunks_after</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span> <span class="o">=</span> <span class="n">ReformerModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">ReformerOnlyLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">new_embeddings</span>

<div class="viewcode-block" id="ReformerModelWithLMHead.forward"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerModelWithLMHead.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_model_forward</span><span class="p">(</span><span class="n">REFORMER_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">CausalLMOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_buckets_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[-100, 0,</span>
<span class="sd">                ..., config.vocab_size - 1]`. All labels set to ``-100`` are ignored (masked), the loss is only</span>
<span class="sd">                computed for labels in ``[0, ..., config.vocab_size]``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">reformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">past_buckets_states</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Shift so that tokens &lt; n predict n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># Flatten the tokens</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">ReformerModelWithLMHeadOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">past_buckets_states</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">past_buckets_states</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># only last token for inputs_ids if past is defined in kwargs</span>
        <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="n">inputs_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
            <span class="s2">&quot;past_buckets_states&quot;</span><span class="p">:</span> <span class="n">past</span><span class="p">,</span>
            <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">use_cache</span><span class="p">,</span>
            <span class="s2">&quot;num_hashes&quot;</span><span class="p">:</span> <span class="n">num_hashes</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">inputs_dict</span>

    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reord_past_buckets_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">:</span>
            <span class="c1"># buckets</span>
            <span class="k">if</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">reord_buckets</span> <span class="o">=</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reord_buckets</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># hidden states</span>
            <span class="n">reord_hidden_states</span> <span class="o">=</span> <span class="n">layer_past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>
            <span class="n">reord_past_buckets_states</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">reord_buckets</span><span class="p">,</span> <span class="n">reord_hidden_states</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">reord_past_buckets_states</span></div>


<div class="viewcode-block" id="ReformerForMaskedLM"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForMaskedLM">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Reformer Model with a `language modeling` head on top. &quot;&quot;&quot;</span><span class="p">,</span> <span class="n">REFORMER_START_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">ReformerForMaskedLM</span><span class="p">(</span><span class="n">ReformerPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span>
        <span class="p">),</span> <span class="s2">&quot;If you want to use `ReformerForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span> <span class="o">=</span> <span class="n">ReformerModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">ReformerOnlyLMHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">new_embeddings</span>

<div class="viewcode-block" id="ReformerForMaskedLM.forward"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForMaskedLM.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_model_forward</span><span class="p">(</span><span class="n">REFORMER_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">MaskedLMOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">                Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,</span>
<span class="sd">                config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored</span>
<span class="sd">                (masked), the loss is only computed for the tokens with labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">reformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># no causal mask</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># -100 index = padding token</span>
            <span class="n">masked_lm_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">masked_lm_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">masked_lm_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">MaskedLMOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">masked_lm_loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="ReformerForSequenceClassification"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForSequenceClassification">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reformer Model transformer with a sequence classification/regression head on top (a linear layer on top of the</span>
<span class="sd">    pooled output) e.g. for GLUE tasks.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">REFORMER_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">ReformerForSequenceClassification</span><span class="p">(</span><span class="n">ReformerPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span> <span class="o">=</span> <span class="n">ReformerModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">ReformerClassificationHead</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;You might want to disable causal masking for sequence classification&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="ReformerForSequenceClassification.forward"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForSequenceClassification.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_model_forward</span><span class="p">(</span><span class="n">REFORMER_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">SequenceClassifierOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,</span>
<span class="sd">            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),</span>
<span class="sd">            If :obj:`config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1">#  We are doing regression</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">SequenceClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">ReformerClassificationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Head for sentence-level classification tasks.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># take &lt;s&gt; token (equiv. to [CLS])</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<div class="viewcode-block" id="ReformerForQuestionAnswering"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForQuestionAnswering">[docs]</a><span class="nd">@add_start_docstrings</span><span class="p">(</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reformer Model with a span classification head on top for extractive question-answering tasks like SQuAD / TriviaQA</span>
<span class="sd">    ( a linear layer on top of hidden-states output to compute `span start logits` and `span end logits`.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">REFORMER_START_DOCSTRING</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">ReformerForQuestionAnswering</span><span class="p">(</span><span class="n">ReformerPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span> <span class="o">=</span> <span class="n">ReformerModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># 2 * config.hidden_size because we use reversible residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

<div class="viewcode-block" id="ReformerForQuestionAnswering.forward"><a class="viewcode-back" href="../../../../model_doc/reformer.html#transformers.ReformerForQuestionAnswering.forward">[docs]</a>    <span class="nd">@add_start_docstrings_to_model_forward</span><span class="p">(</span><span class="n">REFORMER_INPUTS_DOCSTRING</span><span class="p">)</span>
    <span class="nd">@add_code_sample_docstrings</span><span class="p">(</span>
        <span class="n">tokenizer_class</span><span class="o">=</span><span class="n">_TOKENIZER_FOR_DOC</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="s2">&quot;google/reformer-crime-and-punishment&quot;</span><span class="p">,</span>
        <span class="n">output_type</span><span class="o">=</span><span class="n">QuestionAnsweringModelOutput</span><span class="p">,</span>
        <span class="n">config_class</span><span class="o">=</span><span class="n">_CONFIG_FOR_DOC</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_hashes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">end_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for position (index) of the start of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the</span>
<span class="sd">            sequence are not taken into account for computing the loss.</span>
<span class="sd">        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for position (index) of the end of the labelled span for computing the token classification loss.</span>
<span class="sd">            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the</span>
<span class="sd">            sequence are not taken into account for computing the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">reformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reformer</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">num_hashes</span><span class="o">=</span><span class="n">num_hashes</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># no causal mask</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qa_outputs</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">start_logits</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">end_logits</span> <span class="o">=</span> <span class="n">end_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we are on multi-GPU, split add a dimension</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_positions</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">end_positions</span> <span class="o">=</span> <span class="n">end_positions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sometimes the start/end positions are outside our model inputs, we ignore these terms</span>
            <span class="n">ignored_index</span> <span class="o">=</span> <span class="n">start_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">start_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">end_positions</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ignored_index</span><span class="p">)</span>

            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">ignored_index</span><span class="p">)</span>
            <span class="n">start_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>
            <span class="n">end_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_loss</span> <span class="o">+</span> <span class="n">end_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">end_logits</span><span class="p">)</span> <span class="o">+</span> <span class="n">reformer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">total_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">total_loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">QuestionAnsweringModelOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">,</span>
            <span class="n">start_logits</span><span class="o">=</span><span class="n">start_logits</span><span class="p">,</span>
            <span class="n">end_logits</span><span class="o">=</span><span class="n">end_logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">reformer_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>