

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.integrations &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.integrations</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.integrations</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 The HuggingFace Team. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Integrations with other Python libraries.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">importlib.util</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">SimpleNamespace</span>

<span class="kn">from</span> <span class="nn">.trainer_utils</span> <span class="kn">import</span> <span class="n">SchedulerType</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="c1"># comet_ml requires to be imported before any ML frameworks</span>
<span class="n">_has_comet</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;comet_ml&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;COMET_MODE&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;DISABLED&quot;</span>
<span class="k">if</span> <span class="n">_has_comet</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">comet_ml</span>  <span class="c1"># noqa: F401</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">comet_ml</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">comet_ml</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_config</span><span class="p">(</span><span class="s2">&quot;comet.api_key&quot;</span><span class="p">):</span>
            <span class="n">_has_comet</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;COMET_MODE&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;DISABLED&quot;</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;comet_ml is installed but `COMET_API_KEY` is not set.&quot;</span><span class="p">)</span>
            <span class="n">_has_comet</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">_has_comet</span> <span class="o">=</span> <span class="kc">False</span>

<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="n">ENV_VARS_TRUE_VALUES</span><span class="p">,</span> <span class="n">is_torch_tpu_available</span>  <span class="c1"># noqa: E402</span>
<span class="kn">from</span> <span class="nn">.trainer_callback</span> <span class="kn">import</span> <span class="n">TrainerCallback</span>  <span class="c1"># noqa: E402</span>
<span class="kn">from</span> <span class="nn">.trainer_utils</span> <span class="kn">import</span> <span class="n">PREFIX_CHECKPOINT_DIR</span><span class="p">,</span> <span class="n">BestRun</span><span class="p">,</span> <span class="n">EvaluationStrategy</span>  <span class="c1"># noqa: E402</span>


<span class="c1"># Integration functions:</span>
<span class="k">def</span> <span class="nf">is_wandb_available</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WANDB_DISABLED&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;wandb&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_comet_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">_has_comet</span>


<span class="k">def</span> <span class="nf">is_tensorboard_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;tensorboardX&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_optuna_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;optuna&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_ray_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;ray&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_ray_tune_available</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_ray_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;ray.tune&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_azureml_available</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;azureml&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;azureml.core&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;azureml.core.run&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_mlflow_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;mlflow&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_fairscale_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;fairscale&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_deepspeed_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;deepspeed&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">hp_params</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_optuna_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">optuna</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">optuna</span><span class="o">.</span><span class="n">Trial</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">trial</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">is_ray_tune_available</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">trial</span>

    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown type for trial </span><span class="si">{</span><span class="n">trial</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">default_hp_search_backend</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">is_optuna_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;optuna&quot;</span>
    <span class="k">elif</span> <span class="n">is_ray_tune_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;ray&quot;</span>


<span class="k">def</span> <span class="nf">run_hp_search_optuna</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">direction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BestRun</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">optuna</span>

    <span class="k">def</span> <span class="nf">_objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">subdir</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">subdir</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">PREFIX_CHECKPOINT_DIR</span><span class="p">):</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">subdir</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trial</span><span class="o">=</span><span class="n">trial</span><span class="p">)</span>
        <span class="c1"># If there hasn&#39;t been any evaluation during the training loop.</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;objective&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">compute_objective</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">trainer</span><span class="o">.</span><span class="n">objective</span>

    <span class="n">timeout</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;timeout&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">n_jobs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="n">direction</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">_objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">)</span>
    <span class="n">best_trial</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
    <span class="k">return</span> <span class="n">BestRun</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">best_trial</span><span class="o">.</span><span class="n">number</span><span class="p">),</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">run_hp_search_ray</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">direction</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BestRun</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">ray</span>

    <span class="k">def</span> <span class="nf">_objective</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">model_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">subdir</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">subdir</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">PREFIX_CHECKPOINT_DIR</span><span class="p">):</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">subdir</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trial</span><span class="o">=</span><span class="n">trial</span><span class="p">)</span>
        <span class="c1"># If there hasn&#39;t been any evaluation during the training loop.</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;objective&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">objective</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">compute_objective</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">_tune_save_checkpoint</span><span class="p">()</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">tune</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">objective</span><span class="p">,</span> <span class="o">**</span><span class="n">metrics</span><span class="p">,</span> <span class="n">done</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># The model and TensorBoard writer do not pickle so we have to remove them (if they exists)</span>
    <span class="c1"># while doing the ray hp search.</span>

    <span class="n">_tb_writer</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">pop_callback</span><span class="p">(</span><span class="n">TensorBoardCallback</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Setup default `resources_per_trial` and `reporter`.</span>
    <span class="k">if</span> <span class="s2">&quot;resources_per_trial&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># `args.n_gpu` is considered the total number of GPUs that will be split</span>
        <span class="c1"># among the `n_jobs`</span>
        <span class="n">n_jobs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_jobs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">num_gpus_per_trial</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span>
        <span class="k">if</span> <span class="n">num_gpus_per_trial</span> <span class="o">/</span> <span class="n">n_jobs</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">num_gpus_per_trial</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_gpus_per_trial</span> <span class="o">/</span> <span class="n">n_jobs</span><span class="p">))</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;resources_per_trial&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gpu&quot;</span><span class="p">:</span> <span class="n">num_gpus_per_trial</span><span class="p">}</span>

    <span class="k">if</span> <span class="s2">&quot;progress_reporter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">ray.tune</span> <span class="kn">import</span> <span class="n">CLIReporter</span>

        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;progress_reporter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CLIReporter</span><span class="p">(</span><span class="n">metric_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;objective&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="s2">&quot;keep_checkpoints_num&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;keep_checkpoints_num&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># `keep_checkpoints_num=0` would disabled checkpointing</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">use_tune_checkpoints</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;keep_checkpoints_num&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Currently keeping </span><span class="si">{}</span><span class="s2"> checkpoints for each trial. Checkpoints are usually huge, &quot;</span>
                <span class="s2">&quot;consider setting `keep_checkpoints_num=1`.&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;scheduler&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">ray.tune.schedulers</span> <span class="kn">import</span> <span class="n">ASHAScheduler</span><span class="p">,</span> <span class="n">HyperBandForBOHB</span><span class="p">,</span> <span class="n">MedianStoppingRule</span><span class="p">,</span> <span class="n">PopulationBasedTraining</span>

        <span class="c1"># Check if checkpointing is enabled for PopulationBasedTraining</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">],</span> <span class="n">PopulationBasedTraining</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">trainer</span><span class="o">.</span><span class="n">use_tune_checkpoints</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;You are using PopulationBasedTraining but you haven&#39;t enabled checkpointing. &quot;</span>
                    <span class="s2">&quot;This means your trials will train from scratch everytime they are exploiting &quot;</span>
                    <span class="s2">&quot;new configurations. Consider enabling checkpointing by passing &quot;</span>
                    <span class="s2">&quot;`keep_checkpoints_num=1` as an additional argument to `Trainer.hyperparameter_search`.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Check for `do_eval` and `eval_during_training` for schedulers that require intermediate reporting.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">],</span> <span class="p">(</span><span class="n">ASHAScheduler</span><span class="p">,</span> <span class="n">MedianStoppingRule</span><span class="p">,</span> <span class="n">HyperBandForBOHB</span><span class="p">,</span> <span class="n">PopulationBasedTraining</span><span class="p">)</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">do_eval</span> <span class="ow">or</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">evaluation_strategy</span> <span class="o">==</span> <span class="n">EvaluationStrategy</span><span class="o">.</span><span class="n">NO</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;You are using </span><span class="si">{cls}</span><span class="s2"> as a scheduler but you haven&#39;t enabled evaluation during training. &quot;</span>
                <span class="s2">&quot;This means your trials will not report intermediate results to Ray Tune, and &quot;</span>
                <span class="s2">&quot;can thus not be stopped early or used to exploit other trials parameters. &quot;</span>
                <span class="s2">&quot;If this is what you want, do not use </span><span class="si">{cls}</span><span class="s2">. If you would like to use </span><span class="si">{cls}</span><span class="s2">, &quot;</span>
                <span class="s2">&quot;make sure you pass `do_eval=True` and `evaluation_strategy=&#39;steps&#39;` in the &quot;</span>
                <span class="s2">&quot;Trainer `args`.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">cls</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="n">analysis</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">tune</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_objective</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">hp_space</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">best_trial</span> <span class="o">=</span> <span class="n">analysis</span><span class="o">.</span><span class="n">get_best_trial</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;objective&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">direction</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">best_run</span> <span class="o">=</span> <span class="n">BestRun</span><span class="p">(</span><span class="n">best_trial</span><span class="o">.</span><span class="n">trial_id</span><span class="p">,</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">last_result</span><span class="p">[</span><span class="s2">&quot;objective&quot;</span><span class="p">],</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_tb_writer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span><span class="n">_tb_writer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">best_run</span>


<span class="k">def</span> <span class="nf">rewrite_logs</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">new_d</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">eval_prefix</span> <span class="o">=</span> <span class="s2">&quot;eval_&quot;</span>
    <span class="n">eval_prefix_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_prefix</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">eval_prefix</span><span class="p">):</span>
            <span class="n">new_d</span><span class="p">[</span><span class="s2">&quot;eval/&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">[</span><span class="n">eval_prefix_len</span><span class="p">:]]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_d</span><span class="p">[</span><span class="s2">&quot;train/&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">new_d</span>


<span class="k">def</span> <span class="nf">init_deepspeed</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Init DeepSpeed, after converting any relevant Trainer&#39;s args into DeepSpeed configuration</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer: Trainer object</span>
<span class="sd">        num_training_steps: per single gpu</span>

<span class="sd">    Returns: model, optimizer, lr_scheduler</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">deepspeed</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">args</span>
    <span class="n">ds_config_file</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">deepspeed</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span>

    <span class="k">with</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">ds_config_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># The following code translates relevant trainer&#39;s cl args into the DS config</span>

    <span class="c1"># First to ensure that there is no mismatch between cl args values and presets in the config</span>
    <span class="c1"># file, ask to not set in ds config file:</span>
    <span class="c1"># - &quot;train_batch_size&quot;,</span>
    <span class="c1"># - &quot;train_micro_batch_size_per_gpu&quot;,</span>
    <span class="c1"># - &quot;gradient_accumulation_steps&quot;</span>
    <span class="n">bs_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">,</span> <span class="s2">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bs_keys</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">()]):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Do not include </span><span class="si">{</span><span class="n">bs_keys</span><span class="si">}</span><span class="s2"> entries in the ds config file, as they will be set via --per_device_train_batch_size or its default&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;gradient_accumulation_steps&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Do not include gradient_accumulation_steps entries in the ds config file, as they will be set via --gradient_accumulation_steps or its default&quot;</span>
        <span class="p">)</span>

    <span class="c1"># DeepSpeed does:</span>
    <span class="c1">#   train_batch_size = n_gpus * train_micro_batch_size_per_gpu * gradient_accumulation_steps</span>
    <span class="c1"># therefore we just need to set:</span>
    <span class="n">config</span><span class="p">[</span><span class="s2">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span>
    <span class="n">config</span><span class="p">[</span><span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>

    <span class="k">if</span> <span class="s2">&quot;gradient_clipping&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Keeping the `gradient_clipping` config from </span><span class="si">{</span><span class="n">ds_config_file</span><span class="si">}</span><span class="s2"> intact, ignoring any gradient clipping-specific cl args&quot;</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># override only if the ds config doesn&#39;t already have this section</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;gradient_clipping&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span>

    <span class="k">if</span> <span class="s2">&quot;optimizer&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Keeping the `optimizer` config from </span><span class="si">{</span><span class="n">ds_config_file</span><span class="si">}</span><span class="s2"> intact, ignoring any optimizer-specific cl args&quot;</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># override only if the ds config doesn&#39;t already have this section</span>
        <span class="c1"># ds supports Adam, OneBitAdam, and Lamb optimizers and can import other optimizers from torch.</span>
        <span class="c1"># But trainer uses AdamW by default.</span>
        <span class="c1"># To use other optimizers so using a different scheduler requires voiding warranty with: `zero_allow_untested_optimizer`</span>

        <span class="n">optimizer_configs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;AdamW&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="s2">&quot;betas&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">adam_beta1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_beta2</span><span class="p">],</span>
                <span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">,</span>
                <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="s2">&quot;AdamW&quot;</span>

        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;zero_allow_untested_optimizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">,</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">optimizer_configs</span><span class="p">[</span><span class="n">optimizer</span><span class="p">],</span>
        <span class="p">}</span>

    <span class="c1"># DS schedulers (deepspeed/runtime/lr_schedules.py):</span>
    <span class="c1">#</span>
    <span class="c1"># DS name      | --lr_scheduler_type  | HF func                           | Notes</span>
    <span class="c1"># -------------| ---------------------|-----------------------------------|--------------------</span>
    <span class="c1"># LRRangeTest  | na                   | na                                | LRRT</span>
    <span class="c1"># OneCycle     | na                   | na                                | 1CLR</span>
    <span class="c1"># WarmupLR     | constant_with_warmup | get_constant_schedule_with_warmup | w/ warmup_min_lr=0</span>
    <span class="c1"># WarmupDecayLR| linear               | get_linear_schedule_with_warmup   |</span>
    <span class="k">if</span> <span class="s2">&quot;scheduler&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Keeping the `scheduler` config from </span><span class="si">{</span><span class="n">ds_config_file</span><span class="si">}</span><span class="s2"> intact, ignoring any scheduler-specific cl args&quot;</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># override only if the ds config doesn&#39;t already have this section</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_type</span> <span class="o">==</span> <span class="n">SchedulerType</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">:</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="s2">&quot;WarmupDecayLR&quot;</span>
            <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;last_batch_iteration&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="s2">&quot;total_num_steps&quot;</span><span class="p">:</span> <span class="n">num_training_steps</span><span class="p">,</span>
                <span class="s2">&quot;warmup_min_lr&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;warmup_max_lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="s2">&quot;warmup_num_steps&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_type</span> <span class="o">==</span> <span class="n">SchedulerType</span><span class="o">.</span><span class="n">CONSTANT_WITH_WARMUP</span><span class="p">:</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="s2">&quot;WarmupLR&quot;</span>
            <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;warmup_min_lr&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s2">&quot;warmup_max_lr&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="s2">&quot;warmup_num_steps&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">lr_scheduler_type</span><span class="si">}</span><span class="s2"> scheduler type is not supported by DeepSpeed&quot;</span><span class="p">)</span>

        <span class="n">config</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">scheduler</span><span class="p">,</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="c1"># fp16</span>
    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fp16_backend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Deepspeed has 2 possible fp16 config entries:</span>
        <span class="c1"># - `fp16`: for the native amp - it has a bunch of optional params but we won&#39;t set any here unless the user did the work</span>
        <span class="c1"># - `amp`: which delegates amp work to apex (which needs to be available), but it cannot be used with any ZeRO features, so probably best to be avoided.</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fp16_backend</span> <span class="o">==</span> <span class="s2">&quot;apex&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;amp&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Keeping the `amp` config from </span><span class="si">{</span><span class="n">ds_config_file</span><span class="si">}</span><span class="s2"> intact, ignoring any amp-specific cl args&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;amp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="s2">&quot;opt_level&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">fp16_opt_level</span><span class="p">,</span>
                <span class="p">}</span>
        <span class="k">elif</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fp16_backend</span> <span class="o">==</span> <span class="s2">&quot;amp&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;fp16&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Keeping the `fp16` config from </span><span class="si">{</span><span class="n">ds_config_file</span><span class="si">}</span><span class="s2"> intact, ignoring any fp16-specific cl args&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span><span class="p">[</span><span class="s2">&quot;fp16&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="p">}</span>

    <span class="c1"># for clarity extract the specific cl args that are being passed to deepspeed</span>
    <span class="n">ds_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>

    <span class="c1"># init that takes part of the config via `args`, and the bulk of it via `config_params`</span>
    <span class="n">model_parameters</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span>
        <span class="n">args</span><span class="o">=</span><span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="n">ds_args</span><span class="p">),</span>  <span class="c1"># expects an obj</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="o">=</span><span class="n">model_parameters</span><span class="p">,</span>
        <span class="n">config_params</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_scheduler</span>


<div class="viewcode-block" id="TensorBoardCallback"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.TensorBoardCallback">[docs]</a><span class="k">class</span> <span class="nc">TensorBoardCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A :class:`~transformers.TrainerCallback` that sends the logs to `TensorBoard</span>
<span class="sd">    &lt;https://www.tensorflow.org/tensorboard&gt;`__.</span>

<span class="sd">    Args:</span>
<span class="sd">        tb_writer (:obj:`SummaryWriter`, `optional`):</span>
<span class="sd">            The writer to use. Will instantiate one if not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tb_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">has_tensorboard</span> <span class="o">=</span> <span class="n">is_tensorboard_available</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">has_tensorboard</span>
        <span class="p">),</span> <span class="s2">&quot;TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.&quot;</span>
        <span class="k">if</span> <span class="n">has_tensorboard</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>  <span class="c1"># noqa: F401</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span> <span class="o">=</span> <span class="n">SummaryWriter</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span> <span class="o">=</span> <span class="n">SummaryWriter</span>
                <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span> <span class="o">=</span> <span class="n">tb_writer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span> <span class="o">=</span> <span class="n">SummaryWriter</span>

    <span class="k">def</span> <span class="nf">_init_summary_writer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">log_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">log_dir</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_dir</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SummaryWriter</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">log_dir</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_hyper_param_search</span><span class="p">:</span>
            <span class="n">trial_name</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">trial_name</span>
            <span class="k">if</span> <span class="n">trial_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">logging_dir</span><span class="p">,</span> <span class="n">trial_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_summary_writer</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">add_text</span><span class="p">(</span><span class="s2">&quot;args&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">to_json_string</span><span class="p">())</span>
            <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">model_config_json</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_json_string</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">add_text</span><span class="p">(</span><span class="s2">&quot;model_config&quot;</span><span class="p">,</span> <span class="n">model_config_json</span><span class="p">)</span>
            <span class="c1"># Version of TensorBoard coming from tensorboardX does not have this method.</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="p">,</span> <span class="s2">&quot;add_hparams&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">add_hparams</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">to_sanitized_dict</span><span class="p">(),</span> <span class="n">metric_dict</span><span class="o">=</span><span class="p">{})</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_summary_writer</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logs</span> <span class="o">=</span> <span class="n">rewrite_logs</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Trainer is attempting to log a value of &quot;</span>
                        <span class="s1">&#39;&quot;</span><span class="si">%s</span><span class="s1">&quot; of type </span><span class="si">%s</span><span class="s1"> for key &quot;</span><span class="si">%s</span><span class="s1">&quot; as a scalar. &#39;</span>
                        <span class="s2">&quot;This invocation of Tensorboard&#39;s writer.add_scalar() &quot;</span>
                        <span class="s2">&quot;is incorrect so we dropped this attribute.&quot;</span><span class="p">,</span>
                        <span class="n">v</span><span class="p">,</span>
                        <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
                        <span class="n">k</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tb_writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>


<div class="viewcode-block" id="WandbCallback"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.WandbCallback">[docs]</a><span class="k">class</span> <span class="nc">WandbCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A :class:`~transformers.TrainerCallback` that sends the logs to `Weight and Biases &lt;https://www.wandb.com/&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">has_wandb</span> <span class="o">=</span> <span class="n">is_wandb_available</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">has_wandb</span><span class="p">,</span> <span class="s2">&quot;WandbCallback requires wandb to be installed. Run `pip install wandb`.&quot;</span>
        <span class="k">if</span> <span class="n">has_wandb</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">wandb</span>

            <span class="n">wandb</span><span class="o">.</span><span class="n">ensure_configured</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">api_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">has_wandb</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;W&amp;B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="o">=</span> <span class="n">wandb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="WandbCallback.setup"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.WandbCallback.setup">[docs]</a>    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">reinit</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the optional Weights &amp; Biases (`wandb`) integration.</span>

<span class="sd">        One can subclass and override this method to customize the setup if needed. Find more information `here</span>
<span class="sd">        &lt;https://docs.wandb.com/huggingface&gt;`__. You can also override the following environment variables:</span>

<span class="sd">        Environment:</span>
<span class="sd">            WANDB_LOG_MODEL (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to log model as artifact at the end of training.</span>
<span class="sd">            WANDB_WATCH (:obj:`str`, `optional` defaults to :obj:`&quot;gradients&quot;`):</span>
<span class="sd">                Can be :obj:`&quot;gradients&quot;`, :obj:`&quot;all&quot;` or :obj:`&quot;false&quot;`. Set to :obj:`&quot;false&quot;` to disable gradient</span>
<span class="sd">                logging or :obj:`&quot;all&quot;` to log gradients and parameters.</span>
<span class="sd">            WANDB_PROJECT (:obj:`str`, `optional`, defaults to :obj:`&quot;huggingface&quot;`):</span>
<span class="sd">                Set this to a custom string to store results in a different project.</span>
<span class="sd">            WANDB_DISABLED (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to disable wandb entirely.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s1">&#39;Automatic Weights &amp; Biases logging enabled, to disable set os.environ[&quot;WANDB_DISABLED&quot;] = &quot;true&quot;&#39;</span>
            <span class="p">)</span>
            <span class="n">combined_dict</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">args</span><span class="o">.</span><span class="n">to_sanitized_dict</span><span class="p">()}</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
                <span class="n">combined_dict</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">model_config</span><span class="p">,</span> <span class="o">**</span><span class="n">combined_dict</span><span class="p">}</span>
            <span class="n">trial_name</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">trial_name</span>
            <span class="n">init_args</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">trial_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">run_name</span> <span class="o">=</span> <span class="n">trial_name</span>
                <span class="n">init_args</span><span class="p">[</span><span class="s2">&quot;group&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">run_name</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">run_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">run_name</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                <span class="n">project</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WANDB_PROJECT&quot;</span><span class="p">,</span> <span class="s2">&quot;huggingface&quot;</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="n">combined_dict</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">run_name</span><span class="p">,</span>
                <span class="n">reinit</span><span class="o">=</span><span class="n">reinit</span><span class="p">,</span>
                <span class="o">**</span><span class="n">init_args</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># keep track of model topology and gradients, unsupported on TPU</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_torch_tpu_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WANDB_WATCH&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;false&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WANDB_WATCH&quot;</span><span class="p">,</span> <span class="s2">&quot;gradients&quot;</span><span class="p">),</span> <span class="n">log_freq</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">logging_steps</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># log outputs</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_model</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WANDB_LOG_MODEL&quot;</span><span class="p">,</span> <span class="s2">&quot;FALSE&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">in</span> <span class="n">ENV_VARS_TRUE_VALUES</span><span class="o">.</span><span class="n">union</span><span class="p">({</span><span class="s2">&quot;TRUE&quot;</span><span class="p">})</span></div>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">hp_search</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">is_hyper_param_search</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="ow">or</span> <span class="n">hp_search</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">reinit</span><span class="o">=</span><span class="n">hp_search</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="c1"># commit last step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({})</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_model</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>

            <span class="n">fake_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">temp_dir</span><span class="p">:</span>
                <span class="n">fake_trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span>
                <span class="c1"># use run name and ensure it&#39;s a valid Artifact name</span>
                <span class="n">artifact_name</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[^a-zA-Z0-9_\.\-]&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">v</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">summary</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
                    <span class="p">}</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">load_best_model_at_end</span>
                    <span class="k">else</span> <span class="p">{</span>
                        <span class="sa">f</span><span class="s2">&quot;eval/</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">metric_for_best_model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">state</span><span class="o">.</span><span class="n">best_metric</span><span class="p">,</span>
                        <span class="s2">&quot;train/total_floss&quot;</span><span class="p">:</span> <span class="n">state</span><span class="o">.</span><span class="n">total_flos</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">)</span>
                <span class="n">artifact</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">Artifact</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;run-</span><span class="si">{</span><span class="n">artifact_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
                        <span class="k">with</span> <span class="n">artifact</span><span class="o">.</span><span class="n">new_file</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fa</span><span class="p">:</span>
                            <span class="n">fa</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">())</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">artifact</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">reinit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="n">logs</span> <span class="o">=</span> <span class="n">rewrite_logs</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span></div>


<div class="viewcode-block" id="CometCallback"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.CometCallback">[docs]</a><span class="k">class</span> <span class="nc">CometCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A :class:`~transformers.TrainerCallback` that sends the logs to `Comet ML &lt;https://www.comet.ml/site/&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">_has_comet</span><span class="p">,</span> <span class="s2">&quot;CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="CometCallback.setup"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.CometCallback.setup">[docs]</a>    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the optional Comet.ml integration.</span>

<span class="sd">        Environment:</span>
<span class="sd">            COMET_MODE (:obj:`str`, `optional`):</span>
<span class="sd">                &quot;OFFLINE&quot;, &quot;ONLINE&quot;, or &quot;DISABLED&quot;</span>
<span class="sd">            COMET_PROJECT_NAME (:obj:`str`, `optional`):</span>
<span class="sd">                Comet.ml project name for experiments</span>
<span class="sd">            COMET_OFFLINE_DIRECTORY (:obj:`str`, `optional`):</span>
<span class="sd">                Folder to use for saving offline experiments when :obj:`COMET_MODE` is &quot;OFFLINE&quot;</span>

<span class="sd">        For a number of configurable items in the environment, see `here</span>
<span class="sd">        &lt;https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables&gt;`__.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="n">comet_mode</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;COMET_MODE&quot;</span><span class="p">,</span> <span class="s2">&quot;ONLINE&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;project_name&quot;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;COMET_PROJECT_NAME&quot;</span><span class="p">,</span> <span class="s2">&quot;huggingface&quot;</span><span class="p">)}</span>
            <span class="n">experiment</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">comet_mode</span> <span class="o">==</span> <span class="s2">&quot;ONLINE&quot;</span><span class="p">:</span>
                <span class="n">experiment</span> <span class="o">=</span> <span class="n">comet_ml</span><span class="o">.</span><span class="n">Experiment</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Automatic Comet.ml online logging enabled&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">comet_mode</span> <span class="o">==</span> <span class="s2">&quot;OFFLINE&quot;</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;offline_directory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;COMET_OFFLINE_DIRECTORY&quot;</span><span class="p">,</span> <span class="s2">&quot;./&quot;</span><span class="p">)</span>
                <span class="n">experiment</span> <span class="o">=</span> <span class="n">comet_ml</span><span class="o">.</span><span class="n">OfflineExperiment</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Automatic Comet.ml offline logging enabled; use `comet upload` when finished&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">experiment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">experiment</span><span class="o">.</span><span class="n">_set_model_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span>
                <span class="n">experiment</span><span class="o">.</span><span class="n">_log_parameters</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;args/&quot;</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">):</span>
                    <span class="n">experiment</span><span class="o">.</span><span class="n">_log_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;config/&quot;</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="n">experiment</span> <span class="o">=</span> <span class="n">comet_ml</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_global_experiment</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">experiment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">experiment</span><span class="o">.</span><span class="n">_log_metrics</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">framework</span><span class="o">=</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="AzureMLCallback"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.AzureMLCallback">[docs]</a><span class="k">class</span> <span class="nc">AzureMLCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A :class:`~transformers.TrainerCallback` that sends the logs to `AzureML</span>
<span class="sd">    &lt;https://pypi.org/project/azureml-sdk/&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">azureml_run</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">is_azureml_available</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;AzureMLCallback requires azureml to be installed. Run `pip install azureml-sdk`.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">azureml_run</span> <span class="o">=</span> <span class="n">azureml_run</span>

    <span class="k">def</span> <span class="nf">on_init_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">azureml.core.run</span> <span class="kn">import</span> <span class="n">Run</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">azureml_run</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">azureml_run</span> <span class="o">=</span> <span class="n">Run</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">azureml_run</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">azureml_run</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="n">k</span><span class="p">)</span></div>


<div class="viewcode-block" id="MLflowCallback"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.MLflowCallback">[docs]</a><span class="k">class</span> <span class="nc">MLflowCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A :class:`~transformers.TrainerCallback` that sends the logs to `MLflow &lt;https://www.mlflow.org/&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">MAX_LOG_SIZE</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">is_mlflow_available</span><span class="p">(),</span> <span class="s2">&quot;MLflowCallback requires mlflow to be installed. Run `pip install mlflow`.&quot;</span>
        <span class="kn">import</span> <span class="nn">mlflow</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_artifacts</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span> <span class="o">=</span> <span class="n">mlflow</span>

<div class="viewcode-block" id="MLflowCallback.setup"><a class="viewcode-back" href="../../main_classes/callback.html#transformers.integrations.MLflowCallback.setup">[docs]</a>    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setup the optional MLflow integration.</span>

<span class="sd">        Environment:</span>
<span class="sd">            HF_MLFLOW_LOG_ARTIFACTS (:obj:`str`, `optional`):</span>
<span class="sd">                Whether to use MLflow .log_artifact() facility to log artifacts.</span>

<span class="sd">                This only makes sense if logging to a remote server, e.g. s3 or GCS. If set to `True` or `1`, will copy</span>
<span class="sd">                whatever is in TrainerArgument&#39;s output_dir to the local or remote artifact storage. Using it without a</span>
<span class="sd">                remote storage will just copy the files to your artifact location.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">log_artifacts</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HF_MLFLOW_LOG_ARTIFACTS&quot;</span><span class="p">,</span> <span class="s2">&quot;FALSE&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">log_artifacts</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;TRUE&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">}:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_artifacts</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">start_run</span><span class="p">()</span>
            <span class="n">combined_dict</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">model_config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
                <span class="n">combined_dict</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">model_config</span><span class="p">,</span> <span class="o">**</span><span class="n">combined_dict</span><span class="p">}</span>
            <span class="c1"># MLflow cannot log more than 100 values in one go, so we have to split it</span>
            <span class="n">combined_dict_items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combined_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">combined_dict_items</span><span class="p">),</span> <span class="n">MLflowCallback</span><span class="o">.</span><span class="n">MAX_LOG_SIZE</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">combined_dict_items</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">MLflowCallback</span><span class="o">.</span><span class="n">MAX_LOG_SIZE</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span></div>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="n">logs</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Trainer is attempting to log a value of &quot;</span>
                        <span class="s1">&#39;&quot;</span><span class="si">%s</span><span class="s1">&quot; of type </span><span class="si">%s</span><span class="s1"> for key &quot;</span><span class="si">%s</span><span class="s1">&quot; as a metric. &#39;</span>
                        <span class="s2">&quot;MLflow&#39;s log_metric() only accepts float and &quot;</span>
                        <span class="s2">&quot;int types so we dropped this attribute.&quot;</span><span class="p">,</span>
                        <span class="n">v</span><span class="p">,</span>
                        <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
                        <span class="n">k</span><span class="p">,</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">is_world_process_zero</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_artifacts</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Logging artifacts. This may take time.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">log_artifacts</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">end_run</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># if the previous run is not terminated correctly, the fluent API will</span>
        <span class="c1"># not let you start a new run before the previous one is killed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">active_run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ml_flow</span><span class="o">.</span><span class="n">end_run</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="s2">&quot;KILLED&quot;</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>