

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>transformers.modeling_utils &mdash; transformers 2.6.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/custom.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">AutoModels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.modeling_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.modeling_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;PyTorch BERT model.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">typing</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">.activations</span> <span class="kn">import</span> <span class="n">get_activation</span>
<span class="kn">from</span> <span class="nn">.configuration_utils</span> <span class="kn">import</span> <span class="n">PretrainedConfig</span>
<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DUMMY_INPUTS</span><span class="p">,</span>
    <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">TF_WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">cached_path</span><span class="p">,</span>
    <span class="n">hf_bucket_url</span><span class="p">,</span>
    <span class="n">is_remote_url</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># Older PyTorch compatibility</span>
    <span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A placeholder identity operator that is argument-insensitive.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">input</span>


<span class="k">class</span> <span class="nc">ModuleUtilsMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A few utilities for torch.nn.Modules, to be used as a mixin.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">num_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">only_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get number of (optionally, trainable) parameters in the module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="n">only_trainable</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_hook_rss_memory_pre_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">psutil</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;You need to install psutil (pip install psutil) to use memory tracing.&quot;</span><span class="p">)</span>

        <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span> <span class="o">=</span> <span class="n">mem</span><span class="o">.</span><span class="n">rss</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_hook_rss_memory_post_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">psutil</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;You need to install psutil (pip install psutil) to use memory tracing.&quot;</span><span class="p">)</span>

        <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">=</span> <span class="n">mem</span><span class="o">.</span><span class="n">rss</span>
        <span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">-</span> <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="n">mem_rss_diff</span> <span class="o">+</span> <span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;mem_rss_diff&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">add_memory_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</span>
<span class="sd">            Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero with `model.reset_memory_hooks_state()`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_rss_memory_pre_forward</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_rss_memory_post_forward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_memory_hooks_state</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_memory_hooks_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>


<div class="viewcode-block" id="PreTrainedModel"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel">[docs]</a><span class="k">class</span> <span class="nc">PreTrainedModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModuleUtilsMixin</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Base class for all models.</span>

<span class="sd">        :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models</span>
<span class="sd">        as well as a few methods common to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.</span>

<span class="sd">        Class attributes (overridden by derived classes):</span>
<span class="sd">            - ``config_class``: a class derived from :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.</span>
<span class="sd">            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.</span>
<span class="sd">            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:</span>

<span class="sd">                - ``model``: an instance of the relevant subclass of :class:`~transformers.PreTrainedModel`,</span>
<span class="sd">                - ``config``: an instance of the relevant subclass of :class:`~transformers.PretrainedConfig`,</span>
<span class="sd">                - ``path``: a path (string) to the TensorFlow checkpoint.</span>

<span class="sd">            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">pretrained_model_archive_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Dummy inputs to do a forward pass in the network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor with dummy inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DUMMY_INPUTS</span><span class="p">)}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Parameter config in `</span><span class="si">{}</span><span class="s2">(config)` should be an instance of class `PretrainedConfig`. &quot;</span>
                <span class="s2">&quot;To create a model from a pretrained model use &quot;</span>
                <span class="s2">&quot;`model = </span><span class="si">{}</span><span class="s2">.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Save config in model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedModel.get_input_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.get_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s input embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`nn.Module`:</span>
<span class="sd">                A torch module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">base_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedModel.set_input_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.set_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set model&#39;s input embeddings</span>

<span class="sd">        Args:</span>
<span class="sd">            value (:obj:`nn.Module`):</span>
<span class="sd">                A module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">base_model</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedModel.get_output_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s output embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`nn.Module`:</span>
<span class="sd">                A torch module mapping hidden states to vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Overwrite for models with output embeddings</span></div>

<div class="viewcode-block" id="PreTrainedModel.tie_weights"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.tie_weights">[docs]</a>    <span class="k">def</span> <span class="nf">tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tie the weights between the input embeddings and the output embeddings.</span>
<span class="sd">        If the `torchscript` flag is set in the configuration, can&#39;t handle parameter sharing so we are cloning</span>
<span class="sd">        the weights instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">())</span></div>

    <span class="k">def</span> <span class="nf">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_embeddings</span><span class="p">,</span> <span class="n">input_embeddings</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Tie or clone module weights depending of weither we are using TorchScript or not</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">torchscript</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">input_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span>
                <span class="s2">&quot;constant&quot;</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;out_features&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">,</span> <span class="s2">&quot;num_embeddings&quot;</span><span class="p">):</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">num_embeddings</span>

<div class="viewcode-block" id="PreTrainedModel.resize_token_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.resize_token_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.</span>
<span class="sd">        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.</span>

<span class="sd">        Arguments:</span>

<span class="sd">            new_num_tokens: (`optional`) int:</span>
<span class="sd">                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.</span>
<span class="sd">                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.</span>

<span class="sd">        Return: ``torch.nn.Embeddings``</span>
<span class="sd">            Pointer to the input tokens Embeddings Module of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>  <span class="c1"># get the base model if needed</span>
        <span class="n">model_embeds</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">_resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_embeds</span>

        <span class="c1"># Update base model and current model config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">new_num_tokens</span>
        <span class="n">base_model</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">new_num_tokens</span>

        <span class="c1"># Tie weights again if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">model_embeds</span></div>

    <span class="k">def</span> <span class="nf">_resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">):</span>
        <span class="n">old_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_embeddings</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_resized_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_embeddings</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Build a resized Embedding Module from a provided token Embedding Module.</span>
<span class="sd">            Increasing the size will add newly initialized vectors at the end</span>
<span class="sd">            Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            new_num_tokens: (`optional`) int</span>
<span class="sd">                New number of tokens in the embedding matrix.</span>
<span class="sd">                Increasing the size will add newly initialized vectors at the end</span>
<span class="sd">                Reducing the size will remove vectors from the end</span>
<span class="sd">                If not provided or None: return the provided token Embedding Module.</span>
<span class="sd">        Return: ``torch.nn.Embeddings``</span>
<span class="sd">            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">old_embedding_dim</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">old_num_tokens</span> <span class="o">==</span> <span class="n">new_num_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="c1"># Build new embeddings</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">old_embedding_dim</span><span class="p">)</span>
        <span class="n">new_embeddings</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># initialize all new embeddings (in particular added tokens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

        <span class="c1"># Copy token embeddings from the previous weights</span>
        <span class="n">num_tokens_to_copy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">new_embeddings</span>

<div class="viewcode-block" id="PreTrainedModel.init_weights"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.init_weights">[docs]</a>    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Initialize and prunes weights if needed. &quot;&quot;&quot;</span>
        <span class="c1"># Initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span>

        <span class="c1"># Prune heads if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>

        <span class="c1"># Tie weights if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span></div>

<div class="viewcode-block" id="PreTrainedModel.prune_heads"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.prune_heads">[docs]</a>    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Prunes heads of the base model.</span>

<span class="sd">            Arguments:</span>

<span class="sd">                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).</span>
<span class="sd">                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">union_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">union_heads</span><span class="p">)</span>  <span class="c1"># Unfortunately we have to store it as list for JSON</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">_prune_heads</span><span class="p">(</span><span class="n">heads_to_prune</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedModel.save_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.save_pretrained">[docs]</a>    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Save a model and its configuration file to a directory, so that it</span>
<span class="sd">            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span>
            <span class="n">save_directory</span>
        <span class="p">),</span> <span class="s2">&quot;Saving path should be a directory where the model and configuration can be saved&quot;</span>

        <span class="c1"># Only save the model itself if we are using distributed training</span>
        <span class="n">model_to_save</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;module&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span>

        <span class="c1"># Attach architecture to the config</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_to_save</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>

        <span class="c1"># Save configuration file</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="c1"># If we save using the predefined names, we can load using `from_pretrained`</span>
        <span class="n">output_model_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">output_model_file</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model weights saved in </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">))</span></div>

<div class="viewcode-block" id="PreTrainedModel.from_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Instantiate a pretrained pytorch model from a pre-trained model configuration.</span>

<span class="sd">        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)</span>
<span class="sd">        To train the model, you should first set it back in training mode with ``model.train()``</span>

<span class="sd">        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.</span>
<span class="sd">        It is up to you to train those weights with a downstream fine-tuning task.</span>

<span class="sd">        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path: either:</span>
<span class="sd">              - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.</span>
<span class="sd">              - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.</span>
<span class="sd">              - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.</span>
<span class="sd">              - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</span>
<span class="sd">              - None if you are both providing the configuration and state dictionary (resp. with keyword arguments ``config`` and ``state_dict``)</span>

<span class="sd">            model_args: (`optional`) Sequence of positional arguments:</span>
<span class="sd">                All remaning positional arguments will be passed to the underlying model&#39;s ``__init__`` method</span>

<span class="sd">            config: (`optional`) one of:</span>
<span class="sd">                - an instance of a class derived from :class:`~transformers.PretrainedConfig`, or</span>
<span class="sd">                - a string valid as input to :func:`~transformers.PretrainedConfig.from_pretrained()`</span>
<span class="sd">                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</span>
<span class="sd">                    - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or</span>
<span class="sd">                    - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.</span>
<span class="sd">                    - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.</span>

<span class="sd">            state_dict: (`optional`) dict:</span>
<span class="sd">                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.</span>
<span class="sd">                This option can be used if you want to create a model from a pretrained configuration but load your own weights.</span>
<span class="sd">                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.</span>

<span class="sd">            cache_dir: (`optional`) string:</span>
<span class="sd">                Path to a directory in which a downloaded pre-trained model</span>
<span class="sd">                configuration should be cached if the standard cache should not be used.</span>

<span class="sd">            force_download: (`optional`) boolean, default False:</span>
<span class="sd">                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</span>

<span class="sd">            resume_download: (`optional`) boolean, default False:</span>
<span class="sd">                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</span>

<span class="sd">            proxies: (`optional`) dict, default None:</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {&#39;http&#39;: &#39;foo.bar:3128&#39;, &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}.</span>
<span class="sd">                The proxies are used on each request.</span>

<span class="sd">            output_loading_info: (`optional`) boolean:</span>
<span class="sd">                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.</span>

<span class="sd">            kwargs: (`optional`) Remaining dictionary of keyword arguments:</span>
<span class="sd">                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:</span>

<span class="sd">                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model&#39;s ``__init__`` method (we assume all relevant updates to the configuration have already been done)</span>
<span class="sd">                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model&#39;s ``__init__`` function.</span>

<span class="sd">        Examples::</span>

<span class="sd">            # For example purposes. Not runnable.</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;./test/saved_model/&#39;)  # E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_attention=True)  # Update configuration during loading</span>
<span class="sd">            assert model.config.output_attention == True</span>
<span class="sd">            # Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="sd">            config = BertConfig.from_json_file(&#39;./tf_model/my_tf_model_config.json&#39;)</span>
<span class="sd">            model = BertModel.from_pretrained(&#39;./tf_model/my_tf_checkpoint.ckpt.index&#39;, from_tf=True, config=config)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;state_dict&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_tf</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_tf&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">output_loading_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_loading_info&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Load config if we don&#39;t provide a configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="n">config_path</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">config_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">model_args</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Load model</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">:</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">from_tf</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">)):</span>
                    <span class="c1"># Load from a TF 1.0 checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">from_tf</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a TF 2.0 checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a PyTorch checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="s2">&quot;Error no file named </span><span class="si">{}</span><span class="s2"> found in directory </span><span class="si">{}</span><span class="s2"> or `from_tf` set to False&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">],</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">from_tf</span>
                <span class="p">),</span> <span class="s2">&quot;We found a TensorFlow checkpoint at </span><span class="si">{}</span><span class="s2">, please set from_tf to True to load from this checkpoint&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
                <span class="p">)</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">hf_bucket_url</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">postfix</span><span class="o">=</span><span class="p">(</span><span class="n">TF2_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">from_tf</span> <span class="k">else</span> <span class="n">WEIGHTS_NAME</span><span class="p">),</span>
                <span class="p">)</span>

            <span class="c1"># redirect to the cache, if necessary</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_path</span><span class="p">(</span>
                    <span class="n">archive_file</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">EnvironmentError</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Couldn&#39;t reach server at &#39;</span><span class="si">{}</span><span class="s2">&#39; to download pretrained weights.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="s2">&quot;Model name &#39;</span><span class="si">{}</span><span class="s2">&#39; was not found in model name list (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                        <span class="s2">&quot;We assumed &#39;</span><span class="si">{}</span><span class="s2">&#39; was a path or url to model weight files named one of </span><span class="si">{}</span><span class="s2"> but &quot;</span>
                        <span class="s2">&quot;couldn&#39;t find any such file at this path or url.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                            <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pretrained_model_archive_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                            <span class="n">archive_file</span><span class="p">,</span>
                            <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span><span class="p">],</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="o">==</span> <span class="n">archive_file</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2"> from cache at </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Instantiate model.</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">from_tf</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to load weights from pytorch checkpoint file. &quot;</span>
                    <span class="s2">&quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;</span>
                <span class="p">)</span>

        <span class="n">missing_keys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">error_msgs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">from_tf</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">resolved_archive_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.index&quot;</span><span class="p">):</span>
                <span class="c1"># Load from a TensorFlow 1.X checkpoint - provided by original authors</span>
                <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">load_tf_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">[:</span><span class="o">-</span><span class="mi">6</span><span class="p">])</span>  <span class="c1"># Remove the &#39;.index&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Load from our TensorFlow 2.0 checkpoints</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">load_tf2_checkpoint_in_pytorch_model</span>

                    <span class="n">model</span> <span class="o">=</span> <span class="n">load_tf2_checkpoint_in_pytorch_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">allow_missing_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                        <span class="s2">&quot;Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see &quot;</span>
                        <span class="s2">&quot;https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">raise</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Convert old format to new format if needed from a PyTorch state_dict</span>
            <span class="n">old_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">new_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">new_key</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="s2">&quot;gamma&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;beta&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_key</span><span class="p">:</span>
                    <span class="n">old_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                    <span class="n">new_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_key</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">old_key</span><span class="p">,</span> <span class="n">new_key</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_keys</span><span class="p">,</span> <span class="n">new_keys</span><span class="p">):</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old_key</span><span class="p">)</span>

            <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s2">&quot;_metadata&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>

            <span class="c1"># PyTorch&#39;s `_load_from_state_dict` does not copy parameters in a module&#39;s descendants</span>
            <span class="c1"># so we need to apply the function recursively.</span>
            <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
                <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                    <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>

            <span class="c1"># Make sure we are able to load base models as well as derived models (with heads)</span>
            <span class="n">start_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">model_to_load</span> <span class="o">=</span> <span class="n">model</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">start_prefix</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="n">model_to_load</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span>

            <span class="n">load</span><span class="p">(</span><span class="n">model_to_load</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">start_prefix</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="n">model_to_load</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">:</span>
                <span class="n">base_model_state_dict</span> <span class="o">=</span> <span class="n">model_to_load</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="n">head_model_state_dict_without_base_prefix</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">]</span>

                <span class="n">missing_keys</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">head_model_state_dict_without_base_prefix</span> <span class="o">-</span> <span class="n">base_model_state_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Weights of </span><span class="si">{}</span><span class="s2"> not initialized from pretrained model: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">missing_keys</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Weights from pretrained model not used in </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">unexpected_keys</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s2">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>  <span class="c1"># make sure token embedding weights are still tied if needed</span>

        <span class="c1"># Set model in evaluation mode to desactivate DropOut modules by default</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">output_loading_info</span><span class="p">:</span>
            <span class="n">loading_info</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;missing_keys&quot;</span><span class="p">:</span> <span class="n">missing_keys</span><span class="p">,</span>
                <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">:</span> <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="s2">&quot;error_msgs&quot;</span><span class="p">:</span> <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loading_info</span>

        <span class="k">return</span> <span class="n">model</span></div>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">prepare_scores_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scores</span>

    <span class="k">def</span> <span class="nf">_do_output_past</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;During generation, decide whether to pass the `past` variable to the next forward pass.&quot;&quot;&quot;</span>
        <span class="n">has_output_past</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;output_past&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">mem_len</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;mem_len&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">mem_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">has_output_past</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>

<div class="viewcode-block" id="PreTrainedModel.enforce_repetition_penalty_"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.enforce_repetition_penalty_">[docs]</a>    <span class="k">def</span> <span class="nf">enforce_repetition_penalty_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lprobs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">prev_output_tokens</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858). &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">previous_token</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">prev_output_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
                <span class="c1"># if score &lt; 0 then repetition penalty has to multiplied to reduce the previous token probability</span>
                <span class="k">if</span> <span class="n">lprobs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">lprobs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">*=</span> <span class="n">repetition_penalty</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">lprobs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">previous_token</span><span class="p">]</span> <span class="o">/=</span> <span class="n">repetition_penalty</span></div>

<div class="viewcode-block" id="PreTrainedModel.generate"><a class="viewcode-back" href="../../main_classes/model.html#transformers.PreTrainedModel.generate">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.</span>

<span class="sd">        Adapted in part from `Facebook&#39;s XLM beam search code`_.</span>

<span class="sd">        .. _`Facebook&#39;s XLM beam search code`:</span>
<span class="sd">           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529</span>


<span class="sd">        Parameters:</span>

<span class="sd">            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`</span>
<span class="sd">                The sequence used as a prompt for the generation. If `None` the method initializes</span>
<span class="sd">                it as an empty `torch.LongTensor` of shape `(1,)`.</span>

<span class="sd">            max_length: (`optional`) int</span>
<span class="sd">                The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.</span>

<span class="sd">            min_length: (`optional`) int</span>
<span class="sd">                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.</span>

<span class="sd">            do_sample: (`optional`) bool</span>
<span class="sd">                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.</span>

<span class="sd">            early_stopping: (`optional`) bool</span>
<span class="sd">                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.</span>

<span class="sd">            num_beams: (`optional`) int</span>
<span class="sd">                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.</span>

<span class="sd">            temperature: (`optional`) float</span>
<span class="sd">                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.</span>

<span class="sd">            top_k: (`optional`) int</span>
<span class="sd">                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.</span>

<span class="sd">            top_p: (`optional`) float</span>
<span class="sd">                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.</span>

<span class="sd">            repetition_penalty: (`optional`) float</span>
<span class="sd">                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.</span>

<span class="sd">            pad_token_id: (`optional`) int</span>
<span class="sd">                Padding token. Default to specicic model pad_token_id or None if it does not exist.</span>

<span class="sd">            bos_token_id: (`optional`) int</span>
<span class="sd">                BOS token. Defaults to bos_token_id as defined in the models config.</span>

<span class="sd">            pad_token_id: (`optional`) int</span>
<span class="sd">                Pad token. Defaults to pad_token_id as defined in the models config.</span>

<span class="sd">            eos_token_ids: (`optional`) int or list of int</span>
<span class="sd">                End of sequence token or list of tokens to stop the generation. Default to eos_token_ids as defined in the models config.</span>

<span class="sd">            length_penalty: (`optional`) float</span>
<span class="sd">                Exponential penalty to the length. Default to 1.</span>

<span class="sd">            no_repeat_ngram_size: (`optional`) int</span>
<span class="sd">                If set to int &gt; 0, all ngrams of size `no_repeat_ngram_size` can only occur once.</span>

<span class="sd">            num_return_sequences: (`optional`) int</span>
<span class="sd">                The number of independently computed returned sequences for each element in the batch. Default to 1.</span>

<span class="sd">            attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`</span>
<span class="sd">                Mask to avoid performing attention on padding token indices.</span>
<span class="sd">                Mask values selected in ``[0, 1]``:</span>
<span class="sd">                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.</span>
<span class="sd">                Defaults to `None`.</span>

<span class="sd">            `What are attention masks? &lt;../glossary.html#attention-mask&gt;`__</span>

<span class="sd">            decoder_start_token_id=None: (`optional`) int</span>
<span class="sd">                If an encoder-decoder model starts decoding with a different token than BOS.</span>
<span class="sd">                Defaults to `None` and is changed to `BOS` later.</span>

<span class="sd">        Return:</span>

<span class="sd">            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`</span>
<span class="sd">                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`</span>

<span class="sd">        Examples::</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = AutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            outputs = model.generate(max_length=40)  # do greedy decoding</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;openai-gpt&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = AutoModelWithLMHead.from_pretrained(&#39;openai-gpt&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;pt&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#39;The dog&#39;</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;distilgpt2&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = AutoModelWithLMHead.from_pretrained(&#39;distilgpt2&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;The dog&#39;</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;pt&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling</span>
<span class="sd">            for i in range(3): #  3 output sequences were generated</span>
<span class="sd">                print(&#39;Generated {}: {}&#39;.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))</span>

<span class="sd">            tokenizer = AutoTokenizer.from_pretrained(&#39;ctrl&#39;)   # Initialize tokenizer</span>
<span class="sd">            model = AutoModelWithLMHead.from_pretrained(&#39;ctrl&#39;)    # Download model and configuration from S3 and cache.</span>
<span class="sd">            input_context = &#39;Legal My neighbor is&#39;  # &quot;Legal&quot; is one of the control codes for ctrl</span>
<span class="sd">            input_ids = tokenizer.encode(input_context, return_tensors=&#39;pt&#39;)  # encode input context</span>
<span class="sd">            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences</span>
<span class="sd">            print(&#39;Generated: {}&#39;.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># We cannot generate if the model does not have a LM head</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;You tried to generate sequences with a model that does not have a LM Head.&quot;</span>
                <span class="s2">&quot;Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )&quot;</span>
            <span class="p">)</span>

        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span>
        <span class="n">min_length</span> <span class="o">=</span> <span class="n">min_length</span> <span class="k">if</span> <span class="n">min_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_length</span>
        <span class="n">do_sample</span> <span class="o">=</span> <span class="n">do_sample</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span>
        <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span> <span class="k">if</span> <span class="n">early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">early_stopping</span>
        <span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span> <span class="k">if</span> <span class="n">num_beams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_beams</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span> <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="n">top_k</span> <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span>
        <span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span> <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span>
        <span class="n">repetition_penalty</span> <span class="o">=</span> <span class="n">repetition_penalty</span> <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span>
        <span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span> <span class="k">if</span> <span class="n">bos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bos_token_id</span>
        <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span>
        <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span> <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
        <span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span> <span class="k">if</span> <span class="n">length_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">no_repeat_ngram_size</span> <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span>
        <span class="p">)</span>
        <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">num_return_sequences</span> <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_return_sequences</span>
        <span class="p">)</span>
        <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decoder_start_token_id</span> <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># overriden by the input batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`max_length` should be a strictly positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">min_length</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`min_length` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">do_sample</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`do_sample` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">early_stopping</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s2">&quot;`early_stopping` should be a boolean.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`num_beams` should be a strictly positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`temperature` should be strictly positive.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`top_k` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;`top_p` should be between 0 and 1.&quot;</span>
        <span class="k">assert</span> <span class="n">repetition_penalty</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;`repetition_penalty` should be &gt;= 1.&quot;</span>
        <span class="k">assert</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;If input_ids is not defined, `bos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`pad_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;`eos_token_id` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="n">length_penalty</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;`length_penalty` should be strictly positive.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;`no_repeat_ngram_size` should be a positive integer.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_return_sequences</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;`num_return_sequences` should be a strictly positive integer.&quot;</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bos_token_id</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bos_token_id</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;you should either supply a context to complete as `input_ids` input &quot;</span>
                <span class="s2">&quot;or a `bos_token_id` (integer &gt;= 0) as a first token to start the generation.&quot;</span>
            <span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bos_token_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input prompt should be of shape (batch_size, sequence length).&quot;</span>

        <span class="c1"># not allow to duplicate outputs when greedy decoding</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_beams</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># no_beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_return_sequences</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences &gt; 1. Please set num_return_sequences = 1&quot;</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># beam_search greedy generation conditions</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">num_beams</span> <span class="o">&gt;=</span> <span class="n">num_return_sequences</span>
                <span class="p">),</span> <span class="s2">&quot;Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams &gt;= num_return_sequences&quot;</span>

        <span class="c1"># create attention mask if necessary</span>
        <span class="c1"># TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">pad_token_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">):</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># set pad_token_id to eos_token_id if not set. Important that this is done after</span>
        <span class="c1"># attention_mask is created</span>
        <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Setting `pad_token_id` to </span><span class="si">{}</span><span class="s2"> (first `eos_token_id`) to generate sequence&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eos_token_id</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>

        <span class="c1"># current position and vocab size</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>

        <span class="c1"># set effective batch size and effective batch multiplier according to do_sample</span>
        <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="n">num_return_sequences</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="n">effective_batch_mult</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>

            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">decoder_start_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation&quot;</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;get_encoder&quot;</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should have a &#39;get_encoder&#39; function defined&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">),</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> should be a method&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">)</span>

            <span class="c1"># get encoder and store encoder outputs</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()</span>

            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># Expand input ids if num_beams &gt; 1 or num_return_sequences &gt; 1</span>
        <span class="k">if</span> <span class="n">num_return_sequences</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">input_ids_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">effective_batch_mult</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span>
            <span class="p">)</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids_len</span>
            <span class="p">)</span>  <span class="c1"># shape: (batch_size * num_return_sequences * num_beams, cur_len)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span><span class="p">:</span>
            <span class="c1"># create empty decoder_input_ids</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
                <span class="p">(</span><span class="n">effective_batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">decoder_start_token_id</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">batch_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_outputs_batch_dim_idx</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">batch_size</span> <span class="o">==</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;expected encoder_outputs[0] to have 1st dimension bs=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="n">expanded_idx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">effective_batch_mult</span><span class="p">)</span>
                <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">expanded_idx</span><span class="p">),</span> <span class="o">*</span><span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">num_beams</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">num_return_sequences</span><span class="p">,</span>
                <span class="n">length_penalty</span><span class="o">=</span><span class="n">length_penalty</span><span class="p">,</span>
                <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_no_beam_search</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">min_length</span><span class="o">=</span><span class="n">min_length</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="n">no_repeat_ngram_size</span><span class="p">,</span>
                <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="n">decoder_start_token_id</span><span class="p">,</span>
                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
                <span class="n">encoder_outputs</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span></div>

    <span class="k">def</span> <span class="nf">_generate_no_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generate sequences for each example without beam search (num_beams == 1).</span>
<span class="sd">            All returned sequence are generated independantly.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># length of generated sentences / unfinished sentences</span>
        <span class="n">unfinished_sents</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>  <span class="c1"># defined for encoder-decoder models, None for decoder-only models</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_output_past</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">enforce_repetition_penalty_</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">banned_tokens</span> <span class="o">=</span> <span class="n">calc_banned_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="n">next_token_logits</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">banned_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="n">next_token_logits</span><span class="p">[:,</span> <span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="c1"># Temperature (higher temperature =&gt; more likely to sample low probability tokens)</span>
                <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>
                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">top_k_top_p_filtering</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">)</span>
                <span class="c1"># Sample</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Greedy decoding</span>
                <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># update generations and finished sentences</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pad finished sentences if eos_token_id exist</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span> <span class="o">*</span> <span class="n">unfinished_sents</span> <span class="o">+</span> <span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">unfinished_sents</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">next_token</span>

            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokens_to_add</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">eos_in_sents</span> <span class="o">=</span> <span class="n">tokens_to_add</span> <span class="o">==</span> <span class="n">eos_token_id</span>
                <span class="c1"># if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length</span>
                <span class="n">is_sents_unfinished_and_token_to_add_is_eos</span> <span class="o">=</span> <span class="n">unfinished_sents</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">eos_in_sents</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
                <span class="n">sent_lengths</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">is_sents_unfinished_and_token_to_add_is_eos</span><span class="p">,</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="c1"># unfinished_sents is set to zero if eos in sentence</span>
                <span class="n">unfinished_sents</span><span class="o">.</span><span class="n">mul_</span><span class="p">((</span><span class="o">~</span><span class="n">eos_in_sents</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

            <span class="c1"># stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length</span>
            <span class="k">if</span> <span class="n">unfinished_sents</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># extend attention_mask for new generated input if only decoder</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># if there are different sentences lengths in the batch, some batches have to be padded</span>
        <span class="k">if</span> <span class="n">sent_lengths</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">sent_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined if batches have different lengths&quot;</span>
            <span class="c1"># finished sents are filled with pad_token</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sent_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">input_ids</span>

        <span class="k">for</span> <span class="n">hypo_idx</span><span class="p">,</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">):</span>
            <span class="n">decoded</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">,</span> <span class="p">:</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">hypo</span><span class="p">[:</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]]</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="k">def</span> <span class="nf">_generate_beam_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">,</span>
        <span class="n">cur_len</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">,</span>
        <span class="n">min_length</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="p">,</span>
        <span class="n">bos_token_id</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="p">,</span>
        <span class="n">eos_token_id</span><span class="p">,</span>
        <span class="n">decoder_start_token_id</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">num_return_sequences</span><span class="p">,</span>
        <span class="n">length_penalty</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">encoder_outputs</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generate sequences for each example with beam search.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># generated hypotheses</span>
        <span class="n">generated_hyps</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">BeamHypotheses</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># scores for each sentence in the beam</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times</span>
        <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">beam_scores</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (batch_size * num_beams,)</span>

        <span class="c1"># cache compute states</span>
        <span class="n">past</span> <span class="o">=</span> <span class="n">encoder_outputs</span>  <span class="c1"># defined for encoder-decoder models, None for decoder-only models</span>

        <span class="c1"># done sentences</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>

        <span class="k">while</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">past</span><span class="o">=</span><span class="n">past</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, cur_len, vocab_size)</span>
            <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

            <span class="c1"># if model has past, then set the past variable to speed up decoding</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_output_past</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                <span class="n">past</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="c1"># repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)</span>
            <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">enforce_repetition_penalty_</span><span class="p">(</span>
                    <span class="n">next_token_logits</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">next_token_logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="c1"># TODO (PVP) still a bit hacky here - there might be a better solutino</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_scores_for_generation</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>

            <span class="c1"># set eos token prob to zero if min_length is not reached</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cur_len</span> <span class="o">&lt;</span> <span class="n">min_length</span><span class="p">:</span>
                <span class="n">scores</span><span class="p">[:,</span> <span class="n">eos_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">no_repeat_ngram_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate a list of banned tokens to prevent repetitively generating the same ngrams</span>
                <span class="n">num_batch_hypotheses</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
                <span class="c1"># from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345</span>
                <span class="n">banned_batch_tokens</span> <span class="o">=</span> <span class="n">calc_banned_tokens</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">num_batch_hypotheses</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">banned_batch_tokens</span><span class="p">):</span>
                    <span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">banned_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">),</span> <span class="s2">&quot;Shapes of scores: </span><span class="si">{}</span><span class="s2"> != </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">do_sample</span><span class="p">:</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>
                <span class="c1"># Top-p/top-k filtering</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">top_k_top_p_filtering</span><span class="p">(</span>
                    <span class="n">_scores</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">2</span>
                <span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>
                <span class="c1"># re-organize to group the beam together to sample from all beam_idxs</span>
                <span class="n">_scores</span> <span class="o">=</span> <span class="n">_scores</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, num_beams * vocab_size)</span>

                <span class="c1"># Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
                <span class="c1"># Compute next scores</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">_scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>
                <span class="c1"># sort the sampled vector to make sure that the first num_beams samples are the best</span>
                <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_scores_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_scores_indices</span><span class="p">)</span>  <span class="c1"># (batch_size, num_beams * 2)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (batch_size * num_beams, vocab_size)</span>

                <span class="c1"># re-organize to group the beam together (we are keeping top hypothesis accross beams)</span>
                <span class="n">next_scores</span> <span class="o">=</span> <span class="n">next_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span>
                <span class="p">)</span>  <span class="c1"># (batch_size, num_beams * vocab_size)</span>

                <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">assert</span> <span class="n">next_scores</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">next_tokens</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>

            <span class="c1"># next batch beam content</span>
            <span class="n">next_batch_beam</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># for each sentence</span>
            <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>

                <span class="c1"># if we are done with this sentence</span>
                <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                    <span class="p">),</span> <span class="s2">&quot;Batch can only be done if at least </span><span class="si">{}</span><span class="s2"> beams have been generated&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_beams</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="p">(</span>
                        <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">),</span> <span class="s2">&quot;generated beams &gt;= num_beams -&gt; eos_token_id and pad_token have to be defined&quot;</span>
                    <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span>  <span class="c1"># pad the batch</span>
                    <span class="k">continue</span>

                <span class="c1"># next sentence beam content</span>
                <span class="n">next_sent_beam</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="c1"># next tokens for this sentence</span>
                <span class="k">for</span> <span class="n">beam_token_rank</span><span class="p">,</span> <span class="p">(</span><span class="n">beam_token_id</span><span class="p">,</span> <span class="n">beam_token_score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                    <span class="nb">zip</span><span class="p">(</span><span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>
                <span class="p">):</span>
                    <span class="c1"># get beam and word IDs</span>
                    <span class="n">beam_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">//</span> <span class="n">vocab_size</span>
                    <span class="n">token_id</span> <span class="o">=</span> <span class="n">beam_token_id</span> <span class="o">%</span> <span class="n">vocab_size</span>

                    <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>

                    <span class="c1"># add to generated hypotheses if end of sentence</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">token_id</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="ow">is</span> <span class="n">eos_token_id</span><span class="p">):</span>
                        <span class="c1"># if beam_token does not belong to top num_beams tokens, it should not be added</span>
                        <span class="n">is_beam_token_worse_than_top_num_beams</span> <span class="o">=</span> <span class="n">beam_token_rank</span> <span class="o">&gt;=</span> <span class="n">num_beams</span>
                        <span class="k">if</span> <span class="n">is_beam_token_worse_than_top_num_beams</span><span class="p">:</span>
                            <span class="k">continue</span>
                        <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                            <span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">beam_token_score</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># add next predicted word if it is not eos_token</span>
                        <span class="n">next_sent_beam</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">beam_token_score</span><span class="p">,</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">effective_beam_id</span><span class="p">))</span>

                    <span class="c1"># the beam for next step is full</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">:</span>
                        <span class="k">break</span>

                <span class="c1"># Check if were done so that we can save a pad step if all(done)</span>
                <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="ow">or</span> <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">is_done</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">cur_len</span><span class="o">=</span><span class="n">cur_len</span>
                <span class="p">)</span>

                <span class="c1"># update next beam content</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span><span class="p">,</span> <span class="s2">&quot;Beam should always be full&quot;</span>
                <span class="n">next_batch_beam</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">next_sent_beam</span><span class="p">)</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># stop when we are done with each sentence</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">done</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="c1"># sanity check / prepare next batch</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">next_batch_beam</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_beams</span>
            <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">new</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">])</span>
            <span class="n">beam_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">])</span>
            <span class="n">beam_idx</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">next_batch_beam</span><span class="p">])</span>

            <span class="c1"># re-order batch</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">beam_idx</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_tokens</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># re-order internal states</span>
            <span class="k">if</span> <span class="n">past</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">past</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">)</span>

            <span class="c1"># extend attention_mask for new generated input if only decoder</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
                <span class="p">)</span>

            <span class="c1"># update current length</span>
            <span class="n">cur_len</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># finalize all open beam hypotheses and end to generated hypotheses</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                <span class="k">continue</span>

            <span class="c1"># test that beam scores match previously calculated scores if not eos and batch_idx not done</span>
            <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                <span class="p">(</span><span class="n">token_id</span> <span class="o">%</span> <span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">eos_token_id</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">next_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">]</span> <span class="o">==</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">)[</span><span class="n">batch_idx</span><span class="p">]</span>
                <span class="p">),</span> <span class="s2">&quot;If batch_idx is not done, final next scores: </span><span class="si">{}</span><span class="s2"> have to equal to accumulated beam_scores: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">next_scores</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_beams</span><span class="p">][</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">)[</span><span class="n">batch_idx</span><span class="p">],</span>
                <span class="p">)</span>

            <span class="c1"># need to add best num_beams hypotheses to generated hyps</span>
            <span class="k">for</span> <span class="n">beam_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beams</span><span class="p">):</span>
                <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>
                <span class="n">final_score</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">final_tokens</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">effective_beam_id</span><span class="p">]</span>
                <span class="n">generated_hyps</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">final_tokens</span><span class="p">,</span> <span class="n">final_score</span><span class="p">)</span>

        <span class="c1"># depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch</span>
        <span class="n">output_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_return_sequences</span>
        <span class="n">output_num_return_sequences_per_batch</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">do_sample</span> <span class="k">else</span> <span class="n">num_return_sequences</span>

        <span class="c1"># select the best hypotheses</span>
        <span class="n">sent_lengths</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">output_batch_size</span><span class="p">)</span>
        <span class="n">best</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># retrieve best hypotheses</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generated_hyps</span><span class="p">):</span>
            <span class="n">sorted_hyps</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">hypotheses</span><span class="o">.</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_num_return_sequences_per_batch</span><span class="p">):</span>
                <span class="n">effective_batch_idx</span> <span class="o">=</span> <span class="n">output_num_return_sequences_per_batch</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="n">j</span>
                <span class="n">best_hyp</span> <span class="o">=</span> <span class="n">sorted_hyps</span><span class="o">.</span><span class="n">pop</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">sent_lengths</span><span class="p">[</span><span class="n">effective_batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>
                <span class="n">best</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_hyp</span><span class="p">)</span>

        <span class="c1"># shorter batches are filled with pad_token</span>
        <span class="k">if</span> <span class="n">sent_lengths</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">sent_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`Pad_token_id` has to be defined&quot;</span>
            <span class="n">sent_max_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">sent_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">output_batch_size</span><span class="p">,</span> <span class="n">sent_max_len</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span>

            <span class="c1"># fill with hypothesis and eos_token_id if necessary</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best</span><span class="p">):</span>
                <span class="n">decoded</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">hypo</span>
                <span class="k">if</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_length</span><span class="p">:</span>
                    <span class="n">decoded</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">sent_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">eos_token_id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># none of the hypotheses have an eos_token</span>
            <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hypo</span><span class="p">)</span> <span class="o">==</span> <span class="n">max_length</span> <span class="k">for</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="n">best</span><span class="p">)</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">best</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoded</span>

    <span class="c1"># force one of token_ids to be generated by setting prob of all other tokens to 0.</span>
    <span class="k">def</span> <span class="nf">_force_token_ids_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">token_ids</span><span class="p">]</span>
        <span class="n">all_but_token_ids_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;scores should be of rank 2 with shape: [batch_size, vocab_size]&quot;</span>
        <span class="n">scores</span><span class="p">[:,</span> <span class="n">all_but_token_ids_mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_reorder_cache</span><span class="p">(</span><span class="n">past</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">):</span>
        <span class="n">reordered_past</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer_past</span> <span class="ow">in</span> <span class="n">past</span><span class="p">:</span>
            <span class="c1"># get the correct batch idx from layer past batch dim</span>
            <span class="c1"># batch dim of `past` and `mems` is at 2nd position</span>
            <span class="n">reordered_layer_past</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer_past</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">beam_idx</span><span class="p">]</span>
            <span class="n">reordered_layer_past</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">reordered_layer_past</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># check that shape matches</span>
            <span class="k">assert</span> <span class="n">reordered_layer_past</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_past</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">reordered_past</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reordered_layer_past</span><span class="p">)</span>
        <span class="n">past</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">reordered_past</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">past</span></div>


<span class="k">def</span> <span class="nf">calc_banned_tokens</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">,</span> <span class="n">num_hypos</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="p">,</span> <span class="n">cur_len</span><span class="p">):</span>
    <span class="c1"># Copied from fairseq for no_repeat_ngram in beam_search&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">no_repeat_ngram_size</span><span class="p">:</span>
        <span class="c1"># return no banned tokens if we haven&#39;t generated no_repeat_ngram_size tokens yet</span>
        <span class="k">return</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="n">generated_ngrams</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">):</span>
        <span class="n">gen_tokens</span> <span class="o">=</span> <span class="n">prev_input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">generated_ngram</span> <span class="o">=</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">gen_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">no_repeat_ngram_size</span><span class="p">)]):</span>
            <span class="n">prev_ngram_tuple</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ngram</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">generated_ngram</span><span class="p">[</span><span class="n">prev_ngram_tuple</span><span class="p">]</span> <span class="o">=</span> <span class="n">generated_ngram</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prev_ngram_tuple</span><span class="p">,</span> <span class="p">[])</span> <span class="o">+</span> <span class="p">[</span><span class="n">ngram</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">):</span>
        <span class="c1"># Before decoding the next token, prevent decoding of ngrams that have already appeared</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">cur_len</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">no_repeat_ngram_size</span>
        <span class="n">ngram_idx</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prev_input_ids</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">cur_len</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">generated_ngrams</span><span class="p">[</span><span class="n">hypo_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ngram_idx</span><span class="p">,</span> <span class="p">[])</span>

    <span class="n">banned_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_generated_ngrams</span><span class="p">(</span><span class="n">hypo_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hypos</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">banned_tokens</span>


<span class="k">def</span> <span class="nf">top_k_top_p_filtering</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">filter_value</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">),</span> <span class="n">min_tokens_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</span>
<span class="sd">        Args:</span>
<span class="sd">            logits: logits distribution shape (batch size, vocabulary size)</span>
<span class="sd">            if top_k &gt; 0: keep only top k tokens with highest probability (top-k filtering).</span>
<span class="sd">            if top_p &lt; 1.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</span>
<span class="sd">                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)</span>
<span class="sd">            Make sure we keep at least min_tokens_to_keep per batch example in the output</span>
<span class="sd">        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">top_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_tokens_to_keep</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Safety check</span>
        <span class="c1"># Remove all tokens with a probability less than the last token of the top-k</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">logits</span><span class="p">[</span><span class="n">indices_to_remove</span><span class="p">]</span> <span class="o">=</span> <span class="n">filter_value</span>

    <span class="k">if</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Remove tokens with cumulative probability above the threshold (token with 0 are kept)</span>
        <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>
        <span class="k">if</span> <span class="n">min_tokens_to_keep</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)</span>
            <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">min_tokens_to_keep</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Shift the indices to the right to keep also the first token above the threshold</span>
        <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scatter sorted tensors to original indexing</span>
        <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">sorted_indices_to_remove</span><span class="p">)</span>
        <span class="n">logits</span><span class="p">[</span><span class="n">indices_to_remove</span><span class="p">]</span> <span class="o">=</span> <span class="n">filter_value</span>
    <span class="k">return</span> <span class="n">logits</span>


<span class="k">class</span> <span class="nc">BeamHypotheses</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">length_penalty</span><span class="p">,</span> <span class="n">early_stopping</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize n-best list of hypotheses.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># ignoring bos_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span> <span class="o">=</span> <span class="n">length_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="o">=</span> <span class="n">num_beams</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="mf">1e9</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Number of hypotheses in the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">sum_logprobs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a new hypothesis to the list.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">sum_logprobs</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">hyp</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span> <span class="ow">or</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">hyp</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
                <span class="n">sorted_scores</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([(</span><span class="n">s</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">)])</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">beams</span><span class="p">[</span><span class="n">sorted_scores</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="n">sorted_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_done</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">best_sum_logprobs</span><span class="p">,</span> <span class="n">cur_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If there are enough hypotheses and that none of the hypotheses being generated</span>
<span class="sd">        can become better than the worst one in the heap, then we are done with this sentence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_beams</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cur_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cur_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span>
            <span class="n">cur_score</span> <span class="o">=</span> <span class="n">best_sum_logprobs</span> <span class="o">/</span> <span class="n">cur_len</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">length_penalty</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worst_score</span> <span class="o">&gt;=</span> <span class="n">cur_score</span>
            <span class="k">return</span> <span class="n">ret</span>


<span class="k">class</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">nx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)</span>
<span class="sd">            Basically works like a Linear layer but the weights are transposed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">nf</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nf</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">size_out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">,)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">size_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">PoolerStartLogits</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Compute SQuAD start_logits from sequence hidden states. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Args:</span>
<span class="sd">            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`</span>
<span class="sd">                invalid position mask such as query and special symbols (PAD, SEP, CLS)</span>
<span class="sd">                1.0 means token should be masked.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">65500</span> <span class="o">*</span> <span class="n">p_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1e30</span> <span class="o">*</span> <span class="n">p_mask</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">PoolerEndLogits</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Compute SQuAD end_logits from sequence hidden states and start token hidden state.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Args:</span>
<span class="sd">            One of ``start_states``, ``start_positions`` should be not None.</span>
<span class="sd">            If both are set, ``start_positions`` overrides ``start_states``.</span>

<span class="sd">            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states</span>
<span class="sd">                hidden states of the first tokens for the labeled span.</span>
<span class="sd">            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``</span>
<span class="sd">                position of the first token for the labeled span:</span>
<span class="sd">            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``</span>
<span class="sd">                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)</span>
<span class="sd">                1.0 means token should be masked.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">start_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;One of start_states, start_positions should be not None&quot;</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">slen</span><span class="p">,</span> <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">start_states</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, hsz)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">65500</span> <span class="o">*</span> <span class="n">p_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1e30</span> <span class="o">*</span> <span class="n">p_mask</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">PoolerAnswerClass</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Compute SQuAD 2.0 answer class from classification and start tokens hidden states. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            One of ``start_states``, ``start_positions`` should be not None.</span>
<span class="sd">            If both are set, ``start_positions`` overrides ``start_states``.</span>

<span class="sd">            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.</span>
<span class="sd">                hidden states of the first tokens for the labeled span.</span>
<span class="sd">            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``</span>
<span class="sd">                position of the first token for the labeled span.</span>
<span class="sd">            **cls_index**: torch.LongTensor of shape ``(batch_size,)``</span>
<span class="sd">                position of the CLS token. If None, take the last token.</span>

<span class="sd">            note(Original repo):</span>
<span class="sd">                no dependency on end_feature so that we can obtain one single `cls_logits`</span>
<span class="sd">                for each sample</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">start_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;One of start_states, start_positions should be not None&quot;</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, hsz)</span>

        <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">cls_token_state</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, hsz)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cls_token_state</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># shape (bsz, hsz)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">start_states</span><span class="p">,</span> <span class="n">cls_token_state</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">SQuADHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; A SQuAD head inspired by XLNet.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        config (:class:`~transformers.XLNetConfig`): Model configuration class with all the parameters of the model.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``</span>
<span class="sd">            hidden states of sequence tokens</span>
<span class="sd">        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``</span>
<span class="sd">            position of the first token for the labeled span.</span>
<span class="sd">        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``</span>
<span class="sd">            position of the last token for the labeled span.</span>
<span class="sd">        **cls_index**: torch.LongTensor of shape ``(batch_size,)``</span>
<span class="sd">            position of the CLS token. If None, take the last token.</span>
<span class="sd">        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``</span>
<span class="sd">            Whether the question has a possible answer in the paragraph or not.</span>
<span class="sd">        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``</span>
<span class="sd">            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)</span>
<span class="sd">            1.0 means token should be masked.</span>

<span class="sd">    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:</span>
<span class="sd">        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:</span>
<span class="sd">            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.</span>
<span class="sd">        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)</span>
<span class="sd">            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``</span>
<span class="sd">            Log probabilities for the top config.start_n_top start token possibilities (beam-search).</span>
<span class="sd">        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)</span>
<span class="sd">            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``</span>
<span class="sd">            Indices for the top config.start_n_top start token possibilities (beam-search).</span>
<span class="sd">        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)</span>
<span class="sd">            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``</span>
<span class="sd">            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).</span>
<span class="sd">        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)</span>
<span class="sd">            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``</span>
<span class="sd">            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).</span>
<span class="sd">        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)</span>
<span class="sd">            ``torch.FloatTensor`` of shape ``(batch_size,)``</span>
<span class="sd">            Log probabilities for the ``is_impossible`` label of the answers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">start_n_top</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">end_n_top</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">start_logits</span> <span class="o">=</span> <span class="n">PoolerStartLogits</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span> <span class="o">=</span> <span class="n">PoolerEndLogits</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span> <span class="o">=</span> <span class="n">PoolerAnswerClass</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_impossible</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">()</span>

        <span class="n">start_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we are on multi-GPU, let&#39;s remove the dimension added by batch splitting</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">,</span> <span class="n">is_impossible</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># during training, compute the end logits based on the ground truth of the start position</span>
            <span class="n">end_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>

            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">start_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>
            <span class="n">end_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_loss</span> <span class="o">+</span> <span class="n">end_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">is_impossible</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Predict answerability from the representation of CLS and START</span>
                <span class="n">cls_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="n">cls_index</span><span class="p">)</span>
                <span class="n">loss_fct_cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
                <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">loss_fct_cls</span><span class="p">(</span><span class="n">cls_logits</span><span class="p">,</span> <span class="n">is_impossible</span><span class="p">)</span>

                <span class="c1"># note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss</span>
                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">cls_loss</span> <span class="o">*</span> <span class="mf">0.5</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># during inference, compute the end logits based on beam search</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">start_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen)</span>

            <span class="n">start_top_log_probs</span><span class="p">,</span> <span class="n">start_top_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">start_log_probs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top)</span>
            <span class="n">start_top_index_exp</span> <span class="o">=</span> <span class="n">start_top_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_top_index_exp</span><span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">start_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top, hsz)</span>

            <span class="n">hidden_states_expanded</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span>
                <span class="n">start_states</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top, hsz)</span>
            <span class="n">p_mask</span> <span class="o">=</span> <span class="n">p_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">end_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span><span class="p">(</span><span class="n">hidden_states_expanded</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="n">start_states</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>
            <span class="n">end_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top)</span>

            <span class="n">end_top_log_probs</span><span class="p">,</span> <span class="n">end_top_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">end_log_probs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, end_n_top, start_n_top)</span>
            <span class="n">end_top_log_probs</span> <span class="o">=</span> <span class="n">end_top_log_probs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">)</span>
            <span class="n">end_top_index</span> <span class="o">=</span> <span class="n">end_top_index</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">)</span>

            <span class="n">start_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;blh,bl-&gt;bh&quot;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_log_probs</span><span class="p">)</span>
            <span class="n">cls_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="n">start_states</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="n">cls_index</span><span class="p">)</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_top_log_probs</span><span class="p">,</span> <span class="n">start_top_index</span><span class="p">,</span> <span class="n">end_top_log_probs</span><span class="p">,</span> <span class="n">end_top_index</span><span class="p">,</span> <span class="n">cls_logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="c1"># return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits</span>
        <span class="c1"># or (if labels are provided) (total_loss,)</span>
        <span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">SequenceSummary</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Compute a single vector summary of a sequence hidden states according to various possibilities:</span>
<span class="sd">        Args of the config class:</span>
<span class="sd">            summary_type:</span>
<span class="sd">                - &#39;last&#39; =&gt; [default] take the last token hidden state (like XLNet)</span>
<span class="sd">                - &#39;first&#39; =&gt; take the first token hidden state (like Bert)</span>
<span class="sd">                - &#39;mean&#39; =&gt; take the mean of all tokens hidden states</span>
<span class="sd">                - &#39;cls_index&#39; =&gt; supply a Tensor of classification token position (GPT/GPT-2)</span>
<span class="sd">                - &#39;attn&#39; =&gt; Not implemented now, use multi-head attention</span>
<span class="sd">            summary_use_proj: Add a projection after the vector extraction</span>
<span class="sd">            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.</span>
<span class="sd">            summary_activation: &#39;tanh&#39; or another string =&gt; add an activation to the output, Other =&gt; no activation. Default</span>
<span class="sd">            summary_first_dropout: Add a dropout before the projection and activation</span>
<span class="sd">            summary_last_dropout: Add a dropout after the projection and activation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_type&quot;</span><span class="p">,</span> <span class="s2">&quot;last&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="c1"># We should use a standard multi-head attention module with absolute positional embedding for that.</span>
            <span class="c1"># Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276</span>
            <span class="c1"># We can probably just use the multi-head attention module of PyTorch &gt;=1.1.0</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_use_proj&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_use_proj</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_proj_to_labels&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_proj_to_labels</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="n">activation_string</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_activation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">get_activation</span><span class="p">(</span><span class="n">activation_string</span><span class="p">)</span> <span class="k">if</span> <span class="n">activation_string</span> <span class="k">else</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># type: typing.Callable</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_first_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_last_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.</span>
<span class="sd">            cls_index: [optional] position of the classification token if summary_type == &#39;cls_index&#39;,</span>
<span class="sd">                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.</span>
<span class="sd">                if summary_type == &#39;cls_index&#39; and cls_index is None:</span>
<span class="sd">                    we take the last token of the sequence as classification token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;first&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;cls_index&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">cls_index</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),))</span>
            <span class="c1"># shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, XX, hidden_size)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">create_position_ids_from_input_ids</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Replace non-padding symbols with their position numbers. Position numbers begin at</span>
<span class="sd">    padding_idx+1. Padding symbols are ignored. This is modified from fairseq&#39;s</span>
<span class="sd">    `utils.make_positions`.</span>

<span class="sd">    :param torch.Tensor x:</span>
<span class="sd">    :return torch.Tensor:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="n">incremental_indicies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="n">incremental_indicies</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="o">+</span> <span class="n">padding_idx</span>


<span class="k">def</span> <span class="nf">prune_linear_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Prune a linear layer (a model parameters) to keep only entries in index.</span>
<span class="sd">        Return the pruned layer as a new layer with requires_grad=True.</span>
<span class="sd">        Used to remove heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">new_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">new_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">new_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">new_layer</span>


<span class="k">def</span> <span class="nf">prune_conv1d_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Prune a Conv1D layer (a model parameters) to keep only entries in index.</span>
<span class="sd">        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.</span>
<span class="sd">        Return the pruned layer as a new layer with requires_grad=True.</span>
<span class="sd">        Used to remove heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">new_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">new_layer</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">new_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">new_layer</span>


<span class="k">def</span> <span class="nf">prune_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.</span>
<span class="sd">        Return the pruned layer as a new layer with requires_grad=True.</span>
<span class="sd">        Used to remove heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">prune_conv1d_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t prune layer of class </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="p">))</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, huggingface

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>