

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>transformers.modeling_utils &mdash; transformers 4.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/huggingface.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/code-snippets.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/hidesidebar.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/custom.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> transformers
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_doc/xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>transformers.modeling_utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for transformers.modeling_utils</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">.activations</span> <span class="kn">import</span> <span class="n">get_activation</span>
<span class="kn">from</span> <span class="nn">.configuration_utils</span> <span class="kn">import</span> <span class="n">PretrainedConfig</span>
<span class="kn">from</span> <span class="nn">.file_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DUMMY_INPUTS</span><span class="p">,</span>
    <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">TF_WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">WEIGHTS_NAME</span><span class="p">,</span>
    <span class="n">ModelOutput</span><span class="p">,</span>
    <span class="n">cached_path</span><span class="p">,</span>
    <span class="n">hf_bucket_url</span><span class="p">,</span>
    <span class="n">is_remote_url</span><span class="p">,</span>
    <span class="n">is_torch_tpu_available</span><span class="p">,</span>
    <span class="n">replace_return_docstrings</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.generation_utils</span> <span class="kn">import</span> <span class="n">GenerationMixin</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">logging</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># Older PyTorch compatibility</span>
    <span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A placeholder identity operator that is argument-insensitive.&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">input</span>


<div class="viewcode-block" id="find_pruneable_heads_and_indices"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.find_pruneable_heads_and_indices">[docs]</a><span class="k">def</span> <span class="nf">find_pruneable_heads_and_indices</span><span class="p">(</span>
    <span class="n">heads</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">already_pruned_heads</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the heads and their indices taking :obj:`already_pruned_heads` into account.</span>

<span class="sd">    Args:</span>
<span class="sd">        heads (:obj:`List[int]`): List of the indices of heads to prune.</span>
<span class="sd">        n_heads (:obj:`int`): The number of heads in the model.</span>
<span class="sd">        head_size (:obj:`int`): The size of each head.</span>
<span class="sd">        already_pruned_heads (:obj:`Set[int]`): A set of already pruned heads.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`Tuple[Set[int], torch.LongTensor]`: A tuple with the remaining heads and their corresponding indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">-</span> <span class="n">already_pruned_heads</span>  <span class="c1"># Convert to set and remove already pruned heads</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
        <span class="c1"># Compute how many pruned heads are before the head and move the index accordingly</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">head</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">already_pruned_heads</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">head</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="p">))[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">heads</span><span class="p">,</span> <span class="n">index</span></div>


<div class="viewcode-block" id="ModuleUtilsMixin"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin">[docs]</a><span class="k">class</span> <span class="nc">ModuleUtilsMixin</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A few utilities for :obj:`torch.nn.Modules`, to be used as a mixin.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_hook_rss_memory_pre_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">psutil</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;You need to install psutil (pip install psutil) to use memory tracing.&quot;</span><span class="p">)</span>

        <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span> <span class="o">=</span> <span class="n">mem</span><span class="o">.</span><span class="n">rss</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_hook_rss_memory_post_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">psutil</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ImportError</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;You need to install psutil (pip install psutil) to use memory tracing.&quot;</span><span class="p">)</span>

        <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">=</span> <span class="n">mem</span><span class="o">.</span><span class="n">rss</span>
        <span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">-</span> <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span>
        <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="n">mem_rss_diff</span> <span class="o">+</span> <span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;mem_rss_diff&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

<div class="viewcode-block" id="ModuleUtilsMixin.add_memory_hooks"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks">[docs]</a>    <span class="k">def</span> <span class="nf">add_memory_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</span>

<span class="sd">        Increase in memory consumption is stored in a :obj:`mem_rss_diff` attribute for each module and can be reset to</span>
<span class="sd">        zero with :obj:`model.reset_memory_hooks_state()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_rss_memory_pre_forward</span><span class="p">)</span>
            <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_rss_memory_post_forward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_memory_hooks_state</span><span class="p">()</span></div>

<div class="viewcode-block" id="ModuleUtilsMixin.reset_memory_hooks_state"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state">[docs]</a>    <span class="k">def</span> <span class="nf">reset_memory_hooks_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reset the :obj:`mem_rss_diff` attribute of each module (see</span>
<span class="sd">        :func:`~transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_diff</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_post_forward</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">module</span><span class="o">.</span><span class="n">mem_rss_pre_forward</span> <span class="o">=</span> <span class="mi">0</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">device</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same</span>
<span class="sd">        device).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="c1"># For nn.DataParallel compatibility in PyTorch 1.5</span>

            <span class="k">def</span> <span class="nf">find_tensor_attributes</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
                <span class="n">tuples</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span>
                <span class="k">return</span> <span class="n">tuples</span>

            <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span><span class="n">get_members_fn</span><span class="o">=</span><span class="n">find_tensor_attributes</span><span class="p">)</span>
            <span class="n">first_tuple</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">first_tuple</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="c1"># For nn.DataParallel compatibility in PyTorch 1.5</span>

            <span class="k">def</span> <span class="nf">find_tensor_attributes</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
                <span class="n">tuples</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span>
                <span class="k">return</span> <span class="n">tuples</span>

            <span class="n">gen</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_members</span><span class="p">(</span><span class="n">get_members_fn</span><span class="o">=</span><span class="n">find_tensor_attributes</span><span class="p">)</span>
            <span class="n">first_tuple</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">first_tuple</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>

<div class="viewcode-block" id="ModuleUtilsMixin.invert_attention_mask"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask">[docs]</a>    <span class="k">def</span> <span class="nf">invert_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invert an attention mask (e.g., switches 0. and 1.).</span>

<span class="sd">        Args:</span>
<span class="sd">            encoder_attention_mask (:obj:`torch.Tensor`): An attention mask.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.Tensor`: The inverted attention mask.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="c1"># T5 has a mask that can compare sequence ids, we can simulate this here with this transposition</span>
        <span class="c1"># Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow</span>
        <span class="c1"># /transformer/transformer_layers.py#L270</span>
        <span class="c1"># encoder_extended_attention_mask = (encoder_extended_attention_mask ==</span>
        <span class="c1"># encoder_extended_attention_mask.transpose(-1, -2))</span>
        <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="n">encoder_extended_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># fp16 compatibility</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">encoder_extended_attention_mask</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e4</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">encoder_extended_attention_mask</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> not recognized. `dtype` should be set to either `torch.float32` or `torch.float16`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoder_extended_attention_mask</span></div>

<div class="viewcode-block" id="ModuleUtilsMixin.get_extended_attention_mask"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask">[docs]</a>    <span class="k">def</span> <span class="nf">get_extended_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">device</span><span class="p">:</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            attention_mask (:obj:`torch.Tensor`):</span>
<span class="sd">                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</span>
<span class="sd">            input_shape (:obj:`Tuple[int]`):</span>
<span class="sd">                The shape of the input to the model.</span>
<span class="sd">            device: (:obj:`torch.device`):</span>
<span class="sd">                The device of the input to the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
        <span class="c1"># ourselves in which case we just need to make it broadcastable to all heads.</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># Provided a padding mask of dimensions [batch_size, seq_length]</span>
            <span class="c1"># - if the model is a decoder, apply a causal mask in addition to the padding mask</span>
            <span class="c1"># - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span>
                <span class="n">seq_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">seq_ids</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">seq_ids</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
                <span class="c1"># in case past_key_values are used we need to add a prefix ones mask to the causal mask</span>
                <span class="c1"># causal and attention masks must have same type with pytorch version &lt; 1.3</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="n">prefix_seq_len</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">prefix_seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">causal_mask</span><span class="o">.</span><span class="n">dtype</span>
                            <span class="p">),</span>
                            <span class="n">causal_mask</span><span class="p">,</span>
                        <span class="p">],</span>
                        <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>

                <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Wrong shape for input_ids (shape </span><span class="si">{}</span><span class="s2">) or attention_mask (shape </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">input_shape</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span>
        <span class="c1"># masked positions, this operation will create a tensor which is 0.0 for</span>
        <span class="c1"># positions we want to attend and -10000.0 for masked positions.</span>
        <span class="c1"># Since we are adding it to the raw scores before the softmax, this is</span>
        <span class="c1"># effectively the same as removing these entirely.</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">extended_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># fp16 compatibility</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">extended_attention_mask</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
        <span class="k">return</span> <span class="n">extended_attention_mask</span></div>

<div class="viewcode-block" id="ModuleUtilsMixin.get_head_mask"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.get_head_mask">[docs]</a>    <span class="k">def</span> <span class="nf">get_head_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">is_attention_chunked</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare the head mask if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            head_mask (:obj:`torch.Tensor` with shape :obj:`[num_heads]` or :obj:`[num_hidden_layers x num_heads]`, `optional`):</span>
<span class="sd">                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).</span>
<span class="sd">            num_hidden_layers (:obj:`int`):</span>
<span class="sd">                The number of hidden layers in the model.</span>
<span class="sd">            is_attention_chunked: (:obj:`bool`, `optional, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not the attentions scores are computed by chunks or not.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.Tensor` with shape :obj:`[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or</span>
<span class="sd">            list with :obj:`[None]` for each layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_head_mask_to_5d</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_attention_chunked</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">head_mask</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_hidden_layers</span>

        <span class="k">return</span> <span class="n">head_mask</span></div>

    <span class="k">def</span> <span class="nf">_convert_head_mask_to_5d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_mask</span><span class="p">,</span> <span class="n">num_hidden_layers</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;-&gt; [num_hidden_layers x batch x num_heads x seq_length x seq_length]&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># We can specify head_mask for each layer</span>
        <span class="k">assert</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;head_mask.dim != 5, instead </span><span class="si">{</span><span class="n">head_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># switch to float if need + fp16 compatibility</span>
        <span class="k">return</span> <span class="n">head_mask</span>

<div class="viewcode-block" id="ModuleUtilsMixin.num_parameters"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.num_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">num_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">only_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">exclude_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get number of (optionally, trainable or non-embeddings) parameters in the module.</span>

<span class="sd">        Args:</span>
<span class="sd">            only_trainable (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to return only the number of trainable parameters</span>

<span class="sd">            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to return only the number of non-embeddings parameters</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: The number of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">parameter_filter</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">only_trainable</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span> <span class="ow">and</span> <span class="n">exclude_embeddings</span>
            <span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="n">parameter_filter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="n">only_trainable</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span></div>

<div class="viewcode-block" id="ModuleUtilsMixin.estimate_tokens"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.estimate_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">estimate_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function to estimate the total number of tokens from the model inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (:obj:`dict`): The model inputs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: The total number of tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">token_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">input_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;input&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">token_inputs</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">token_input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">token_input</span> <span class="ow">in</span> <span class="n">token_inputs</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Could not estimate the number of tokens of the input, floating-point operations will not be computed&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="mi">0</span></div>

<div class="viewcode-block" id="ModuleUtilsMixin.floating_point_ops"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.ModuleUtilsMixin.floating_point_ops">[docs]</a>    <span class="k">def</span> <span class="nf">floating_point_ops</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">exclude_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a</span>
<span class="sd">        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of</span>
<span class="sd">        tokens (valid if :obj:`12 * d_model &lt;&lt; sequence_length`) as laid out in `this paper</span>
<span class="sd">        &lt;https://arxiv.org/pdf/2001.08361.pdf&gt;`__ section 2.1. Should be overridden for transformers with parameter</span>
<span class="sd">        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (:obj:`int`):</span>
<span class="sd">                The batch size for the forward pass.</span>

<span class="sd">            sequence_length (:obj:`int`):</span>
<span class="sd">                The number of tokens in each line of the batch.</span>

<span class="sd">            exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
<span class="sd">                Whether or not to count embedding and softmax operations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: The number of floating-point operations.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="mi">6</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_tokens</span><span class="p">(</span><span class="n">input_dict</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_parameters</span><span class="p">(</span><span class="n">exclude_embeddings</span><span class="o">=</span><span class="n">exclude_embeddings</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="PreTrainedModel"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel">[docs]</a><span class="k">class</span> <span class="nc">PreTrainedModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModuleUtilsMixin</span><span class="p">,</span> <span class="n">GenerationMixin</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for all models.</span>

<span class="sd">    :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods</span>
<span class="sd">    for loading, downloading and saving models as well as a few methods common to all models to:</span>

<span class="sd">        * resize the input embeddings,</span>
<span class="sd">        * prune heads in the self-attention heads.</span>

<span class="sd">    Class attributes (overridden by derived classes):</span>

<span class="sd">        - **config_class** (:class:`~transformers.PretrainedConfig`) -- A subclass of</span>
<span class="sd">          :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.</span>
<span class="sd">        - **load_tf_weights** (:obj:`Callable`) -- A python `method` for loading a TensorFlow checkpoint in a PyTorch</span>
<span class="sd">          model, taking as arguments:</span>

<span class="sd">            - **model** (:class:`~transformers.PreTrainedModel`) -- An instance of the model on which to load the</span>
<span class="sd">              TensorFlow checkpoint.</span>
<span class="sd">            - **config** (:class:`~transformers.PreTrainedConfig`) -- An instance of the configuration associated to</span>
<span class="sd">              the model.</span>
<span class="sd">            - **path** (:obj:`str`) -- A path to the TensorFlow checkpoint.</span>

<span class="sd">        - **base_model_prefix** (:obj:`str`) -- A string indicating the attribute associated to the base model in</span>
<span class="sd">          derived classes of the same architecture adding modules on top of the base model.</span>
<span class="sd">        - **is_parallelizable** (:obj:`bool`) -- A flag indicating whether this model supports model parallelization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="c1"># a list of re pattern of tensor names to ignore from the model when loading the model weights</span>
    <span class="c1"># (and avoid unnecessary warnings).</span>
    <span class="n">_keys_to_ignore_on_load_missing</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># a list of re pattern of tensor names to ignore from the weights when loading the model weights</span>
    <span class="c1"># (and avoid unnecessary warnings).</span>
    <span class="n">_keys_to_ignore_on_load_unexpected</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># a list of of tensor names to ignore when saving the model (useful for keys that aren&#39;t</span>
    <span class="c1"># trained, but which are deterministic)</span>
    <span class="n">_keys_to_ignore_on_save</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dummy_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">DUMMY_INPUTS</span><span class="p">)}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Parameter config in `</span><span class="si">{}</span><span class="s2">(config)` should be an instance of class `PretrainedConfig`. &quot;</span>
                <span class="s2">&quot;To create a model from a pretrained model use &quot;</span>
                <span class="s2">&quot;`model = </span><span class="si">{}</span><span class="s2">.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Save config and origin of the pretrained weights if given in model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">name_or_path</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :obj:`torch.nn.Module`: The main body of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="PreTrainedModel.get_input_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.get_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s input embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`nn.Module`: A torch module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">base_model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedModel.set_input_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.set_input_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set model&#39;s input embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (:obj:`nn.Module`): A module mapping vocabulary to hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">base_model</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="PreTrainedModel.get_output_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.get_output_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the model&#39;s output embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`nn.Module`: A torch module mapping hidden states to vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Overwrite for models with output embeddings</span></div>

<div class="viewcode-block" id="PreTrainedModel.tie_weights"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.tie_weights">[docs]</a>    <span class="k">def</span> <span class="nf">tie_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tie the weights between the input embeddings and the output embeddings.</span>

<span class="sd">        If the :obj:`torchscript` flag is set in the configuration, can&#39;t handle parameter sharing so we are cloning</span>
<span class="sd">        the weights instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_or_clone_weights</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">())</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_encoder_decoder</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_encoder_decoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">):</span>
                <span class="bp">self</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tie_encoder_decoder_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_tie_encoder_decoder_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">decoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">base_model_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">uninitialized_encoder_weights</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">decoder</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">!=</span> <span class="n">encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">decoder</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="si">}</span><span class="s2"> are not equal. In this case make sure that all encoder weights are correctly initialized.&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">tie_encoder_to_decoder_recursively</span><span class="p">(</span>
            <span class="n">decoder_pointer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">encoder_pointer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">module_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">uninitialized_encoder_weights</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
            <span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">encoder_pointer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">decoder_pointer</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">encoder_pointer</span><span class="si">}</span><span class="s2"> have to be of type torch.nn.Module&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">encoder_pointer</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
                <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">weight</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">decoder_pointer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">encoder_pointer</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span>
                    <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">bias</span>
                <span class="k">return</span>

            <span class="n">encoder_modules</span> <span class="o">=</span> <span class="n">encoder_pointer</span><span class="o">.</span><span class="n">_modules</span>
            <span class="n">decoder_modules</span> <span class="o">=</span> <span class="n">decoder_pointer</span><span class="o">.</span><span class="n">_modules</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">encoder_modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Encoder module </span><span class="si">{</span><span class="n">encoder_pointer</span><span class="si">}</span><span class="s2"> does not match decoder module </span><span class="si">{</span><span class="n">decoder_pointer</span><span class="si">}</span><span class="s2">&quot;</span>

                <span class="n">all_encoder_weights</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">sub_name</span> <span class="k">for</span> <span class="n">sub_name</span> <span class="ow">in</span> <span class="n">encoder_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
                <span class="n">encoder_layer_pos</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">decoder_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
                        <span class="n">encoder_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_layer_pos</span><span class="p">)</span>
                        <span class="n">decoder_name</span> <span class="o">=</span> <span class="n">name</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">[</span><span class="n">decoder_name</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">encoder_modules</span><span class="p">[</span><span class="n">encoder_name</span><span class="p">]))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span>
                            <span class="n">encoder_modules</span>
                        <span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder_modules</span><span class="p">):</span>
                            <span class="c1"># this can happen if the name corresponds to the position in a list module list of layers</span>
                            <span class="c1"># in this case the decoder has added a cross-attention that the encoder does not have</span>
                            <span class="c1"># thus skip this step and subtract one layer pos from encoder</span>
                            <span class="n">encoder_layer_pos</span> <span class="o">-=</span> <span class="mi">1</span>
                            <span class="k">continue</span>
                    <span class="k">elif</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder_modules</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">elif</span> <span class="n">depth</span> <span class="o">&gt;</span> <span class="mi">500</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is a circular dependency between two or more `nn.Modules` of your model.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">decoder_name</span> <span class="o">=</span> <span class="n">encoder_name</span> <span class="o">=</span> <span class="n">name</span>
                    <span class="n">tie_encoder_to_decoder_recursively</span><span class="p">(</span>
                        <span class="n">decoder_modules</span><span class="p">[</span><span class="n">decoder_name</span><span class="p">],</span>
                        <span class="n">encoder_modules</span><span class="p">[</span><span class="n">encoder_name</span><span class="p">],</span>
                        <span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span>
                        <span class="n">uninitialized_encoder_weights</span><span class="p">,</span>
                        <span class="n">depth</span><span class="o">=</span><span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">all_encoder_weights</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">encoder_name</span><span class="p">)</span>

                <span class="n">uninitialized_encoder_weights</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_encoder_weights</span><span class="p">)</span>

        <span class="c1"># tie weights recursively</span>
        <span class="n">tie_encoder_to_decoder_recursively</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">base_model_prefix</span><span class="p">,</span> <span class="n">uninitialized_encoder_weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">uninitialized_encoder_weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following encoder weights were not tied to the decoder </span><span class="si">{</span><span class="n">uninitialized_encoder_weights</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_tie_or_clone_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_embeddings</span><span class="p">,</span> <span class="n">input_embeddings</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Tie or clone module weights depending of whether we are using TorchScript or not&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">torchscript</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">input_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">weight</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="mi">0</span><span class="p">,</span>
                    <span class="n">output_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">output_embeddings</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="p">),</span>
                <span class="s2">&quot;constant&quot;</span><span class="p">,</span>
                <span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">output_embeddings</span><span class="p">,</span> <span class="s2">&quot;out_features&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">input_embeddings</span><span class="p">,</span> <span class="s2">&quot;num_embeddings&quot;</span><span class="p">):</span>
            <span class="n">output_embeddings</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">input_embeddings</span><span class="o">.</span><span class="n">num_embeddings</span>

<div class="viewcode-block" id="PreTrainedModel.resize_token_embeddings"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.resize_token_embeddings">[docs]</a>    <span class="k">def</span> <span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.</span>

<span class="sd">        Takes care of tying weights embeddings afterwards if the model class has a :obj:`tie_weights()` method.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            new_num_tokens (:obj:`int`, `optional`):</span>
<span class="sd">                The number of new tokens in the embedding matrix. Increasing the size will add newly initialized</span>
<span class="sd">                vectors at the end. Reducing the size will remove vectors from the end. If not provided or :obj:`None`,</span>
<span class="sd">                just returns a pointer to the input tokens :obj:`torch.nn.Embedding` module of the model without doing</span>
<span class="sd">                anything.</span>

<span class="sd">        Return:</span>
<span class="sd">            :obj:`torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resize_token_embeddings</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_embeds</span>

        <span class="c1"># Update base model and current model config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">new_num_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">new_num_tokens</span>

        <span class="c1"># Tie weights again if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">model_embeds</span></div>

    <span class="k">def</span> <span class="nf">_resize_token_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">):</span>
        <span class="n">old_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_embeddings</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_input_embeddings</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

        <span class="c1"># if word embeddings are not tied, make sure that lm head is resized as well</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">tie_word_embeddings</span><span class="p">:</span>
            <span class="n">old_lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_embeddings</span><span class="p">()</span>
            <span class="n">new_lm_head</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_resized_lm_head</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_output_embeddings</span><span class="p">(</span><span class="n">new_lm_head</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_resized_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">old_embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly</span>
<span class="sd">        initialized vectors at the end. Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            old_embeddings (:obj:`torch.nn.Embedding`):</span>
<span class="sd">                Old embeddings to be resized.</span>
<span class="sd">            new_num_tokens (:obj:`int`, `optional`):</span>
<span class="sd">                New number of tokens in the embedding matrix.</span>

<span class="sd">                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove</span>
<span class="sd">                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens</span>
<span class="sd">                :obj:`torch.nn.Embedding`` module of the model without doing anything.</span>

<span class="sd">        Return:</span>
<span class="sd">            :obj:`torch.nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if</span>
<span class="sd">            :obj:`new_num_tokens` is :obj:`None`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">old_embedding_dim</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">old_num_tokens</span> <span class="o">==</span> <span class="n">new_num_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_embeddings</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Old embeddings are of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">old_embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2">, which is not an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;You should either use a different resize function or make sure that `old_embeddings` are an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Build new embeddings</span>
        <span class="n">new_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">old_embedding_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># initialize all new embeddings (in particular added tokens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">new_embeddings</span><span class="p">)</span>

        <span class="c1"># Copy token embeddings from the previous weights</span>
        <span class="n">num_tokens_to_copy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>
        <span class="n">new_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">old_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span>

        <span class="k">return</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">_get_resized_lm_head</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">old_lm_head</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">transposed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized</span>
<span class="sd">        vectors at the end. Reducing the size will remove vectors from the end</span>

<span class="sd">        Args:</span>
<span class="sd">            old_lm_head (:obj:`torch.nn.Linear`):</span>
<span class="sd">                Old lm head liner layer to be resized.</span>
<span class="sd">            new_num_tokens (:obj:`int`, `optional`):</span>
<span class="sd">                New number of tokens in the linear matrix.</span>

<span class="sd">                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove</span>
<span class="sd">                vectors from the end. If not provided or :obj:`None`, just returns a pointer to the input tokens</span>
<span class="sd">                :obj:`torch.nn.Linear`` module of the model without doing anything.</span>
<span class="sd">            transposed (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether ``old_lm_head`` is transposed or not. If True ``old_lm_head.size()`` is ``lm_head_dim,</span>
<span class="sd">                vocab_size`` else ``vocab_size, lm_head_dim``.</span>

<span class="sd">        Return:</span>
<span class="sd">            :obj:`torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if</span>
<span class="sd">            :obj:`new_num_tokens` is :obj:`None`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">new_num_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_lm_head</span>

        <span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">old_lm_head_dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span> <span class="k">else</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">old_num_tokens</span> <span class="o">==</span> <span class="n">new_num_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">old_lm_head</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Old language model head is of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">old_lm_head</span><span class="p">)</span><span class="si">}</span><span class="s2">, which is not an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;You should either use a different resize function or make sure that `old_embeddings` are an instance of </span><span class="si">{</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Build new lm head</span>
        <span class="n">new_lm_head_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">old_lm_head_dim</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span> <span class="k">else</span> <span class="p">(</span><span class="n">new_num_tokens</span><span class="p">,</span> <span class="n">old_lm_head_dim</span><span class="p">)</span>
        <span class="n">has_new_lm_head_bias</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">new_lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="o">*</span><span class="n">new_lm_head_shape</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">has_new_lm_head_bias</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># initialize new lm head (in particular added tokens)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">(</span><span class="n">new_lm_head</span><span class="p">)</span>

        <span class="n">num_tokens_to_copy</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">old_num_tokens</span><span class="p">,</span> <span class="n">new_num_tokens</span><span class="p">)</span>

        <span class="c1"># Copy old lm head weights to new lm head</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span>

        <span class="c1"># Copy bias weights to new lm head</span>
        <span class="k">if</span> <span class="n">has_new_lm_head_bias</span><span class="p">:</span>
            <span class="n">new_lm_head</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_lm_head</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="n">num_tokens_to_copy</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">new_lm_head</span>

<div class="viewcode-block" id="PreTrainedModel.init_weights"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.init_weights">[docs]</a>    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes and prunes weights if needed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span>

        <span class="c1"># Prune heads if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)</span>

        <span class="c1"># Tie weights if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span></div>

<div class="viewcode-block" id="PreTrainedModel.prune_heads"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.prune_heads">[docs]</a>    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the base model.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            heads_to_prune (:obj:`Dict[int, List[int]]`):</span>
<span class="sd">                Dictionary with keys being selected layer indices (:obj:`int`) and associated values being the list of</span>
<span class="sd">                heads to prune in said layer (list of :obj:`int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads</span>
<span class="sd">                0 and 2 on layer 1 and heads 2 and 3 on layer 2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">union_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">union_heads</span><span class="p">)</span>  <span class="c1"># Unfortunately we have to store it as list for JSON</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="o">.</span><span class="n">_prune_heads</span><span class="p">(</span><span class="n">heads_to_prune</span><span class="p">)</span></div>

<div class="viewcode-block" id="PreTrainedModel.save_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.save_pretrained">[docs]</a>    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save a model and its configuration file to a directory, so that it can be re-loaded using the</span>
<span class="sd">        `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (:obj:`str` or :obj:`os.PathLike`):</span>
<span class="sd">                Directory to which to save. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Provided path (</span><span class="si">{}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">save_directory</span><span class="p">))</span>
            <span class="k">return</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Only save the model itself if we are using distributed training</span>
        <span class="n">model_to_save</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;module&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span>

        <span class="c1"># Attach architecture to the config</span>
        <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_to_save</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model_to_save</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># Handle the case where some state_dict keys shouldn&#39;t be saved</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keys_to_ignore_on_save</span><span class="p">}</span>

        <span class="c1"># If we save using the predefined names, we can load using `from_pretrained`</span>
        <span class="n">output_model_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;xla_device&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_torch_tpu_available</span><span class="p">():</span>
            <span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

            <span class="k">if</span> <span class="n">xm</span><span class="o">.</span><span class="n">is_master_ordinal</span><span class="p">():</span>
                <span class="c1"># Save configuration file</span>
                <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
            <span class="c1"># xm.save takes care of saving only from master</span>
            <span class="n">xm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">output_model_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_to_save</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">output_model_file</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model weights saved in </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_model_file</span><span class="p">))</span></div>

<div class="viewcode-block" id="PreTrainedModel.from_pretrained"><a class="viewcode-back" href="../../main_classes/model.html#transformers.modeling_utils.PreTrainedModel.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]],</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a pretrained pytorch model from a pre-trained model configuration.</span>

<span class="sd">        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated). To</span>
<span class="sd">        train the model, you should first set it back in training mode with ``model.train()``.</span>

<span class="sd">        The warning `Weights from XXX not initialized from pretrained model` means that the weights of XXX do not come</span>
<span class="sd">        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning</span>
<span class="sd">        task.</span>

<span class="sd">        The warning `Weights from XXX not used in YYY` means that the layer XXX is not used by YYY, therefore those</span>
<span class="sd">        weights are discarded.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`, `optional`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.</span>
<span class="sd">                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under</span>
<span class="sd">                      a user or organization name, like ``dbmdz/bert-base-german-cased``.</span>
<span class="sd">                    - A path to a `directory` containing model weights saved using</span>
<span class="sd">                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.</span>
<span class="sd">                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In</span>
<span class="sd">                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided</span>
<span class="sd">                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in</span>
<span class="sd">                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</span>
<span class="sd">                    - :obj:`None` if you are both providing the configuration and state dictionary (resp. with keyword</span>
<span class="sd">                      arguments ``config`` and ``state_dict``).</span>
<span class="sd">            model_args (sequence of positional arguments, `optional`):</span>
<span class="sd">                All remaning positional arguments will be passed to the underlying model&#39;s ``__init__`` method.</span>
<span class="sd">            config (:obj:`Union[PretrainedConfig, str, os.PathLike]`, `optional`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - an instance of a class derived from :class:`~transformers.PretrainedConfig`,</span>
<span class="sd">                    - a string or path valid as input to :func:`~transformers.PretrainedConfig.from_pretrained`.</span>

<span class="sd">                Configuration for the model to use instead of an automatically loaded configuation. Configuration can</span>
<span class="sd">                be automatically loaded when:</span>

<span class="sd">                    - The model is a model provided by the library (loaded with the `model id` string of a pretrained</span>
<span class="sd">                      model).</span>
<span class="sd">                    - The model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded</span>
<span class="sd">                      by supplying the save directory.</span>
<span class="sd">                    - The model is loaded by supplying a local directory as ``pretrained_model_name_or_path`` and a</span>
<span class="sd">                      configuration JSON file named `config.json` is found in the directory.</span>
<span class="sd">            state_dict (:obj:`Dict[str, torch.Tensor]`, `optional`):</span>
<span class="sd">                A state dictionary to use instead of a state dictionary loaded from saved weights file.</span>

<span class="sd">                This option can be used if you want to create a model from a pretrained configuration but load your own</span>
<span class="sd">                weights. In this case though, you should check if using</span>
<span class="sd">                :func:`~transformers.PreTrainedModel.save_pretrained` and</span>
<span class="sd">                :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.</span>
<span class="sd">            cache_dir (:obj:`Union[str, os.PathLike]`, `optional`):</span>
<span class="sd">                Path to a directory in which a downloaded pretrained model configuration should be cached if the</span>
<span class="sd">                standard cache should not be used.</span>
<span class="sd">            from_tf (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Load the model weights from a TensorFlow checkpoint save file (see docstring of</span>
<span class="sd">                ``pretrained_model_name_or_path`` argument).</span>
<span class="sd">            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>
<span class="sd">            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to delete incompletely received files. Will attempt to resume the download if such a</span>
<span class="sd">                file exists.</span>
<span class="sd">            proxies (:obj:`Dict[str, str], `optional`):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            output_loading_info(:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</span>
<span class="sd">            local_files_only(:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to only look at local files (i.e., do not try to download the model).</span>
<span class="sd">            use_auth_token (:obj:`str` or `bool`, `optional`):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token</span>
<span class="sd">                generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).</span>
<span class="sd">            revision(:obj:`str`, `optional`, defaults to :obj:`&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a</span>
<span class="sd">                git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any</span>
<span class="sd">                identifier allowed by git.</span>
<span class="sd">            mirror(:obj:`str`, `optional`, defaults to :obj:`None`):</span>
<span class="sd">                Mirror source to accelerate downloads in China. If you are from China and have an accessibility</span>
<span class="sd">                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.</span>
<span class="sd">                Please refer to the mirror site for more information.</span>
<span class="sd">            kwargs (remaining dictionary of keyword arguments, `optional`):</span>
<span class="sd">                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,</span>
<span class="sd">                :obj:`output_attentions=True`). Behaves differently depending on whether a ``config`` is provided or</span>
<span class="sd">                automatically loaded:</span>

<span class="sd">                    - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the</span>
<span class="sd">                      underlying model&#39;s ``__init__`` method (we assume all relevant updates to the configuration have</span>
<span class="sd">                      already been done)</span>
<span class="sd">                    - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class</span>
<span class="sd">                      initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of</span>
<span class="sd">                      ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute</span>
<span class="sd">                      with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration</span>
<span class="sd">                      attribute will be passed to the underlying model&#39;s ``__init__`` function.</span>

<span class="sd">        .. note::</span>

<span class="sd">            Passing :obj:`use_auth_token=True` is required when you want to use a private model.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; from transformers import BertConfig, BertModel</span>
<span class="sd">            &gt;&gt;&gt; # Download model and configuration from huggingface.co and cache.</span>
<span class="sd">            &gt;&gt;&gt; model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span>
<span class="sd">            &gt;&gt;&gt; # Model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)` (for example purposes, not runnable).</span>
<span class="sd">            &gt;&gt;&gt; model = BertModel.from_pretrained(&#39;./test/saved_model/&#39;)</span>
<span class="sd">            &gt;&gt;&gt; # Update configuration during loading.</span>
<span class="sd">            &gt;&gt;&gt; model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_attentions=True)</span>
<span class="sd">            &gt;&gt;&gt; assert model.config.output_attentions == True</span>
<span class="sd">            &gt;&gt;&gt; # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="sd">            &gt;&gt;&gt; config = BertConfig.from_json_file(&#39;./tf_model/my_tf_model_config.json&#39;)</span>
<span class="sd">            &gt;&gt;&gt; model = BertModel.from_pretrained(&#39;./tf_model/my_tf_checkpoint.ckpt.index&#39;, from_tf=True, config=config)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;state_dict&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_tf</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;from_tf&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">output_loading_info</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;output_loading_info&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">use_auth_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_auth_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">mirror</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;mirror&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Load config if we don&#39;t provide a configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
            <span class="n">config_path</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">config</span><span class="p">,</span> <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">config_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">model_args</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Load model</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">from_tf</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">)):</span>
                    <span class="c1"># Load from a TF 1.0 checkpoint in priority if from_tf</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">from_tf</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a TF 2.0 checkpoint in priority if from_tf</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)):</span>
                    <span class="c1"># Load from a PyTorch checkpoint</span>
                    <span class="n">archive_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">WEIGHTS_NAME</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                        <span class="s2">&quot;Error no file named </span><span class="si">{}</span><span class="s2"> found in directory </span><span class="si">{}</span><span class="s2"> or `from_tf` set to False&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="p">[</span><span class="n">WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF2_WEIGHTS_NAME</span><span class="p">,</span> <span class="n">TF_WEIGHTS_NAME</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">],</span>
                            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span><span class="p">):</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">from_tf</span>
                <span class="p">),</span> <span class="s2">&quot;We found a TensorFlow checkpoint at </span><span class="si">{}</span><span class="s2">, please set from_tf to True to load from this checkpoint&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
                <span class="p">)</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">archive_file</span> <span class="o">=</span> <span class="n">hf_bucket_url</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">filename</span><span class="o">=</span><span class="p">(</span><span class="n">TF2_WEIGHTS_NAME</span> <span class="k">if</span> <span class="n">from_tf</span> <span class="k">else</span> <span class="n">WEIGHTS_NAME</span><span class="p">),</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">mirror</span><span class="o">=</span><span class="n">mirror</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Load from URL or cache if already cached</span>
                <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="n">cached_path</span><span class="p">(</span>
                    <span class="n">archive_file</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">EnvironmentError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Can&#39;t load weights for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39;. Make sure that:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;- &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is a correct model identifier listed on &#39;https://huggingface.co/models&#39;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;- or &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is the correct path to a directory containing a file named one of </span><span class="si">{</span><span class="n">WEIGHTS_NAME</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">TF2_WEIGHTS_NAME</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">TF_WEIGHTS_NAME</span><span class="si">}</span><span class="s2">.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">resolved_archive_file</span> <span class="o">==</span> <span class="n">archive_file</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading weights file </span><span class="si">{}</span><span class="s2"> from cache at </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">archive_file</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">resolved_archive_file</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">config</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

        <span class="c1"># Instantiate model.</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">model_args</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">from_tf</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unable to load weights from pytorch checkpoint file for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;at &#39;</span><span class="si">{</span><span class="n">resolved_archive_file</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
                    <span class="s2">&quot;If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. &quot;</span>
                <span class="p">)</span>

        <span class="n">missing_keys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">error_msgs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">from_tf</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">resolved_archive_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.index&quot;</span><span class="p">):</span>
                <span class="c1"># Load from a TensorFlow 1.X checkpoint - provided by original authors</span>
                <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">load_tf_weights</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">[:</span><span class="o">-</span><span class="mi">6</span><span class="p">])</span>  <span class="c1"># Remove the &#39;.index&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Load from our TensorFlow 2.0 checkpoints</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span> <span class="nn">.modeling_tf_pytorch_utils</span> <span class="kn">import</span> <span class="n">load_tf2_checkpoint_in_pytorch_model</span>

                    <span class="n">model</span> <span class="o">=</span> <span class="n">load_tf2_checkpoint_in_pytorch_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">resolved_archive_file</span><span class="p">,</span> <span class="n">allow_missing_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                        <span class="s2">&quot;Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see &quot;</span>
                        <span class="s2">&quot;https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.&quot;</span>
                    <span class="p">)</span>
                    <span class="k">raise</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Convert old format to new format if needed from a PyTorch state_dict</span>
            <span class="n">old_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">new_keys</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">new_key</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="s2">&quot;gamma&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;beta&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">new_key</span><span class="p">:</span>
                    <span class="n">old_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                    <span class="n">new_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_key</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">old_key</span><span class="p">,</span> <span class="n">new_key</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_keys</span><span class="p">,</span> <span class="n">new_keys</span><span class="p">):</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old_key</span><span class="p">)</span>

            <span class="c1"># copy state_dict so _load_from_state_dict can modify it</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s2">&quot;_metadata&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">metadata</span>

            <span class="c1"># PyTorch&#39;s `_load_from_state_dict` does not copy parameters in a module&#39;s descendants</span>
            <span class="c1"># so we need to apply the function recursively.</span>
            <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
                <span class="n">local_metadata</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">metadata</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">{})</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                    <span class="n">state_dict</span><span class="p">,</span>
                    <span class="n">prefix</span><span class="p">,</span>
                    <span class="n">local_metadata</span><span class="p">,</span>
                    <span class="kc">True</span><span class="p">,</span>
                    <span class="n">missing_keys</span><span class="p">,</span>
                    <span class="n">unexpected_keys</span><span class="p">,</span>
                    <span class="n">error_msgs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">child</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">load</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>

            <span class="c1"># Make sure we are able to load base models as well as derived models (with heads)</span>
            <span class="n">start_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">model_to_load</span> <span class="o">=</span> <span class="n">model</span>
            <span class="n">has_prefix_module</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">has_prefix_module</span><span class="p">:</span>
                <span class="n">start_prefix</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">has_prefix_module</span><span class="p">:</span>
                <span class="n">model_to_load</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span><span class="p">)</span>

            <span class="n">load</span><span class="p">(</span><span class="n">model_to_load</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">start_prefix</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="n">model_to_load</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">:</span>
                <span class="n">base_model_state_dict</span> <span class="o">=</span> <span class="n">model_to_load</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="n">head_model_state_dict_without_base_prefix</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">base_model_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">]</span>
                <span class="n">missing_keys</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">head_model_state_dict_without_base_prefix</span> <span class="o">-</span> <span class="n">base_model_state_dict</span><span class="p">)</span>

            <span class="c1"># Some models may have keys that are not in the state by design, removing them before needlessly warning</span>
            <span class="c1"># the user.</span>
            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_missing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">pat</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_missing</span><span class="p">:</span>
                    <span class="n">missing_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_unexpected</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">pat</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_keys_to_ignore_on_load_unexpected</span><span class="p">:</span>
                    <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">]</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unexpected_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Some weights of the model checkpoint at </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> were not used when &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;- This IS expected if you are initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> from the checkpoint of a model trained on another task &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;- This IS NOT expected if you are initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> from the checkpoint of a model that you expect &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All model checkpoint weights were used when initializing </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Some weights of </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> were not initialized from the model checkpoint at </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;and are newly initialized: </span><span class="si">{</span><span class="n">missing_keys</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;All the weights of </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> were initialized from the model checkpoint at </span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;If your task is similar to the task the model of the checkpoint was trained on, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;you can already use </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> for predictions without further training.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Error(s) in loading state_dict for </span><span class="si">{}</span><span class="s2">:</span><span class="se">\n\t</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">error_msgs</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="c1"># make sure token embedding weights are still tied if needed</span>
        <span class="n">model</span><span class="o">.</span><span class="n">tie_weights</span><span class="p">()</span>

        <span class="c1"># Set model in evaluation mode to deactivate DropOut modules by default</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">output_loading_info</span><span class="p">:</span>
            <span class="n">loading_info</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;missing_keys&quot;</span><span class="p">:</span> <span class="n">missing_keys</span><span class="p">,</span>
                <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">:</span> <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="s2">&quot;error_msgs&quot;</span><span class="p">:</span> <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">loading_info</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;xla_device&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">xla_device</span> <span class="ow">and</span> <span class="n">is_torch_tpu_available</span><span class="p">():</span>
            <span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

            <span class="n">model</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">send_cpu_data_to_device</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
            <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">model</span></div></div>


<div class="viewcode-block" id="Conv1D"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.Conv1D">[docs]</a><span class="k">class</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).</span>

<span class="sd">    Basically works like a linear layer but the weights are transposed.</span>

<span class="sd">    Args:</span>
<span class="sd">        nf (:obj:`int`): The number of output features.</span>
<span class="sd">        nx (:obj:`int`): The number of input features.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nf</span><span class="p">,</span> <span class="n">nx</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nf</span> <span class="o">=</span> <span class="n">nf</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">nf</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nf</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">size_out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nf</span><span class="p">,)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">size_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="PoolerStartLogits"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerStartLogits">[docs]</a><span class="k">class</span> <span class="nc">PoolerStartLogits</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute SQuAD start logits from sequence hidden states.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`~transformers.PretrainedConfig`):</span>
<span class="sd">            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="PoolerStartLogits.forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerStartLogits.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):</span>
<span class="sd">                The final hidden states of the model.</span>
<span class="sd">            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):</span>
<span class="sd">                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token</span>
<span class="sd">                should be masked.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.FloatTensor`: The start logits for SQuAD.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">65500</span> <span class="o">*</span> <span class="n">p_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1e30</span> <span class="o">*</span> <span class="n">p_mask</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="PoolerEndLogits"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerEndLogits">[docs]</a><span class="k">class</span> <span class="nc">PoolerEndLogits</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute SQuAD end logits from sequence hidden states.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`~transformers.PretrainedConfig`):</span>
<span class="sd">            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the</span>
<span class="sd">            :obj:`layer_norm_eps` to use.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="PoolerEndLogits.forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerEndLogits.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">start_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):</span>
<span class="sd">                The final hidden states of the model.</span>
<span class="sd">            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):</span>
<span class="sd">                The hidden states of the first tokens for the labeled span.</span>
<span class="sd">            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                The position of the first token for the labeled span.</span>
<span class="sd">            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):</span>
<span class="sd">                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token</span>
<span class="sd">                should be masked.</span>

<span class="sd">        .. note::</span>

<span class="sd">            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,</span>
<span class="sd">            ``start_positions`` overrides ``start_states``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.FloatTensor`: The end logits for SQuAD.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">start_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;One of start_states, start_positions should be not None&quot;</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">slen</span><span class="p">,</span> <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
            <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">start_states</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, hsz)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">65500</span> <span class="o">*</span> <span class="n">p_mask</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1e30</span> <span class="o">*</span> <span class="n">p_mask</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="PoolerAnswerClass"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerAnswerClass">[docs]</a><span class="k">class</span> <span class="nc">PoolerAnswerClass</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`~transformers.PretrainedConfig`):</span>
<span class="sd">            The config used by the model, will be used to grab the :obj:`hidden_size` of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<div class="viewcode-block" id="PoolerAnswerClass.forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.PoolerAnswerClass.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">start_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):</span>
<span class="sd">                The final hidden states of the model.</span>
<span class="sd">            start_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`, `optional`):</span>
<span class="sd">                The hidden states of the first tokens for the labeled span.</span>
<span class="sd">            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                The position of the first token for the labeled span.</span>
<span class="sd">            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.</span>

<span class="sd">        .. note::</span>

<span class="sd">            One of ``start_states`` or ``start_positions`` should be not obj:`None`. If both are set,</span>
<span class="sd">            ``start_positions`` overrides ``start_states``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.FloatTensor`: The SQuAD 2.0 answer class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.</span>
        <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">start_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;One of start_states, start_positions should be not None&quot;</span>
        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start_positions</span> <span class="o">=</span> <span class="n">start_positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, hsz)</span>

        <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, 1, hsz)</span>
            <span class="n">cls_token_state</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, hsz)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cls_token_state</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># shape (bsz, hsz)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_0</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">start_states</span><span class="p">,</span> <span class="n">cls_token_state</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="SquadHeadOutput"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.SquadHeadOutput">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SquadHeadOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for outputs of question answering models using a :class:`~transformers.modeling_utils.SQuADHead`.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned if both :obj:`start_positions` and :obj:`end_positions` are provided):</span>
<span class="sd">            Classification loss as the sum of start token, end token (and is_impossible if provided) classification</span>
<span class="sd">            losses.</span>
<span class="sd">        start_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):</span>
<span class="sd">            Log probabilities for the top config.start_n_top start token possibilities (beam-search).</span>
<span class="sd">        start_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):</span>
<span class="sd">            Indices for the top config.start_n_top start token possibilities (beam-search).</span>
<span class="sd">        end_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):</span>
<span class="sd">            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities</span>
<span class="sd">            (beam-search).</span>
<span class="sd">        end_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):</span>
<span class="sd">            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).</span>
<span class="sd">        cls_logits (``torch.FloatTensor`` of shape ``(batch_size,)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):</span>
<span class="sd">            Log probabilities for the ``is_impossible`` label of the answers.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">start_top_log_probs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">start_top_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">end_top_log_probs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">end_top_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">cls_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="SQuADHead"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.SQuADHead">[docs]</a><span class="k">class</span> <span class="nc">SQuADHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A SQuAD head inspired by XLNet.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`~transformers.PretrainedConfig`):</span>
<span class="sd">            The config used by the model, will be used to grab the :obj:`hidden_size` of the model and the</span>
<span class="sd">            :obj:`layer_norm_eps` to use.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">start_n_top</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">end_n_top</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">start_logits</span> <span class="o">=</span> <span class="n">PoolerStartLogits</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span> <span class="o">=</span> <span class="n">PoolerEndLogits</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span> <span class="o">=</span> <span class="n">PoolerAnswerClass</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<div class="viewcode-block" id="SQuADHead.forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.SQuADHead.forward">[docs]</a>    <span class="nd">@replace_return_docstrings</span><span class="p">(</span><span class="n">output_type</span><span class="o">=</span><span class="n">SquadHeadOutput</span><span class="p">,</span> <span class="n">config_class</span><span class="o">=</span><span class="n">PretrainedConfig</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">start_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">end_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_impossible</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">p_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">SquadHeadOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len, hidden_size)`):</span>
<span class="sd">                Final hidden states of the model on the sequence tokens.</span>
<span class="sd">            start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Positions of the first token for the labeled span.</span>
<span class="sd">            end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Positions of the last token for the labeled span.</span>
<span class="sd">            cls_index (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Position of the CLS token for each sentence in the batch. If :obj:`None`, takes the last token.</span>
<span class="sd">            is_impossible (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">                Whether the question has a possible answer in the paragraph or not.</span>
<span class="sd">            p_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `optional`):</span>
<span class="sd">                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token</span>
<span class="sd">                should be masked.</span>
<span class="sd">            return_dict (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
<span class="sd">                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.</span>

<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">start_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">start_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">end_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we are on multi-GPU, let&#39;s remove the dimension added by batch splitting</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">,</span> <span class="n">is_impossible</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">x</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># during training, compute the end logits based on the ground truth of the start position</span>
            <span class="n">end_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>

            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">start_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">start_positions</span><span class="p">)</span>
            <span class="n">end_loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">end_positions</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_loss</span> <span class="o">+</span> <span class="n">end_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">is_impossible</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Predict answerability from the representation of CLS and START</span>
                <span class="n">cls_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_positions</span><span class="o">=</span><span class="n">start_positions</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="n">cls_index</span><span class="p">)</span>
                <span class="n">loss_fct_cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
                <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">loss_fct_cls</span><span class="p">(</span><span class="n">cls_logits</span><span class="p">,</span> <span class="n">is_impossible</span><span class="p">)</span>

                <span class="c1"># note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss</span>
                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">cls_loss</span> <span class="o">*</span> <span class="mf">0.5</span>

            <span class="k">return</span> <span class="n">SquadHeadOutput</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="p">(</span><span class="n">total_loss</span><span class="p">,)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># during inference, compute the end logits based on beam search</span>
            <span class="n">bsz</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">hsz</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">start_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">start_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen)</span>

            <span class="n">start_top_log_probs</span><span class="p">,</span> <span class="n">start_top_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">start_log_probs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top)</span>
            <span class="n">start_top_index_exp</span> <span class="o">=</span> <span class="n">start_top_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hsz</span><span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">start_top_index_exp</span><span class="p">)</span>  <span class="c1"># shape (bsz, start_n_top, hsz)</span>
            <span class="n">start_states</span> <span class="o">=</span> <span class="n">start_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top, hsz)</span>

            <span class="n">hidden_states_expanded</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span>
                <span class="n">start_states</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top, hsz)</span>
            <span class="n">p_mask</span> <span class="o">=</span> <span class="n">p_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">p_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">end_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_logits</span><span class="p">(</span><span class="n">hidden_states_expanded</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="n">start_states</span><span class="p">,</span> <span class="n">p_mask</span><span class="o">=</span><span class="n">p_mask</span><span class="p">)</span>
            <span class="n">end_log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">end_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bsz, slen, start_n_top)</span>

            <span class="n">end_top_log_probs</span><span class="p">,</span> <span class="n">end_top_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
                <span class="n">end_log_probs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># shape (bsz, end_n_top, start_n_top)</span>
            <span class="n">end_top_log_probs</span> <span class="o">=</span> <span class="n">end_top_log_probs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">)</span>
            <span class="n">end_top_index</span> <span class="o">=</span> <span class="n">end_top_index</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_n_top</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_n_top</span><span class="p">)</span>

            <span class="n">start_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;blh,bl-&gt;bh&quot;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_log_probs</span><span class="p">)</span>
            <span class="n">cls_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer_class</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">start_states</span><span class="o">=</span><span class="n">start_states</span><span class="p">,</span> <span class="n">cls_index</span><span class="o">=</span><span class="n">cls_index</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">start_top_log_probs</span><span class="p">,</span> <span class="n">start_top_index</span><span class="p">,</span> <span class="n">end_top_log_probs</span><span class="p">,</span> <span class="n">end_top_index</span><span class="p">,</span> <span class="n">cls_logits</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">SquadHeadOutput</span><span class="p">(</span>
                    <span class="n">start_top_log_probs</span><span class="o">=</span><span class="n">start_top_log_probs</span><span class="p">,</span>
                    <span class="n">start_top_index</span><span class="o">=</span><span class="n">start_top_index</span><span class="p">,</span>
                    <span class="n">end_top_log_probs</span><span class="o">=</span><span class="n">end_top_log_probs</span><span class="p">,</span>
                    <span class="n">end_top_index</span><span class="o">=</span><span class="n">end_top_index</span><span class="p">,</span>
                    <span class="n">cls_logits</span><span class="o">=</span><span class="n">cls_logits</span><span class="p">,</span>
                <span class="p">)</span></div></div>


<div class="viewcode-block" id="SequenceSummary"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.SequenceSummary">[docs]</a><span class="k">class</span> <span class="nc">SequenceSummary</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute a single vector summary of a sequence hidden states.</span>

<span class="sd">    Args:</span>
<span class="sd">        config (:class:`~transformers.PretrainedConfig`):</span>
<span class="sd">            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual</span>
<span class="sd">            config class of your model for the default values it uses):</span>

<span class="sd">            - **summary_type** (:obj:`str`) -- The method to use to make this summary. Accepted values are:</span>

<span class="sd">                - :obj:`&quot;last&quot;` -- Take the last token hidden state (like XLNet)</span>
<span class="sd">                - :obj:`&quot;first&quot;` -- Take the first token hidden state (like Bert)</span>
<span class="sd">                - :obj:`&quot;mean&quot;` -- Take the mean of all tokens hidden states</span>
<span class="sd">                - :obj:`&quot;cls_index&quot;` -- Supply a Tensor of classification token position (GPT/GPT-2)</span>
<span class="sd">                - :obj:`&quot;attn&quot;` -- Not implemented now, use multi-head attention</span>

<span class="sd">            - **summary_use_proj** (:obj:`bool`) -- Add a projection after the vector extraction.</span>
<span class="sd">            - **summary_proj_to_labels** (:obj:`bool`) -- If :obj:`True`, the projection outputs to</span>
<span class="sd">              :obj:`config.num_labels` classes (otherwise to :obj:`config.hidden_size`).</span>
<span class="sd">            - **summary_activation** (:obj:`Optional[str]`) -- Set to :obj:`&quot;tanh&quot;` to add a tanh activation to the</span>
<span class="sd">              output, another string or :obj:`None` will add no activation.</span>
<span class="sd">            - **summary_first_dropout** (:obj:`float`) -- Optional dropout probability before the projection and</span>
<span class="sd">              activation.</span>
<span class="sd">            - **summary_last_dropout** (:obj:`float`)-- Optional dropout probability after the projection and</span>
<span class="sd">              activation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_type&quot;</span><span class="p">,</span> <span class="s2">&quot;last&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="c1"># We should use a standard multi-head attention module with absolute positional embedding for that.</span>
            <span class="c1"># Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276</span>
            <span class="c1"># We can probably just use the multi-head attention module of PyTorch &gt;=1.1.0</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_use_proj&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_use_proj</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_proj_to_labels&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_proj_to_labels</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_classes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="n">activation_string</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_activation&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">activation_string</span><span class="p">)</span> <span class="k">if</span> <span class="n">activation_string</span> <span class="k">else</span> <span class="n">Identity</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_first_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_first_dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;summary_last_dropout&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">summary_last_dropout</span><span class="p">)</span>

<div class="viewcode-block" id="SequenceSummary.forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.SequenceSummary.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute a single vector summary of a sequence hidden states.</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_states (:obj:`torch.FloatTensor` of shape :obj:`[batch_size, seq_len, hidden_size]`):</span>
<span class="sd">                The hidden states of the last layer.</span>
<span class="sd">            cls_index (:obj:`torch.LongTensor` of shape :obj:`[batch_size]` or :obj:`[batch_size, ...]` where ... are optional leading dimensions of :obj:`hidden_states`, `optional`):</span>
<span class="sd">                Used if :obj:`summary_type == &quot;cls_index&quot;` and takes the last token of the sequence as classification</span>
<span class="sd">                token.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`torch.FloatTensor`: The summary of the sequence hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;last&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;first&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;cls_index&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cls_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
                    <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">cls_index</span> <span class="o">=</span> <span class="n">cls_index</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">cls_index</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),))</span>
            <span class="c1"># shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">cls_index</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape (bsz, XX, hidden_size)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_type</span> <span class="o">==</span> <span class="s2">&quot;attn&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="prune_linear_layer"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.prune_linear_layer">[docs]</a><span class="k">def</span> <span class="nf">prune_linear_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prune a linear layer to keep only entries in index.</span>

<span class="sd">    Used to remove heads.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer (:obj:`torch.nn.Linear`): The layer to prune.</span>
<span class="sd">        index (:obj:`torch.LongTensor`): The indices to keep in the layer.</span>
<span class="sd">        dim (:obj:`int`, `optional`, defaults to 0): The dimension on which to keep the indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`torch.nn.Linear`: The pruned layer as a new layer with :obj:`requires_grad=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">new_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">new_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">new_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
        <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">new_layer</span></div>


<div class="viewcode-block" id="prune_conv1d_layer"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.prune_conv1d_layer">[docs]</a><span class="k">def</span> <span class="nf">prune_conv1d_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Conv1D</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights</span>
<span class="sd">    are transposed.</span>

<span class="sd">    Used to remove heads.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer (:class:`~transformers.modeling_utils.Conv1D`): The layer to prune.</span>
<span class="sd">        index (:obj:`torch.LongTensor`): The indices to keep in the layer.</span>
<span class="sd">        dim (:obj:`int`, `optional`, defaults to 1): The dimension on which to keep the indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with :obj:`requires_grad=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">new_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">new_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">new_layer</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">new_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
    <span class="n">new_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">new_layer</span></div>


<div class="viewcode-block" id="prune_layer"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.prune_layer">[docs]</a><span class="k">def</span> <span class="nf">prune_layer</span><span class="p">(</span>
    <span class="n">layer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">],</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prune a Conv1D or linear layer to keep only entries in index.</span>

<span class="sd">    Used to remove heads.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer (:obj:`Union[torch.nn.Linear, Conv1D]`): The layer to prune.</span>
<span class="sd">        index (:obj:`torch.LongTensor`): The indices to keep in the layer.</span>
<span class="sd">        dim (:obj:`int`, `optional`): The dimension on which to keep the indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`torch.nn.Linear` or :class:`~transformers.modeling_utils.Conv1D`: The pruned layer as a new layer with</span>
<span class="sd">        :obj:`requires_grad=True`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">prune_conv1d_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t prune layer of class </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="p">))</span></div>


<div class="viewcode-block" id="apply_chunking_to_forward"><a class="viewcode-back" href="../../internal/modeling_utils.html#transformers.modeling_utils.apply_chunking_to_forward">[docs]</a><span class="k">def</span> <span class="nf">apply_chunking_to_forward</span><span class="p">(</span>
    <span class="n">forward_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="n">input_tensors</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function chunks the :obj:`input_tensors` into smaller input tensor parts of size :obj:`chunk_size` over the</span>
<span class="sd">    dimension :obj:`chunk_dim`. It then applies a layer :obj:`forward_fn` to each chunk independently to save memory.</span>

<span class="sd">    If the :obj:`forward_fn` is independent across the :obj:`chunk_dim` this function will yield the same result as</span>
<span class="sd">    directly applying :obj:`forward_fn` to :obj:`input_tensors`.</span>

<span class="sd">    Args:</span>
<span class="sd">        forward_fn (:obj:`Callable[..., torch.Tensor]`):</span>
<span class="sd">            The forward function of the model.</span>
<span class="sd">        chunk_size (:obj:`int`):</span>
<span class="sd">            The chunk size of a chunked tensor: :obj:`num_chunks = len(input_tensors[0]) / chunk_size`.</span>
<span class="sd">        chunk_dim (:obj:`int`):</span>
<span class="sd">            The dimension over which the :obj:`input_tensors` should be chunked.</span>
<span class="sd">        input_tensors (:obj:`Tuple[torch.Tensor]`):</span>
<span class="sd">            The input tensors of ``forward_fn`` which will be chunked</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`torch.Tensor`: A tensor with the same shape as the :obj:`forward_fn` would have given if applied`.</span>


<span class="sd">    Examples::</span>

<span class="sd">        # rename the usual forward() fn to forward_chunk()</span>
<span class="sd">        def forward_chunk(self, hidden_states):</span>
<span class="sd">            hidden_states = self.decoder(hidden_states)</span>
<span class="sd">            return hidden_states</span>

<span class="sd">        # implement a chunked forward function</span>
<span class="sd">        def forward(self, hidden_states):</span>
<span class="sd">            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> has to be a tuple/list of tensors&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
    <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">chunk_dim</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">chunk_dim</span><span class="p">]</span> <span class="o">==</span> <span class="n">tensor_shape</span> <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">input_tensors</span>
    <span class="p">),</span> <span class="s2">&quot;All input tenors have to be of the same shape&quot;</span>

    <span class="c1"># inspect.signature exist since python 3.5 and is a python method -&gt; no problem with backward compatibility</span>
    <span class="n">num_args_in_forward_chunk_fn</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">num_args_in_forward_chunk_fn</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
        <span class="n">input_tensors</span>
    <span class="p">),</span> <span class="s2">&quot;forward_chunk_fn expects </span><span class="si">{}</span><span class="s2"> arguments, but only </span><span class="si">{}</span><span class="s2"> input tensors are given&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">num_args_in_forward_chunk_fn</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">chunk_dim</span><span class="p">]</span> <span class="o">%</span> <span class="n">chunk_size</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;The dimension to be chunked </span><span class="si">{}</span><span class="s2"> has to be a multiple of the chunk size </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">chunk_dim</span><span class="p">],</span> <span class="n">chunk_size</span>
        <span class="p">)</span>

        <span class="n">num_chunks</span> <span class="o">=</span> <span class="n">input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">chunk_dim</span><span class="p">]</span> <span class="o">//</span> <span class="n">chunk_size</span>

        <span class="c1"># chunk input tensor into tuples</span>
        <span class="n">input_tensors_chunks</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">num_chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">chunk_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_tensor</span> <span class="ow">in</span> <span class="n">input_tensors</span><span class="p">)</span>
        <span class="c1"># apply forward fn to every tuple</span>
        <span class="n">output_chunks</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">(</span><span class="o">*</span><span class="n">input_tensors_chunk</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_tensors_chunk</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">input_tensors_chunks</span><span class="p">))</span>
        <span class="c1"># concatenate output at same dimension</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">output_chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">chunk_dim</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">forward_fn</span><span class="p">(</span><span class="o">*</span><span class="n">input_tensors</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>