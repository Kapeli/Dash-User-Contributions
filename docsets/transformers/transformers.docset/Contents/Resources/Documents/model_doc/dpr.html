
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>DPR — transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="electra.html" rel="next" title="ELECTRA"/>
<link href="distilbert.html" rel="prev" title="DistilBERT"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using 🤗 Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">🤗 Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">DPR</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprconfig">DPRConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprcontextencodertokenizer">DPRContextEncoderTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprcontextencodertokenizerfast">DPRContextEncoderTokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprquestionencodertokenizer">DPRQuestionEncoderTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprquestionencodertokenizerfast">DPRQuestionEncoderTokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprreadertokenizer">DPRReaderTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprreadertokenizerfast">DPRReaderTokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dpr-specific-outputs">DPR specific outputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprcontextencoder">DPRContextEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprquestionencoder">DPRQuestionEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dprreader">DPRReader</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdprcontextencoder">TFDPRContextEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdprquestionencoder">TFDPRQuestionEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfdprreader">TFDPRReader</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> »</li>
<li>DPR</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/dpr.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="dpr">
<h1>DPR<a class="headerlink" href="#dpr" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&amp;A research. It was
introduced in <a class="reference external" href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> by
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.</p>
<p>The abstract from the paper is the following:</p>
<p><em>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.</em></p>
<p>The original code can be found <a class="reference external" href="https://github.com/facebookresearch/DPR">here</a>.</p>
</div>
<div class="section" id="dprconfig">
<h2>DPRConfig<a class="headerlink" href="#dprconfig" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRConfig"><a name="//apple_ref/cpp/Class/transformers.DPRConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_size</span><span class="o">=</span><span class="default_value">30522</span></em>, <em class="sig-param"><span class="n">hidden_size</span><span class="o">=</span><span class="default_value">768</span></em>, <em class="sig-param"><span class="n">num_hidden_layers</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">num_attention_heads</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">intermediate_size</span><span class="o">=</span><span class="default_value">3072</span></em>, <em class="sig-param"><span class="n">hidden_act</span><span class="o">=</span><span class="default_value">'gelu'</span></em>, <em class="sig-param"><span class="n">hidden_dropout_prob</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">attention_probs_dropout_prob</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">max_position_embeddings</span><span class="o">=</span><span class="default_value">512</span></em>, <em class="sig-param"><span class="n">type_vocab_size</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">initializer_range</span><span class="o">=</span><span class="default_value">0.02</span></em>, <em class="sig-param"><span class="n">layer_norm_eps</span><span class="o">=</span><span class="default_value">1e-12</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">gradient_checkpointing</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">position_embedding_type</span><span class="o">=</span><span class="default_value">'absolute'</span></em>, <em class="sig-param"><span class="n">projection_dim</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/configuration_dpr.html#DPRConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRConfig" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a> is the configuration class to store the configuration of a <cite>DPRModel</cite>.</p>
<p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.DPRContextEncoder" title="transformers.DPRContextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoder</span></code></a>,
<a class="reference internal" href="#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a>, or a <a class="reference internal" href="#transformers.DPRReader" title="transformers.DPRReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReader</span></code></a>. It is used to instantiate the
components of the DPR model.</p>
<p>This class is a subclass of <a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a>. Please check the superclass for the documentation of
all kwargs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 30522) – Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <cite>inputs_ids</cite>
passed to the forward method of <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</p></li>
<li><p><strong>hidden_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 768) – Dimensionality of the encoder layers and the pooler layer.</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) – Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>num_attention_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) – Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>intermediate_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 3072) – Dimensionality of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.</p></li>
<li><p><strong>hidden_act</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu"</span></code>) – The non-linear activation function (function or string) in the encoder and pooler. If string,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"relu"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"silu"</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gelu_new"</span></code> are supported.</p></li>
<li><p><strong>hidden_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>attention_probs_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – The dropout ratio for the attention probabilities.</p></li>
<li><p><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 512) – The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>type_vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 2) – The vocabulary size of the <cite>token_type_ids</cite> passed into <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) – The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>layer_norm_eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-12) – The epsilon used by the layer normalization layers.</p></li>
<li><p><strong>gradient_checkpointing</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – If True, use gradient checkpointing to save memory at the expense of slower backward pass.</p></li>
<li><p><strong>position_embedding_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"absolute"</span></code>) – Type of position embedding. Choose one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">"absolute"</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">"relative_key"</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"relative_key_query"</span></code>. For positional embeddings use <code class="xref py py-obj docutils literal notranslate"><span class="pre">"absolute"</span></code>. For more information on
<code class="xref py py-obj docutils literal notranslate"><span class="pre">"relative_key"</span></code>, please refer to <a class="reference external" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations (Shaw et al.)</a>. For more information on <code class="xref py py-obj docutils literal notranslate"><span class="pre">"relative_key_query"</span></code>, please refer to
<cite>Method 4</cite> in <a class="reference external" href="https://arxiv.org/abs/2009.13658">Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)</a>.</p></li>
<li><p><strong>projection_dim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 0) – Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="dprcontextencodertokenizer">
<h2>DPRContextEncoderTokenizer<a class="headerlink" href="#dprcontextencodertokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRContextEncoderTokenizer"><a name="//apple_ref/cpp/Class/transformers.DPRContextEncoderTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRContextEncoderTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">do_basic_tokenize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">never_split</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr.html#DPRContextEncoderTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRContextEncoderTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a DPRContextEncoder tokenizer.</p>
<p><a class="reference internal" href="#transformers.DPRContextEncoderTokenizer" title="transformers.DPRContextEncoderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoderTokenizer</span></code></a> is identical to <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> and runs
end-to-end tokenization: punctuation splitting and wordpiece.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> for usage examples and documentation concerning
parameters.</p>
</dd></dl>
</div>
<div class="section" id="dprcontextencodertokenizerfast">
<h2>DPRContextEncoderTokenizerFast<a class="headerlink" href="#dprcontextencodertokenizerfast" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRContextEncoderTokenizerFast"><a name="//apple_ref/cpp/Class/transformers.DPRContextEncoderTokenizerFast"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRContextEncoderTokenizerFast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">tokenizer_file</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr_fast.html#DPRContextEncoderTokenizerFast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRContextEncoderTokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a “fast” DPRContextEncoder tokenizer (backed by HuggingFace’s <cite>tokenizers</cite> library).</p>
<p><a class="reference internal" href="#transformers.DPRContextEncoderTokenizerFast" title="transformers.DPRContextEncoderTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoderTokenizerFast</span></code></a> is identical to <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> and
runs end-to-end tokenization: punctuation splitting and wordpiece.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> for usage examples and documentation concerning
parameters.</p>
<dl class="py attribute">
<dt id="transformers.DPRContextEncoderTokenizerFast.slow_tokenizer_class"><a name="//apple_ref/cpp/Attribute/transformers.DPRContextEncoderTokenizerFast.slow_tokenizer_class"></a>
<code class="sig-name descname">slow_tokenizer_class</code><a class="headerlink" href="#transformers.DPRContextEncoderTokenizerFast.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.dpr.tokenization_dpr.DPRContextEncoderTokenizer</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dprquestionencodertokenizer">
<h2>DPRQuestionEncoderTokenizer<a class="headerlink" href="#dprquestionencodertokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRQuestionEncoderTokenizer"><a name="//apple_ref/cpp/Class/transformers.DPRQuestionEncoderTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRQuestionEncoderTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">do_basic_tokenize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">never_split</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr.html#DPRQuestionEncoderTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRQuestionEncoderTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a DPRQuestionEncoder tokenizer.</p>
<p><a class="reference internal" href="#transformers.DPRQuestionEncoderTokenizer" title="transformers.DPRQuestionEncoderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoderTokenizer</span></code></a> is identical to <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> and runs
end-to-end tokenization: punctuation splitting and wordpiece.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> for usage examples and documentation concerning
parameters.</p>
</dd></dl>
</div>
<div class="section" id="dprquestionencodertokenizerfast">
<h2>DPRQuestionEncoderTokenizerFast<a class="headerlink" href="#dprquestionencodertokenizerfast" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRQuestionEncoderTokenizerFast"><a name="//apple_ref/cpp/Class/transformers.DPRQuestionEncoderTokenizerFast"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRQuestionEncoderTokenizerFast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">tokenizer_file</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr_fast.html#DPRQuestionEncoderTokenizerFast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRQuestionEncoderTokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a “fast” DPRQuestionEncoder tokenizer (backed by HuggingFace’s <cite>tokenizers</cite> library).</p>
<p><a class="reference internal" href="#transformers.DPRQuestionEncoderTokenizerFast" title="transformers.DPRQuestionEncoderTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoderTokenizerFast</span></code></a> is identical to <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> and
runs end-to-end tokenization: punctuation splitting and wordpiece.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> for usage examples and documentation concerning
parameters.</p>
<dl class="py attribute">
<dt id="transformers.DPRQuestionEncoderTokenizerFast.slow_tokenizer_class"><a name="//apple_ref/cpp/Attribute/transformers.DPRQuestionEncoderTokenizerFast.slow_tokenizer_class"></a>
<code class="sig-name descname">slow_tokenizer_class</code><a class="headerlink" href="#transformers.DPRQuestionEncoderTokenizerFast.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.dpr.tokenization_dpr.DPRQuestionEncoderTokenizer</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dprreadertokenizer">
<h2>DPRReaderTokenizer<a class="headerlink" href="#dprreadertokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRReaderTokenizer"><a name="//apple_ref/cpp/Class/transformers.DPRReaderTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRReaderTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">do_basic_tokenize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">never_split</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr.html#DPRReaderTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRReaderTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a DPRReader tokenizer.</p>
<p><a class="reference internal" href="#transformers.DPRReaderTokenizer" title="transformers.DPRReaderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderTokenizer</span></code></a> is almost identical to <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> and runs
end-to-end tokenization: punctuation splitting and wordpiece. The difference is that is has three inputs strings:
question, titles and texts that are combined to be fed to the <a class="reference internal" href="#transformers.DPRReader" title="transformers.DPRReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReader</span></code></a> model.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> for usage examples and documentation concerning
parameters.</p>
<p>Return a dictionary with the token ids of the input strings and other information to give to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">decode_best_spans</span></code>. It converts the strings of a question and different passages (title and text) in a
sequence of IDs (integers), using the tokenizer and vocabulary. The resulting <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> is a matrix of size
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code> with the format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="o">&lt;</span><span class="n">question</span> <span class="n">token</span> <span class="n">ids</span><span class="o">&gt;</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span> <span class="o">&lt;</span><span class="n">titles</span> <span class="n">ids</span><span class="o">&gt;</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span> <span class="o">&lt;</span><span class="n">texts</span> <span class="n">ids</span><span class="o">&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>questions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code class="xref py py-obj docutils literal notranslate"><span class="pre">[questions]</span> <span class="pre">*</span> <span class="pre">n_passages</span></code>. Otherwise you have to specify as many questions as
in <code class="xref py py-obj docutils literal notranslate"><span class="pre">titles</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">texts</span></code>.</p></li>
<li><p><strong>titles</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The passages titles to be encoded. This can be a string or a list of strings if there are several passages.</p></li>
<li><p><strong>texts</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The passages texts to be encoded. This can be a string or a list of strings if there are several passages.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair if a
pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided. This will only truncate
the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with sequence
lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) – <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – <p>Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A dictionary with the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code>: List of token ids to be fed to a model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>: List of indices specifying which tokens should be attended to by the model.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[List[int]]]</span></code></p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="dprreadertokenizerfast">
<h2>DPRReaderTokenizerFast<a class="headerlink" href="#dprreadertokenizerfast" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRReaderTokenizerFast"><a name="//apple_ref/cpp/Class/transformers.DPRReaderTokenizerFast"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRReaderTokenizerFast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">tokenizer_file</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">do_lower_case</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'[UNK]'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'[SEP]'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'[PAD]'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'[CLS]'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'[MASK]'</span></em>, <em class="sig-param"><span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">strip_accents</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/tokenization_dpr_fast.html#DPRReaderTokenizerFast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRReaderTokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a “fast” DPRReader tokenizer (backed by HuggingFace’s <cite>tokenizers</cite> library).</p>
<p><a class="reference internal" href="#transformers.DPRReaderTokenizerFast" title="transformers.DPRReaderTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderTokenizerFast</span></code></a> is almost identical to <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> and
runs end-to-end tokenization: punctuation splitting and wordpiece. The difference is that is has three inputs
strings: question, titles and texts that are combined to be fed to the <a class="reference internal" href="#transformers.DPRReader" title="transformers.DPRReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReader</span></code></a> model.</p>
<p>Refer to superclass <a class="reference internal" href="bert.html#transformers.BertTokenizerFast" title="transformers.BertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizerFast</span></code></a> for usage examples and documentation concerning
parameters.</p>
<p>Return a dictionary with the token ids of the input strings and other information to give to
<code class="xref py py-obj docutils literal notranslate"><span class="pre">decode_best_spans</span></code>. It converts the strings of a question and different passages (title and text) in a
sequence of IDs (integers), using the tokenizer and vocabulary. The resulting <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> is a matrix of size
<code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code> with the format:</p>
<p>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>questions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code class="xref py py-obj docutils literal notranslate"><span class="pre">[questions]</span> <span class="pre">*</span> <span class="pre">n_passages</span></code>. Otherwise you have to specify as many questions as
in <code class="xref py py-obj docutils literal notranslate"><span class="pre">titles</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">texts</span></code>.</p></li>
<li><p><strong>titles</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The passages titles to be encoded. This can be a string or a list of strings if there are several passages.</p></li>
<li><p><strong>texts</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>) – The passages texts to be encoded. This can be a string or a list of strings if there are several passages.</p></li>
<li><p><strong>padding</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.PaddingStrategy" title="transformers.tokenization_utils_base.PaddingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PaddingStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Activates and controls padding. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest'</span></code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'max_length'</span></code>: Pad to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_pad'</span></code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</p></li>
</ul>
</p></li>
<li><p><strong>truncation</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TruncationStrategy" title="transformers.tokenization_utils_base.TruncationStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncationStrategy</span></code></a>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) – <p>Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'longest_first'</span></code>: Truncate to a maximum length specified with the argument
<code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the maximum acceptable input length for the model if that argument is not
provided. This will truncate token by token, removing a token from the longest sequence in the pair if a
pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_first'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to the
maximum acceptable input length for the model if that argument is not provided. This will only truncate
the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'only_second'</span></code>: Truncate to a maximum length specified with the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_length</span></code> or to
the maximum acceptable input length for the model if that argument is not provided. This will only
truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">'do_not_truncate'</span></code> (default): No truncation (i.e., can output batch with sequence
lengths greater than the model maximum admissible input size).</p></li>
</ul>
</p></li>
<li><p><strong>max_length</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – <p>Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.</p>
</p></li>
<li><p><strong>return_tensors</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <a class="reference internal" href="../internal/tokenization_utils.html#transformers.tokenization_utils_base.TensorType" title="transformers.tokenization_utils_base.TensorType"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorType</span></code></a>, <cite>optional</cite>) – <p>If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'tf'</span></code>: Return TensorFlow <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.constant</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'pt'</span></code>: Return PyTorch <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">'np'</span></code>: Return Numpy <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code> objects.</p></li>
</ul>
</p></li>
<li><p><strong>return_attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – <p>Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer’s default, defined by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">return_outputs</span></code> attribute.</p>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A dictionary with the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_ids</span></code>: List of token ids to be fed to a model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>: List of indices specifying which tokens should be attended to by the model.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">List[List[int]]]</span></code></p>
</dd>
</dl>
<dl class="py attribute">
<dt id="transformers.DPRReaderTokenizerFast.slow_tokenizer_class"><a name="//apple_ref/cpp/Attribute/transformers.DPRReaderTokenizerFast.slow_tokenizer_class"></a>
<code class="sig-name descname">slow_tokenizer_class</code><a class="headerlink" href="#transformers.DPRReaderTokenizerFast.slow_tokenizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.dpr.tokenization_dpr.DPRReaderTokenizer</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dpr-specific-outputs">
<h2>DPR specific outputs<a class="headerlink" href="#dpr-specific-outputs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"><a name="//apple_ref/cpp/Class/transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.models.dpr.modeling_dpr.</code><code class="sig-name descname">DPRContextEncoderOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pooler_output</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRContextEncoderOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput" title="Permalink to this definition">¶</a></dt>
<dd><p>Class for outputs of <a class="reference internal" href="#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pooler_output</strong> – (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>):
The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"><a name="//apple_ref/cpp/Class/transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.models.dpr.modeling_dpr.</code><code class="sig-name descname">DPRQuestionEncoderOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pooler_output</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRQuestionEncoderOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput" title="Permalink to this definition">¶</a></dt>
<dd><p>Class for outputs of <a class="reference internal" href="#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pooler_output</strong> – (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>):
The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py class">
<dt id="transformers.models.dpr.modeling_dpr.DPRReaderOutput"><a name="//apple_ref/cpp/Class/transformers.models.dpr.modeling_dpr.DPRReaderOutput"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.models.dpr.modeling_dpr.</code><code class="sig-name descname">DPRReaderOutput</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start_logits</span><span class="p">:</span> <span class="n">torch.FloatTensor</span></em>, <em class="sig-param"><span class="n">end_logits</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">relevance_logits</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.FloatTensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Tuple<span class="p">[</span>torch.FloatTensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRReaderOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.models.dpr.modeling_dpr.DPRReaderOutput" title="Permalink to this definition">¶</a></dt>
<dd><p>Class for outputs of <a class="reference internal" href="#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_logits</strong> – (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>):
Logits of the start index of the span for each passage.</p></li>
<li><p><strong>end_logits</strong> – (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>):
Logits of the end index of the span for each passage.</p></li>
<li><p><strong>relevance_logits</strong> – (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor`</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">)</span></code>):
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</p></li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – <p>Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="dprcontextencoder">
<h2>DPRContextEncoder<a class="headerlink" href="#dprcontextencoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRContextEncoder"><a name="//apple_ref/cpp/Class/transformers.DPRContextEncoder"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRContextEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.dpr.configuration_dpr.DPRConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRContextEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRContextEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRContextEncoder transformer outputting pooler outputs as context representations.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.DPRContextEncoder.forward"><a name="//apple_ref/cpp/Method/transformers.DPRContextEncoder.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput">transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a><span class="p">, </span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRContextEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRContextEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DPRContextEncoder" title="transformers.DPRContextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoder</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<ol class="loweralpha simple">
<li><p>For sequence pairs (for a pair title+text for example):</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">:</span>         <span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="ow">is</span> <span class="n">this</span> <span class="n">jack</span> <span class="c1">##son ##ville ? [SEP] no it is not . [SEP]</span>
<span class="n">token_type_ids</span><span class="p">:</span>   <span class="mi">0</span>   <span class="mi">0</span>  <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>     <span class="mi">0</span>       <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>   <span class="mi">1</span>   <span class="mi">1</span>
</pre></div>
</div>
<ol class="loweralpha simple" start="2">
<li><p>For single sequences (for a question for example):</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">:</span>         <span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="n">the</span> <span class="n">dog</span> <span class="ow">is</span> <span class="n">hairy</span> <span class="o">.</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span>
<span class="n">token_type_ids</span><span class="p">:</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span>  <span class="mi">0</span>     <span class="mi">0</span>   <span class="mi">0</span>
</pre></div>
</div>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">DPRTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for
details. attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>,
<cite>optional</cite>): Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul class="simple">
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a> token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>): Segment token indices to indicate first and second
portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul class="simple">
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a> inputs_embeds (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>): Optionally, instead of passing
<code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation. This is useful if you want
more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated vectors than the model’s internal
embedding lookup matrix. output_attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>): Whether or not to return the
attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned tensors for more detail.
output_hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>): Whether or not to return the hidden states of all layers.
See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for more detail. return_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):
Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoderOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>pooler_output:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>) – The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DPRContextEncoder</span><span class="p">,</span> <span class="n">DPRContextEncoderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRContextEncoderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-ctx_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DPRContextEncoder</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-ctx_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, is my dog cute ?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRContextEncoderOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dprquestionencoder">
<h2>DPRQuestionEncoder<a class="headerlink" href="#dprquestionencoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRQuestionEncoder"><a name="//apple_ref/cpp/Class/transformers.DPRQuestionEncoder"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRQuestionEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.dpr.configuration_dpr.DPRConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRQuestionEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRQuestionEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.DPRQuestionEncoder.forward"><a name="//apple_ref/cpp/Method/transformers.DPRQuestionEncoder.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput">transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a><span class="p">, </span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRQuestionEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRQuestionEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<ol class="loweralpha simple">
<li><p>For sequence pairs (for a pair title+text for example):</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">:</span>         <span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="ow">is</span> <span class="n">this</span> <span class="n">jack</span> <span class="c1">##son ##ville ? [SEP] no it is not . [SEP]</span>
<span class="n">token_type_ids</span><span class="p">:</span>   <span class="mi">0</span>   <span class="mi">0</span>  <span class="mi">0</span>    <span class="mi">0</span>    <span class="mi">0</span>     <span class="mi">0</span>       <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>   <span class="mi">1</span>   <span class="mi">1</span>
</pre></div>
</div>
<ol class="loweralpha simple" start="2">
<li><p>For single sequences (for a question for example):</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span><span class="p">:</span>         <span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="n">the</span> <span class="n">dog</span> <span class="ow">is</span> <span class="n">hairy</span> <span class="o">.</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span>
<span class="n">token_type_ids</span><span class="p">:</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span>   <span class="mi">0</span>  <span class="mi">0</span>     <span class="mi">0</span>   <span class="mi">0</span>
</pre></div>
</div>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">DPRTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for
details. attention_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>,
<cite>optional</cite>): Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul class="simple">
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a> token_type_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>): Segment token indices to indicate first and second
portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul class="simple">
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a> inputs_embeds (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>): Optionally, instead of passing
<code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation. This is useful if you want
more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated vectors than the model’s internal
embedding lookup matrix. output_attentions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>): Whether or not to return the
attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned tensors for more detail.
output_hidden_states (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>): Whether or not to return the hidden states of all layers.
See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for more detail. return_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>):
Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p>
</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoderOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>pooler_output:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>) – The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DPRQuestionEncoder</span><span class="p">,</span> <span class="n">DPRQuestionEncoderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRQuestionEncoderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-question_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DPRQuestionEncoder</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-question_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, is my dog cute ?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput" title="transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoderOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="dprreader">
<h2>DPRReader<a class="headerlink" href="#dprreader" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.DPRReader"><a name="//apple_ref/cpp/Class/transformers.DPRReader"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">DPRReader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.dpr.configuration_dpr.DPRConfig</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRReader" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRReader transformer outputting span predictions.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel" title="transformers.PreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></a>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the model
weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.DPRReader.forward"><a name="//apple_ref/cpp/Method/transformers.DPRReader.forward"></a>
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRReaderOutput" title="transformers.models.dpr.modeling_dpr.DPRReaderOutput">transformers.models.dpr.modeling_dpr.DPRReaderOutput</a><span class="p">, </span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_dpr.html#DPRReader.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.DPRReader.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.DPRReader" title="transformers.DPRReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReader</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> – <p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.LongTensor]</span></code> of shapes <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>):
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> sequence
should be formatted with [CLS] and [SEP] with the format:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">&lt;question</span> <span class="pre">token</span> <span class="pre">ids&gt;</span> <span class="pre">[SEP]</span> <span class="pre">&lt;titles</span> <span class="pre">ids&gt;</span> <span class="pre">[SEP]</span> <span class="pre">&lt;texts</span> <span class="pre">ids&gt;</span></code></p>
</div></blockquote>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DPRReaderTokenizer" title="transformers.DPRReaderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderTokenizer</span></code></a>. See this class documentation for
more details.</p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRReaderOutput" title="transformers.models.dpr.modeling_dpr.DPRReaderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderOutput</span></code></a> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>
comprising various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>start_logits:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>) – Logits of the start index of the span for each passage.</p></li>
<li><p><strong>end_logits:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>) – Logits of the end index of the span for each passage.</p></li>
<li><p><strong>relevance_logits:</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor`</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">)</span></code>) – Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for the output of the embeddings + one for the output of each layer)
of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DPRReader</span><span class="p">,</span> <span class="n">DPRReaderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRReaderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-reader-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DPRReader</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-reader-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">questions</span><span class="o">=</span><span class="p">[</span><span class="s2">"What is love ?"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Haddaway"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s2">"'What Is Love' is a song recorded by the artist Haddaway"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'pt'</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">stat_logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relevance_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">relevance_logits</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#transformers.models.dpr.modeling_dpr.DPRReaderOutput" title="transformers.models.dpr.modeling_dpr.DPRReaderOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderOutput</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdprcontextencoder">
<h2>TFDPRContextEncoder<a class="headerlink" href="#tfdprcontextencoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFDPRContextEncoder"><a name="//apple_ref/cpp/Class/transformers.TFDPRContextEncoder"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDPRContextEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRContextEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRContextEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRContextEncoder transformer outputting pooler outputs as context representations.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)</p>
<p>This model is also a Tensorflow <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
<p>This second option is useful when using <code class="xref py py-meth docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having all
the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({"input_ids":</span> <span class="pre">input_ids,</span> <span class="pre">"token_type_ids":</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel.from_pretrained" title="transformers.TFPreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the
model weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TFDPRContextEncoder.call"><a name="//apple_ref/cpp/Method/transformers.TFDPRContextEncoder.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">training</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput<span class="p">, </span>Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRContextEncoder.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRContextEncoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDPRContextEncoder" title="transformers.TFDPRContextEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRContextEncoder</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<ol class="loweralpha">
<li><p>For sequence pairs (for a pair title+text for example):</p>
<p><code class="docutils literal notranslate"><span class="pre">tokens:</span> <span class="pre">[CLS]</span> <span class="pre">is</span> <span class="pre">this</span> <span class="pre">jack</span> <span class="pre">##son</span> <span class="pre">##ville</span> <span class="pre">?</span> <span class="pre">[SEP]</span> <span class="pre">no</span> <span class="pre">it</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">.</span> <span class="pre">[SEP]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">token_type_ids:</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span></code></p>
</li>
<li><p>For single sequences (for a question for example):</p>
<p><code class="docutils literal notranslate"><span class="pre">tokens:</span> <span class="pre">[CLS]</span> <span class="pre">the</span> <span class="pre">dog</span> <span class="pre">is</span> <span class="pre">hairy</span> <span class="pre">.</span> <span class="pre">[SEP]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">token_type_ids:</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span></code></p>
</li>
</ol>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">DPRTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for
details.</p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRContextEncoderOutput</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> comprising
various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>pooler_output:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>) – The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer) of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span>
<span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDPRContextEncoder</span><span class="p">,</span> <span class="n">DPRContextEncoderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRContextEncoderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-ctx_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDPRContextEncoder</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-ctx_encoder-single-nq-base'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, is my dog cute ?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRContextEncoderOutput</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdprquestionencoder">
<h2>TFDPRQuestionEncoder<a class="headerlink" href="#tfdprquestionencoder" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFDPRQuestionEncoder"><a name="//apple_ref/cpp/Class/transformers.TFDPRQuestionEncoder"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDPRQuestionEncoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRQuestionEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRQuestionEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)</p>
<p>This model is also a Tensorflow <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
<p>This second option is useful when using <code class="xref py py-meth docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having all
the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({"input_ids":</span> <span class="pre">input_ids,</span> <span class="pre">"token_type_ids":</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel.from_pretrained" title="transformers.TFPreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the
model weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TFDPRQuestionEncoder.call"><a name="//apple_ref/cpp/Method/transformers.TFDPRQuestionEncoder.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">training</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput<span class="p">, </span>Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRQuestionEncoder.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRQuestionEncoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDPRQuestionEncoder" title="transformers.TFDPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRQuestionEncoder</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>) – <p>Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<ol class="loweralpha">
<li><p>For sequence pairs (for a pair title+text for example):</p>
<p><code class="docutils literal notranslate"><span class="pre">tokens:</span> <span class="pre">[CLS]</span> <span class="pre">is</span> <span class="pre">this</span> <span class="pre">jack</span> <span class="pre">##son</span> <span class="pre">##ville</span> <span class="pre">?</span> <span class="pre">[SEP]</span> <span class="pre">no</span> <span class="pre">it</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">.</span> <span class="pre">[SEP]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">token_type_ids:</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span> <span class="pre">1</span></code></p>
</li>
<li><p>For single sequences (for a question for example):</p>
<p><code class="docutils literal notranslate"><span class="pre">tokens:</span> <span class="pre">[CLS]</span> <span class="pre">the</span> <span class="pre">dog</span> <span class="pre">is</span> <span class="pre">hairy</span> <span class="pre">.</span> <span class="pre">[SEP]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">token_type_ids:</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span></code></p>
</li>
</ol>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <code class="xref py py-class docutils literal notranslate"><span class="pre">DPRTokenizer</span></code>. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.encode()</span></code> and <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__" title="transformers.PreTrainedTokenizer.__call__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transformers.PreTrainedTokenizer.__call__()</span></code></a> for
details.</p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>token_type_ids</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code class="docutils literal notranslate"><span class="pre">[0,</span>
<span class="pre">1]</span></code>:</p>
<ul>
<li><p>0 corresponds to a <cite>sentence A</cite> token,</p></li>
<li><p>1 corresponds to a <cite>sentence B</cite> token.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#token-type-ids">What are token type IDs?</a></p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRQuestionEncoderOutput</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> comprising
various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>pooler_output:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embeddings_size)</span></code>) – The DPR encoder outputs the <cite>pooler_output</cite> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer) of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span> <span class="pre">sequence_length,</span>
<span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDPRQuestionEncoder</span><span class="p">,</span> <span class="n">DPRQuestionEncoderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRQuestionEncoderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-question_encoder-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDPRQuestionEncoder</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-question_encoder-single-nq-base'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, is my dog cute ?"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span><span class="p">)[</span><span class="s2">"input_ids"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRQuestionEncoderOutput</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfdprreader">
<h2>TFDPRReader<a class="headerlink" href="#tfdprreader" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFDPRReader"><a name="//apple_ref/cpp/Class/transformers.TFDPRReader"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFDPRReader</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRReader" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare DPRReader transformer outputting span predictions.</p>
<p>This model inherits from <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel" title="transformers.TFPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPreTrainedModel</span></code></a>. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)</p>
<p>This model is also a Tensorflow <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model">tf.keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TF 2.0 models accepts two formats as inputs:</p>
<ul class="simple">
<li><p>having all inputs as keyword arguments (like PyTorch models), or</p></li>
<li><p>having all inputs as a list, tuple or dict in the first positional arguments.</p></li>
</ul>
<p>This second option is useful when using <code class="xref py py-meth docutils literal notranslate"><span class="pre">tf.keras.Model.fit()</span></code> method which currently requires having all
the tensors in the first argument of the model call function: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs)</span></code>.</p>
<p>If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :</p>
<ul class="simple">
<li><p>a single Tensor with <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> only and nothing else: <code class="xref py py-obj docutils literal notranslate"><span class="pre">model(inputs_ids)</span></code></p></li>
<li><p>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask])</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">model([input_ids,</span> <span class="pre">attention_mask,</span> <span class="pre">token_type_ids])</span></code></p></li>
<li><p>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code class="xref py py-obj docutils literal notranslate"><span class="pre">model({"input_ids":</span> <span class="pre">input_ids,</span> <span class="pre">"token_type_ids":</span> <span class="pre">token_type_ids})</span></code></p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) – Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a class="reference internal" href="../main_classes/model.html#transformers.TFPreTrainedModel.from_pretrained" title="transformers.TFPreTrainedModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method to load the
model weights.</p>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.TFDPRReader.call"><a name="//apple_ref/cpp/Method/transformers.TFDPRReader.call"></a>
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_ids</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>bool<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_dict</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">training</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span> → Union<span class="p">[</span>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput<span class="p">, </span>Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/dpr/modeling_tf_dpr.html#TFDPRReader.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFDPRReader.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#transformers.TFDPRReader" title="transformers.TFDPRReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRReader</span></code></a> forward method, overrides the <code class="xref py py-func docutils literal notranslate"><span class="pre">__call__()</span></code> special method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> – <p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shapes <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>):
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> sequence
should be formatted with [CLS] and [SEP] with the format:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span> <span class="pre">&lt;question</span> <span class="pre">token</span> <span class="pre">ids&gt;</span> <span class="pre">[SEP]</span> <span class="pre">&lt;titles</span> <span class="pre">ids&gt;</span> <span class="pre">[SEP]</span> <span class="pre">&lt;texts</span> <span class="pre">ids&gt;</span></code></p>
</div></blockquote>
<p>DPR is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a class="reference internal" href="#transformers.DPRReaderTokenizer" title="transformers.DPRReaderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRReaderTokenizer</span></code></a>. See this class documentation for
more details.</p>
</p></li>
<li><p><strong>attention_mask</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>, <cite>optional</cite>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p><a class="reference external" href="../glossary.html#attention-mask">What are attention masks?</a></p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Numpy</span> <span class="pre">array</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>, <cite>optional</cite>) – Optionally, instead of passing <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code class="xref py py-obj docutils literal notranslate"><span class="pre">input_ids</span></code> indices into associated
vectors than the model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return the attentions tensors of all attention layers. See <code class="docutils literal notranslate"><span class="pre">attentions</span></code> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to rturn the hidden states of all layers. See <code class="docutils literal notranslate"><span class="pre">hidden_states</span></code> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>) – Whether or not to return a <a class="reference internal" href="../main_classes/output.html#transformers.file_utils.ModelOutput" title="transformers.file_utils.ModelOutput"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelOutput</span></code></a> instead of a plain tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRReaderOutput</span></code> (if
<code class="docutils literal notranslate"><span class="pre">return_dict=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.return_dict=True</span></code>) or a tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> comprising
various elements depending on the configuration (<a class="reference internal" href="#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a>) and inputs.</p>
<ul>
<li><p><strong>start_logits:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>) – Logits of the start index of the span for each passage.</p></li>
<li><p><strong>end_logits:</strong> (:obj:<code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">sequence_length)</span></code>) – Logits of the end index of the span for each passage.</p></li>
<li><p><strong>relevance_logits:</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor`</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(n_passages,</span> <span class="pre">)</span></code>) – Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p></li>
<li><p><strong>hidden_states</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_hidden_states=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_hidden_states=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code> (one for the output of the embeddings + one for the output of each layer) of
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">hidden_size)</span></code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li><p><strong>attentions</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(torch.FloatTensor)</span></code>, <cite>optional</cite>, returned when <code class="docutils literal notranslate"><span class="pre">output_attentions=True</span></code> is passed or when <code class="docutils literal notranslate"><span class="pre">config.output_attentions=True</span></code>) – Tuple of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> (one for each layer) of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_heads,</span>
<span class="pre">sequence_length,</span> <span class="pre">sequence_length)</span></code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDPRReader</span><span class="p">,</span> <span class="n">DPRReaderTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DPRReaderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-reader-single-nq-base'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFDPRReader</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'facebook/dpr-reader-single-nq-base'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">questions</span><span class="o">=</span><span class="p">[</span><span class="s2">"What is love ?"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Haddaway"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s2">"'What Is Love' is a song recorded by the artist Haddaway"</span><span class="p">],</span>
<span class="gp">... </span>        <span class="n">return_tensors</span><span class="o">=</span><span class="s1">'tf'</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relevance_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">relevance_logits</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRReaderOutput</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple(tf.Tensor)</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="electra.html" rel="next" title="ELECTRA">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="distilbert.html" rel="prev" title="DistilBERT"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>