
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>AutoModels — transformers 2.6.0 documentation</title>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<script src="../_static/js/modernizr.min.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/language_data.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="bert.html" rel="next" title="BERT"/>
<link href="../main_classes/processors.html" rel="prev" title="Processors"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model upload and sharing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Loading Google AI or OpenAI pre-trained weights or PyTorch dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html#serialization-best-practices">Serialization best-practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from pytorch-pretrained-bert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchscript.html">TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#schedules">Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html#gradient-strategies">Gradient Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">AutoModels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#autoconfig"><code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#autotokenizer"><code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodel"><code class="docutils literal notranslate"><span class="pre">AutoModel</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforpretraining"><code class="docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelwithlmhead"><code class="docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforsequenceclassification"><code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforquestionanswering"><code class="docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelfortokenclassification"><code class="docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">Bart</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a href="../index.html">Docs</a> »</li>
<li>AutoModels</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/auto.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="automodels">
<h1>AutoModels<a class="headerlink" href="#automodels" title="Permalink to this headline">¶</a></h1>
<p>In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the <code class="docutils literal notranslate"><span class="pre">from_pretrained</span></code> method.</p>
<p>AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:</p>
<p>Instantiating one of <code class="docutils literal notranslate"><span class="pre">AutoModel</span></code>, <code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> will directly create a class of the relevant architecture (ex: <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">AutoModel.from_pretrained('bert-base-cased')</span></code> will create a instance of <code class="docutils literal notranslate"><span class="pre">BertModel</span></code>).</p>
<div class="section" id="autoconfig">
<h2><code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code><a class="headerlink" href="#autoconfig" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoConfig"><a name="//apple_ref/cpp/Class/transformers.AutoConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoConfig</code><a class="reference internal" href="../_modules/transformers/configuration_auto.html#AutoConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoConfig" title="transformers.AutoConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoConfig</span></code></a> is a generic configuration class
that will be instantiated as one of the configuration classes of the library
when created with the <a class="reference internal" href="#transformers.AutoConfig.from_pretrained" title="transformers.AutoConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
<p>The <a class="reference internal" href="#transformers.AutoConfig.from_pretrained" title="transformers.AutoConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<dl class="method">
<dt id="transformers.AutoConfig.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoConfig.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/configuration_auto.html#AutoConfig.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the configuration classes of the library
from a pre-trained model configuration.</p>
<p>The configuration class to instantiate is selected
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: <a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite> : <a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> (CTRL model)</p></li>
<li><p>contains <cite>flaubert</cite> : <a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> (Flaubert model)</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>) – <dl class="simple">
<dt>Is either: </dt><dd><ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model configuration to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model configuration that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing a configuration file saved using the <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.save_pretrained" title="transformers.PretrainedConfig.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a saved configuration JSON <cite>file</cite>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/configuration.json</span></code>.</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, optional, defaults to <cite>None</cite>) – Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) – Force to (re-)download the model weights and configuration files and override the cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) – Do not delete incompletely received file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, optional, defaults to <cite>None</cite>) – A dictionary of proxy servers to use by protocol or endpoint, e.g.: <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span> <span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>.
The proxies are used on each request. See <a class="reference external" href="https://requests.readthedocs.io/en/master/user/advanced/#proxies">the requests documentation</a> for usage.</p></li>
<li><p><strong>return_unused_kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">boolean</span></code>, optional, defaults to <cite>False</cite>) – <ul>
<li><p>If False, then this function returns just the final configuration object.</p></li>
<li><p>If True, then this functions returns a tuple <cite>(config, unused_kwargs)</cite> where <cite>unused_kwargs</cite> is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update <cite>config</cite> and is otherwise ignored.</p></li>
</ul>
</p></li>
<li><p><strong>kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">any]</span></code>, optional, defaults to <cite>{}</cite>) – key/value pairs with which to update the configuration object after loading.
- The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.
- Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled by the <cite>return_unused_kwargs</cite> keyword parameter.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>  <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/'</span><span class="p">)</span>  <span class="c1"># E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/my_configuration.json'</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="n">config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                   <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="k">assert</span> <span class="n">unused_kwargs</span> <span class="o">==</span> <span class="p">{</span><span class="s1">'foo'</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="autotokenizer">
<h2><code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code><a class="headerlink" href="#autotokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoTokenizer"><a name="//apple_ref/cpp/Class/transformers.AutoTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoTokenizer</code><a class="reference internal" href="../_modules/transformers/tokenization_auto.html#AutoTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoTokenizer" title="transformers.AutoTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoTokenizer</span></code></a> is a generic tokenizer class
that will be instantiated as one of the tokenizer classes of the library
when created with the <cite>AutoTokenizer.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>The <cite>from_pretrained()</cite> method take care of returning the correct tokenizer class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The tokenizer class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: T5Tokenizer (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: DistilBertTokenizer (DistilBert model)</p></li>
<li><p>contains <cite>albert</cite>: AlbertTokenizer (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: CamembertTokenizer (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: XLMRobertaTokenizer (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: RobertaTokenizer (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: BertTokenizer (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: OpenAIGPTTokenizer (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: GPT2Tokenizer (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: TransfoXLTokenizer (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: XLNetTokenizer (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: XLMTokenizer (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite>: CTRLTokenizer (Salesforce CTRL model)</p></li>
</ul>
</div></blockquote>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throw an error).</p>
<dl class="method">
<dt id="transformers.AutoTokenizer.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoTokenizer.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/tokenization_auto.html#AutoTokenizer.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate one of the tokenizer classes of the library
from a pre-trained model vocabulary.</p>
<p>The tokenizer class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: T5Tokenizer (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: DistilBertTokenizer (DistilBert model)</p></li>
<li><p>contains <cite>albert</cite>: AlbertTokenizer (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: CamembertTokenizer (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: XLMRobertaTokenizer (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: RobertaTokenizer (RoBERTa model)</p></li>
<li><p>contains <cite>bert-base-japanese</cite>: BertJapaneseTokenizer (Bert model)</p></li>
<li><p>contains <cite>bert</cite>: BertTokenizer (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: OpenAIGPTTokenizer (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: GPT2Tokenizer (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: TransfoXLTokenizer (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: XLNetTokenizer (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: XLMTokenizer (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite>: CTRLTokenizer (Salesforce CTRL model)</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Params:</dt><dd><p>pretrained_model_name_or_path: either:</p>
<blockquote>
<div><ul class="simple">
<li><p>a string with the <cite>shortcut name</cite> of a predefined tokenizer to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a predefined tokenizer that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved using the <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer.save_pretrained" title="transformers.PreTrainedTokenizer.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>(not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>cache_dir: (<cite>optional</cite>) string:</dt><dd><p>Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.</p>
</dd>
<dt>force_download: (<cite>optional</cite>) boolean, default False:</dt><dd><p>Force to (re-)download the vocabulary files and override the cached versions if they exists.</p>
</dd>
<dt>resume_download: (<cite>optional</cite>) boolean, default False:</dt><dd><p>Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p>
</dd>
<dt>proxies: (<cite>optional</cite>) dict, default None:</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p>
</dd>
<dt>use_fast: (<cite>optional</cite>) boolean, default False:</dt><dd><p>Indicate if transformers should try to load the fast version of the tokenizer (True) or use the Python one (False).</p>
</dd>
</dl>
<p>inputs: (<cite>optional</cite>) positional arguments: will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>kwargs: (<cite>optional</cite>) keyword arguments: will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. Can be used to set special tokens like <code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>, <code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the doc string of <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> for details.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download vocabulary from S3 and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="c1"># Download vocabulary from S3 (user-uploaded) and cache.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'dbmdz/bert-base-german-cased'</span><span class="p">)</span>

<span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/'</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodel">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModel</span></code><a class="headerlink" href="#automodel" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModel"><a name="//apple_ref/cpp/Class/transformers.AutoModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModel</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModel" title="transformers.AutoModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModel</span></code></a> is a generic model class
that will be instantiated as one of the base model classes of the library
when created with the <cite>AutoModel.from_pretrained(pretrained_model_name_or_path)</cite>
or the <cite>AutoModel.from_config(config)</cite> class methods.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModel.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModel.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (Bert model)</p></li>
<li><p>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (Salesforce CTRL  model)</p></li>
<li><p>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</p></li>
<li><p>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertModel" title="transformers.FlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertModel</span></code></a> (XLM model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The base model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a> (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertModel" title="transformers.AlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertModel</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertModel" title="transformers.CamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModel</span></code></a> (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaModel" title="transformers.XLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModel</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (Salesforce CTRL  model)</p></li>
<li><p>contains <cite>flaubert</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">Flaubert</span></code> (Flaubert  model)</p></li>
</ul>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforpretraining">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code><a class="headerlink" href="#automodelforpretraining" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForPreTraining"><a name="//apple_ref/cpp/Class/transformers.AutoModelForPreTraining"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForPreTraining</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForPreTraining" title="transformers.AutoModelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForPreTraining</span></code></a> is a generic model class
that will be instantiated as one of the model classes of the library -with the architecture used for pretraining this model– when created with the <cite>AutoModelForPreTraining.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModelForPreTraining.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForPreTraining.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (Bert model)</p></li>
<li><p>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL  model)</p></li>
<li><p>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModelForPreTraining.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForPreTraining.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForPreTraining.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the model classes of the library -with the architecture used for pretraining this model– from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">T5ModelWithLMHead</span></code> (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL model)</p></li>
<li><p>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>Either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely received file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelwithlmhead">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code><a class="headerlink" href="#automodelwithlmhead" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelWithLMHead"><a name="//apple_ref/cpp/Class/transformers.AutoModelWithLMHead"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelWithLMHead</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelWithLMHead" title="transformers.AutoModelWithLMHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelWithLMHead</span></code></a> is a generic model class
that will be instantiated as one of the language modeling model classes of the library
when created with the <cite>AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModelWithLMHead.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelWithLMHead.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (Bert model)</p></li>
<li><p>isInstance of <cite>openai-gpt</cite> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>isInstance of <cite>gpt2</cite> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>isInstance of <cite>ctrl</cite> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL  model)</p></li>
<li><p>isInstance of <cite>transfo-xl</cite> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModelWithLMHead.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelWithLMHead.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelWithLMHead.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelWithLMHead.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the language modeling model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>t5</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">T5ModelWithLMHead</span></code> (T5 model)</p></li>
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>openai-gpt</cite>: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p>contains <cite>gpt2</cite>: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p>contains <cite>transfo-xl</cite>: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p>contains <cite>ctrl</cite>: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (Salesforce CTRL model)</p></li>
<li><p>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (Flaubert model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>Either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely received file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforsequenceclassification">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code><a class="headerlink" href="#automodelforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.AutoModelForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForSequenceClassification</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForSequenceClassification" title="transformers.AutoModelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code></a> is a generic model class
that will be instantiated as one of the sequence classification model classes of the library
when created with the <cite>AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModelForSequenceClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSequenceClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>albert</cite> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p>isInstance of <cite>camembert</cite> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p>isInstance of <cite>xlm roberta</cite> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>isInstance of <cite>roberta</cite> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (Bert model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForSequenceClassification" title="transformers.XLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForSequenceClassification</span></code></a> (XLM model)</p></li>
<li><p>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (Flaubert model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModelForSequenceClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSequenceClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForSequenceClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the sequence classification model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (Flaubert model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaining positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforquestionanswering">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code><a class="headerlink" href="#automodelforquestionanswering" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.AutoModelForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForQuestionAnswering</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForQuestionAnswering" title="transformers.AutoModelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForQuestionAnswering</span></code></a> is a generic model class
that will be instantiated as one of the question answering model classes of the library
when created with the <cite>AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModelForQuestionAnswering.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForQuestionAnswering.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>albert</cite> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModelForQuestionAnswering</span></code> (Bert model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnswering" title="transformers.XLNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnswering</span></code></a> (XLNet model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnswering" title="transformers.XLMForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnswering</span></code></a> (XLM model)</p></li>
<li><p>isInstance of <cite>flaubert</cite> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnswering" title="transformers.FlaubertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnswering</span></code></a> (XLM model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModelForQuestionAnswering.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForQuestionAnswering.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForQuestionAnswering.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the question answering model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>distilbert</cite>: <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p>contains <cite>albert</cite>: <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForQuestionAnswering" title="transformers.BertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnswering" title="transformers.XLNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnswering</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>xlm</cite>: <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnswering" title="transformers.XLMForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnswering</span></code></a> (XLM model)</p></li>
<li><p>contains <cite>flaubert</cite>: <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnswering" title="transformers.FlaubertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnswering</span></code></a> (XLM model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelfortokenclassification">
<h2><code class="docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code><a class="headerlink" href="#automodelfortokenclassification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="transformers.AutoModelForTokenClassification"><a name="//apple_ref/cpp/Class/transformers.AutoModelForTokenClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForTokenClassification</code><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#transformers.AutoModelForTokenClassification" title="transformers.AutoModelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModelForTokenClassification</span></code></a> is a generic model class
that will be instantiated as one of the token classification model classes of the library
when created with the <cite>AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path)</cite>
class method.</p>
<p>This class cannot be instantiated using <cite>__init__()</cite> (throws an error).</p>
<dl class="method">
<dt id="transformers.AutoModelForTokenClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTokenClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) – <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p>isInstance of <cite>distilbert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModelForTokenClassification</span></code> (DistilBERT model)</p></li>
<li><p>isInstance of <cite>xlm</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code> (XLM model)</p></li>
<li><p>isInstance of <cite>xlm roberta</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModelForTokenClassification</span></code> (XLMRoberta model)</p></li>
<li><p>isInstance of <cite>bert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModelForTokenClassification</span></code> (Bert model)</p></li>
<li><p>isInstance of <cite>albert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForTokenClassification</span></code> (AlBert model)</p></li>
<li><p>isInstance of <cite>xlnet</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModelForTokenClassification</span></code> (XLNet model)</p></li>
<li><p>isInstance of <cite>camembert</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModelForTokenClassification</span></code> (Camembert model)</p></li>
<li><p>isInstance of <cite>roberta</cite> configuration class: <code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModelForTokenClassification</span></code> (Roberta model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="transformers.AutoModelForTokenClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTokenClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param">pretrained_model_name_or_path</em>, <em class="sig-param">*model_args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/modeling_auto.html#AutoModelForTokenClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates one of the question answering model classes of the library
from a pre-trained model configuration.</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string.</p>
<p>The model class to instantiate is selected as the first pattern matching
in the <cite>pretrained_model_name_or_path</cite> string (in the following order):</p>
<blockquote>
<div><ul class="simple">
<li><p>contains <cite>distilbert</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForTokenClassification</span></code> (DistilBERT model)</p></li>
<li><p>contains <cite>xlm</cite>: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code> (XLM model)</p></li>
<li><p>contains <cite>xlm-roberta</cite>: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForTokenClassification" title="transformers.XLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa?Para model)</p></li>
<li><p>contains <cite>camembert</cite>: <a class="reference internal" href="camembert.html#transformers.CamembertForTokenClassification" title="transformers.CamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForTokenClassification</span></code></a> (Camembert model)</p></li>
<li><p>contains <cite>bert</cite>: <a class="reference internal" href="bert.html#transformers.BertForTokenClassification" title="transformers.BertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code></a> (Bert model)</p></li>
<li><p>contains <cite>xlnet</cite>: <a class="reference internal" href="xlnet.html#transformers.XLNetForTokenClassification" title="transformers.XLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForTokenClassification</span></code></a> (XLNet model)</p></li>
<li><p>contains <cite>roberta</cite>: <a class="reference internal" href="roberta.html#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a> (Roberta model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>Either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaning positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionnary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_model/'</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained('./test/saved_model/')`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="bert.html" rel="next" title="BERT">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="../main_classes/processors.html" rel="prev" title="Processors"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, huggingface

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    ga('send', 'pageview');
    </script>
</body>
</html>