
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Auto Classes â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="bart.html" rel="next" title="BART"/>
<link href="albert.html" rel="prev" title="ALBERT"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Auto Classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#autoconfig">AutoConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#autotokenizer">AutoTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodel">AutoModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforpretraining">AutoModelForPreTraining</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforcausallm">AutoModelForCausalLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelformaskedlm">AutoModelForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforseq2seqlm">AutoModelForSeq2SeqLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforsequenceclassification">AutoModelForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelformultiplechoice">AutoModelForMultipleChoice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelfornextsentenceprediction">AutoModelForNextSentencePrediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelfortokenclassification">AutoModelForTokenClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelforquestionanswering">AutoModelForQuestionAnswering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automodelfortablequestionanswering">AutoModelForTableQuestionAnswering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodel">TFAutoModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelforpretraining">TFAutoModelForPreTraining</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelforcausallm">TFAutoModelForCausalLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelformaskedlm">TFAutoModelForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelforseq2seqlm">TFAutoModelForSeq2SeqLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelforsequenceclassification">TFAutoModelForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelformultiplechoice">TFAutoModelForMultipleChoice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelfortokenclassification">TFAutoModelForTokenClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfautomodelforquestionanswering">TFAutoModelForQuestionAnswering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#flaxautomodel">FlaxAutoModel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Auto Classes</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/auto.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="auto-classes">
<h1>Auto Classes<a class="headerlink" href="#auto-classes" title="Permalink to this headline">Â¶</a></h1>
<p>In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the <code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.</p>
<p>Instantiating one of <a class="reference internal" href="#transformers.AutoConfig" title="transformers.AutoConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoConfig</span></code></a>, <a class="reference internal" href="#transformers.AutoModel" title="transformers.AutoModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoModel</span></code></a>, and
<a class="reference internal" href="#transformers.AutoTokenizer" title="transformers.AutoTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">AutoTokenizer</span></code></a> will directly create a class of the relevant architecture. For instance</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-cased'</span><span class="p">)</span>
</pre></div>
</div>
<p>will create a model that is an instance of <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a>.</p>
<p>There is one class of <code class="xref py py-obj docutils literal notranslate"><span class="pre">AutoModel</span></code> for each task, and for each backend (PyTorch or TensorFlow).</p>
<div class="section" id="autoconfig">
<h2>AutoConfig<a class="headerlink" href="#autoconfig" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoConfig"><a name="//apple_ref/cpp/Class/transformers.AutoConfig"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoConfig</code><a class="reference internal" href="../_modules/transformers/models/auto/configuration_auto.html#AutoConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the <a class="reference internal" href="#transformers.AutoConfig.from_pretrained" title="transformers.AutoConfig.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoConfig.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoConfig.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/configuration_auto.html#AutoConfig.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoConfig.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the configuration classes of the library from a pretrained model configuration.</p>
<p>The configuration class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object
that is loaded, or when itâ€™s missing, by falling back to using pattern matching on
<code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> (LED model)</p></li>
<li><p><strong>blenderbot-small</strong> â€“ <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><strong>retribert</strong> â€“ <a class="reference internal" href="retribert.html#transformers.RetriBertConfig" title="transformers.RetriBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertConfig</span></code></a> (RetriBERT model)</p></li>
<li><p><strong>mt5</strong> â€“ <a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> (mT5 model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> (ALBERT model)</p></li>
<li><p><strong>bert-generation</strong> â€“ <a class="reference internal" href="bertgeneration.html#transformers.BertGenerationConfig" title="transformers.BertGenerationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationConfig</span></code></a> (Bert Generation model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>pegasus</strong> â€“ <a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> (Pegasus model)</p></li>
<li><p><strong>marian</strong> â€“ <a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> (Marian model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> (BART model)</p></li>
<li><p><strong>blenderbot</strong> â€“ <a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> (Blenderbot model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>deberta</strong> â€“ <a class="reference internal" href="deberta.html#transformers.DebertaConfig" title="transformers.DebertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaConfig</span></code></a> (DeBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>fsmt</strong> â€“ <a class="reference internal" href="fsmt.html#transformers.FSMTConfig" title="transformers.FSMTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTConfig</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm-prophetnet</strong> â€“ <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetConfig" title="transformers.XLMProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetConfig</span></code></a> (XLMProphetNet model)</p></li>
<li><p><strong>prophetnet</strong> â€“ <a class="reference internal" href="prophetnet.html#transformers.ProphetNetConfig" title="transformers.ProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetConfig</span></code></a> (ProphetNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>encoder-decoder</strong> â€“ <a class="reference internal" href="encoderdecoder.html#transformers.EncoderDecoderConfig" title="transformers.EncoderDecoderConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">EncoderDecoderConfig</span></code></a> (Encoder decoder model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> (LXMERT model)</p></li>
<li><p><strong>dpr</strong> â€“ <a class="reference internal" href="dpr.html#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a> (DPR model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>rag</strong> â€“ <a class="reference internal" href="rag.html#transformers.RagConfig" title="transformers.RagConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RagConfig</span></code></a> (RAG model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or
namespaced under a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing a configuration file saved using the
<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.save_pretrained" title="transformers.PretrainedConfig.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, or the
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> method, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a saved configuration JSON <cite>file</cite>, e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/configuration.json</span></code>.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, <cite>optional</cite>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>return_unused_kwargs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ <p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>, then this function returns just the final configuration object.</p>
<p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>, then this functions returns a <code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(config,</span> <span class="pre">unused_kwargs)</span></code> where <cite>unused_kwargs</cite>
is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,
the part of <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> which has not been used to update <code class="docutils literal notranslate"><span class="pre">config</span></code> and is otherwise ignored.</p>
</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code class="docutils literal notranslate"><span class="pre">return_unused_kwargs</span></code> keyword parameter.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'dbmdz/bert-base-german-cased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If configuration file is in a directory (e.g., was saved using `save_pretrained('./test/saved_model/')`).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load a specific configuration file.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/my_configuration.json'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Change some config attributes when loading a pretrained config.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">.</span><span class="n">unused_kwargs</span>
<span class="go">{'foo': False}</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="autotokenizer">
<h2>AutoTokenizer<a class="headerlink" href="#autotokenizer" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoTokenizer"><a name="//apple_ref/cpp/Class/transformers.AutoTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoTokenizer</code><a class="reference internal" href="../_modules/transformers/models/auto/tokenization_auto.html#AutoTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the <a class="reference internal" href="#transformers.AutoTokenizer.from_pretrained" title="transformers.AutoTokenizer.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">AutoTokenizer.from_pretrained()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoTokenizer.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoTokenizer.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/tokenization_auto.html#AutoTokenizer.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoTokenizer.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.</p>
<p>The tokenizer class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object
(either passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s
missing, by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDTokenizer" title="transformers.LEDTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDTokenizer</span></code></a> (LED model)</p></li>
<li><p><strong>blenderbot-small</strong> â€“ <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallTokenizer" title="transformers.BlenderbotSmallTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallTokenizer</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><strong>retribert</strong> â€“ <a class="reference internal" href="retribert.html#transformers.RetriBertTokenizer" title="transformers.RetriBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertTokenizer</span></code></a> (RetriBERT model)</p></li>
<li><p><strong>mt5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5TokenizerFast" title="transformers.T5TokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5TokenizerFast</span></code></a> (mT5 model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5TokenizerFast" title="transformers.T5TokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5TokenizerFast</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertTokenizer" title="transformers.MobileBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertTokenizer</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertTokenizer" title="transformers.DistilBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertTokenizer</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertTokenizerFast" title="transformers.AlbertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertTokenizerFast</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertTokenizerFast" title="transformers.CamembertTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertTokenizerFast</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaTokenizerFast" title="transformers.XLMRobertaTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaTokenizerFast</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>pegasus</strong> â€“ <a class="reference internal" href="pegasus.html#transformers.PegasusTokenizerFast" title="transformers.PegasusTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusTokenizerFast</span></code></a> (Pegasus model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartTokenizerFast" title="transformers.MBartTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartTokenizerFast</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetTokenizer" title="transformers.MPNetTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetTokenizer</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartTokenizer" title="transformers.BartTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartTokenizer</span></code></a> (BART model)</p></li>
<li><p><strong>blenderbot</strong> â€“ <a class="reference internal" href="blenderbot.html#transformers.BlenderbotTokenizer" title="transformers.BlenderbotTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotTokenizer</span></code></a> (Blenderbot model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerTokenizerFast" title="transformers.ReformerTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerTokenizerFast</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerTokenizer" title="transformers.LongformerTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerTokenizer</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaTokenizer" title="transformers.RobertaTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaTokenizer</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>deberta</strong> â€“ <a class="reference internal" href="deberta.html#transformers.DebertaTokenizer" title="transformers.DebertaTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaTokenizer</span></code></a> (DeBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertTokenizer" title="transformers.FlaubertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertTokenizer</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>fsmt</strong> â€“ <a class="reference internal" href="fsmt.html#transformers.FSMTTokenizer" title="transformers.FSMTTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTTokenizer</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertTokenizer" title="transformers.SqueezeBertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertTokenizer</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertTokenizer" title="transformers.BertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertTokenizer</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTTokenizer" title="transformers.OpenAIGPTTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTTokenizer</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2Tokenizer" title="transformers.GPT2Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Tokenizer</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLTokenizer" title="transformers.TransfoXLTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLTokenizer</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetTokenizerFast" title="transformers.XLNetTokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetTokenizerFast</span></code></a> (XLNet model)</p></li>
<li><p><strong>prophetnet</strong> â€“ <a class="reference internal" href="prophetnet.html#transformers.ProphetNetTokenizer" title="transformers.ProphetNetTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetTokenizer</span></code></a> (ProphetNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMTokenizer" title="transformers.XLMTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMTokenizer</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLTokenizer" title="transformers.CTRLTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLTokenizer</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraTokenizer" title="transformers.ElectraTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraTokenizer</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelTokenizer" title="transformers.FunnelTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelTokenizer</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.LxmertTokenizer" title="transformers.LxmertTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertTokenizer</span></code></a> (LXMERT model)</p></li>
<li><p><strong>dpr</strong> â€“ <a class="reference internal" href="dpr.html#transformers.DPRQuestionEncoderTokenizer" title="transformers.DPRQuestionEncoderTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoderTokenizer</span></code></a> (DPR model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMTokenizer" title="transformers.LayoutLMTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMTokenizer</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>rag</strong> â€“ <a class="reference internal" href="rag.html#transformers.RagTokenizer" title="transformers.RagTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">RagTokenizer</span></code></a> (RAG model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasTokenizer" title="transformers.TapasTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasTokenizer</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Params:</dt><dd><dl>
<dt>pretrained_model_name_or_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>A string, the <cite>model id</cite> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing vocabulary files required by the tokenizer, for instance saved
using the <code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code> method, e.g.,
<code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/vocab.txt</span></code>. (Not
applicable to all derived classes)</p></li>
</ul>
</div></blockquote>
</dd>
<dt>inputs (additional positional arguments, <cite>optional</cite>):</dt><dd><p>Will be passed along to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p>
</dd>
<dt>config (<code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedConfig</span></code>, <cite>optional</cite>)</dt><dd><p>The configuration object used to dertermine the tokenizer class to instantiate.</p>
</dd>
<dt>cache_dir (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>):</dt><dd><p>Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p>
</dd>
<dt>force_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.</p>
</dd>
<dt>resume_download (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>):</dt><dd><p>Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p>
</dd>
<dt>proxies (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str]</span></code>, <cite>optional</cite>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p>
</dd>
<dt>revision(<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p>
</dd>
<dt>subfolder (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.</p>
</dd>
<dt>use_fast (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>):</dt><dd><p>Whether or not to try to load the fast version of the tokenizer.</p>
</dd>
<dt>kwargs (additional keyword arguments, <cite>optional</cite>):</dt><dd><p>Will be passed to the Tokenizer <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method. Can be used to set special tokens like
<code class="docutils literal notranslate"><span class="pre">bos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">eos_token</span></code>, <code class="docutils literal notranslate"><span class="pre">unk_token</span></code>, <code class="docutils literal notranslate"><span class="pre">sep_token</span></code>, <code class="docutils literal notranslate"><span class="pre">pad_token</span></code>, <code class="docutils literal notranslate"><span class="pre">cls_token</span></code>,
<code class="docutils literal notranslate"><span class="pre">mask_token</span></code>, <code class="docutils literal notranslate"><span class="pre">additional_special_tokens</span></code>. See parameters in the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> for more details.</p>
</dd>
</dl>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download vocabulary from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'dbmdz/bert-base-german-cased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./test/bert_saved_model/'</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodel">
<h2>AutoModel<a class="headerlink" href="#automodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModel"><a name="//apple_ref/cpp/Class/transformers.AutoModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModel</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the base model classes of the library when
created with the <a class="reference internal" href="#transformers.AutoModel.from_pretrained" title="transformers.AutoModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModel.from_config" title="transformers.AutoModel.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class methods.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModel.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModel.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModel.from_pretrained" title="transformers.AutoModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.LEDModel" title="transformers.LEDModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDModel</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallModel" title="transformers.BlenderbotSmallModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallModel</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><a class="reference internal" href="retribert.html#transformers.RetriBertConfig" title="transformers.RetriBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertConfig</span></code></a> configuration class: <a class="reference internal" href="retribert.html#transformers.RetriBertModel" title="transformers.RetriBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertModel</span></code></a> (RetriBERT model)</p></li>
<li><p><a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> configuration class: <a class="reference internal" href="mt5.html#transformers.MT5Model" title="transformers.MT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Model</span></code></a> (mT5 model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> configuration class: <a class="reference internal" href="pegasus.html#transformers.PegasusModel" title="transformers.PegasusModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusModel</span></code></a> (Pegasus model)</p></li>
<li><p><a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> configuration class: <a class="reference internal" href="marian.html#transformers.MarianModel" title="transformers.MarianModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianModel</span></code></a> (Marian model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.MBartModel" title="transformers.MBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartModel</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot.html#transformers.BlenderbotModel" title="transformers.BlenderbotModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotModel</span></code></a> (Blenderbot model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertModel" title="transformers.AlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertModel</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertModel" title="transformers.CamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModel</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaModel" title="transformers.XLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModel</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartModel" title="transformers.BartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartModel</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerModel" title="transformers.LongformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerModel</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> configuration class: <a class="reference internal" href="layoutlm.html#transformers.LayoutLMModel" title="transformers.LayoutLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMModel</span></code></a> (LayoutLM model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertModel" title="transformers.SqueezeBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertModel</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertModel" title="transformers.MobileBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertModel</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertModel" title="transformers.FlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="fsmt.html#transformers.FSMTConfig" title="transformers.FSMTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTConfig</span></code></a> configuration class: <a class="reference internal" href="fsmt.html#transformers.FSMTModel" title="transformers.FSMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTModel</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraModel" title="transformers.ElectraModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraModel</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> configuration class: <a class="reference internal" href="reformer.html#transformers.ReformerModel" title="transformers.ReformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerModel</span></code></a> (Reformer model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelModel" title="transformers.FunnelModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelModel</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> configuration class: <a class="reference internal" href="lxmert.html#transformers.LxmertModel" title="transformers.LxmertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertModel</span></code></a> (LXMERT model)</p></li>
<li><p><a class="reference internal" href="bertgeneration.html#transformers.BertGenerationConfig" title="transformers.BertGenerationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationConfig</span></code></a> configuration class: <a class="reference internal" href="bertgeneration.html#transformers.BertGenerationEncoder" title="transformers.BertGenerationEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationEncoder</span></code></a> (Bert Generation model)</p></li>
<li><p><a class="reference internal" href="deberta.html#transformers.DebertaConfig" title="transformers.DebertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaConfig</span></code></a> configuration class: <a class="reference internal" href="deberta.html#transformers.DebertaModel" title="transformers.DebertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaModel</span></code></a> (DeBERTa model)</p></li>
<li><p><a class="reference internal" href="dpr.html#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a> configuration class: <a class="reference internal" href="dpr.html#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a> (DPR model)</p></li>
<li><p><a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetConfig" title="transformers.XLMProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetModel" title="transformers.XLMProphetNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetModel</span></code></a> (XLMProphetNet model)</p></li>
<li><p><a class="reference internal" href="prophetnet.html#transformers.ProphetNetConfig" title="transformers.ProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="prophetnet.html#transformers.ProphetNetModel" title="transformers.ProphetNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetModel</span></code></a> (ProphetNet model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetModel" title="transformers.MPNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetModel</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> configuration class: <a class="reference internal" href="tapas.html#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a> (TAPAS model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the base model classes of the library from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDModel" title="transformers.LEDModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDModel</span></code></a> (LED model)</p></li>
<li><p><strong>blenderbot-small</strong> â€“ <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallModel" title="transformers.BlenderbotSmallModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallModel</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><strong>retribert</strong> â€“ <a class="reference internal" href="retribert.html#transformers.RetriBertModel" title="transformers.RetriBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertModel</span></code></a> (RetriBERT model)</p></li>
<li><p><strong>mt5</strong> â€“ <a class="reference internal" href="mt5.html#transformers.MT5Model" title="transformers.MT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Model</span></code></a> (mT5 model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertModel" title="transformers.MobileBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertModel</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertModel" title="transformers.DistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertModel" title="transformers.AlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertModel</span></code></a> (ALBERT model)</p></li>
<li><p><strong>bert-generation</strong> â€“ <a class="reference internal" href="bertgeneration.html#transformers.BertGenerationEncoder" title="transformers.BertGenerationEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationEncoder</span></code></a> (Bert Generation model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertModel" title="transformers.CamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertModel</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaModel" title="transformers.XLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaModel</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>pegasus</strong> â€“ <a class="reference internal" href="pegasus.html#transformers.PegasusModel" title="transformers.PegasusModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusModel</span></code></a> (Pegasus model)</p></li>
<li><p><strong>marian</strong> â€“ <a class="reference internal" href="marian.html#transformers.MarianModel" title="transformers.MarianModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianModel</span></code></a> (Marian model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartModel" title="transformers.MBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartModel</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetModel" title="transformers.MPNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetModel</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartModel" title="transformers.BartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartModel</span></code></a> (BART model)</p></li>
<li><p><strong>blenderbot</strong> â€“ <a class="reference internal" href="blenderbot.html#transformers.BlenderbotModel" title="transformers.BlenderbotModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotModel</span></code></a> (Blenderbot model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerModel" title="transformers.ReformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerModel</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerModel" title="transformers.LongformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerModel</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>deberta</strong> â€“ <a class="reference internal" href="deberta.html#transformers.DebertaModel" title="transformers.DebertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaModel</span></code></a> (DeBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertModel" title="transformers.FlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>fsmt</strong> â€“ <a class="reference internal" href="fsmt.html#transformers.FSMTModel" title="transformers.FSMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTModel</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertModel" title="transformers.SqueezeBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertModel</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertModel" title="transformers.BertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTModel" title="transformers.OpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2Model" title="transformers.GPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLModel" title="transformers.TransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetModel" title="transformers.XLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm-prophetnet</strong> â€“ <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetModel" title="transformers.XLMProphetNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetModel</span></code></a> (XLMProphetNet model)</p></li>
<li><p><strong>prophetnet</strong> â€“ <a class="reference internal" href="prophetnet.html#transformers.ProphetNetModel" title="transformers.ProphetNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetModel</span></code></a> (ProphetNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMModel" title="transformers.XLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLModel" title="transformers.CTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLModel</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraModel" title="transformers.ElectraModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraModel</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelModel" title="transformers.FunnelModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelModel</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.LxmertModel" title="transformers.LxmertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertModel</span></code></a> (LXMERT model)</p></li>
<li><p><strong>dpr</strong> â€“ <a class="reference internal" href="dpr.html#transformers.DPRQuestionEncoder" title="transformers.DPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRQuestionEncoder</span></code></a> (DPR model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMModel" title="transformers.LayoutLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMModel</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasModel" title="transformers.TapasModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasModel</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforpretraining">
<h2>AutoModelForPreTraining<a class="headerlink" href="#automodelforpretraining" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForPreTraining"><a name="//apple_ref/cpp/Class/transformers.AutoModelForPreTraining"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForPreTraining</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForPreTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with the
architecture used for pretraining this modelâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForPreTraining.from_pretrained" title="transformers.AutoModelForPreTraining.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForPreTraining.from_config" title="transformers.AutoModelForPreTraining.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForPreTraining.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForPreTraining.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForPreTraining.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with the architecture used for pretraining this
modelâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForPreTraining.from_pretrained" title="transformers.AutoModelForPreTraining.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> configuration class: <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForMaskedLM" title="transformers.LayoutLMForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForMaskedLM</span></code></a> (LayoutLM model)</p></li>
<li><p><a class="reference internal" href="retribert.html#transformers.RetriBertConfig" title="transformers.RetriBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertConfig</span></code></a> configuration class: <a class="reference internal" href="retribert.html#transformers.RetriBertModel" title="transformers.RetriBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertModel</span></code></a> (RetriBERT model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.T5ForConditionalGeneration" title="transformers.T5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForPreTraining" title="transformers.AlbertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForPreTraining</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="fsmt.html#transformers.FSMTConfig" title="transformers.FSMTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTConfig</span></code></a> configuration class: <a class="reference internal" href="fsmt.html#transformers.FSMTForConditionalGeneration" title="transformers.FSMTForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForMaskedLM" title="transformers.LongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMaskedLM" title="transformers.SqueezeBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMaskedLM</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForPreTraining" title="transformers.MobileBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForPreTraining</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForPreTraining" title="transformers.ElectraForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForPreTraining</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> configuration class: <a class="reference internal" href="lxmert.html#transformers.LxmertForPreTraining" title="transformers.LxmertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertForPreTraining</span></code></a> (LXMERT model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForPreTraining" title="transformers.FunnelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForPreTraining</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForMaskedLM" title="transformers.MPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> configuration class: <a class="reference internal" href="tapas.html#transformers.TapasForMaskedLM" title="transformers.TapasForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForMaskedLM</span></code></a> (TAPAS model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForPreTraining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForPreTraining.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForPreTraining.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForPreTraining.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForPreTraining.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with the architecture used for pretraining this modelâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>retribert</strong> â€“ <a class="reference internal" href="retribert.html#transformers.RetriBertModel" title="transformers.RetriBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetriBertModel</span></code></a> (RetriBERT model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5ForConditionalGeneration" title="transformers.T5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForPreTraining" title="transformers.MobileBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForPreTraining</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForPreTraining" title="transformers.AlbertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForPreTraining</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForMaskedLM" title="transformers.MPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForMaskedLM" title="transformers.LongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>fsmt</strong> â€“ <a class="reference internal" href="fsmt.html#transformers.FSMTForConditionalGeneration" title="transformers.FSMTForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMaskedLM" title="transformers.SqueezeBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMaskedLM</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForPreTraining" title="transformers.BertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForPreTraining</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForPreTraining" title="transformers.ElectraForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForPreTraining</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForPreTraining" title="transformers.FunnelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForPreTraining</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.LxmertForPreTraining" title="transformers.LxmertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertForPreTraining</span></code></a> (LXMERT model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForMaskedLM" title="transformers.LayoutLMForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForMaskedLM</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasForMaskedLM" title="transformers.TapasForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForMaskedLM</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForPreTraining</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforcausallm">
<h2>AutoModelForCausalLM<a class="headerlink" href="#automodelforcausallm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForCausalLM"><a name="//apple_ref/cpp/Class/transformers.AutoModelForCausalLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForCausalLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForCausalLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForCausalLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a causal
language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForCausalLM.from_pretrained" title="transformers.AutoModelForCausalLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForCausalLM.from_config" title="transformers.AutoModelForCausalLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForCausalLM.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForCausalLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForCausalLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForCausalLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a causal language modeling headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForCausalLM.from_pretrained" title="transformers.AutoModelForCausalLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForCausalLM" title="transformers.CamembertForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForCausalLM</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForCausalLM" title="transformers.XLMRobertaForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForCausalLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForCausalLM" title="transformers.RobertaForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForCausalLM</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertLMHeadModel" title="transformers.BertLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertLMHeadModel</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> configuration class: <a class="reference internal" href="reformer.html#transformers.ReformerModelWithLMHead" title="transformers.ReformerModelWithLMHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerModelWithLMHead</span></code></a> (Reformer model)</p></li>
<li><p><a class="reference internal" href="bertgeneration.html#transformers.BertGenerationConfig" title="transformers.BertGenerationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationConfig</span></code></a> configuration class: <a class="reference internal" href="bertgeneration.html#transformers.BertGenerationDecoder" title="transformers.BertGenerationDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationDecoder</span></code></a> (Bert Generation model)</p></li>
<li><p><a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetConfig" title="transformers.XLMProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetForCausalLM" title="transformers.XLMProphetNetForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetForCausalLM</span></code></a> (XLMProphetNet model)</p></li>
<li><p><a class="reference internal" href="prophetnet.html#transformers.ProphetNetConfig" title="transformers.ProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="prophetnet.html#transformers.ProphetNetForCausalLM" title="transformers.ProphetNetForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetForCausalLM</span></code></a> (ProphetNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForCausalLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForCausalLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForCausalLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForCausalLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a causal language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>bert-generation</strong> â€“ <a class="reference internal" href="bertgeneration.html#transformers.BertGenerationDecoder" title="transformers.BertGenerationDecoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertGenerationDecoder</span></code></a> (Bert Generation model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForCausalLM" title="transformers.CamembertForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForCausalLM</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForCausalLM" title="transformers.XLMRobertaForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForCausalLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerModelWithLMHead" title="transformers.ReformerModelWithLMHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerModelWithLMHead</span></code></a> (Reformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForCausalLM" title="transformers.RobertaForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForCausalLM</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertLMHeadModel" title="transformers.BertLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertLMHeadModel</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTLMHeadModel" title="transformers.OpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2LMHeadModel" title="transformers.GPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLLMHeadModel" title="transformers.TransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetLMHeadModel" title="transformers.XLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm-prophetnet</strong> â€“ <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetForCausalLM" title="transformers.XLMProphetNetForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetForCausalLM</span></code></a> (XLMProphetNet model)</p></li>
<li><p><strong>prophetnet</strong> â€“ <a class="reference internal" href="prophetnet.html#transformers.ProphetNetForCausalLM" title="transformers.ProphetNetForCausalLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetForCausalLM</span></code></a> (ProphetNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLLMHeadModel" title="transformers.CTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/gpt2_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/gpt2_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelformaskedlm">
<h2>AutoModelForMaskedLM<a class="headerlink" href="#automodelformaskedlm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForMaskedLM"><a name="//apple_ref/cpp/Class/transformers.AutoModelForMaskedLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForMaskedLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMaskedLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a masked
language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForMaskedLM.from_pretrained" title="transformers.AutoModelForMaskedLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForMaskedLM.from_config" title="transformers.AutoModelForMaskedLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForMaskedLM.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForMaskedLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMaskedLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMaskedLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a masked language modeling headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForMaskedLM.from_pretrained" title="transformers.AutoModelForMaskedLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> configuration class: <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForMaskedLM" title="transformers.LayoutLMForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForMaskedLM</span></code></a> (LayoutLM model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForMaskedLM" title="transformers.LongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMaskedLM" title="transformers.SqueezeBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMaskedLM</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForMaskedLM" title="transformers.MobileBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForMaskedLM</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForMaskedLM" title="transformers.ElectraForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForMaskedLM</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> configuration class: <a class="reference internal" href="reformer.html#transformers.ReformerForMaskedLM" title="transformers.ReformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForMaskedLM</span></code></a> (Reformer model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForMaskedLM" title="transformers.FunnelForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForMaskedLM</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForMaskedLM" title="transformers.MPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> configuration class: <a class="reference internal" href="tapas.html#transformers.TapasForMaskedLM" title="transformers.TapasForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForMaskedLM</span></code></a> (TAPAS model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForMaskedLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForMaskedLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMaskedLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMaskedLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a masked language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForMaskedLM" title="transformers.MobileBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForMaskedLM</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForMaskedLM" title="transformers.DistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForMaskedLM" title="transformers.AlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForMaskedLM" title="transformers.CamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMaskedLM" title="transformers.XLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForMaskedLM" title="transformers.MPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerForMaskedLM" title="transformers.ReformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForMaskedLM</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForMaskedLM" title="transformers.LongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertWithLMHeadModel" title="transformers.FlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMaskedLM" title="transformers.SqueezeBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMaskedLM</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForMaskedLM" title="transformers.BertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code></a> (BERT model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMWithLMHeadModel" title="transformers.XLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForMaskedLM" title="transformers.ElectraForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForMaskedLM</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForMaskedLM" title="transformers.FunnelForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForMaskedLM</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForMaskedLM" title="transformers.LayoutLMForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForMaskedLM</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasForMaskedLM" title="transformers.TapasForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForMaskedLM</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforseq2seqlm">
<h2>AutoModelForSeq2SeqLM<a class="headerlink" href="#automodelforseq2seqlm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForSeq2SeqLM"><a name="//apple_ref/cpp/Class/transformers.AutoModelForSeq2SeqLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForSeq2SeqLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSeq2SeqLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSeq2SeqLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
sequence-to-sequence language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForSeq2SeqLM.from_pretrained" title="transformers.AutoModelForSeq2SeqLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForSeq2SeqLM.from_config" title="transformers.AutoModelForSeq2SeqLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForSeq2SeqLM.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSeq2SeqLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSeq2SeqLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSeq2SeqLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a sequence-to-sequence language modeling
headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForSeq2SeqLM.from_pretrained" title="transformers.AutoModelForSeq2SeqLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.LEDForConditionalGeneration" title="transformers.LEDForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForConditionalGeneration</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallForConditionalGeneration" title="transformers.BlenderbotSmallForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallForConditionalGeneration</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> configuration class: <a class="reference internal" href="mt5.html#transformers.MT5ForConditionalGeneration" title="transformers.MT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5ForConditionalGeneration</span></code></a> (mT5 model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.T5ForConditionalGeneration" title="transformers.T5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> configuration class: <a class="reference internal" href="pegasus.html#transformers.PegasusForConditionalGeneration" title="transformers.PegasusForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusForConditionalGeneration</span></code></a> (Pegasus model)</p></li>
<li><p><a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> configuration class: <a class="reference internal" href="marian.html#transformers.MarianMTModel" title="transformers.MarianMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianMTModel</span></code></a> (Marian model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.MBartForConditionalGeneration" title="transformers.MBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForConditionalGeneration</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot.html#transformers.BlenderbotForConditionalGeneration" title="transformers.BlenderbotForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotForConditionalGeneration</span></code></a> (Blenderbot model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="fsmt.html#transformers.FSMTConfig" title="transformers.FSMTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTConfig</span></code></a> configuration class: <a class="reference internal" href="fsmt.html#transformers.FSMTForConditionalGeneration" title="transformers.FSMTForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><a class="reference internal" href="encoderdecoder.html#transformers.EncoderDecoderConfig" title="transformers.EncoderDecoderConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">EncoderDecoderConfig</span></code></a> configuration class: <a class="reference internal" href="encoderdecoder.html#transformers.EncoderDecoderModel" title="transformers.EncoderDecoderModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">EncoderDecoderModel</span></code></a> (Encoder decoder model)</p></li>
<li><p><a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetConfig" title="transformers.XLMProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetForConditionalGeneration" title="transformers.XLMProphetNetForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetForConditionalGeneration</span></code></a> (XLMProphetNet model)</p></li>
<li><p><a class="reference internal" href="prophetnet.html#transformers.ProphetNetConfig" title="transformers.ProphetNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetConfig</span></code></a> configuration class: <a class="reference internal" href="prophetnet.html#transformers.ProphetNetForConditionalGeneration" title="transformers.ProphetNetForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetForConditionalGeneration</span></code></a> (ProphetNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForSeq2SeqLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSeq2SeqLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSeq2SeqLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSeq2SeqLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a sequence-to-sequence language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDForConditionalGeneration" title="transformers.LEDForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForConditionalGeneration</span></code></a> (LED model)</p></li>
<li><p><strong>blenderbot-small</strong> â€“ <a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallForConditionalGeneration" title="transformers.BlenderbotSmallForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallForConditionalGeneration</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><strong>mt5</strong> â€“ <a class="reference internal" href="mt5.html#transformers.MT5ForConditionalGeneration" title="transformers.MT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5ForConditionalGeneration</span></code></a> (mT5 model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.T5ForConditionalGeneration" title="transformers.T5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><strong>pegasus</strong> â€“ <a class="reference internal" href="pegasus.html#transformers.PegasusForConditionalGeneration" title="transformers.PegasusForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusForConditionalGeneration</span></code></a> (Pegasus model)</p></li>
<li><p><strong>marian</strong> â€“ <a class="reference internal" href="marian.html#transformers.MarianMTModel" title="transformers.MarianMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianMTModel</span></code></a> (Marian model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartForConditionalGeneration" title="transformers.MBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForConditionalGeneration</span></code></a> (mBART model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartForConditionalGeneration" title="transformers.BartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><strong>blenderbot</strong> â€“ <a class="reference internal" href="blenderbot.html#transformers.BlenderbotForConditionalGeneration" title="transformers.BlenderbotForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotForConditionalGeneration</span></code></a> (Blenderbot model)</p></li>
<li><p><strong>fsmt</strong> â€“ <a class="reference internal" href="fsmt.html#transformers.FSMTForConditionalGeneration" title="transformers.FSMTForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FSMTForConditionalGeneration</span></code></a> (FairSeq Machine-Translation model)</p></li>
<li><p><strong>xlm-prophetnet</strong> â€“ <a class="reference internal" href="xlmprophetnet.html#transformers.XLMProphetNetForConditionalGeneration" title="transformers.XLMProphetNetForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMProphetNetForConditionalGeneration</span></code></a> (XLMProphetNet model)</p></li>
<li><p><strong>prophetnet</strong> â€“ <a class="reference internal" href="prophetnet.html#transformers.ProphetNetForConditionalGeneration" title="transformers.ProphetNetForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProphetNetForConditionalGeneration</span></code></a> (ProphetNet model)</p></li>
<li><p><strong>encoder-decoder</strong> â€“ <a class="reference internal" href="encoderdecoder.html#transformers.EncoderDecoderModel" title="transformers.EncoderDecoderModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">EncoderDecoderModel</span></code></a> (Encoder decoder model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5-base'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5-base'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/t5_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/t5_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforsequenceclassification">
<h2>AutoModelForSequenceClassification<a class="headerlink" href="#automodelforsequenceclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.AutoModelForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForSequenceClassification</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
sequence classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForSequenceClassification.from_pretrained" title="transformers.AutoModelForSequenceClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForSequenceClassification.from_config" title="transformers.AutoModelForSequenceClassification.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForSequenceClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSequenceClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSequenceClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a sequence classification headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForSequenceClassification.from_pretrained" title="transformers.AutoModelForSequenceClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.LEDForSequenceClassification" title="transformers.LEDForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForSequenceClassification</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.MBartForSequenceClassification" title="transformers.MBartForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForSequenceClassification</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartForSequenceClassification" title="transformers.BartForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForSequenceClassification</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForSequenceClassification" title="transformers.LongformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForSequenceClassification</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForSequenceClassification" title="transformers.SqueezeBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForSequenceClassification</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> configuration class: <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForSequenceClassification" title="transformers.LayoutLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForSequenceClassification</span></code></a> (LayoutLM model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForSequenceClassification" title="transformers.MobileBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForSequenceClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForSequenceClassification" title="transformers.XLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForSequenceClassification</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForSequenceClassification" title="transformers.ElectraForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForSequenceClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForSequenceClassification" title="transformers.FunnelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForSequenceClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="deberta.html#transformers.DebertaConfig" title="transformers.DebertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaConfig</span></code></a> configuration class: <a class="reference internal" href="deberta.html#transformers.DebertaForSequenceClassification" title="transformers.DebertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaForSequenceClassification</span></code></a> (DeBERTa model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.GPT2ForSequenceClassification" title="transformers.GPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2ForSequenceClassification</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.OpenAIGPTForSequenceClassification" title="transformers.OpenAIGPTForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTForSequenceClassification</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> configuration class: <a class="reference internal" href="reformer.html#transformers.ReformerForSequenceClassification" title="transformers.ReformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForSequenceClassification</span></code></a> (Reformer model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.CTRLForSequenceClassification" title="transformers.CTRLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLForSequenceClassification</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TransfoXLForSequenceClassification" title="transformers.TransfoXLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLForSequenceClassification</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForSequenceClassification" title="transformers.MPNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForSequenceClassification</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> configuration class: <a class="reference internal" href="tapas.html#transformers.TapasForSequenceClassification" title="transformers.TapasForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForSequenceClassification</span></code></a> (TAPAS model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForSequenceClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForSequenceClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForSequenceClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForSequenceClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a sequence classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDForSequenceClassification" title="transformers.LEDForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForSequenceClassification</span></code></a> (LED model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForSequenceClassification" title="transformers.MobileBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForSequenceClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForSequenceClassification" title="transformers.DistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForSequenceClassification" title="transformers.AlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForSequenceClassification" title="transformers.CamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForSequenceClassification" title="transformers.XLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartForSequenceClassification" title="transformers.MBartForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForSequenceClassification</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForSequenceClassification" title="transformers.MPNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForSequenceClassification</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartForSequenceClassification" title="transformers.BartForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForSequenceClassification</span></code></a> (BART model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerForSequenceClassification" title="transformers.ReformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForSequenceClassification</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForSequenceClassification" title="transformers.LongformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForSequenceClassification</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>deberta</strong> â€“ <a class="reference internal" href="deberta.html#transformers.DebertaForSequenceClassification" title="transformers.DebertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DebertaForSequenceClassification</span></code></a> (DeBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertForSequenceClassification" title="transformers.FlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForSequenceClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForSequenceClassification" title="transformers.SqueezeBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForSequenceClassification</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForSequenceClassification" title="transformers.BertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.OpenAIGPTForSequenceClassification" title="transformers.OpenAIGPTForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTForSequenceClassification</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.GPT2ForSequenceClassification" title="transformers.GPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2ForSequenceClassification</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TransfoXLForSequenceClassification" title="transformers.TransfoXLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLForSequenceClassification</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetForSequenceClassification" title="transformers.XLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMForSequenceClassification" title="transformers.XLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForSequenceClassification</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.CTRLForSequenceClassification" title="transformers.CTRLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLForSequenceClassification</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForSequenceClassification" title="transformers.ElectraForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForSequenceClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForSequenceClassification" title="transformers.FunnelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForSequenceClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForSequenceClassification" title="transformers.LayoutLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForSequenceClassification</span></code></a> (LayoutLM model)</p></li>
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasForSequenceClassification" title="transformers.TapasForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForSequenceClassification</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelformultiplechoice">
<h2>AutoModelForMultipleChoice<a class="headerlink" href="#automodelformultiplechoice" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForMultipleChoice"><a name="//apple_ref/cpp/Class/transformers.AutoModelForMultipleChoice"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForMultipleChoice</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMultipleChoice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMultipleChoice" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
multiple choice classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForMultipleChoice.from_pretrained" title="transformers.AutoModelForMultipleChoice.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForMultipleChoice.from_config" title="transformers.AutoModelForMultipleChoice.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForMultipleChoice.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForMultipleChoice.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMultipleChoice.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMultipleChoice.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForMultipleChoice.from_pretrained" title="transformers.AutoModelForMultipleChoice.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the
model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForMultipleChoice" title="transformers.CamembertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMultipleChoice</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForMultipleChoice" title="transformers.ElectraForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForMultipleChoice</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMultipleChoice" title="transformers.XLMRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMultipleChoice</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForMultipleChoice" title="transformers.LongformerForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMultipleChoice</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForMultipleChoice" title="transformers.RobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMultipleChoice</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMultipleChoice" title="transformers.SqueezeBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMultipleChoice</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForMultipleChoice" title="transformers.BertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMultipleChoice</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForMultipleChoice" title="transformers.DistilBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMultipleChoice</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForMultipleChoice" title="transformers.MobileBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForMultipleChoice</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForMultipleChoice" title="transformers.XLNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForMultipleChoice</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForMultipleChoice" title="transformers.AlbertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMultipleChoice</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForMultipleChoice" title="transformers.XLMForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForMultipleChoice</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForMultipleChoice" title="transformers.FlaubertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForMultipleChoice</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForMultipleChoice" title="transformers.FunnelForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForMultipleChoice</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForMultipleChoice" title="transformers.MPNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMultipleChoice</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForMultipleChoice</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForMultipleChoice.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForMultipleChoice.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForMultipleChoice.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForMultipleChoice.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForMultipleChoice" title="transformers.MobileBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForMultipleChoice</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForMultipleChoice" title="transformers.DistilBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForMultipleChoice</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForMultipleChoice" title="transformers.AlbertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForMultipleChoice</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForMultipleChoice" title="transformers.CamembertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForMultipleChoice</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForMultipleChoice" title="transformers.XLMRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForMultipleChoice</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForMultipleChoice" title="transformers.MPNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForMultipleChoice</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForMultipleChoice" title="transformers.LongformerForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForMultipleChoice</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForMultipleChoice" title="transformers.RobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMultipleChoice</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertForMultipleChoice" title="transformers.FlaubertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForMultipleChoice</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForMultipleChoice" title="transformers.SqueezeBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForMultipleChoice</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForMultipleChoice" title="transformers.BertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForMultipleChoice</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetForMultipleChoice" title="transformers.XLNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForMultipleChoice</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMForMultipleChoice" title="transformers.XLMForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForMultipleChoice</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForMultipleChoice" title="transformers.ElectraForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForMultipleChoice</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForMultipleChoice" title="transformers.FunnelForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForMultipleChoice</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForMultipleChoice</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelfornextsentenceprediction">
<h2>AutoModelForNextSentencePrediction<a class="headerlink" href="#automodelfornextsentenceprediction" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForNextSentencePrediction"><a name="//apple_ref/cpp/Class/transformers.AutoModelForNextSentencePrediction"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForNextSentencePrediction</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForNextSentencePrediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForNextSentencePrediction" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
multiple choice classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForNextSentencePrediction.from_pretrained" title="transformers.AutoModelForNextSentencePrediction.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForNextSentencePrediction.from_config" title="transformers.AutoModelForNextSentencePrediction.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForNextSentencePrediction.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForNextSentencePrediction.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForNextSentencePrediction.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForNextSentencePrediction.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForNextSentencePrediction.from_pretrained" title="transformers.AutoModelForNextSentencePrediction.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForNextSentencePrediction" title="transformers.BertForNextSentencePrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForNextSentencePrediction</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForNextSentencePrediction" title="transformers.MobileBertForNextSentencePrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForNextSentencePrediction</span></code></a> (MobileBERT model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForNextSentencePrediction</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForNextSentencePrediction</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForNextSentencePrediction.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForNextSentencePrediction.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForNextSentencePrediction.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForNextSentencePrediction.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForNextSentencePrediction" title="transformers.MobileBertForNextSentencePrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForNextSentencePrediction</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForNextSentencePrediction" title="transformers.BertForNextSentencePrediction"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForNextSentencePrediction</span></code></a> (BERT model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForNextSentencePrediction</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForNextSentencePrediction</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForNextSentencePrediction</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForNextSentencePrediction</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelfortokenclassification">
<h2>AutoModelForTokenClassification<a class="headerlink" href="#automodelfortokenclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForTokenClassification"><a name="//apple_ref/cpp/Class/transformers.AutoModelForTokenClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForTokenClassification</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a token
classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.AutoModelForTokenClassification.from_pretrained" title="transformers.AutoModelForTokenClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.AutoModelForTokenClassification.from_config" title="transformers.AutoModelForTokenClassification.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForTokenClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTokenClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTokenClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a token classification headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForTokenClassification.from_pretrained" title="transformers.AutoModelForTokenClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="layoutlm.html#transformers.LayoutLMConfig" title="transformers.LayoutLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMConfig</span></code></a> configuration class: <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForTokenClassification" title="transformers.LayoutLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForTokenClassification</span></code></a> (LayoutLM model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForTokenClassification" title="transformers.DistilBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForTokenClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForTokenClassification" title="transformers.CamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForTokenClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForTokenClassification" title="transformers.FlaubertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForTokenClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForTokenClassification" title="transformers.XLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForTokenClassification" title="transformers.XLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForTokenClassification" title="transformers.LongformerForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForTokenClassification</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForTokenClassification" title="transformers.SqueezeBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForTokenClassification</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForTokenClassification" title="transformers.BertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForTokenClassification" title="transformers.MobileBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForTokenClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForTokenClassification" title="transformers.XLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForTokenClassification</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForTokenClassification" title="transformers.AlbertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForTokenClassification</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForTokenClassification" title="transformers.ElectraForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForTokenClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForTokenClassification" title="transformers.FunnelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForTokenClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForTokenClassification" title="transformers.MPNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForTokenClassification</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForTokenClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTokenClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTokenClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTokenClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a token classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForTokenClassification" title="transformers.MobileBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForTokenClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForTokenClassification" title="transformers.DistilBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForTokenClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForTokenClassification" title="transformers.AlbertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForTokenClassification</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForTokenClassification" title="transformers.CamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForTokenClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForTokenClassification" title="transformers.XLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForTokenClassification" title="transformers.MPNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForTokenClassification</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForTokenClassification" title="transformers.LongformerForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForTokenClassification</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertForTokenClassification" title="transformers.FlaubertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForTokenClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForTokenClassification" title="transformers.SqueezeBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForTokenClassification</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForTokenClassification" title="transformers.BertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForTokenClassification</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetForTokenClassification" title="transformers.XLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForTokenClassification</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMForTokenClassification" title="transformers.XLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForTokenClassification</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForTokenClassification" title="transformers.ElectraForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForTokenClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForTokenClassification" title="transformers.FunnelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForTokenClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>layoutlm</strong> â€“ <a class="reference internal" href="layoutlm.html#transformers.LayoutLMForTokenClassification" title="transformers.LayoutLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayoutLMForTokenClassification</span></code></a> (LayoutLM model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelforquestionanswering">
<h2>AutoModelForQuestionAnswering<a class="headerlink" href="#automodelforquestionanswering" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.AutoModelForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForQuestionAnswering</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
question answering headâ€”when created with the when created with the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method or the
<a class="reference internal" href="#transformers.AutoModelForQuestionAnswering.from_config" title="transformers.AutoModelForQuestionAnswering.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForQuestionAnswering.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForQuestionAnswering.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForQuestionAnswering.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a question answering headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForQuestionAnswering.from_pretrained" title="transformers.AutoModelForQuestionAnswering.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the
model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.LEDForQuestionAnswering" title="transformers.LEDForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForQuestionAnswering</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.CamembertForQuestionAnswering" title="transformers.CamembertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForQuestionAnswering</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.BartForQuestionAnswering" title="transformers.BartForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForQuestionAnswering</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.MBartForQuestionAnswering" title="transformers.MBartForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForQuestionAnswering</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.LongformerForQuestionAnswering" title="transformers.LongformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForQuestionAnswering</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForQuestionAnswering" title="transformers.XLMRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForQuestionAnswering</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.RobertaForQuestionAnswering" title="transformers.RobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForQuestionAnswering</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="squeezebert.html#transformers.SqueezeBertConfig" title="transformers.SqueezeBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertConfig</span></code></a> configuration class: <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForQuestionAnswering" title="transformers.SqueezeBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForQuestionAnswering</span></code></a> (SqueezeBERT model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.BertForQuestionAnswering" title="transformers.BertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnsweringSimple" title="transformers.XLNetForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnsweringSimple</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnsweringSimple" title="transformers.FlaubertForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnsweringSimple</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.MobileBertForQuestionAnswering" title="transformers.MobileBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForQuestionAnswering</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnsweringSimple" title="transformers.XLMForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnsweringSimple</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.ElectraForQuestionAnswering" title="transformers.ElectraForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForQuestionAnswering</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="reformer.html#transformers.ReformerConfig" title="transformers.ReformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerConfig</span></code></a> configuration class: <a class="reference internal" href="reformer.html#transformers.ReformerForQuestionAnswering" title="transformers.ReformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForQuestionAnswering</span></code></a> (Reformer model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.FunnelForQuestionAnswering" title="transformers.FunnelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForQuestionAnswering</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> configuration class: <a class="reference internal" href="lxmert.html#transformers.LxmertForQuestionAnswering" title="transformers.LxmertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertForQuestionAnswering</span></code></a> (LXMERT model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.MPNetForQuestionAnswering" title="transformers.MPNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForQuestionAnswering</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForQuestionAnswering.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForQuestionAnswering.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForQuestionAnswering.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForQuestionAnswering.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a question answering headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.LEDForQuestionAnswering" title="transformers.LEDForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDForQuestionAnswering</span></code></a> (LED model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.MobileBertForQuestionAnswering" title="transformers.MobileBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertForQuestionAnswering</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.DistilBertForQuestionAnswering" title="transformers.DistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.AlbertForQuestionAnswering" title="transformers.AlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.CamembertForQuestionAnswering" title="transformers.CamembertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertForQuestionAnswering</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaForQuestionAnswering" title="transformers.XLMRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaForQuestionAnswering</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.MBartForQuestionAnswering" title="transformers.MBartForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartForQuestionAnswering</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.MPNetForQuestionAnswering" title="transformers.MPNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetForQuestionAnswering</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.BartForQuestionAnswering" title="transformers.BartForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartForQuestionAnswering</span></code></a> (BART model)</p></li>
<li><p><strong>reformer</strong> â€“ <a class="reference internal" href="reformer.html#transformers.ReformerForQuestionAnswering" title="transformers.ReformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReformerForQuestionAnswering</span></code></a> (Reformer model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.LongformerForQuestionAnswering" title="transformers.LongformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForQuestionAnswering</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.RobertaForQuestionAnswering" title="transformers.RobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForQuestionAnswering</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.FlaubertForQuestionAnsweringSimple" title="transformers.FlaubertForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertForQuestionAnsweringSimple</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>squeezebert</strong> â€“ <a class="reference internal" href="squeezebert.html#transformers.SqueezeBertForQuestionAnswering" title="transformers.SqueezeBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeBertForQuestionAnswering</span></code></a> (SqueezeBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.BertForQuestionAnswering" title="transformers.BertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertForQuestionAnswering</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.XLNetForQuestionAnsweringSimple" title="transformers.XLNetForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForQuestionAnsweringSimple</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.XLMForQuestionAnsweringSimple" title="transformers.XLMForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForQuestionAnsweringSimple</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.ElectraForQuestionAnswering" title="transformers.ElectraForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraForQuestionAnswering</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.FunnelForQuestionAnswering" title="transformers.FunnelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelForQuestionAnswering</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.LxmertForQuestionAnswering" title="transformers.LxmertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertForQuestionAnswering</span></code></a> (LXMERT model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/bert_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="automodelfortablequestionanswering">
<h2>AutoModelForTableQuestionAnswering<a class="headerlink" href="#automodelfortablequestionanswering" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.AutoModelForTableQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.AutoModelForTableQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">AutoModelForTableQuestionAnswering</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTableQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTableQuestionAnswering" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a table
question answering headâ€”when created with the when created with the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method or the
<a class="reference internal" href="#transformers.AutoModelForTableQuestionAnswering.from_config" title="transformers.AutoModelForTableQuestionAnswering.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.AutoModelForTableQuestionAnswering.from_config"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTableQuestionAnswering.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTableQuestionAnswering.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTableQuestionAnswering.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a table question answering headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.AutoModelForTableQuestionAnswering.from_pretrained" title="transformers.AutoModelForTableQuestionAnswering.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="tapas.html#transformers.TapasConfig" title="transformers.TapasConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasConfig</span></code></a> configuration class: <a class="reference internal" href="tapas.html#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> (TAPAS model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForTableQuestionAnswering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTableQuestionAnswering</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.AutoModelForTableQuestionAnswering.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.AutoModelForTableQuestionAnswering.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_auto.html#AutoModelForTableQuestionAnswering.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.AutoModelForTableQuestionAnswering.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a table question answering headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>tapas</strong> â€“ <a class="reference internal" href="tapas.html#transformers.TapasForQuestionAnswering" title="transformers.TapasForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TapasForQuestionAnswering</span></code></a> (TAPAS model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>) â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>tensorflow index checkpoint file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./tf_model/model.ckpt.index</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by supplying the save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">os.PathLike</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForTableQuestionAnswering</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTableQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTableQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'google/tapas-base-finetuned-wtq'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./tf_model/tapas_tf_checkpoint.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./tf_model/tapas_tf_checkpoint.ckpt.index'</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodel">
<h2>TFAutoModel<a class="headerlink" href="#tfautomodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModel"><a name="//apple_ref/cpp/Class/transformers.TFAutoModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModel</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the base model classes of the library when
created with the when created with the <a class="reference internal" href="#transformers.TFAutoModel.from_pretrained" title="transformers.TFAutoModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModel.from_config" title="transformers.TFAutoModel.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class methods.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModel.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModel.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModel.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the base model classes of the library from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModel.from_pretrained" title="transformers.TFAutoModel.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.TFLEDModel" title="transformers.TFLEDModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLEDModel</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> configuration class: <a class="reference internal" href="lxmert.html#transformers.TFLxmertModel" title="transformers.TFLxmertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLxmertModel</span></code></a> (LXMERT model)</p></li>
<li><p><a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> configuration class: <a class="reference internal" href="mt5.html#transformers.TFMT5Model" title="transformers.TFMT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMT5Model</span></code></a> (mT5 model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.TFT5Model" title="transformers.TFT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5Model</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertModel" title="transformers.TFDistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertModel" title="transformers.TFAlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertModel</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.TFBartModel" title="transformers.TFBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartModel</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertModel" title="transformers.TFCamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertModel</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaModel" title="transformers.TFXLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaModel</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerModel" title="transformers.TFLongformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerModel</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaModel" title="transformers.TFRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertModel" title="transformers.TFBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertModel</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTModel" title="transformers.TFOpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.TFGPT2Model" title="transformers.TFGPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertModel" title="transformers.TFMobileBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertModel</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLModel" title="transformers.TFTransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetModel" title="transformers.TFXLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertModel" title="transformers.TFFlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMModel" title="transformers.TFXLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.TFCTRLModel" title="transformers.TFCTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLModel</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraModel" title="transformers.TFElectraModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraModel</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelModel" title="transformers.TFFunnelModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelModel</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="dpr.html#transformers.DPRConfig" title="transformers.DPRConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DPRConfig</span></code></a> configuration class: <a class="reference internal" href="dpr.html#transformers.TFDPRQuestionEncoder" title="transformers.TFDPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRQuestionEncoder</span></code></a> (DPR model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetModel" title="transformers.TFMPNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetModel</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.TFMBartModel" title="transformers.TFMBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMBartModel</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> configuration class: <a class="reference internal" href="marian.html#transformers.TFMarianModel" title="transformers.TFMarianModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMarianModel</span></code></a> (Marian model)</p></li>
<li><p><a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> configuration class: <a class="reference internal" href="pegasus.html#transformers.TFPegasusModel" title="transformers.TFPegasusModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPegasusModel</span></code></a> (Pegasus model)</p></li>
<li><p><a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot.html#transformers.TFBlenderbotModel" title="transformers.TFBlenderbotModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotModel</span></code></a> (Blenderbot model)</p></li>
<li><p><a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot_small.html#transformers.TFBlenderbotSmallModel" title="transformers.TFBlenderbotSmallModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotSmallModel</span></code></a> (BlenderbotSmall model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">TFAutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModel.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModel.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the base model classes of the library from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>led</strong> â€“ <a class="reference internal" href="led.html#transformers.TFLEDModel" title="transformers.TFLEDModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLEDModel</span></code></a> (LED model)</p></li>
<li><p><strong>blenderbot-small</strong> â€“ <a class="reference internal" href="blenderbot_small.html#transformers.TFBlenderbotSmallModel" title="transformers.TFBlenderbotSmallModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotSmallModel</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><strong>mt5</strong> â€“ <a class="reference internal" href="mt5.html#transformers.TFMT5Model" title="transformers.TFMT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMT5Model</span></code></a> (mT5 model)</p></li>
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.TFT5Model" title="transformers.TFT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5Model</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertModel" title="transformers.TFMobileBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertModel</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertModel" title="transformers.TFDistilBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertModel</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertModel" title="transformers.TFAlbertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertModel</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertModel" title="transformers.TFCamembertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertModel</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaModel" title="transformers.TFXLMRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaModel</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>pegasus</strong> â€“ <a class="reference internal" href="pegasus.html#transformers.TFPegasusModel" title="transformers.TFPegasusModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPegasusModel</span></code></a> (Pegasus model)</p></li>
<li><p><strong>marian</strong> â€“ <a class="reference internal" href="marian.html#transformers.TFMarianModel" title="transformers.TFMarianModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMarianModel</span></code></a> (Marian model)</p></li>
<li><p><strong>mbart</strong> â€“ <a class="reference internal" href="mbart.html#transformers.TFMBartModel" title="transformers.TFMBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMBartModel</span></code></a> (mBART model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetModel" title="transformers.TFMPNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetModel</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.TFBartModel" title="transformers.TFBartModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartModel</span></code></a> (BART model)</p></li>
<li><p><strong>blenderbot</strong> â€“ <a class="reference internal" href="blenderbot.html#transformers.TFBlenderbotModel" title="transformers.TFBlenderbotModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotModel</span></code></a> (Blenderbot model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerModel" title="transformers.TFLongformerModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerModel</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaModel" title="transformers.TFRobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaModel</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertModel" title="transformers.TFFlaubertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertModel" title="transformers.TFBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertModel</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTModel" title="transformers.TFOpenAIGPTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.TFGPT2Model" title="transformers.TFGPT2Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2Model</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLModel" title="transformers.TFTransfoXLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetModel" title="transformers.TFXLNetModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMModel" title="transformers.TFXLMModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.TFCTRLModel" title="transformers.TFCTRLModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLModel</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraModel" title="transformers.TFElectraModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraModel</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelModel" title="transformers.TFFunnelModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelModel</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.TFLxmertModel" title="transformers.TFLxmertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLxmertModel</span></code></a> (LXMERT model)</p></li>
<li><p><strong>dpr</strong> â€“ <a class="reference internal" href="dpr.html#transformers.TFDPRQuestionEncoder" title="transformers.TFDPRQuestionEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDPRQuestionEncoder</span></code></a> (DPR model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelforpretraining">
<h2>TFAutoModelForPreTraining<a class="headerlink" href="#tfautomodelforpretraining" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForPreTraining"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForPreTraining"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForPreTraining</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForPreTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForPreTraining" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with the
architecture used for pretraining this modelâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForPreTraining.from_pretrained" title="transformers.TFAutoModelForPreTraining.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForPreTraining.from_config" title="transformers.TFAutoModelForPreTraining.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForPreTraining.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForPreTraining.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForPreTraining.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForPreTraining.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with the architecture used for pretraining this
modelâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForPreTraining.from_pretrained" title="transformers.TFAutoModelForPreTraining.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the
model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="lxmert.html#transformers.LxmertConfig" title="transformers.LxmertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmertConfig</span></code></a> configuration class: <a class="reference internal" href="lxmert.html#transformers.TFLxmertForPreTraining" title="transformers.TFLxmertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLxmertForPreTraining</span></code></a> (LXMERT model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.TFT5ForConditionalGeneration" title="transformers.TFT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMaskedLM" title="transformers.TFDistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForPreTraining" title="transformers.TFAlbertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForPreTraining</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.TFBartForConditionalGeneration" title="transformers.TFBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForMaskedLM" title="transformers.TFCamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMaskedLM" title="transformers.TFXLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForMaskedLM" title="transformers.TFRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForPreTraining" title="transformers.TFBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForPreTraining</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTLMHeadModel" title="transformers.TFOpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.TFGPT2LMHeadModel" title="transformers.TFGPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForPreTraining" title="transformers.TFMobileBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForPreTraining</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLLMHeadModel" title="transformers.TFTransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetLMHeadModel" title="transformers.TFXLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertWithLMHeadModel" title="transformers.TFFlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.TFCTRLLMHeadModel" title="transformers.TFCTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForPreTraining" title="transformers.TFElectraForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForPreTraining</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForPreTraining" title="transformers.TFFunnelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForPreTraining</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMaskedLM" title="transformers.TFMPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForPreTraining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForPreTraining</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForPreTraining.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForPreTraining.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForPreTraining.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForPreTraining.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with the architecture used for pretraining this modelâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>t5</strong> â€“ <a class="reference internal" href="t5.html#transformers.TFT5ForConditionalGeneration" title="transformers.TFT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForPreTraining" title="transformers.TFMobileBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForPreTraining</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMaskedLM" title="transformers.TFDistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForPreTraining" title="transformers.TFAlbertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForPreTraining</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForMaskedLM" title="transformers.TFCamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMaskedLM" title="transformers.TFXLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMaskedLM" title="transformers.TFMPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><strong>bart</strong> â€“ <a class="reference internal" href="bart.html#transformers.TFBartForConditionalGeneration" title="transformers.TFBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartForConditionalGeneration</span></code></a> (BART model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForMaskedLM" title="transformers.TFRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertWithLMHeadModel" title="transformers.TFFlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForPreTraining" title="transformers.TFBertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForPreTraining</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTLMHeadModel" title="transformers.TFOpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.TFGPT2LMHeadModel" title="transformers.TFGPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLLMHeadModel" title="transformers.TFTransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetLMHeadModel" title="transformers.TFXLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.TFCTRLLMHeadModel" title="transformers.TFCTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForPreTraining" title="transformers.TFElectraForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForPreTraining</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForPreTraining" title="transformers.TFFunnelForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForPreTraining</span></code></a> (Funnel Transformer model)</p></li>
<li><p><strong>lxmert</strong> â€“ <a class="reference internal" href="lxmert.html#transformers.TFLxmertForPreTraining" title="transformers.TFLxmertForPreTraining"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLxmertForPreTraining</span></code></a> (LXMERT model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForPreTraining</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelforcausallm">
<h2>TFAutoModelForCausalLM<a class="headerlink" href="#tfautomodelforcausallm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForCausalLM"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForCausalLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForCausalLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForCausalLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForCausalLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a causal
language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForCausalLM.from_pretrained" title="transformers.TFAutoModelForCausalLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForCausalLM.from_config" title="transformers.TFAutoModelForCausalLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForCausalLM.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForCausalLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForCausalLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForCausalLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a causal language modeling headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForCausalLM.from_pretrained" title="transformers.TFAutoModelForCausalLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertLMHeadModel" title="transformers.TFBertLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertLMHeadModel</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTLMHeadModel" title="transformers.TFOpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.TFGPT2LMHeadModel" title="transformers.TFGPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLLMHeadModel" title="transformers.TFTransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetLMHeadModel" title="transformers.TFXLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.TFCTRLLMHeadModel" title="transformers.TFCTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForCausalLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForCausalLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForCausalLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForCausalLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForCausalLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a causal language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertLMHeadModel" title="transformers.TFBertLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertLMHeadModel</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTLMHeadModel" title="transformers.TFOpenAIGPTLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTLMHeadModel</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.TFGPT2LMHeadModel" title="transformers.TFGPT2LMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2LMHeadModel</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLLMHeadModel" title="transformers.TFTransfoXLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLLMHeadModel</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetLMHeadModel" title="transformers.TFXLNetLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetLMHeadModel</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.TFCTRLLMHeadModel" title="transformers.TFCTRLLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLLMHeadModel</span></code></a> (CTRL model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForCausalLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'gpt2'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/gpt2_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/gpt2_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelformaskedlm">
<h2>TFAutoModelForMaskedLM<a class="headerlink" href="#tfautomodelformaskedlm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForMaskedLM"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForMaskedLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForMaskedLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMaskedLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMaskedLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a masked
language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForMaskedLM.from_pretrained" title="transformers.TFAutoModelForMaskedLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForMaskedLM.from_config" title="transformers.TFAutoModelForMaskedLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForMaskedLM.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForMaskedLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMaskedLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMaskedLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a masked language modeling headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForMaskedLM.from_pretrained" title="transformers.TFAutoModelForMaskedLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMaskedLM" title="transformers.TFDistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForMaskedLM" title="transformers.TFAlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForMaskedLM" title="transformers.TFCamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMaskedLM" title="transformers.TFXLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerForMaskedLM" title="transformers.TFLongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForMaskedLM" title="transformers.TFRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForMaskedLM" title="transformers.TFBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForMaskedLM</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForMaskedLM" title="transformers.TFMobileBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForMaskedLM</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertWithLMHeadModel" title="transformers.TFFlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForMaskedLM" title="transformers.TFElectraForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForMaskedLM</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForMaskedLM" title="transformers.TFFunnelForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForMaskedLM</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMaskedLM" title="transformers.TFMPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForMaskedLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForMaskedLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForMaskedLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMaskedLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMaskedLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a masked language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForMaskedLM" title="transformers.TFMobileBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForMaskedLM</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMaskedLM" title="transformers.TFDistilBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMaskedLM</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForMaskedLM" title="transformers.TFAlbertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForMaskedLM</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForMaskedLM" title="transformers.TFCamembertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMaskedLM</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMaskedLM" title="transformers.TFXLMRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMaskedLM</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMaskedLM" title="transformers.TFMPNetForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMaskedLM</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerForMaskedLM" title="transformers.TFLongformerForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForMaskedLM</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForMaskedLM" title="transformers.TFRobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMaskedLM</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertWithLMHeadModel" title="transformers.TFFlaubertWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertWithLMHeadModel</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForMaskedLM" title="transformers.TFBertForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForMaskedLM</span></code></a> (BERT model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMWithLMHeadModel" title="transformers.TFXLMWithLMHeadModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMWithLMHeadModel</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForMaskedLM" title="transformers.TFElectraForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForMaskedLM</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForMaskedLM" title="transformers.TFFunnelForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForMaskedLM</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForMaskedLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelforseq2seqlm">
<h2>TFAutoModelForSeq2SeqLM<a class="headerlink" href="#tfautomodelforseq2seqlm" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForSeq2SeqLM"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForSeq2SeqLM"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForSeq2SeqLM</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSeq2SeqLM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSeq2SeqLM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
sequence-to-sequence language modeling headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForSeq2SeqLM.from_pretrained" title="transformers.TFAutoModelForSeq2SeqLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForSeq2SeqLM.from_config" title="transformers.TFAutoModelForSeq2SeqLM.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForSeq2SeqLM.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForSeq2SeqLM.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSeq2SeqLM.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSeq2SeqLM.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a sequence-to-sequence language modeling
headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForSeq2SeqLM.from_pretrained" title="transformers.TFAutoModelForSeq2SeqLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the model
weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.TFLEDForConditionalGeneration" title="transformers.TFLEDForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLEDForConditionalGeneration</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> configuration class: <a class="reference internal" href="mt5.html#transformers.TFMT5ForConditionalGeneration" title="transformers.TFMT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMT5ForConditionalGeneration</span></code></a> (mT5 model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.TFT5ForConditionalGeneration" title="transformers.TFT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> configuration class: <a class="reference internal" href="marian.html#transformers.TFMarianMTModel" title="transformers.TFMarianMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMarianMTModel</span></code></a> (Marian model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.TFMBartForConditionalGeneration" title="transformers.TFMBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMBartForConditionalGeneration</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> configuration class: <a class="reference internal" href="pegasus.html#transformers.TFPegasusForConditionalGeneration" title="transformers.TFPegasusForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPegasusForConditionalGeneration</span></code></a> (Pegasus model)</p></li>
<li><p><a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot.html#transformers.TFBlenderbotForConditionalGeneration" title="transformers.TFBlenderbotForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotForConditionalGeneration</span></code></a> (Blenderbot model)</p></li>
<li><p><a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot_small.html#transformers.TFBlenderbotSmallForConditionalGeneration" title="transformers.TFBlenderbotSmallForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotSmallForConditionalGeneration</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.TFBartForConditionalGeneration" title="transformers.TFBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartForConditionalGeneration</span></code></a> (BART model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForSeq2SeqLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForSeq2SeqLM.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForSeq2SeqLM.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSeq2SeqLM.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSeq2SeqLM.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a sequence-to-sequence language modeling headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<ul class="simple">
<li><p><a class="reference internal" href="led.html#transformers.LEDConfig" title="transformers.LEDConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LEDConfig</span></code></a> configuration class: <a class="reference internal" href="led.html#transformers.TFLEDForConditionalGeneration" title="transformers.TFLEDForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLEDForConditionalGeneration</span></code></a> (LED model)</p></li>
<li><p><a class="reference internal" href="mt5.html#transformers.MT5Config" title="transformers.MT5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Config</span></code></a> configuration class: <a class="reference internal" href="mt5.html#transformers.TFMT5ForConditionalGeneration" title="transformers.TFMT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMT5ForConditionalGeneration</span></code></a> (mT5 model)</p></li>
<li><p><a class="reference internal" href="t5.html#transformers.T5Config" title="transformers.T5Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Config</span></code></a> configuration class: <a class="reference internal" href="t5.html#transformers.TFT5ForConditionalGeneration" title="transformers.TFT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5ForConditionalGeneration</span></code></a> (T5 model)</p></li>
<li><p><a class="reference internal" href="marian.html#transformers.MarianConfig" title="transformers.MarianConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarianConfig</span></code></a> configuration class: <a class="reference internal" href="marian.html#transformers.TFMarianMTModel" title="transformers.TFMarianMTModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMarianMTModel</span></code></a> (Marian model)</p></li>
<li><p><a class="reference internal" href="mbart.html#transformers.MBartConfig" title="transformers.MBartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MBartConfig</span></code></a> configuration class: <a class="reference internal" href="mbart.html#transformers.TFMBartForConditionalGeneration" title="transformers.TFMBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMBartForConditionalGeneration</span></code></a> (mBART model)</p></li>
<li><p><a class="reference internal" href="pegasus.html#transformers.PegasusConfig" title="transformers.PegasusConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PegasusConfig</span></code></a> configuration class: <a class="reference internal" href="pegasus.html#transformers.TFPegasusForConditionalGeneration" title="transformers.TFPegasusForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFPegasusForConditionalGeneration</span></code></a> (Pegasus model)</p></li>
<li><p><a class="reference internal" href="blenderbot.html#transformers.BlenderbotConfig" title="transformers.BlenderbotConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot.html#transformers.TFBlenderbotForConditionalGeneration" title="transformers.TFBlenderbotForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotForConditionalGeneration</span></code></a> (Blenderbot model)</p></li>
<li><p><a class="reference internal" href="blenderbot_small.html#transformers.BlenderbotSmallConfig" title="transformers.BlenderbotSmallConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlenderbotSmallConfig</span></code></a> configuration class: <a class="reference internal" href="blenderbot_small.html#transformers.TFBlenderbotSmallForConditionalGeneration" title="transformers.TFBlenderbotSmallForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBlenderbotSmallForConditionalGeneration</span></code></a> (BlenderbotSmall model)</p></li>
<li><p><a class="reference internal" href="bart.html#transformers.BartConfig" title="transformers.BartConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BartConfig</span></code></a> configuration class: <a class="reference internal" href="bart.html#transformers.TFBartForConditionalGeneration" title="transformers.TFBartForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBartForConditionalGeneration</span></code></a> (BART model)</p></li>
</ul>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForSeq2SeqLM</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5-base'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'t5-base'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/t5_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/t5_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelforsequenceclassification">
<h2>TFAutoModelForSequenceClassification<a class="headerlink" href="#tfautomodelforsequenceclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForSequenceClassification"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForSequenceClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForSequenceClassification</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSequenceClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
sequence classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForSequenceClassification.from_pretrained" title="transformers.TFAutoModelForSequenceClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForSequenceClassification.from_config" title="transformers.TFAutoModelForSequenceClassification.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForSequenceClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForSequenceClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSequenceClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSequenceClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a sequence classification headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForSequenceClassification.from_pretrained" title="transformers.TFAutoModelForSequenceClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to
load the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForSequenceClassification" title="transformers.TFDistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForSequenceClassification" title="transformers.TFAlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForSequenceClassification" title="transformers.TFCamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForSequenceClassification" title="transformers.TFXLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerForSequenceClassification" title="transformers.TFLongformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForSequenceClassification</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForSequenceClassification" title="transformers.TFRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForSequenceClassification" title="transformers.TFBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForSequenceClassification</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetForSequenceClassification" title="transformers.TFXLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForSequenceClassification" title="transformers.TFMobileBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForSequenceClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForSequenceClassification" title="transformers.TFFlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForSequenceClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMForSequenceClassification" title="transformers.TFXLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForSequenceClassification</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForSequenceClassification" title="transformers.TFElectraForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForSequenceClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForSequenceClassification" title="transformers.TFFunnelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForSequenceClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="gpt2.html#transformers.GPT2Config" title="transformers.GPT2Config"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPT2Config</span></code></a> configuration class: <a class="reference internal" href="gpt2.html#transformers.TFGPT2ForSequenceClassification" title="transformers.TFGPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2ForSequenceClassification</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForSequenceClassification" title="transformers.TFMPNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForSequenceClassification</span></code></a> (MPNet model)</p></li>
<li><p><a class="reference internal" href="gpt.html#transformers.OpenAIGPTConfig" title="transformers.OpenAIGPTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenAIGPTConfig</span></code></a> configuration class: <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTForSequenceClassification" title="transformers.TFOpenAIGPTForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTForSequenceClassification</span></code></a> (OpenAI GPT model)</p></li>
<li><p><a class="reference internal" href="transformerxl.html#transformers.TransfoXLConfig" title="transformers.TransfoXLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransfoXLConfig</span></code></a> configuration class: <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLForSequenceClassification" title="transformers.TFTransfoXLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLForSequenceClassification</span></code></a> (Transformer-XL model)</p></li>
<li><p><a class="reference internal" href="ctrl.html#transformers.CTRLConfig" title="transformers.CTRLConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTRLConfig</span></code></a> configuration class: <a class="reference internal" href="ctrl.html#transformers.TFCTRLForSequenceClassification" title="transformers.TFCTRLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLForSequenceClassification</span></code></a> (CTRL model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForSequenceClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForSequenceClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForSequenceClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForSequenceClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForSequenceClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a sequence classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForSequenceClassification" title="transformers.TFMobileBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForSequenceClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForSequenceClassification" title="transformers.TFDistilBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForSequenceClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForSequenceClassification" title="transformers.TFAlbertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForSequenceClassification</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForSequenceClassification" title="transformers.TFCamembertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForSequenceClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForSequenceClassification" title="transformers.TFXLMRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForSequenceClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForSequenceClassification" title="transformers.TFMPNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForSequenceClassification</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerForSequenceClassification" title="transformers.TFLongformerForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForSequenceClassification</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForSequenceClassification" title="transformers.TFRobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForSequenceClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForSequenceClassification" title="transformers.TFFlaubertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForSequenceClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForSequenceClassification" title="transformers.TFBertForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForSequenceClassification</span></code></a> (BERT model)</p></li>
<li><p><strong>openai-gpt</strong> â€“ <a class="reference internal" href="gpt.html#transformers.TFOpenAIGPTForSequenceClassification" title="transformers.TFOpenAIGPTForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFOpenAIGPTForSequenceClassification</span></code></a> (OpenAI GPT model)</p></li>
<li><p><strong>gpt2</strong> â€“ <a class="reference internal" href="gpt2.html#transformers.TFGPT2ForSequenceClassification" title="transformers.TFGPT2ForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFGPT2ForSequenceClassification</span></code></a> (OpenAI GPT-2 model)</p></li>
<li><p><strong>transfo-xl</strong> â€“ <a class="reference internal" href="transformerxl.html#transformers.TFTransfoXLForSequenceClassification" title="transformers.TFTransfoXLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTransfoXLForSequenceClassification</span></code></a> (Transformer-XL model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetForSequenceClassification" title="transformers.TFXLNetForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForSequenceClassification</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMForSequenceClassification" title="transformers.TFXLMForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForSequenceClassification</span></code></a> (XLM model)</p></li>
<li><p><strong>ctrl</strong> â€“ <a class="reference internal" href="ctrl.html#transformers.TFCTRLForSequenceClassification" title="transformers.TFCTRLForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCTRLForSequenceClassification</span></code></a> (CTRL model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForSequenceClassification" title="transformers.TFElectraForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForSequenceClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForSequenceClassification" title="transformers.TFFunnelForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForSequenceClassification</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForSequenceClassification</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelformultiplechoice">
<h2>TFAutoModelForMultipleChoice<a class="headerlink" href="#tfautomodelformultiplechoice" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForMultipleChoice"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForMultipleChoice"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForMultipleChoice</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMultipleChoice"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMultipleChoice" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
multiple choice classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForMultipleChoice.from_pretrained" title="transformers.TFAutoModelForMultipleChoice.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForMultipleChoice.from_config" title="transformers.TFAutoModelForMultipleChoice.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForMultipleChoice.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForMultipleChoice.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMultipleChoice.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMultipleChoice.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForMultipleChoice.from_pretrained" title="transformers.TFAutoModelForMultipleChoice.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load the
model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForMultipleChoice" title="transformers.TFCamembertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMultipleChoice</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMForMultipleChoice" title="transformers.TFXLMForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForMultipleChoice</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMultipleChoice" title="transformers.TFXLMRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMultipleChoice</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerForMultipleChoice" title="transformers.TFLongformerForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForMultipleChoice</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForMultipleChoice" title="transformers.TFRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMultipleChoice</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForMultipleChoice" title="transformers.TFBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForMultipleChoice</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMultipleChoice" title="transformers.TFDistilBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMultipleChoice</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForMultipleChoice" title="transformers.TFMobileBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForMultipleChoice</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetForMultipleChoice" title="transformers.TFXLNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForMultipleChoice</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForMultipleChoice" title="transformers.TFFlaubertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForMultipleChoice</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForMultipleChoice" title="transformers.TFAlbertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForMultipleChoice</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForMultipleChoice" title="transformers.TFElectraForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForMultipleChoice</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForMultipleChoice" title="transformers.TFFunnelForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForMultipleChoice</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMultipleChoice" title="transformers.TFMPNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMultipleChoice</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForMultipleChoice</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForMultipleChoice.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForMultipleChoice.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForMultipleChoice.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForMultipleChoice.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a multiple choice classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForMultipleChoice" title="transformers.TFMobileBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForMultipleChoice</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForMultipleChoice" title="transformers.TFDistilBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForMultipleChoice</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForMultipleChoice" title="transformers.TFAlbertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForMultipleChoice</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForMultipleChoice" title="transformers.TFCamembertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForMultipleChoice</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForMultipleChoice" title="transformers.TFXLMRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForMultipleChoice</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForMultipleChoice" title="transformers.TFMPNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForMultipleChoice</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerForMultipleChoice" title="transformers.TFLongformerForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForMultipleChoice</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForMultipleChoice" title="transformers.TFRobertaForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForMultipleChoice</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForMultipleChoice" title="transformers.TFFlaubertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForMultipleChoice</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForMultipleChoice" title="transformers.TFBertForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForMultipleChoice</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetForMultipleChoice" title="transformers.TFXLNetForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForMultipleChoice</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMForMultipleChoice" title="transformers.TFXLMForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForMultipleChoice</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForMultipleChoice" title="transformers.TFElectraForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForMultipleChoice</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForMultipleChoice" title="transformers.TFFunnelForMultipleChoice"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForMultipleChoice</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForMultipleChoice</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelfortokenclassification">
<h2>TFAutoModelForTokenClassification<a class="headerlink" href="#tfautomodelfortokenclassification" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForTokenClassification"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForTokenClassification"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForTokenClassification</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForTokenClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a token
classification headâ€”when created with the when created with the
<a class="reference internal" href="#transformers.TFAutoModelForTokenClassification.from_pretrained" title="transformers.TFAutoModelForTokenClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForTokenClassification.from_config" title="transformers.TFAutoModelForTokenClassification.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForTokenClassification.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForTokenClassification.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForTokenClassification.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForTokenClassification.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a token classification headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForTokenClassification.from_pretrained" title="transformers.TFAutoModelForTokenClassification.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForTokenClassification" title="transformers.TFDistilBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForTokenClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForTokenClassification" title="transformers.TFAlbertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForTokenClassification</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForTokenClassification" title="transformers.TFCamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForTokenClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForTokenClassification" title="transformers.TFFlaubertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForTokenClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMForTokenClassification" title="transformers.TFXLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForTokenClassification</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForTokenClassification" title="transformers.TFXLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerForTokenClassification" title="transformers.TFLongformerForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForTokenClassification</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForTokenClassification" title="transformers.TFRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForTokenClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForTokenClassification" title="transformers.TFBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForTokenClassification</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForTokenClassification" title="transformers.TFMobileBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForTokenClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetForTokenClassification" title="transformers.TFXLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForTokenClassification</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForTokenClassification" title="transformers.TFElectraForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForTokenClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForTokenClassification" title="transformers.TFFunnelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForTokenClassification</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForTokenClassification" title="transformers.TFMPNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForTokenClassification</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForTokenClassification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForTokenClassification.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForTokenClassification.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForTokenClassification.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForTokenClassification.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a token classification headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForTokenClassification" title="transformers.TFMobileBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForTokenClassification</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForTokenClassification" title="transformers.TFDistilBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForTokenClassification</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForTokenClassification" title="transformers.TFAlbertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForTokenClassification</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForTokenClassification" title="transformers.TFCamembertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForTokenClassification</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForTokenClassification" title="transformers.TFXLMRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForTokenClassification</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForTokenClassification" title="transformers.TFMPNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForTokenClassification</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerForTokenClassification" title="transformers.TFLongformerForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForTokenClassification</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForTokenClassification" title="transformers.TFRobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForTokenClassification</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForTokenClassification" title="transformers.TFFlaubertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForTokenClassification</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForTokenClassification" title="transformers.TFBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForTokenClassification</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetForTokenClassification" title="transformers.TFXLNetForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForTokenClassification</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMForTokenClassification" title="transformers.TFXLMForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForTokenClassification</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForTokenClassification" title="transformers.TFElectraForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForTokenClassification</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForTokenClassification" title="transformers.TFFunnelForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForTokenClassification</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForTokenClassification</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfautomodelforquestionanswering">
<h2>TFAutoModelForQuestionAnswering<a class="headerlink" href="#tfautomodelforquestionanswering" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.TFAutoModelForQuestionAnswering"><a name="//apple_ref/cpp/Class/transformers.TFAutoModelForQuestionAnswering"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFAutoModelForQuestionAnswering</code><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForQuestionAnswering" title="Permalink to this definition">Â¶</a></dt>
<dd><p>This is a generic model class that will be instantiated as one of the model classes of the libraryâ€”with a
question answering headâ€”when created with the when created with the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> class method or the
<a class="reference internal" href="#transformers.TFAutoModelForQuestionAnswering.from_config" title="transformers.TFAutoModelForQuestionAnswering.from_config"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_config()</span></code></a> class method.</p>
<p>This class cannot be instantiated directly using <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> (throws an error).</p>
<dl class="py method">
<dt id="transformers.TFAutoModelForQuestionAnswering.from_config"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForQuestionAnswering.from_config"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForQuestionAnswering.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForQuestionAnswering.from_config" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiates one of the model classes of the libraryâ€”with a question answering headâ€”from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
modelâ€™s configuration. Use <a class="reference internal" href="#transformers.TFAutoModelForQuestionAnswering.from_pretrained" title="transformers.TFAutoModelForQuestionAnswering.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> to load
the model weights.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>) â€“ <p>The model class to instantiate is selected based on the configuration class:</p>
<ul class="simple">
<li><p><a class="reference internal" href="distilbert.html#transformers.DistilBertConfig" title="transformers.DistilBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code></a> configuration class: <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForQuestionAnswering" title="transformers.TFDistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p><a class="reference internal" href="albert.html#transformers.AlbertConfig" title="transformers.AlbertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code></a> configuration class: <a class="reference internal" href="albert.html#transformers.TFAlbertForQuestionAnswering" title="transformers.TFAlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p><a class="reference internal" href="camembert.html#transformers.CamembertConfig" title="transformers.CamembertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">CamembertConfig</span></code></a> configuration class: <a class="reference internal" href="camembert.html#transformers.TFCamembertForQuestionAnswering" title="transformers.TFCamembertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForQuestionAnswering</span></code></a> (CamemBERT model)</p></li>
<li><p><a class="reference internal" href="xlmroberta.html#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a> configuration class: <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForQuestionAnswering" title="transformers.TFXLMRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForQuestionAnswering</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><a class="reference internal" href="longformer.html#transformers.LongformerConfig" title="transformers.LongformerConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerConfig</span></code></a> configuration class: <a class="reference internal" href="longformer.html#transformers.TFLongformerForQuestionAnswering" title="transformers.TFLongformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForQuestionAnswering</span></code></a> (Longformer model)</p></li>
<li><p><a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a> configuration class: <a class="reference internal" href="roberta.html#transformers.TFRobertaForQuestionAnswering" title="transformers.TFRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForQuestionAnswering</span></code></a> (RoBERTa model)</p></li>
<li><p><a class="reference internal" href="bert.html#transformers.BertConfig" title="transformers.BertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code></a> configuration class: <a class="reference internal" href="bert.html#transformers.TFBertForQuestionAnswering" title="transformers.TFBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForQuestionAnswering</span></code></a> (BERT model)</p></li>
<li><p><a class="reference internal" href="xlnet.html#transformers.XLNetConfig" title="transformers.XLNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code></a> configuration class: <a class="reference internal" href="xlnet.html#transformers.TFXLNetForQuestionAnsweringSimple" title="transformers.TFXLNetForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForQuestionAnsweringSimple</span></code></a> (XLNet model)</p></li>
<li><p><a class="reference internal" href="mobilebert.html#transformers.MobileBertConfig" title="transformers.MobileBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileBertConfig</span></code></a> configuration class: <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForQuestionAnswering" title="transformers.TFMobileBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForQuestionAnswering</span></code></a> (MobileBERT model)</p></li>
<li><p><a class="reference internal" href="flaubert.html#transformers.FlaubertConfig" title="transformers.FlaubertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FlaubertConfig</span></code></a> configuration class: <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForQuestionAnsweringSimple" title="transformers.TFFlaubertForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForQuestionAnsweringSimple</span></code></a> (FlauBERT model)</p></li>
<li><p><a class="reference internal" href="xlm.html#transformers.XLMConfig" title="transformers.XLMConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code></a> configuration class: <a class="reference internal" href="xlm.html#transformers.TFXLMForQuestionAnsweringSimple" title="transformers.TFXLMForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForQuestionAnsweringSimple</span></code></a> (XLM model)</p></li>
<li><p><a class="reference internal" href="electra.html#transformers.ElectraConfig" title="transformers.ElectraConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElectraConfig</span></code></a> configuration class: <a class="reference internal" href="electra.html#transformers.TFElectraForQuestionAnswering" title="transformers.TFElectraForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForQuestionAnswering</span></code></a> (ELECTRA model)</p></li>
<li><p><a class="reference internal" href="funnel.html#transformers.FunnelConfig" title="transformers.FunnelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">FunnelConfig</span></code></a> configuration class: <a class="reference internal" href="funnel.html#transformers.TFFunnelForQuestionAnswering" title="transformers.TFFunnelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForQuestionAnswering</span></code></a> (Funnel Transformer model)</p></li>
<li><p><a class="reference internal" href="mpnet.html#transformers.MPNetConfig" title="transformers.MPNetConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">MPNetConfig</span></code></a> configuration class: <a class="reference internal" href="mpnet.html#transformers.TFMPNetForQuestionAnswering" title="transformers.TFMPNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForQuestionAnswering</span></code></a> (MPNet model)</p></li>
</ul>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForQuestionAnswering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt id="transformers.TFAutoModelForQuestionAnswering.from_pretrained"><a name="//apple_ref/cpp/Method/transformers.TFAutoModelForQuestionAnswering.from_pretrained"></a>
<em class="property">classmethod </em><code class="sig-name descname">from_pretrained</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pretrained_model_name_or_path</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">model_args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/auto/modeling_tf_auto.html#TFAutoModelForQuestionAnswering.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFAutoModelForQuestionAnswering.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Instantiate one of the model classes of the libraryâ€”with a question answering headâ€”from a pretrained model.</p>
<p>The model class to instantiate is selected based on the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model_type</span></code> property of the config object (either
passed as an argument or loaded from <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> if possible), or when itâ€™s missing,
by falling back to using pattern matching on <code class="xref py py-obj docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>mobilebert</strong> â€“ <a class="reference internal" href="mobilebert.html#transformers.TFMobileBertForQuestionAnswering" title="transformers.TFMobileBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMobileBertForQuestionAnswering</span></code></a> (MobileBERT model)</p></li>
<li><p><strong>distilbert</strong> â€“ <a class="reference internal" href="distilbert.html#transformers.TFDistilBertForQuestionAnswering" title="transformers.TFDistilBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDistilBertForQuestionAnswering</span></code></a> (DistilBERT model)</p></li>
<li><p><strong>albert</strong> â€“ <a class="reference internal" href="albert.html#transformers.TFAlbertForQuestionAnswering" title="transformers.TFAlbertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFAlbertForQuestionAnswering</span></code></a> (ALBERT model)</p></li>
<li><p><strong>camembert</strong> â€“ <a class="reference internal" href="camembert.html#transformers.TFCamembertForQuestionAnswering" title="transformers.TFCamembertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFCamembertForQuestionAnswering</span></code></a> (CamemBERT model)</p></li>
<li><p><strong>xlm-roberta</strong> â€“ <a class="reference internal" href="xlmroberta.html#transformers.TFXLMRobertaForQuestionAnswering" title="transformers.TFXLMRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMRobertaForQuestionAnswering</span></code></a> (XLM-RoBERTa model)</p></li>
<li><p><strong>mpnet</strong> â€“ <a class="reference internal" href="mpnet.html#transformers.TFMPNetForQuestionAnswering" title="transformers.TFMPNetForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMPNetForQuestionAnswering</span></code></a> (MPNet model)</p></li>
<li><p><strong>longformer</strong> â€“ <a class="reference internal" href="longformer.html#transformers.TFLongformerForQuestionAnswering" title="transformers.TFLongformerForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFLongformerForQuestionAnswering</span></code></a> (Longformer model)</p></li>
<li><p><strong>roberta</strong> â€“ <a class="reference internal" href="roberta.html#transformers.TFRobertaForQuestionAnswering" title="transformers.TFRobertaForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFRobertaForQuestionAnswering</span></code></a> (RoBERTa model)</p></li>
<li><p><strong>flaubert</strong> â€“ <a class="reference internal" href="flaubert.html#transformers.TFFlaubertForQuestionAnsweringSimple" title="transformers.TFFlaubertForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFlaubertForQuestionAnsweringSimple</span></code></a> (FlauBERT model)</p></li>
<li><p><strong>bert</strong> â€“ <a class="reference internal" href="bert.html#transformers.TFBertForQuestionAnswering" title="transformers.TFBertForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertForQuestionAnswering</span></code></a> (BERT model)</p></li>
<li><p><strong>xlnet</strong> â€“ <a class="reference internal" href="xlnet.html#transformers.TFXLNetForQuestionAnsweringSimple" title="transformers.TFXLNetForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLNetForQuestionAnsweringSimple</span></code></a> (XLNet model)</p></li>
<li><p><strong>xlm</strong> â€“ <a class="reference internal" href="xlm.html#transformers.TFXLMForQuestionAnsweringSimple" title="transformers.TFXLMForQuestionAnsweringSimple"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFXLMForQuestionAnsweringSimple</span></code></a> (XLM model)</p></li>
<li><p><strong>electra</strong> â€“ <a class="reference internal" href="electra.html#transformers.TFElectraForQuestionAnswering" title="transformers.TFElectraForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFElectraForQuestionAnswering</span></code></a> (ELECTRA model)</p></li>
<li><p><strong>funnel</strong> â€“ <a class="reference internal" href="funnel.html#transformers.TFFunnelForQuestionAnswering" title="transformers.TFFunnelForQuestionAnswering"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFunnelForQuestionAnswering</span></code></a> (Funnel Transformer model)</p></li>
</ul>
</div></blockquote>
<p>The model is set in evaluation mode by default using <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> â€“ <p>Can be either:</p>
<blockquote>
<div><ul>
<li><p>A string, the <cite>model id</cite> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, or namespaced under
a user or organization name, like <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>A path to a <cite>directory</cite> containing model weights saved using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a>, e.g., <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>A path or url to a <cite>PyTorch state_dict save file</cite> (e.g, <code class="docutils literal notranslate"><span class="pre">./pt_model/pytorch_model.bin</span></code>). In
this case, <code class="docutils literal notranslate"><span class="pre">from_pt</span></code> should be set to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code> and a configuration object should be provided
as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>model_args</strong> (additional positional arguments, <cite>optional</cite>) â€“ Will be passed along to the underlying model <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a>, <cite>optional</cite>) â€“ <p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul>
<li><p>The model is a model provided by the library (loaded with the <cite>model id</cite> string of a pretrained
model).</p></li>
<li><p>The model was saved using <a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and is reloaded
by suppyling the save directory.</p></li>
<li><p>The model is loaded by suppyling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a
configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>state_dict</strong> (<cite>Dict[str, torch.Tensor]</cite>, <cite>optional</cite>) â€“ <p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.save_pretrained" title="transformers.PreTrainedModel.save_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code></a> and
<a class="reference internal" href="../main_classes/model.html#transformers.PreTrainedModel.from_pretrained" title="transformers.PreTrainedModel.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a> is not a simpler option.</p>
</p></li>
<li><p><strong>cache_dir</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p></li>
<li><p><strong>from_tf</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> argument).</p></li>
<li><p><strong>force_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p></li>
<li><p><strong>resume_download</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p></li>
<li><p><strong>proxies</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">str],</span> <span class="pre">`optional</span></code>) â€“ A dictionary of proxy servers to use by protocol or endpoint, e.g., <code class="xref py py-obj docutils literal notranslate"><span class="pre">{'http':</span> <span class="pre">'foo.bar:3128',</span>
<span class="pre">'http://hostname':</span> <span class="pre">'foo.bar:4012'}</span></code>. The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>local_files_only</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to only look at local files (e.g., not try downloading the model).</p></li>
<li><p><strong>revision</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"main"</span></code>) â€“ The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code class="docutils literal notranslate"><span class="pre">revision</span></code> can be any
identifier allowed by git.</p></li>
<li><p><strong>kwargs</strong> (additional keyword arguments, <cite>optional</cite>) â€“ <p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">output_attentions=True</span></code>). Behaves differently depending on whether a <code class="docutils literal notranslate"><span class="pre">config</span></code> is provided or
automatically loaded:</p>
<blockquote>
<div><ul>
<li><p>If a configuration is provided with <code class="docutils literal notranslate"><span class="pre">config</span></code>, <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> will be directly passed to the
underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> will be first passed to the configuration class
initialization function (<a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig.from_pretrained" title="transformers.PretrainedConfig.from_pretrained"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a>). Each key of
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying modelâ€™s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">TFAutoModelForQuestionAnswering</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">'./pt_model/bert_pt_model_config.json'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'./pt_model/bert_pytorch_model.bin'</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="flaxautomodel">
<h2>FlaxAutoModel<a class="headerlink" href="#flaxautomodel" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.FlaxAutoModel"><a name="//apple_ref/cpp/Class/transformers.FlaxAutoModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">FlaxAutoModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/utils/dummy_flax_objects.html#FlaxAutoModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.FlaxAutoModel" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="bart.html" rel="next" title="BART">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="albert.html" rel="prev" title="ALBERT"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>