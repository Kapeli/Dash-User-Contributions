
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>MT5 — transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="gpt.html" rel="next" title="OpenAI GPT"/>
<link href="mpnet.html" rel="prev" title="MPNet"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using 🤗 Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">🤗 Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertweet.html">Bertweet</a></li>
<li class="toctree-l1"><a class="reference internal" href="bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpnet.html">MPNet</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">MT5</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5config">MT5Config</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5tokenizer">MT5Tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5tokenizerfast">MT5TokenizerFast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5model">MT5Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5forconditionalgeneration">MT5ForConditionalGeneration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mt5encodermodel">MT5EncoderModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfmt5model">TFMT5Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfmt5forconditionalgeneration">TFMT5ForConditionalGeneration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tfmt5encodermodel">TFMT5EncoderModel</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> »</li>
<li>MT5</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/mt5.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="mt5">
<h1>MT5<a class="headerlink" href="#mt5" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The mT5 model was presented in <a class="reference external" href="https://arxiv.org/abs/2010.11934">mT5: A massively multilingual pre-trained text-to-text transformer</a> by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.</p>
<p>The abstract from the paper is the following:</p>
<p><em>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. All of the code and model checkpoints</em></p>
<p>The original code can be found <a class="reference external" href="https://github.com/google-research/multilingual-t5">here</a>.</p>
</div>
<div class="section" id="mt5config">
<h2>MT5Config<a class="headerlink" href="#mt5config" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.MT5Config"><a name="//apple_ref/cpp/Class/transformers.MT5Config"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5Config</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_size</span><span class="o">=</span><span class="default_value">250112</span></em>, <em class="sig-param"><span class="n">d_model</span><span class="o">=</span><span class="default_value">512</span></em>, <em class="sig-param"><span class="n">d_kv</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">d_ff</span><span class="o">=</span><span class="default_value">1024</span></em>, <em class="sig-param"><span class="n">num_layers</span><span class="o">=</span><span class="default_value">8</span></em>, <em class="sig-param"><span class="n">num_decoder_layers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_heads</span><span class="o">=</span><span class="default_value">6</span></em>, <em class="sig-param"><span class="n">relative_attention_num_buckets</span><span class="o">=</span><span class="default_value">32</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="default_value">1e-06</span></em>, <em class="sig-param"><span class="n">initializer_factor</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">feed_forward_proj</span><span class="o">=</span><span class="default_value">'gated-gelu'</span></em>, <em class="sig-param"><span class="n">is_encoder_decoder</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">use_cache</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">tokenizer_class</span><span class="o">=</span><span class="default_value">'T5Tokenizer'</span></em>, <em class="sig-param"><span class="n">tie_word_embeddings</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">pad_token_id</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">decoder_start_token_id</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/configuration_mt5.html#MT5Config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.MT5Config" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the configuration class to store the configuration of a <a class="reference internal" href="#transformers.MT5Model" title="transformers.MT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">MT5Model</span></code></a> or a
<a class="reference internal" href="#transformers.TFMT5Model" title="transformers.TFMT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFMT5Model</span></code></a>. It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 <a class="reference external" href="https://huggingface.co/google/mt5-small">google/mt5-small</a> architecture.</p>
<p>Configuration objects inherit from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> and can be used to control the model
outputs. Read the documentation from <a class="reference internal" href="../main_classes/configuration.html#transformers.PretrainedConfig" title="transformers.PretrainedConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></a> for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 32128) – Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> passed when calling <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a> or <a class="reference internal" href="t5.html#transformers.TFT5Model" title="transformers.TFT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5Model</span></code></a>.</p></li>
<li><p><strong>d_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 512) – Size of the encoder layers and the pooler layer.</p></li>
<li><p><strong>d_kv</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 64) – Size of the key, query, value projections per attention head. <code class="xref py py-obj docutils literal notranslate"><span class="pre">d_kv</span></code> has to be equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">d_model</span>
<span class="pre">//</span> <span class="pre">num_heads</span></code>.</p></li>
<li><p><strong>d_ff</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1024) – Size of the intermediate feed forward layer in each <code class="xref py py-obj docutils literal notranslate"><span class="pre">T5Block</span></code>.</p></li>
<li><p><strong>num_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 8) – Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>num_decoder_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>) – Number of hidden layers in the Transformer decoder. Will use the same value as <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_layers</span></code> if not
set.</p></li>
<li><p><strong>num_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 6) – Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>relative_attention_num_buckets</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 32) – The number of buckets to use for each attention layer.</p></li>
<li><p><strong>dropout_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) – The ratio for all dropout layers.</p></li>
<li><p><strong>layer_norm_eps</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-6) – The epsilon used by the layer normalization layers.</p></li>
<li><p><strong>initializer_factor</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1) – A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).</p></li>
<li><p><strong>feed_forward_proj</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">string</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gated-gelu"</span></code>) – Type of feed forward layer to be used. Should be one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">"relu"</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">"gated-gelu"</span></code>.</p></li>
<li><p><strong>use_cache</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code>) – Whether or not the model should return the last key/values attentions (not used by all models).</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="mt5tokenizer">
<h2>MT5Tokenizer<a class="headerlink" href="#mt5tokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.MT5Tokenizer"><a name="//apple_ref/cpp/Class/transformers.MT5Tokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5Tokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/utils/dummy_sentencepiece_objects.html#MT5Tokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.MT5Tokenizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<p>See <a class="reference internal" href="t5.html#transformers.T5Tokenizer" title="transformers.T5Tokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Tokenizer</span></code></a> for all details.</p>
</div>
<div class="section" id="mt5tokenizerfast">
<h2>MT5TokenizerFast<a class="headerlink" href="#mt5tokenizerfast" title="Permalink to this headline">¶</a></h2>
<dl class="py attribute">
<dt id="transformers.MT5TokenizerFast"><a name="//apple_ref/cpp/Attribute/transformers.MT5TokenizerFast"></a>
<code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5TokenizerFast</code><a class="headerlink" href="#transformers.MT5TokenizerFast" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.t5.tokenization_t5_fast.T5TokenizerFast</span></code></p>
</dd></dl>
<p>See <a class="reference internal" href="t5.html#transformers.T5TokenizerFast" title="transformers.T5TokenizerFast"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5TokenizerFast</span></code></a> for all details.</p>
</div>
<div class="section" id="mt5model">
<h2>MT5Model<a class="headerlink" href="#mt5model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.MT5Model"><a name="//apple_ref/cpp/Class/transformers.MT5Model"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5Model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.t5.configuration_t5.T5Config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_mt5.html#MT5Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.MT5Model" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.T5Model" title="transformers.T5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5Model</span></code></a>. Please check the superclass for the appropriate documentation
alongside usage examples.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MT5Model</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MT5Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">"Weiter Verhandlung in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="o">=</span><span class="p">[</span><span class="n">article</span><span class="p">],</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="p">[</span><span class="n">summary</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">decoder_input_ids</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py attribute">
<dt id="transformers.MT5Model.config_class"><a name="//apple_ref/cpp/Attribute/transformers.MT5Model.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.MT5Model.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mt5forconditionalgeneration">
<h2>MT5ForConditionalGeneration<a class="headerlink" href="#mt5forconditionalgeneration" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.MT5ForConditionalGeneration"><a name="//apple_ref/cpp/Class/transformers.MT5ForConditionalGeneration"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5ForConditionalGeneration</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_mt5.html#MT5ForConditionalGeneration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.MT5ForConditionalGeneration" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.T5ForConditionalGeneration" title="transformers.T5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5ForConditionalGeneration</span></code></a>. Please check the superclass for the
appropriate documentation alongside usage examples.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MT5ForConditionalGeneration</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MT5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">"Weiter Verhandlung in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="o">=</span><span class="p">[</span><span class="n">article</span><span class="p">],</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="p">[</span><span class="n">summary</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py attribute">
<dt id="transformers.MT5ForConditionalGeneration.config_class"><a name="//apple_ref/cpp/Attribute/transformers.MT5ForConditionalGeneration.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.MT5ForConditionalGeneration.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="mt5encodermodel">
<h2>MT5EncoderModel<a class="headerlink" href="#mt5encodermodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.MT5EncoderModel"><a name="//apple_ref/cpp/Class/transformers.MT5EncoderModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">MT5EncoderModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">transformers.models.t5.configuration_t5.T5Config</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_mt5.html#MT5EncoderModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.MT5EncoderModel" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.T5EncoderModel" title="transformers.T5EncoderModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">T5EncoderModel</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MT5EncoderModel</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MT5EncoderModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="transformers.MT5EncoderModel.config_class"><a name="//apple_ref/cpp/Attribute/transformers.MT5EncoderModel.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.MT5EncoderModel.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfmt5model">
<h2>TFMT5Model<a class="headerlink" href="#tfmt5model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFMT5Model"><a name="//apple_ref/cpp/Class/transformers.TFMT5Model"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFMT5Model</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_tf_mt5.html#TFMT5Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFMT5Model" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.TFT5Model" title="transformers.TFT5Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5Model</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFMT5Model</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFMT5Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">"Weiter Verhandlung in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="o">=</span><span class="p">[</span><span class="n">article</span><span class="p">],</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="p">[</span><span class="n">summary</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"tf"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span><span class="p">[</span><span class="s2">"decoder_input_ids"</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">del</span> <span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py attribute">
<dt id="transformers.TFMT5Model.config_class"><a name="//apple_ref/cpp/Attribute/transformers.TFMT5Model.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.TFMT5Model.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfmt5forconditionalgeneration">
<h2>TFMT5ForConditionalGeneration<a class="headerlink" href="#tfmt5forconditionalgeneration" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFMT5ForConditionalGeneration"><a name="//apple_ref/cpp/Class/transformers.TFMT5ForConditionalGeneration"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFMT5ForConditionalGeneration</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_tf_mt5.html#TFMT5ForConditionalGeneration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFMT5ForConditionalGeneration" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.TFT5ForConditionalGeneration" title="transformers.TFT5ForConditionalGeneration"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5ForConditionalGeneration</span></code></a>. Please check the superclass for the
appropriate documentation alongside usage examples.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFMT5ForConditionalGeneration</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFMT5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span> <span class="o">=</span> <span class="s2">"Weiter Verhandlung in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">prepare_seq2seq_batch</span><span class="p">(</span><span class="n">src_texts</span><span class="o">=</span><span class="p">[</span><span class="n">article</span><span class="p">],</span> <span class="n">tgt_texts</span><span class="o">=</span><span class="p">[</span><span class="n">summary</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"tf"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py attribute">
<dt id="transformers.TFMT5ForConditionalGeneration.config_class"><a name="//apple_ref/cpp/Attribute/transformers.TFMT5ForConditionalGeneration.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.TFMT5ForConditionalGeneration.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="tfmt5encodermodel">
<h2>TFMT5EncoderModel<a class="headerlink" href="#tfmt5encodermodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.TFMT5EncoderModel"><a name="//apple_ref/cpp/Class/transformers.TFMT5EncoderModel"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">TFMT5EncoderModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/mt5/modeling_tf_mt5.html#TFMT5EncoderModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.TFMT5EncoderModel" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="t5.html#transformers.TFT5EncoderModel" title="transformers.TFT5EncoderModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFT5EncoderModel</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFMT5EncoderModel</span><span class="p">,</span> <span class="n">T5Tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TFMT5EncoderModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/mt5-small"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">article</span> <span class="o">=</span> <span class="s2">"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"tf"</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden_state</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</pre></div>
</div>
<dl class="py attribute">
<dt id="transformers.TFMT5EncoderModel.config_class"><a name="//apple_ref/cpp/Attribute/transformers.TFMT5EncoderModel.config_class"></a>
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.TFMT5EncoderModel.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.mt5.configuration_mt5.MT5Config</span></code></p>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="gpt.html" rel="next" title="OpenAI GPT">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="mpnet.html" rel="prev" title="MPNet"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>