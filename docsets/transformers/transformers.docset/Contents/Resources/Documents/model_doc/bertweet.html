
<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Bertweet â€” transformers 4.2.0 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/huggingface.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/code-snippets.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/hidesidebar.css" rel="stylesheet" type="text/css"/>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/js/custom.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="bertgeneration.html" rel="next" title="BertGeneration"/>
<link href="bert.html" rel="prev" title="BERT"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> transformers
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Using ðŸ¤— Transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../task_summary.html">Summary of the tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_summary.html">Summary of the models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Training and fine-tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_sharing.html">Model sharing and uploading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tokenizer_summary.html">Summary of the tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multilingual.html">Multi-lingual models</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pretrained_models.html">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_datasets.html">Fine-tuning with custom datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">ðŸ¤— Transformers Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../converting_tensorflow_models.html">Converting Tensorflow Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration.html">Migrating from previous packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">How to contribute to transformers?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serialization.html">Exporting transformers models</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bertology.html">BERTology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perplexity.html">Perplexity of fixed-length models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Main Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/optimizer_schedules.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/output.html">Model outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/pipelines.html">Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/processors.html">Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/tokenizer.html">Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_classes/trainer.html">Trainer</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="barthez.html">BARThez</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bertweet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bertweettokenizer">BertweetTokenizer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bertgeneration.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot.html">Blenderbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="blenderbot_small.html">Blenderbot Small</a></li>
<li class="toctree-l1"><a class="reference internal" href="camembert.html">CamemBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="ctrl.html">CTRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="dialogpt.html">DialoGPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="dpr.html">DPR</a></li>
<li class="toctree-l1"><a class="reference internal" href="electra.html">ELECTRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="flaubert.html">FlauBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsmt.html">FSMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="funnel.html">Funnel Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="herbert.html">herBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="layoutlm.html">LayoutLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="led.html">LED</a></li>
<li class="toctree-l1"><a class="reference internal" href="longformer.html">Longformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lxmert.html">LXMERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="marian.html">MarianMT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobilebert.html">MobileBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpnet.html">MPNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt.html">OpenAI GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="pegasus.html">Pegasus</a></li>
<li class="toctree-l1"><a class="reference internal" href="phobert.html">PhoBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="prophetnet.html">ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="reformer.html">Reformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="retribert.html">RetriBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="squeezebert.html">SqueezeBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="tapas.html">TAPAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformerxl.html">Transformer XL</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlm.html">XLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmprophetnet.html">XLM-ProphetNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlmroberta.html">XLM-RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="xlnet.html">XLNet</a></li>
</ul>
<p class="caption"><span class="caption-text">Internal Helpers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/modeling_utils.html">Custom Layers and Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/pipelines_utils.html">Utilities for pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/tokenization_utils.html">Utilities for Tokenizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/trainer_utils.html">Utilities for Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internal/generation_utils.html">Utilities for Generation</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">transformers</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="../index.html"></a> Â»</li>
<li>Bertweet</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/model_doc/bertweet.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="section" id="bertweet">
<h1>Bertweet<a class="headerlink" href="#bertweet" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">Â¶</a></h2>
<p>The BERTweet model was proposed in <a class="reference external" href="https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf">BERTweet: A pre-trained language model for English Tweets</a> by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen.</p>
<p>The abstract from the paper is the following:</p>
<p><em>We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.</em></p>
<p>Example of use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">bertweet</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"vinai/bertweet-base"</span><span class="p">)</span>

<span class="c1"># For transformers v4.x+:</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"vinai/bertweet-base"</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># For transformers v3.x:</span>
<span class="c1"># tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")</span>

<span class="c1"># INPUT TWEET IS ALREADY NORMALIZED!</span>
<span class="n">line</span> <span class="o">=</span> <span class="s2">"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">line</span><span class="p">)])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">bertweet</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># Models outputs are now tuples</span>

<span class="c1">## With TensorFlow 2.0+:</span>
<span class="c1"># from transformers import TFAutoModel</span>
<span class="c1"># bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")</span>
</pre></div>
</div>
<p>The original code can be found <a class="reference external" href="https://github.com/VinAIResearch/BERTweet">here</a>.</p>
</div>
<div class="section" id="bertweettokenizer">
<h2>BertweetTokenizer<a class="headerlink" href="#bertweettokenizer" title="Permalink to this headline">Â¶</a></h2>
<dl class="py class">
<dt id="transformers.BertweetTokenizer"><a name="//apple_ref/cpp/Class/transformers.BertweetTokenizer"></a>
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">BertweetTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vocab_file</span></em>, <em class="sig-param"><span class="n">merges_file</span></em>, <em class="sig-param"><span class="n">normalization</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">bos_token</span><span class="o">=</span><span class="default_value">'&lt;s&gt;'</span></em>, <em class="sig-param"><span class="n">eos_token</span><span class="o">=</span><span class="default_value">'&lt;/s&gt;'</span></em>, <em class="sig-param"><span class="n">sep_token</span><span class="o">=</span><span class="default_value">'&lt;/s&gt;'</span></em>, <em class="sig-param"><span class="n">cls_token</span><span class="o">=</span><span class="default_value">'&lt;s&gt;'</span></em>, <em class="sig-param"><span class="n">unk_token</span><span class="o">=</span><span class="default_value">'&lt;unk&gt;'</span></em>, <em class="sig-param"><span class="n">pad_token</span><span class="o">=</span><span class="default_value">'&lt;pad&gt;'</span></em>, <em class="sig-param"><span class="n">mask_token</span><span class="o">=</span><span class="default_value">'&lt;mask&gt;'</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Constructs a BERTweet tokenizer, using Byte-Pair-Encoding.</p>
<p>This tokenizer inherits from <a class="reference internal" href="../main_classes/tokenizer.html#transformers.PreTrainedTokenizer" title="transformers.PreTrainedTokenizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedTokenizer</span></code></a> which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ Path to the vocabulary file.</p></li>
<li><p><strong>merges_file</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ Path to the merges file.</p></li>
<li><p><strong>normalization</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not to apply a normalization preprocess.</p></li>
<li><p><strong>bos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;s&gt;"</span></code>) â€“ <p>The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code class="xref py py-obj docutils literal notranslate"><span class="pre">cls_token</span></code>.</p>
</div>
</p></li>
<li><p><strong>eos_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;/s&gt;"</span></code>) â€“ <p>The end of sequence token.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the <code class="xref py py-obj docutils literal notranslate"><span class="pre">sep_token</span></code>.</p>
</div>
</p></li>
<li><p><strong>sep_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;/s&gt;"</span></code>) â€“ The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.</p></li>
<li><p><strong>cls_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;s&gt;"</span></code>) â€“ The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.</p></li>
<li><p><strong>unk_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;unk&gt;"</span></code>) â€“ The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.</p></li>
<li><p><strong>pad_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;pad&gt;"</span></code>) â€“ The token used for padding, for example when batching sequences of different lengths.</p></li>
<li><p><strong>mask_token</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">"&lt;mask&gt;"</span></code>) â€“ The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.add_from_file"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.add_from_file"></a>
<code class="sig-name descname">add_from_file</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.add_from_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.add_from_file" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Loads a pre-existing dictionary from a text file and adds its symbols to this instance.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.build_inputs_with_special_tokens"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.build_inputs_with_special_tokens"></a>
<code class="sig-name descname">build_inputs_with_special_tokens</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.build_inputs_with_special_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.build_inputs_with_special_tokens" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:</p>
<ul class="simple">
<li><p>single sequence: <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">X</span> <span class="pre">&lt;/s&gt;</span></code></p></li>
<li><p>pair of sequences: <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span> <span class="pre">A</span> <span class="pre">&lt;/s&gt;&lt;/s&gt;</span> <span class="pre">B</span> <span class="pre">&lt;/s&gt;</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) â€“ List of IDs to which the special tokens will be added.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) â€“ Optional second list of IDs for sequence pairs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of <a class="reference external" href="../glossary.html#input-ids">input IDs</a> with the appropriate special tokens.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.convert_tokens_to_string"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.convert_tokens_to_string"></a>
<code class="sig-name descname">convert_tokens_to_string</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tokens</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.convert_tokens_to_string"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.convert_tokens_to_string" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Converts a sequence of tokens (string) in a single string.</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.create_token_type_ids_from_sequences"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.create_token_type_ids_from_sequences"></a>
<code class="sig-name descname">create_token_type_ids_from_sequences</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.create_token_type_ids_from_sequences"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.create_token_type_ids_from_sequences" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) â€“ List of IDs.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) â€“ Optional second list of IDs for sequence pairs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of zeros.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.get_special_tokens_mask"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.get_special_tokens_mask"></a>
<code class="sig-name descname">get_special_tokens_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> â†’ List<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.get_special_tokens_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.get_special_tokens_mask" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code class="docutils literal notranslate"><span class="pre">prepare_for_model</span></code> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>token_ids_0</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>) â€“ List of IDs.</p></li>
<li><p><strong>token_ids_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>, <cite>optional</cite>) â€“ Optional second list of IDs for sequence pairs.</p></li>
<li><p><strong>already_has_special_tokens</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code>) â€“ Whether or not the token list is already formatted with special tokens for the model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.get_vocab"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.get_vocab"></a>
<code class="sig-name descname">get_vocab</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.get_vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.get_vocab" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Returns the vocabulary as a dictionary of token to index.</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.get_vocab()[token]</span></code> is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">tokenizer.convert_tokens_to_ids(token)</span></code> when
<code class="xref py py-obj docutils literal notranslate"><span class="pre">token</span></code> is in the vocab.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The vocabulary.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.normalizeToken"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.normalizeToken"></a>
<code class="sig-name descname">normalizeToken</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">token</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.normalizeToken"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.normalizeToken" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Normalize tokens in a Tweet</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.normalizeTweet"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.normalizeTweet"></a>
<code class="sig-name descname">normalizeTweet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tweet</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.normalizeTweet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.normalizeTweet" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Normalize a raw Tweet</p>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.save_vocabulary"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.save_vocabulary"></a>
<code class="sig-name descname">save_vocabulary</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">save_directory</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> â†’ Tuple<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/transformers/models/bertweet/tokenization_bertweet.html#BertweetTokenizer.save_vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#transformers.BertweetTokenizer.save_vocabulary" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save only the vocabulary of the tokenizer (vocabulary + added tokens).</p>
<p>This method wonâ€™t save the configuration and special token mappings of the tokenizer. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_save_pretrained()</span></code> to save the whole state of the tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_directory</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>) â€“ The directory in which to save the vocabulary.</p></li>
<li><p><strong>filename_prefix</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>) â€“ An optional prefix to add to the named of the saved files.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Paths to the files saved.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple(str)</span></code></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt id="transformers.BertweetTokenizer.vocab_size"><a name="//apple_ref/cpp/Method/transformers.BertweetTokenizer.vocab_size"></a>
<em class="property">property </em><code class="sig-name descname">vocab_size</code><a class="headerlink" href="#transformers.BertweetTokenizer.vocab_size" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Size of the base vocabulary (without the added tokens).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="bertgeneration.html" rel="next" title="BertGeneration">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="bert.html" rel="prev" title="BERT"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License, Version 2.0.

    </p>
</div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Theme Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83738774-2', 'auto');
    
    ga('send', 'pageview');
    </script>
</body>
</html>